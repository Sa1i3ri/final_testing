context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","# Architectural Decision Record: Storefront Image Lazy Loading

## Decision
Adopt native browser lazy loading (via the `loading=""lazy""` attribute) as the primary mechanism for all offscreen images and thumbnails in the Storefront. Complement this with a lightweight, dependency-free JavaScript fallback for browsers that do not support native lazy loading (per [caniuse.com](https://caniuse.com/?search=loading) data) to ensure consistent performance across all supported browser versions. 

Discontinue reliance on third-party lazy loading extensions (e.g., ""lazysizes"") for this functionality.

---

## Rationale
1. **Eliminate Third-Party Dependencies**: Removes reliance on external extensions, reducing bundle size, maintenance overhead, and potential compatibility conflicts with other Storefront components.
2. **Optimized Performance**: Native lazy loading is integrated into the browser’s rendering pipeline, leading to better performance (faster offscreen image loading, reduced main thread blocking) than JavaScript-based third-party solutions.
3. **Broad Modern Browser Support**: As of 2024, native lazy loading has ~92% global browser coverage, meaning most users will benefit immediately without additional overhead.
4. **Backward Compatibility**: The lightweight fallback ensures support for the small subset of users on older browsers (e.g., Safari <15.4) without introducing heavy dependencies.
5. **Core Web Vitals Improvement**: Reduces initial page load time, improves Largest Contentful Paint (LCP) and Cumulative Layout Shift (CLS) metrics, which are critical for SEO and user experience.

---

## Implications
### Positive
- Faster initial page rendering for Storefront pages with many images/thumbnails.
- Lowered maintenance burden from managing third-party extension updates and compatibility.
- Improved Core Web Vitals scores, which positively impact search rankings and user retention.
- No additional hosting or licensing costs for third-party tools.

### Negative
- Minor development effort required to:
  - Add `loading=""lazy""` to all offscreen image/thumbnail tags in Storefront templates.
  - Implement and test the dependency-free JavaScript fallback.
  - Audit above-the-fold images to ensure they do not receive the `loading=""lazy""` attribute (to avoid delaying LCP).
- Need to maintain browser compatibility testing to validate both native and fallback behavior across supported versions.

### Operational
- No changes to hosting infrastructure required; the solution is self-contained within the Storefront codebase.
- Reduced attack surface by removing external dependencies that could introduce vulnerabilities.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","### Architectural Decision Record: Resolution of Unnecessary Association Loading Performance Issues

---

#### Decision
1. **Set Lazy Loading as Default**: Change the default value of the `autoload` parameter for `OneToOneAssociationField` and `ManyToOneAssociationField` from `true` to `false` for all new implementations, library releases, and major version updates. Lazy loading will be implemented as:
   - For ORM-backed models: Use proxy objects to fetch associated data only when the association property is first accessed (if supported by the underlying ORM).
   - For API layer integrations: Omit associated data from default payloads, with explicit inclusion enabled via a standardized mechanism (e.g., `?expand=associationName` query parameter).
2. **Deprecate Implicit Eager Loading**: Mark implicit use of `autoload=true` (i.e., not explicitly setting the parameter) as deprecated. Trigger a runtime warning that directs developers to explicitly specify `autoload=true` only when associated data is guaranteed to be needed in all use cases for the field.
3. **Introduce Performance Tooling**:
   - Add a static analysis linter rule to detect instances where `autoload=true` is set but the associated data is not used in the code path, flagging these as performance anti-patterns.
   - Provide a code migration script that scans existing codebases to identify implicit `autoload=true` usage, suggesting whether to keep `autoload=true` (if data is always used) or switch to lazy loading (if data is rarely accessed).
4. **Document Best Practices**: Update official documentation to:
   - Highlight the performance costs of unnecessary eager loading.
   - Demonstrate explicit eager fetching patterns (e.g., query builder joins with `addSelect()`) for cases where associated data is required for specific workflows.
   - Explain API expansion mechanisms for controlled inclusion of associated data in responses.

---

#### Rationale
- **Directly Addresses Core Performance Issues**: Eliminates unnecessary SQL joins, data transfer, object hydration, and bloated API payloads by loading associated data only when explicitly needed.
- **Promotes Intentional Development**: Forces developers to make conscious decisions about data fetching, aligning with modern performance best practices (e.g., ""fetch only what you need"").
- **Balances Backward Compatibility**: Existing code with explicit `autoload=true` remains functional, while implicit uses are deprecated with clear migration guidance, minimizing breaking changes.
- **Scalability Enabler**: Reduces resource consumption (database load, memory, network bandwidth) in high-traffic scenarios, improving application scalability.

---

#### Trade-Offs
- **Migration Effort**: Teams relying on implicit `autoload=true` will need to audit and update field definitions or suppress warnings, which may require time for large codebases.
- **Learning Curve**: Developers must adopt explicit eager fetching patterns and API expansion mechanisms instead of relying on the previous implicit behavior.
- **Potential for Runtime Errors**: Developers may forget to explicitly fetch associated data when needed, but this is mitigated by clear warnings, documentation, and tooling.

---

#### Consequences
- **Positive**:
  - Significant reduction in unnecessary resource usage and improved application response times.
  - More maintainable code with explicit data fetching intent.
  - Flexible API responses that adapt to client needs via expansion parameters.
- **Negative**:
  - Short-term disruption for teams migrating from older versions.
  - Initial engineering investment in tooling and documentation updates.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","---
## Decision
We will retain the existing folder-by-area structure for logical grouping of Architectural Decision Records (ADRs) to support use case 3 (viewing ADRs for a specific area). To address gaps in use cases 1 (searching by known headline) and 2 (accessing the latest ADRs), we will implement three complementary enhancements:

1. **Unique Sequential ID Filename Prefixing**: Assign a zero-padded, sequential integer ID (e.g., `0001`, `0002`) to every new ADR, prefixed to the filename (e.g., `0012-implement-rate-limiting.md`). IDs are assigned in creation order to correlate with recency, and no two ADRs will share the same ID.
2. **Root-Level Auto-Generated Metadata Index**: Maintain a root-level `ADR-Index.md` file that aggregates metadata for all ADRs across all areas. The index will be a sortable table with columns:
   - Sequential ID
   - ADR Headline
   - Area (folder path)
   - Date Created (extracted from ADR frontmatter)
   - Status (Proposed/Accepted/Rejected/Superseded)
   - Direct relative link to the ADR file
3. **Mandatory Frontmatter & Automation Script**: Require all new ADRs to include standardized YAML frontmatter (fields: `id`, `title`, `area`, `date-created`, `status`, `supersedes` (optional)). A lightweight script stored in `/adr-tools/` will scan folders, parse frontmatter, regenerate the index sorted by date/ID (descending), and auto-generate the next available sequential ID for new ADRs to eliminate manual tracking overhead.

### Supplementary Optional Enhancement
Add auto-generated `area-index.md` files in each area folder to list only the ADRs in that area, sorted by sequential ID, for users browsing specific areas.

---
### Rationale
This decision directly resolves all identified use cases:
1. **Search by known headline**: The centralized root index provides a single location to search for headlines, eliminating duplicate results from folder traversal. Users can find the relevant ADR and click through to its area folder.
2. **Access latest ADRs**: The index is sorted by date/ID descending, so the most recent ADRs appear at the top—no need to access git history via shell/IDE.
3. **View ADRs for a specific area**: The existing folder structure is retained, and the index can be filtered by the ""Area"" column for a consolidated view of area-specific decisions.

---
### Consequences
- **Positive**:
  - All three use cases are fully supported without sacrificing logical grouping.
  - Sequential IDs make relative ADR age visible even within area folders.
  - Automation reduces manual maintenance errors and overhead.
- **Negative**:
  - Initial effort required to retroactively assign IDs and frontmatter to existing ADRs.
  - Users must run the automation script (or integrate it into a pre-commit hook) to keep the index up-to-date.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision

We will implement a **native Vue 3 extensibility system** that combines **registry-based template extension points** and **Composition API-compatible logic extension utilities**—built exclusively on Vue’s public, stable APIs—to retain full plugin flexibility while resolving the drawbacks of the previous Component Factory approach and addressing the limitations of our initial test implementation.

The system is structured into three core pillars:

---

### 1. Template Extensibility: Native Named Slots + Centralized Override Registry
Replace the custom `<sw-block>` component with native Vue named slots for explicit template sections, paired with a registry that lets plugins modify or replace specific slot content (or full components) without custom runtime compilation.

#### Original Component Implementation:
Define named slots with fallback content to mark extensible template regions:
```html
<template>
  <div class=""sw-hello-world"">
    <!-- Named slot for main content with original fallback -->
    <slot name=""sw-hello-world__content"">
      <div>Hello World</div>
    </slot>
    
    <!-- Additional extensible region -->
    <slot name=""sw-hello-world__actions"">
      <button>Default Action</button>
    </slot>
  </div>
</template>
```

#### Plugin Template Overrides:
Plugins register slot or component replacements in a centralized registry:
```js
// Plugin code
import { registerSlotOverride, registerComponentOverride } from '@shopware-ag/frontend-extensibility';

// Override a specific slot
registerSlotOverride('sw-hello-world__content', () => import('./CustomHelloContent.vue'));

// Or replace the entire component
registerComponentOverride('sw-hello-world', () => import('./FullHelloWorldOverride.vue'));
```

The core framework will resolve registry entries at runtime and render overridden content, falling back to original slots if no override exists.

---

### 2. Script & Logic Extensibility: Exposed State + Typed HOC Helpers
To solve data merging and internal access challenges without relying on Vue’s internal logic:
- **Explicitly expose component internals** using Vue’s `defineExpose` for `<script setup>` components.
- **Provide typed, safe higher-order component (HOC) utilities** to wrap original components, merge props, and extend logic while preserving reactivity.
- **Add hook-based extension points** in core components for minor logic modifications (e.g., modifying state) without full component wrapping.

#### Original Component Implementation:
```html
<script setup lang=""ts"">
import { ref, defineExpose } from 'vue';
import { useExtensionHooks } from '@shopware-ag/frontend-extensibility';

// Core state and logic
const message = ref('Hello World');
const updateMessage = (newVal: string) => message.value = newVal;

// Register extension hooks for plugins to tap into
const { beforeMount, afterMount } = useExtensionHooks('sw-hello-world');
await beforeMount({ message, updateMessage });

// Expose internals for HOC extensions
defineExpose({ message, updateMessage });
</script>
```

#### Plugin Logic Extensions:
##### Option A: Minor Modifications via Hooks
```js
// Plugin code
import { registerExtensionHook } from '@shopware-ag/frontend-extensibility';

// Modify state before component mounts
registerExtensionHook('sw-hello-world.beforeMount', (context) => {
  context.message.value = 'Hello Custom World';
});
```

##### Option B: Full Logic Extension via HOC
```js
// Plugin code
import SwHelloWorld from '../original/sw-hello-world.vue';
import { createComponentExtender, inheritProps } from '@shopware-ag/frontend-extensibility';
import { ref } from 'vue';

// Create extended component using typed helper
const ExtendedHelloWorld = createComponentExtender(SwHelloWorld, {
  props: inheritProps(SwHelloWorld), // Safely inherit original props
  setup(originalInternals) {
    // Add custom state/logic
    const customGreeting = ref('Hello Extended!');
    const customAction = () => console.log('Custom action triggered');

    // Merge original and custom logic
    return {
      ...originalInternals,
      customGreeting,
      customAction
    };
  }
});

// Register as full component override
registerComponentOverride('sw-hello-world', () => ExtendedHelloWorld);
```

---

### 3. Core Guarantees for Long-Term Maintainability
- **No Internal Vue Dependencies**: All utilities use only Vue’s public APIs (e.g., `defineExpose`, `h()`, `inheritAttrs`), ensuring compatibility across Vue updates.
- **Full Vue Ecosystem Compatibility**: Precompiled components support linting, static analysis, TypeScript, and Vue DevTools without custom modifications.
- **Performance Optimizations**: Runtime registry resolution is lightweight, and precompiled components eliminate the overhead of the old Component Factory’s runtime compilation.
- **Backward Compatibility**: Provide temporary shims for the old Component Factory API to support existing plugins during migration.

---

## Rationale
1. **Solves Template Extensibility Limitations**: Native named slots eliminate the need for custom block resolution logic and integrate seamlessly with Vue’s tooling, while the registry system retains the flexibility to modify specific template sections.
2. **Resolves Script Logic Merging Challenges**: Explicitly exposing internals via `defineExpose` avoids relying on Vue’s compiler optimizations, and typed HOC utilities ensure safe, reactive merging of original and plugin logic.
3. **Eliminates Update-Safe Risks**: Building on Vue’s public APIs ensures the system won’t break with Vue upgrades, unlike the initial test approach’s reliance on internal logic.
4. **Retains Full Plugin Flexibility**: Hook-based minor modifications and HOC-based full overrides let plugins modify every component layer (template, state, methods) just like the original Component Factory.
5. **Performance Improvements**: Precompiled components remove runtime compilation overhead, and registry-based overrides enable tree-shaking of unused extensions in production.

---

## Implications
- **Core Component Refactoring**: All core components need to be updated to use named slots, expose internals via `defineExpose`, and add extension hooks where appropriate.
- **Plugin Migration Path**: Provide automated migration tools and documentation to help existing plugins transition from the old Component Factory API to the new system.
- **Framework Tooling**: Build a shared extensibility package containing registry utilities, HOC helpers, and hook functions.
- **Testing Strategy**: Add comprehensive tests for the extensibility system to ensure plugin modifications don’t break core behavior and to validate compatibility across Vue versions.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","# Architectural Decision Record: Changelog Workflow Overhaul

## Decision  
Adopt a **per-change, unreleased entry workflow with automated release batching and Jira integration** to resolve the identified trunk-based development pain points, structured as follows:

### 1. Directory-Based Changelog Entry Storage
Replace the single version-locked changelog file with a modular directory structure:
- Create a `.changelog/` root directory containing:
  - `unreleased/`: For developer-submitted entries for changes not yet included in a formal release.
  - `archived/`: A subdirectory per release version (e.g., `archived/v2.1.0/`) to store processed entries for historical tracking.
- Each unreleased entry is a standalone markdown file following the naming convention: `<JIRA-ISSUE-ID>-<kebab-case-short-summary>.md` (e.g., `PROJ-456-fix-db-connection-leak.md`).
- Mandatory fields in each entry:
  - Brief, user-facing summary of the change.
  - Change type (feature, bug fix, breaking change, internal improvement, documentation update).
  - Direct link to the corresponding Jira issue.
  - For breaking changes: Explicit upgrade instructions (e.g., ""Deprecated `config.yaml` field `old_setting`; replace with `new_setting` of type string"").

### 2. Release-Time Version Assignment
- Developers do not specify a release version when creating entries; all `unreleased/` entries are marked as pending inclusion in a future release.
- When cutting a new release (major/minor/patch), use an automated CI/CD step or lightweight CLI tool to:
  a. Collect all valid unreleased entries (skipping those linked to unresolved Jira issues).
  b. Append entries to a root `CHANGELOG.md` file (following the [Keep a Changelog](https://keepachangelog.com/) format) grouped by the new release version and date.
  c. Move processed entry files to the corresponding `archived/<RELEASE-VERSION>/` directory.

### 3. Aligning Jira and Code State
- Enforce a 1:1 link between merged PRs (with user/impactful changes) and changelog entries via CI checks: PRs will fail to merge if they lack a corresponding entry linked to a Jira issue in a ""Ready for Release"" state.
- Automate Jira state sync: When entries are batched for release, trigger a Jira API call to transition linked issues to a ""Released"" state, ensuring Jira reflects the actual code deployment status.
- Add a validation step in the release pipeline to reject entries linked to Jira issues in unresolved states (e.g., ""In Progress"", ""Under Review"") and flag them for follow-up.

### 4. Developer Experience & Maintenance
- Document clear guidelines in `CONTRIBUTING.md` with examples of valid entries and naming conventions.
- Provide a simple CLI template (e.g., `yarn changelog new PROJ-123 ""Add SSO support""`) to generate pre-structured entry files, reducing manual effort.
- Schedule quarterly workflow reviews to gather developer feedback and adjust conventions or automation as needed.

---

## Rationale
- **Eliminates Merge Conflicts**: Modular entry files avoid concurrent edits to the same lines of a single changelog file, a critical fix for trunk-based development.
- **Resolves Version Uncertainty**: Shifting version assignment to release time removes the need for developers to predict future release numbers upfront.
- **Closes Jira-Code State Gap**: Enforced Jira links and automated state sync ensure every code change with impact is tied to a resolved issue, aligning Jira tracking with actual deployed code.
- **Simplifies Maintenance**: Clear conventions, automation, and tooling keep the workflow low-effort for developers while ensuring consistency across releases.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","# Architectural Decision Record: Flow Builder DelayAction Data and Rule Freshness Handling

## Decision

To address the issues of outdated event data and stale/missing rules in delayed Flow Builder actions, we implement the following cohesive solution:

### 1. Entity Reference-Based Data Retrieval (Resolve Outdated Event Data)
- **Replace Full Payload with Immutable References**: For delayed actions, persist only a unique, permanent identifier of the target business entity (e.g., `entityId: ""customer_78901""`) plus metadata about the entity fields required for action execution/rule evaluation, instead of the full original event payload.
- **Fetch Latest Data Pre-Execution**: Immediately before executing the delayed action, retrieve the current state of the entity from the system of record (SOR) using the stored `entityId`.
- **Handle Entity Not Found**: Define configurable fallback behaviors (via Flow Builder UI) for cases where the entity no longer exists: skip action execution, trigger an error handling sub-flow, or log a critical alert with audit details.

### 2. Deferred Rule Evaluation (Resolve Stale/Missing Rules)
- **Avoid Precomputed Rule Results**: Do not store rule evaluation outcomes or static rule copies in the delayed action job. Instead, persist references to relevant rule sets (e.g., rule IDs, rule group tags, or a `latest` version indicator).
- **Fetch Latest Rules at Execution Time**: When the delay expires, retrieve the most recent active version of each referenced rule from the rule repository.
- **Rule Unavailability Handling**: For missing/inactive rules:
  - If the rule is critical to action execution: Trigger a configured error flow (e.g., notify admins, abort the flow).
  - If non-critical: Use a pre-defined default behavior (e.g., proceed with execution using a fallback rule, skip the rule-dependent step).
- **Re-Evaluate Rules with Fresh Data**: Re-run rule evaluations against the newly fetched entity data to ensure alignment with current business logic.

### 3. Persistent Delayed Job Queue with Audit Metadata
- Use a distributed, persistent delayed job queue (e.g., Redis Queue, AWS SQS with Delay Queues, or Celery Beat) to manage delayed action lifecycles. Each job will include:
  - Entity reference (`entityId`, required field list).
  - Rule set references.
  - Flow instance context (flow ID, current step, fallback configurations).
  - Unique job ID for idempotency (prevents duplicate executions during retries).

### 4. Configurable Error Handling & Auditing
- Mandate detailed logging of all delayed action events: delay initiation, expiration, entity fetch status, rule retrieval outcome, rule evaluation result, and execution status.
- Provide Flow Builder users with drag-and-drop error handling options for failure scenarios (entity not found, rule missing, data validation failure) to customize recovery paths without code changes.

## Rationale
- **Addresses Outdated Data**: By relying on entity references instead of static payloads, we ensure actions always use the most current entity state, eliminating dependency on stale event data.
- **Ensures Rule Freshness**: Deferring rule retrieval and evaluation guarantees alignment with the latest business rules, handling cases where rules are updated or deleted during the delay.
- **Scalability & Reliability**: Persistent job queues with minimal metadata reduce storage overhead and ensure delayed actions survive system restarts. Configurable fallbacks make the system resilient to edge cases like deleted entities/rules.
- **Auditability**: Comprehensive logging provides full visibility into delayed action execution, supporting debugging and compliance requirements.

## Consequences
### Positive
- Eliminates reliance on stale event data and rules for delayed actions.
- Improves system resilience to entity/rule changes.
- Reduces storage costs by avoiding large payload persistence.
- Gives users control over failure recovery via configurable fallbacks.

### Negative
- Adds a dependency on the SOR being available at delayed action execution time (requires retry logic for temporary SOR outages).
- Introduces slight latency pre-execution due to data/rule fetch operations (mitigable with caching for frequently accessed entities/rules).
- Requires additional UI/API work to expose entity reference and fallback configuration options in Flow Builder.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","### Architectural Decision  
Integrate the mature app system from the standalone plugin (https://github.com/shopware/app-system) directly into the core of the Shopware 6 platform, with the following actionable implementation steps to realize the stated benefits and ensure a seamless transition:

1. **Core Feature Migration**: Port all production-validated features of the standalone app system plugin to the Shopware 6 platform core, maintaining full functional parity across both cloud and on-premises deployments to ensure a consistent extension mechanism regardless of environment.  
2. **Deprecation & Automated User Transition**: Deprecate the standalone app system plugin and implement a zero-effort migration path for existing users:  
   - Auto-detect and disable the standalone plugin during platform upgrade once the core app system is active.  
   - Guarantee backward compatibility for all existing app integrations (APIs, webhooks, manifest structures, authentication flows) to avoid breaking changes for partners, app manufacturers, and end-users.  
3. **Lifecycle Alignment**: Integrate the app system’s development, release, and maintenance workflows into the official Shopware 6 release cycle, adhering to platform semantic versioning, LTS support policies, bug fix windows, and security patch timelines.  
4. **Stability Signaling & Documentation Update**:  
   - Communicate the core integration to partners, app developers, and end-users as a formal signal of the app system’s production stability.  
   - Revise official documentation, partner onboarding materials, and platform guides to reflect that the app system is now a native core feature (no separate plugin installation required).  
5. **Feedback Loop Integration**: Maintain a structured feedback channel with partners and users for future app system enhancements, routing feature requests and bug reports through the core platform’s contribution and quality assurance pipelines.

This decision addresses all stated goals: unifying the extension mechanism across cloud/on-prem environments, signaling stability to the ecosystem, aligning with reliable platform release cycles, and eliminating the need for an extra plugin for end-users.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","# Architectural Decision: Unified FlowStorer Architecture for Reduced Boilerplate and Plugin Flexibility

## Decision
Adopt a standardized, generic architecture for `FlowStorer` implementations to eliminate boilerplate code, simplify plugin integration, and reduce maintenance overhead:

### 1. Unify on a Single `FlowStorer` Interface
Replace all per-storer-specific interfaces with a single, universal `FlowStorer` interface that all implementations must implement:
```java
public interface FlowStorer<T> {
    // Standardized eligibility check (replaces per-storer interface logic)
    boolean shouldStore(Event event);
    
    // Core storage logic
    void store(Event event);
    
    // Optional restore method (defaults to empty for scalar storers)
    default Optional<T> restore() {
        return Optional.empty();
    }
}
```
This interface centralizes the eligibility check for events and standardizes core storage behavior across all storers.

### 2. Generic `ScalarFlowStorer<T>` Base Class
Create a reusable `ScalarFlowStorer<T>` abstract base class for all scalar-value storers that do not require restore logic. This class:
- Implements the `FlowStorer` interface
- Accepts type-specific configuration via constructor:
  - A predicate to check if the event contains relevant scalar data (e.g., `event -> event.hasMetricValue()`)
  - A function to extract the scalar value from the event (e.g., `event -> event.getMetricValue()`)
- Handles common scalar storage logic (e.g., in-memory persistence, value caching)
- Eliminates the need to write full custom classes for simple scalar storers; new scalar storers can be instantiated via configuration (e.g., `new ScalarFlowStorer<>(Double.class, event -> event.hasMetric(), event -> event.getMetric())`)

### 3. Centralized `FlowStorerRegistry`
Implement a singleton `FlowStorerRegistry` to manage all registered `FlowStorer` instances:
- Provides methods to register/unregister storers (supports plugin-driven registration)
- Exposes a `processEvent(Event event)` method that iterates over all registered storers, checks eligibility via `shouldStore(event)`, and invokes `store(event)` for applicable storers
- Adds plugin-friendly filtering capabilities:
  - Allow plugins to exclude specific storers by unique ID for targeted events
  - Support overriding `shouldStore` predicates for existing storers via registry extensions
  - Enable plugins to register custom storers alongside core implementations

### 4. Legacy Refactor and Deprecation Plan
- Refactor all existing scalar `FlowStorer` implementations to use the generic `ScalarFlowStorer` base class or replace them with configured instances
- Deprecate all per-storer interfaces with backward-compatibility shims (adapters that wrap legacy storers in the standard interface)
- Remove deprecated legacy components in the next major release cycle

## Rationale (Alignment with Context)
- **Eliminates Boilerplate**: The generic scalar base class reduces custom code for simple storers from full classes to configuration snippets. The unified interface removes redundant eligibility logic across per-storer interfaces.
- **Simplifies Plugin Bypassing**: The registry provides a single entry point for plugins to filter or modify storer eligibility, avoiding the need to interact with individual storer-specific interfaces.
- **Maintains Flexibility**: The optional `restore` method supports existing storers that require recovery logic, while the generic base class handles the majority of scalar use cases.
- **Improves Maintainability**: A single interface and registry make it easier to test, debug, and extend storage workflows.

## Key Implications
- **Migration Effort**: Legacy code/plugins will need to be refactored to use the standardized interface. A 2-release deprecation window with shims will mitigate disruption.
- **Configuration Shift**: Teams will transition from writing custom scalar storer classes to configuring generic instances, reducing long-term code maintenance.
- **Centralized Dependency**: The registry becomes a critical system component, requiring thread-safety and lifecycle management (e.g., registration during application startup).","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","### Decision
1. **Adopt CKEditor 5 as the official replacement for the existing sw-text-editor** in the administration interface. This choice is prioritized for its alignment with all core requirements outlined in the context.
2. **Implement a modular, phased integration**:
   - Start with the minimal CKEditor 5 core build (including stable, bug-free basic WYSIWYG features like text formatting, lists, links, and image embedding) to minimize initial complexity and maintenance overhead.
   - Migrate custom domain-specific features (e.g., content validation, brand-aligned formatting) to CKEditor 5’s standardized plugin system, ensuring custom logic is decoupled from core editor code for easy extension and updates.
   - Gradually phase out the legacy sw-text-editor by first replacing high-traffic, bug-prone instances, while using a content migration script to ensure backward compatibility between legacy editor output and CKEditor 5’s standard HTML format.
3. **Establish a maintenance workflow**: Align custom plugin updates with CKEditor 5’s release cycle, leverage official documentation and community support for troubleshooting, and conduct quarterly audits to ensure core functionality remains stable and extensible.

### Rationale Tied to Context
- **Stability**: CKEditor 5’s mature, actively maintained core has extensive test coverage, resolving the low-level basic WYSIWYG bugs plaguing the legacy editor.
- **Maintainability**: Its modular architecture allows selective feature inclusion (reducing code bloat), and official documentation + a large community simplify long-term upkeep.
- **Extendability**: The plugin system provides a standardized, low-effort way to add custom features without modifying core editor logic, supporting future flexibility.
- **Feature Set**: Core builds include all essential WYSIWYG capabilities, with optional plugins available for advanced needs (e.g., table editing, markdown support) as required by the administration team.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision
### Overarching Decision
We will standardize on the value `""not_specified""` as the explicit default salutation for all customer, user, and order entities within the Shopware 6 platform to address unspecified salutation cases and promote inclusive, neutral communication with customers. This replaces implicit defaults (e.g., `null`, empty strings) to ensure consistency across all layers of the system.

### Concrete Implementation Decisions (Aligned with Shopware 6 Architecture)
1.  **Data Layer & Migration**
    - Modify database schema constraints for all salutation-related columns (e.g., `customer.salutation`, `order.salutation`) to set the default value to `""not_specified""` instead of `null` or empty strings.
    - Create a Shopware database migration script to backfill all existing records with unspecified salutations (null, empty strings) to `""not_specified""` to ensure data consistency across historical and new entries.
2.  **Core Services & Validation**
    - Update Shopware’s core salutation management services (e.g., `SalutationLoader`, `CustomerValidator`, `OrderValidator`) to recognize `""not_specified""` as a valid, default value. Enforce that any unspecified input during customer/user creation, order placement, or entity updates is automatically mapped to `""not_specified""` instead of falling back to unstandardized values.
    - Add `""not_specified""` to the list of valid salutation values in core validation logic to prevent invalid or unspecified inputs from bypassing the standardized default.
3.  **Frontend (Storefront & Administration)**
    - **Admin UI**: 
      - Set the salutation dropdown default to `""not_specified""` when creating new customers, users, or orders, with a localized label (e.g., ""Not Specified"") that is respectful and inclusive.
      - Update salutation filter and display components to handle the `""not_specified""` value and sort it appropriately in dropdowns/filters.
    - **Storefront**:
      - Pre-select ""Not Specified"" as the default salutation option in customer registration, account edit, and checkout forms, with a clear, inclusive label.
      - Adjust customer-facing communications (invoices, order confirmations, account emails) to render the `""not_specified""` value as a neutral greeting (e.g., ""Dear Customer"") instead of displaying the raw value, to maintain professionalism while upholding inclusivity.
4.  **Localization**
    - Add translated labels for the `""not_specified""` salutation across all supported Shopware locales, ensuring terminology is culturally sensitive and consistent with inclusive language guidelines.
5.  **API Contracts**
    - Update REST and GraphQL API specifications to explicitly document `""not_specified""` as the default salutation value for relevant endpoints (e.g., customer creation, order placement). Ensure API requests omitting the salutation field are automatically populated with this default.
6.  **Testing & Validation**
    - Implement unit and integration tests for core services to verify unspecified inputs are correctly mapped to `""not_specified""`.
    - Create E2E tests for storefront and admin UIs to validate default salutation selection and proper rendering of neutral greetings for `""not_specified""` entries.
    - Validate database migrations to ensure historical unspecified records are correctly converted without data loss.
7.  **Documentation**
    - Update developer and merchant documentation to explain the new inclusive default salutation, how it is handled across the platform, and how to customize its display if needed (while emphasizing retention of inclusive defaults as a best practice).

This set of decisions ensures consistency, inclusivity, and compatibility with Shopware 6’s core architecture while addressing the requirement to handle unspecified salutation cases explicitly.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","### Decision

1. **Remove the `padaliyajay/php-autoprefixer` Package and Its Integration**  
   Immediately eliminate the `padaliyajay/php-autoprefixer` dependency from the project and remove all code that invokes it during the `theme:compile` workflow. Bootstrap 5 natively includes vendor prefixes for its components aligned with modern browser support matrices (matching our `.browserslist` configuration), so this package no longer provides meaningful value and is the primary cause of `theme:compile` performance degradation in the SaaS environment.

2. **Validate Compiled CSS for Critical Prefix Coverage**  
   Conduct a targeted audit to ensure no necessary vendor prefixes are missing:
   - **Bootstrap Components**: Confirm Bootstrap 5’s compiled CSS includes all required prefixes for browsers in our `.browserslist` (Bootstrap’s build process already respects standard browser support configurations).
   - **Custom Storefront CSS**: Use `autoprefixer-cli` with our `.browserslist` to scan custom SCSS/CS S not part of Bootstrap. For any properties identified as needing prefixes, manually add them to the source files (this ensures only essential prefixes are included without the overhead of an automated tool).

3. **Standardize Manual Prefixing Guidelines for Future Development**  
   Update storefront development documentation to establish:
   - Bootstrap components do not require additional prefixing.
   - For custom CSS, developers must reference [caniuse.com](https://caniuse.com/) to determine if manual vendor prefixes are needed for browsers in our `.browserslist`.
   - Any future need for automated prefixing must use a modern, standards-compliant tool (e.g., PostCSS with `autoprefixer`) that natively respects `.browserslist` and does not introduce unacceptable performance overhead.

4. **Monitor Performance and Compatibility Post-Removal**  
   - Track `theme:compile` execution times to confirm the expected performance improvement (target: reduction in build time proportional to the overhead introduced by the removed package).
   - Perform cross-browser testing across all browsers in our `.browserslist` to rule out styling regressions caused by the removal of the autoprefixer.

### Rationale
- **Performance**: Removing the inefficient, hard-coded `padaliyajay/php-autoprefixer` directly resolves the significant build slowdown in the SaaS environment.
- **Browser Support Alignment**: Bootstrap 5’s native prefixing ensures we adhere to our `.browserslist` without relying on a package that ignores this configuration.
- **Sustainability**: Manual prefixing for custom code (validated via caniuse.com) is more transparent and avoids the long-term maintenance risks of a stagnant, hard-coded prefixing tool.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision
We will implement a two-pronged strategy to address missed vulnerability alerts for pre-release package versions and mitigate associated risks:

1. **SemVer-Compliant Custom Vulnerability Scanning Workflow**
   Integrate a dedicated vulnerability check into our CI/CD pipeline and dependency management process using a strict SemVer 2.0.0-compliant tool (e.g., the official `semver` npm package). This workflow will:
   - Correctly compare pre-release versions (e.g., `1.9.0-alpha1`) against published vulnerability ranges, recognizing that pre-releases of higher stable versions (like `1.9.x`) are newer than lower stable versions (like `1.8.7`).
   - Trigger critical alerts if a pre-release version falls within any vulnerable range (e.g., versions between `1.8.7` and `2.0.0` in the provided scenario, where the fix is exclusive to `2.0.0`).
   - Bypass npm's default non-compliant version comparison logic entirely for vulnerability evaluations of pre-releases.

2. **Mandatory Security Backporting for Active Pre-Release Branches**
   For any vulnerability fixed in a future stable release (e.g., `2.0.0`), we will backport the fix to all active pre-release branches (e.g., `1.9.x`) before publishing new pre-release versions. We will also:
   - Maintain explicit metadata linking pre-release versions to their parent branches and associated vulnerability statuses.
   - Deprecate any unpatched vulnerable pre-release versions in the package registry to warn downstream users.

---

## Rationale
- **Directly Addresses npm's Comparison Flaw**: The custom scanning workflow eliminates missed alerts caused by npm misinterpreting pre-releases as `<0.0.0`, ensuring we detect vulnerabilities in pre-releases that lie within unpatched ranges.
- **Proactive Risk Mitigation**: Backporting fixes to pre-release branches ensures our pre-release versions are not exposed to known vulnerabilities, reducing risk for developers using early-access builds.
- **Controllable and Feasible**: Unlike modifying npm's core logic, these changes are within our project's control, leverage existing tooling, and integrate seamlessly into our development lifecycle.
- **Transparent for Users**: Deprecating unpatched pre-releases ensures downstream consumers are aware of risks even if they bypass our internal scanning.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision

### 1. Query Optimization
Rewrite the DELETE query to use a logically equivalent, index-friendly condition that unifies the original OR clauses:
```sql
DELETE FROM cart
WHERE COALESCE(updated_at, created_at) <= '2023-02-01'
LIMIT 1000;
```
This expression uses `created_at` as a fallback for entries with no `updated_at` value, matching the original logic while simplifying index utilization.

### 2. Index Creation (Database-Aware)
Implement an index to eliminate full-table scans, with options for different database versions:
- **For Databases Supporting Functional Indexes** (PostgreSQL 9.2+, MySQL 8.0.13+, MariaDB 10.5+, etc.):
  Create a functional index directly covering the `COALESCE` expression:
  ```sql
  CREATE INDEX idx_cart_last_activity ON cart (COALESCE(updated_at, created_at));
  ```
  This index allows the database to instantly locate outdated entries without scanning all rows.
  
- **For Older Databases Without Functional Index Support** (MySQL <8.0.13, MariaDB <10.5):
  a. Create a composite index to cover both conditions in the original query:
     ```sql
     CREATE INDEX idx_cart_updated_created ON cart (updated_at, created_at);
     ```
  b. Split the deletion into two batch queries to leverage the composite index effectively:
     ```sql
     -- Delete entries with non-null updated_at first
     DELETE FROM cart
     WHERE updated_at IS NOT NULL AND updated_at <= '2023-02-01'
     LIMIT 1000;

     -- Delete entries with null updated_at (using created_at)
     DELETE FROM cart
     WHERE updated_at IS NULL AND created_at <= '2023-02-01'
     LIMIT 1000;
     ```
  This avoids the OR clause (which query planners often struggle to optimize) and uses the composite index to filter rows efficiently.

### 3. Batch Deletion Best Practices
- Retain the `LIMIT 1000` clause (adjust based on performance testing) to process deletions in small batches. This reduces table lock contention, prevents long-running transactions, and minimizes disruption to high-traffic shop operations.
- For periodic cleanup jobs:
  - Add a 1-2 second delay between batches to avoid overwhelming the database during peak hours.
  - Halt batch processing once no rows are deleted to avoid unnecessary query runs.

### 4. Validation & Monitoring
- After implementation, run `EXPLAIN` on the optimized query to confirm the index is being used (check that `possible_keys` and `key` columns include the new index).
- Monitor query execution times to ensure they drop below acceptable thresholds (e.g., <1 second per batch).
- Track write overhead (INSERT/UPDATE on `cart`): the minor performance cost of the index is justified by the critical improvement to cleanup query speed on high-traffic shops.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","# Architectural Decision Record (ADR) Decision

## Decision
Implement an official **Experimental Feature Framework** that enables the release of early-stage, minimal viable increments of features to users, with explicit disclaimers waiving long-term backwards compatibility commitments. The framework will include a structured lifecycle for feedback collection, iterative refinement, graduation to stable status, or deprecation—aligned with our goal of shipping value sooner, gathering user-centric feedback, and reducing the risk of building misaligned features.

### Key Components of the Framework
To operationalize this decision, we will enforce the following core practices:

1.  **Explicit Labeling and Transparent Communication**
    - All experimental features must be clearly marked across all touchpoints:
      - UI: Visible ""Experimental"" badge next to feature elements, with tooltips explaining the non-stable status.
      - APIs: Prefix experimental endpoints (e.g., `/api/experimental/b2b/employee-management`) or require an opt-in header (e.g., `X-Enable-Experimental-Features: true`).
      - Documentation: Dedicated experimental feature section with prominent disclaimers stating: no backwards compatibility guarantee, potential for unannounced changes/removal, and that experimental features are not covered by standard support SLAs.
    - Communicate the availability of experimental features to interested customers via targeted outreach, support portals, and release notes.

2.  **User Opt-In/Opt-Out Mechanism**
    - Provide account-level or user-level toggles (depending on feature scope) for customers to voluntarily enable/disable experimental features. This ensures mainstream users are not disrupted, while engaged users can participate in feedback.
    - For API-driven features, allow opt-in via account settings or request headers to avoid unintended usage by integration partners.

3.  **Structured Lifecycle for Experimental Features**
    - **Launch Phase:** Release the minimal viable increment of the feature (e.g., basic B2B employee management without budget controls, core advanced search without full customization). Document intended use cases and link to feedback channels.
    - **Iteration & Feedback Phase:** Establish dedicated feedback loops:
      - In-app feedback forms tied directly to experimental features.
      - Dedicated support ticket tags for experimental feature inquiries/feedback.
      - Periodic check-ins with opt-in customers to gather qualitative insights on usability, missing functionality, and ecosystem extendability needs.
      - Prioritize feature iterations using this real user feedback to align development with actual customer needs.
    - **Graduation to Stable:** A feature will graduate to stable status only when:
      - It has validated its core value proposition via consistent positive feedback.
      - Its design and implementation are mature enough to commit to long-term backwards compatibility.
      - Critical ecosystem extendability requirements have been addressed.
      - A migration path (if needed) is provided for users transitioning from the experimental version to the stable release.
    - **Deprecation:** If an experimental feature fails to gain traction or feedback indicates it does not align with user needs, we will:
      - Notify opt-in users of the upcoming deprecation with a 2-release-cycle notice period.
      - Remove the feature entirely in the next major release, with no obligation to maintain compatibility or provide migration paths.

4.  **Governance & Policy Updates**
    - Maintain a centralized register of active experimental features, tracking lifecycle phase, feedback summaries, and next steps for visibility across teams.
    - Update our official Backwards Compatibility Policy to explicitly exclude experimental features, reaffirming that only stable features are covered by our long-term compatibility guarantee.

### Initial Prioritization
We will launch the framework with two pilot features as defined in the context:
1.  B2B multi-account basic employee management (core functionality without advanced budget/access controls).
2.  Core advanced search (without full configuration and customizability options).

This decision balances our need to iterate on early-stage features with our commitment to maintaining reliable, backwards-compatible stable features for all users, while empowering us to build solutions that better align with real customer and ecosystem needs.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Architectural Decision Record: Shopware 6 Stock Handling Refactor

### Decision Statement
We will refactor Shopware 6’s stock handling system to address flexibility, performance, and complexity issues via four core changes:
1.  Implement a modular abstraction layer for stock operations to enable custom integrations.
2.  Replace hard-coded stock adjustments with an event-driven lifecycle model.
3.  Simplify the data model to use a single source of truth for on-hand stock, with a dedicated reserved stock table.
4.  Optimize large-catalog performance via aggregated reserved stock tracking and configurable caching.

---

### Detailed Decision Breakdown & Justifications

#### 1. Modular Abstraction Layer for Stock Operations
**Implementation**:
- Define core interfaces (`StockManagerInterface`, `StockPersistenceInterface`, `StockIntegrationInterface`) that standardize operations like loading stock, adjusting on-hand quantities, and calculating available stock.
- Make all system components (checkout, product listings, admin UI) depend on these interfaces instead of concrete implementations.
- Ship a default `DefaultStockManager` that uses the database as the backend, and allow developers to replace this with custom implementations (e.g., ERP sync) via Shopware’s dependency injection system.

**Solves**:
- Inability to easily replace stock loading with custom ERP logic: Developers can inject a custom `StockManagerInterface` implementation without modifying core code.

#### 2. Event-Driven Stock Adjustments
**Implementation**:
- Replace hard-coded stock modification logic in order/fulfillment flows with domain events (e.g., `OrderPlacedEvent`, `OrderCompletedEvent`, `OrderCancelledEvent`, `ReturnReceivedEvent`).
- Implement core event subscribers to handle default stock operations (e.g., `OrderPlacedStockReserver` reserves stock on order placement; `OrderCompletedStockUpdater` converts reserved stock to on-hand reductions).
- Allow developers to extend/override behavior by adding custom subscribers or disabling default ones via configuration.

**Solves**:
- Inability to modify stock changes across the order lifecycle: Teams can adjust stock behavior by adding/removing subscribers instead of editing core order processing code.

#### 3. Simplified Data Model with Single Source of Truth
**Implementation**:
- Remove redundant `stock` and `availableStock` fields from the `product` table, replacing them with a single `on_hand_stock` field (source of truth for physical stock on hand).
- Create a new `product_reserved_stock` table that tracks aggregated reserved quantities per product (one row per product with `reserved_quantity`).
- Calculate available stock dynamically as: `available_stock = on_hand_stock - reserved_quantity`.

**Solves**:
- Redundant stock fields and complex sync logic: Eliminates the need to maintain two separate stock values and aligns with standard inventory practices.

#### 4. Large-Catalog Performance Optimizations
**Implementation**:
- Update the `product_reserved_stock` table in real time via event subscribers (e.g., increment reserved quantity on order placement, decrement on cancellation).
- Add database indexes on `product_reserved_stock.product_id` and `product.on_hand_stock` to speed up queries.
- Implement a configurable cache for available stock values (using Shopware’s existing caching system) that invalidates automatically when `on_hand_stock` or `reserved_quantity` changes. Allow cache bypass for critical use cases (e.g., checkout) via a flag in `StockManagerInterface`.

**Solves**:
- Slow available stock calculation on large catalogs: Aggregated reserved stock eliminates the need to sum open order line items for every lookup, while caching reduces repeated computation.

---

### Trade-Offs
- **Migration Cost**: Third-party plugins relying on old `stock`/`availableStock` fields will need to migrate. Mitigate with a deprecation period, virtual property backward compatibility, and migration guides.
- **Event Debugging Complexity**: Tracing stock changes requires following event flows instead of hard-coded logic. Mitigate with detailed stock operation logging and an admin dashboard for stock history (on-hand and reserved changes).
- **Cache Staleness Risk**: Failed cache invalidation could lead to incorrect available stock. Mitigate with idempotent event handlers and periodic cache warm-up jobs as a safeguard.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","# Architectural Decision Record: Exception Log Level Adjustment for Client-Side API Errors

## Decision
1. **Leverage and extend the existing exception-to-log-level mapping configuration** (already implemented for the cloud product) to reduce log noise from expected client-side API errors:
   - Add a dynamic rule to the configuration system that automatically maps all `ShopwareHttpException` instances returning an HTTP 4xx status code to the `notice` log level (replacing the default `error` level).
   - Retain full support for explicit class-based exception mappings (e.g., listing specific exception subclass names) to override the dynamic status-based rule for edge cases where a 4xx exception requires error-level logging.
2. **Standardize this configuration across all deployment environments** (cloud and non-cloud) to ensure consistent log behavior and avoid platform-specific noise discrepancies.
3. **Preserve error-level logging for `ShopwareHttpException` instances with HTTP 5xx status codes**, as these indicate server-side failures requiring root-cause analysis.
4. **Document the configuration mechanism** (including both dynamic status rules and explicit class overrides) in the development documentation to enable teams to adjust log levels for new exception types as needed.

### Rationale (Embedded for Clarity)
This decision builds on the proven cloud product configuration pattern, avoiding redundant system development. The dynamic status-based rule eliminates the need to manually list every 4xx-specific `ShopwareHttpException` subclass, ensuring scalability as new client-side error exceptions are added. Explicit override support retains flexibility for critical monitoring use cases, while cross-environment standardization simplifies log analysis and troubleshooting across deployments.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","### Decision
1. **Adopt UUIDv7 as the standard primary key type** for all new database tables and future primary key columns in existing tables to resolve B-tree index inefficiencies caused by UUIDv4’s fully random prefix.
2. **Phase UUIDv4 migration for existing tables**:
   - Prioritize migrating UUIDv4 primary keys to UUIDv7 in high-throughput tables where index fragmentation, excessive page allocation, and performance degradation are most pronounced.
   - Retain UUIDv4 in low-throughput tables unless performance issues emerge or migration is trivial, to avoid unnecessary operational overhead.
3. **Standardize UUIDv7 generation**:
   - Use database-native UUIDv7 generation functions (e.g., PostgreSQL’s `uuid_generate_v7()` from the `uuid-ossp` extension, MySQL 8.0.36+’s `UUID_TO_BIN(UUID_V7(), TRUE)`) where available to ensure consistency, reduce application-level overhead, and align with database optimizations.
   - For databases without native UUIDv7 support, integrate trusted, RFC 9562-compliant libraries (e.g., `uuid7` for Python, Node.js’s `uuid` package with UUIDv7 support) across all application services to guarantee valid, time-ordered UUIDv7 values.
4. **Maintain backward compatibility**:
   - Keep application-level logic capable of processing both UUIDv4 and UUIDv7 primary keys until full migration of all impacted tables is complete, to avoid breaking existing data access patterns and cross-service integrations.
5. **Validate and monitor performance**:
   - Conduct pre- and post-migration performance testing on high-impact tables to measure improvements in index insertion latency, query speed, and index size.
   - Implement ongoing monitoring of index fragmentation and storage usage to ensure the expected compactness and efficiency gains from UUIDv7 are sustained over time.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision
We will implement a dynamic, extensible event registry system for the flow builder to enable external apps to register, manage, and surface custom events within the flow builder interface and execution pipeline. The solution will consist of the following structured components and rules:

1. **Standard Event Contract**:
   Mandate a machine-readable, versioned schema for all events (core and custom) that includes:
   - A unique, namespaced event ID (e.g., `my-app:subscription-expired`) to prevent conflicts with core events
   - Human-readable name, description, and app-specific branding metadata
   - Trigger condition details (user action, system state change, external webhook, etc.)
   - JSON Schema for the event payload to enforce data consistency and type safety
   - Permissions scope (authorized roles/apps allowed to use the event in flows)

2. **Centralized Dynamic Event Registry Service**:
   - Deploy a REST API (with a companion SDK for app developers) supporting CRUD operations for custom event registration.
   - Persist registered events in a dedicated database table, linked to the owning app’s identity.
   - Enforce security checks: authentication via app-specific API keys/OAuth 2.0, conflict validation (blocking duplicate event IDs), and schema sanitization to prevent malicious payloads.
   - Offer real-time webhook notifications for event registry updates (add/remove/modify) to sync with the flow builder UI.

3. **Flow Builder UI & Execution Integration**:
   - Modify the flow builder’s event selection interface to fetch combined core + custom events from the registry API, with visual differentiators for custom events (e.g., app logos, ""Custom"" tags).
   - Implement real-time sync: subscribe the builder to registry webhooks to reflect event changes without requiring a page reload.
   - Refactor the core trigger processing layer to be schema-agnostic, using the event’s payload schema to validate incoming data and route triggers to flow actions seamlessly for both core and custom events.

4. **Developer Enablement & Tooling**:
   - Publish detailed documentation on the event contract, registry API endpoints, and SDK usage.
   - Provide a sandbox environment for app developers to test event registration and trigger execution without affecting production flows.
   - Offer a schema validation CLI tool to help developers ensure their custom events comply with the standard contract before registration.

## Rationale
This approach directly resolves the non-extensible event limitation by:
- Enabling on-the-fly event extension without redeploying the core flow builder, reducing downtime and maintenance overhead.
- Ensuring interoperability and consistency between core and custom events via a standardized schema.
- Maintaining system stability through authentication, conflict checks, and payload validation.
- Reducing friction for app developers via dedicated SDKs, sandbox testing, and clear documentation.
- Supporting real-time updates to the event list, improving the user experience for flow builders working with evolving app integrations.

Alternative approaches (e.g., hardcoding custom events, static plugin bundles) were rejected because they require core system redeployments for each new event, lack flexibility for on-demand updates, and introduce higher maintenance overhead for both core and app teams.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","### Architectural Decision

Implement a dual-validation authentication flow for direct client app-to-App Server communication to verify request legitimacy, customer identity, and shop-app authorization:

1. **Customer Identity Validation via Shopware JWT**:
   - Require client-side apps to include a valid Shopware Customer JWT (obtained via Shopware’s official customer authentication endpoints, e.g., `/api/customer/login`) in requests to the App Server, passed in the `Authorization: Bearer <customer-jwt>` header.
   - The App Server will validate this JWT in one of two ways (configurable per use case):
     - **Signature Validation**: Cache the Shopware instance’s public key (retrieved during app installation or via Shopware’s API) to verify the JWT’s signature and expiry without additional roundtrips.
     - **Introspection Validation**: For high-security use cases, call Shopware’s token introspection endpoint (authenticated using the shop-app secret shared during app installation) to confirm the token’s active status, associated customer ID, and shop context.

2. **App/Shop Legitimacy Validation via Request Signing**:
   - Mandate client apps to sign the request payload (plus a timestamp to prevent replay attacks) using the shop-app secret, and include this signature in a custom header (e.g., `X-App-Request-Signature`).
   - The App Server will recompute the signature using its stored shop-app secret for the target shop and validate it against the header value to ensure the request originates from an authorized app registered with the Shopware instance.

### Rationale
This flow addresses the core gaps in secure direct communication:
- **Trusted Customer Identity**: Validating the Shopware Customer JWT ensures the request is tied to an authenticated, active customer in the target Shopware instance, eliminating reliance on untrusted client-side claims.
- **Authorized App Origin**: Using the shop-app secret to sign requests verifies the request comes from an app officially installed in the shop, preventing spoofed requests from unregistered clients.
- **Efficiency Flexibility**: Signature validation minimizes latency via cached public keys, while introspection provides stricter validation for sensitive operations (e.g., order modifications).
- **Compatibility**: Leverages Shopware’s native customer authentication and app installation mechanisms, avoiding custom identity systems or breaking existing workflows.

### Trade-Offs
- **Complexity**: Adds implementation overhead for client apps (JWT retrieval, request signing) and the App Server (dual validation logic, key caching).
- **Shopware Dependency**: Introspection validation introduces a runtime dependency on Shopware availability (mitigated by caching valid JWTs for short intervals).
- **Replay Attack Risk**: Requires strict timestamp enforcement and short JWT expiry windows to mitigate replay attacks, adding minor client-side logic.

### Next Steps
1. Define standard request headers/payload format for direct app-to-App Server communication and document it for app developers.
2. Implement Shopware public key retrieval/caching and JWT signature validation in the App Server.
3. Build request signature generation/validation utilities using the shop-app secret, including timestamp checks.
4. Add support for Shopware token introspection (authenticated via shop-app secret) as an optional validation mode.
5. Test end-to-end flow: Customer logs into Shopware → client app retrieves JWT → signed request sent to App Server → validation confirms legitimacy → trusted customer data returned.
6. Add monitoring for validation failures to detect unauthorized access attempts.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision Statement
Implement an interactive, side-effect-free Flow Builder Preview feature integrated natively into the Flow Builder editor, designed to visualize step-by-step execution paths and validate flow configurations using user-provided test context, without triggering real system actions.

---

## Key Implementation Details
The feature will include the following core capabilities to address user pain points:

1. **Test Context Management**
   - Offer pre-defined scenario templates (e.g., ""New Paid Order"", ""Customer Registration"", ""Abandoned Cart"") aligned with common flow triggers, plus support for manual input/upload of custom test context data (structured form or JSON) matching the trigger event’s data schema.
   - Allow users to save and name custom test scenarios for repeatable use across flow edits or different flows.

2. **Path Visualization & Execution Logic**
   - For the selected test context, perform a static traversal of the flow graph to highlight the exact execution path:
     - Color-code active nodes (trigger, action, condition, decision) in the path and grey out skipped branches.
     - Add tooltips or inline labels explaining why branches were skipped (e.g., ""Condition 'Order Total > €100' evaluated to False for test order total €80"").

3. **Contextual Step Explanations**
   - For each node in the active path, display human-readable insights:
     - For conditions/decisions: Show the exact logic evaluation (e.g., ""Decision 'Customer is VIP' → True because test customer has a VIP tier"").
     - For actions: Explain what the action would do in a real execution (e.g., ""Action 'Send Order Confirmation' → Would deliver to test customer jane@example.com"") without executing the action.

4. **Proactive Configuration Validation**
   - Detect and highlight invalid or incomplete node configurations during preview (e.g., missing email template in an email action, invalid condition syntax) with actionable error messages (e.g., ""Fix: Select a template from the dropdown to enable this email action"").

5. **Seamless UI Integration**
   - Add a ""Preview"" toggle button in the Flow Builder editor to switch between edit and preview modes.
   - Maintain real-time synchronization: Auto-refresh the preview when the flow structure (nodes, conditions, actions) is modified, ensuring users see up-to-date execution paths.

---

## Rationale
This decision directly resolves the core merchant pain point of debugging opaque flow behavior (e.g., unsent emails) by:
- Eliminating the need to test flows in production/staging with real data (avoiding accidental side effects like duplicate emails or database changes).
- Providing clear, actionable visibility into why a flow follows a specific path or fails to execute intended steps.
- Proactively catching configuration errors before deployment, reducing post-launch debugging time.
- Catering to both novice users (via pre-built scenario templates) and power users (via custom context input), ensuring accessibility and flexibility.

The static simulation approach aligns with the requirement to not execute real flows while delivering meaningful insights into flow execution logic.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃÂÃÂ¡ÃÂÃÂ°realÃÂÃÂ¡ÃÂÃÂ± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","# Architectural Decision Record: Demand-Driven Media URL Generation 
## Title
Decouple Media URL Generation from `media.loaded` Event and Implement Lazy, Context-Aware URL Resolution

## Context Recap
The Shopware media system uses `PathnameStrategyInterface` to define storage path components (hash, cache buster, filename) for media files and thumbnails. URLs for frontend/Store API use are generated via `UrlGeneratorInterface`, currently triggered by a subscriber to the `media.loaded` event—computing URLs every time a `MediaEntity` is hydrated. This approach leads to unnecessary performance overhead in non-frontend contexts (e.g., backend admin tasks) and couples URL generation to the entity loading lifecycle, reducing flexibility for alternative workflows.

## Decision
1. **Remove Event-Driven Precomputation**: Eliminate the `media.loaded` event subscriber responsible for pre-generating media URLs during entity hydration.
2. **Introduce a Dedicated `MediaUrlResolver` Service**:
   - Create a stateless service that acts as a facade, combining the active `PathnameStrategyInterface` and `UrlGeneratorInterface` implementations.
   - The resolver will expose methods mirroring `UrlGeneratorInterface` (e.g., `getAbsoluteMediaUrl(MediaEntity $media)`) but delegate to the configured strategy and generator to compute components and assemble the final URL.
3. **Add Lazy-Loading Methods to `MediaEntity`**:
   - Extend `MediaEntity` with lazy-loading methods (e.g., `getAbsoluteUrl()`, `getRelativeThumbnailUrl(MediaThumbnailEntity $thumbnail)`) that delegate to the injected `MediaUrlResolver` when invoked.
   - Avoid storing computed URLs directly on the entity to ensure dynamic updates when configuration changes (e.g., CDN prefix, path strategy).
4. **Preserve Backward Compatibility for Serialization**:
   - Update the `MediaEntity` normalizer (used in Store API/frontend responses) to explicitly call the lazy URL methods during serialization, ensuring URLs are included in API payloads as before.
5. **Reinforce Separation of Concerns**:
   - Maintain clear boundaries: `PathnameStrategyInterface` remains responsible for storage path components (disk layout), `UrlGeneratorInterface` for URL schema rules (absolute/relative, CDN prefixes), and `MediaUrlResolver` for orchestrating their integration.
   - Ensure `UrlGeneratorInterface` implementations accept path components from any valid `PathnameStrategyInterface` without hardcoding dependencies.

## Consequences
### Positive
- **Performance Optimization**: URLs are only generated when explicitly needed (e.g., frontend templates, Store API responses), eliminating unnecessary computation during backend entity loads.
- **Dynamic Configuration Updates**: URLs reflect the latest `PathnameStrategy` or URL configuration (e.g., CDN switch) whenever requested, avoiding stale URL issues.
- **Increased Flexibility**: URL generation logic can be extended or replaced (e.g., admin-specific URLs) without altering the entity hydration lifecycle.
- **Cleaner Entity Responsibility**: `MediaEntity` remains focused on core media attributes (file type, size) rather than URL metadata.

### Negative
- **Migration Effort**: Existing code accessing precomputed URL properties on `MediaEntity` must be updated to use the new lazy-loading methods.
- **Potential Duplicate Computations**: Repeated requests for the same URL may trigger redundant computations. Mitigate this with in-memory caching in `MediaUrlResolver` for entities/thumbnails processed within the same request context.

### Neutral
- **Preserved Interface Contracts**: `PathnameStrategyInterface` and `UrlGeneratorInterface` remain unchanged, ensuring compatibility with existing custom implementations.
- **Normalizer Overhead**: Serialization of media entities will incur URL computation cost, but this aligns with the original intent of including URLs in frontend/API responses.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision

### Core Decision
Implement a dedicated, versioned, and idempotent one-time data migration framework that is fully separated from Shopware’s indexer system and integrated with the existing update pipeline to ensure data transformation logic runs exactly once per target environment.

### Specific Implementation Actions
1.  **Define a Standardized Data Migration Contract**
    - Create an abstract base class `AbstractDataMigration` (or interface `DataMigrationInterface`) enforcing a consistent structure:
      - A unique timestamp-based version identifier (e.g., `20240520120000`, aligned with Shopware’s schema migration convention) to enforce execution order.
      - An `execute` method containing the one-time data transformation logic.
      - Optional `preCheck` (to validate prerequisites like schema versions, data availability) and `postCheck` (to verify migration success) methods.
    - Mandate idempotency for all data migrations (safe to re-run without unintended side effects) as a fallback guard against accidental re-execution.

2.  **Track Execution Status**
    - Create a dedicated `data_migration` database table to track execution status, with columns:
      - `id` (primary key), `version` (unique timestamp identifier), `class_name` (FQCN of the migration), `executed_at` (datetime), `status` (enum: `pending`, `success`, `failed`), `error_message` (nullable text for failure details).
    - Implement a `DataMigrationTracker` service to manage this table, checking which migrations are pending and marking them as executed upon completion.

3.  **Integrate with Shopware’s Update Pipeline**
    - Insert the data migration execution step into the automated update workflow (triggered via `bin/console update:run` or backend update wizard) immediately after schema migrations are applied, and before any full indexer reindex operations.
    - Introduce a CLI command `bin/console system:data-migration:execute` to run pending migrations manually, with flags to list pending migrations, re-run failed migrations, or force execution for debugging purposes.

4.  **Refactor Existing Indexer-Based Migrations**
    - Audit all indexer classes to identify embedded one-time data transformation logic (e.g., media path updates) and refactor this logic into new data migration classes adhering to the new framework.
    - Remove all one-time data logic from indexer implementations to restore their core purpose: maintaining up-to-date indexes for recurring read operations (search, navigation, product listings).

5.  **Enforce Development Guidelines**
    - Establish a formal rule that one-time data transformations (legacy format conversions, value migrations, post-schema-change data fixes) must use the dedicated data migration framework, not indexers.
    - Document the new framework in Shopware’s developer documentation, including best practices for dependency management, error handling, and testing data migrations.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision

1. **Mandate Feature Identifier in Experimental Annotations**  
   Modify the existing `@experimental` annotation across all supported codebases to require a unique, immutable `featureId` parameter (e.g., `@experimental(featureId = ""automated-invoice-generation-v1"", stableVersion = ""3.0"")`). This `featureId` will serve as the canonical link between all code components (classes, methods, configuration entries, UI elements, etc.) and their parent experimental feature.  
   - Phased rollout: For backward compatibility, flag missing `featureId` as a static analysis warning for one release cycle, then elevate to an error to enforce full compliance.

2. **Build a Centralized Feature Code Inventory Tool**  
   Develop or integrate custom static analysis tooling (e.g., extended lint rules, SonarQube checks, or a CLI utility) to:  
   a. Scan the entire codebase to collect all `@experimental` annotated components, grouping them by `featureId`.  
   b. Maintain a living, machine-readable inventory (JSON report + internal dashboard) mapping each `featureId` to:  
      - Full list of associated code locations (file paths, component types).  
      - Current `stableVersion` value from annotations.  
      - Feature metadata (Jira ticket link, status: active/prolonged/killed, deprecation target version).  
   c. Automate compliance workflows:  
      - When an experiment is prolonged: The tool generates a list of all code components for the target `featureId` to ensure consistent `stableVersion` updates.  
      - When a feature is killed: The tool automatically identifies all linked code, flags unmarked components, and generates a batch script to add `@deprecated` annotations (with context: ""Deprecated; part of killed feature 'X', to be removed in version Y"").

3. **Revise Experimental Feature Workflow to Align with the New System**  
   Update the [""Experimental Features"" document](./2023-05-10-experimental-features.md) to:  
   a. Require `featureId` assignment during experimental feature planning (linked to product roadmap tickets).  
   b. Enforce that ""Killing Feature"" compliance is verified via the centralized inventory before major version releases: all components for killed features must be marked as deprecated, with the tool providing a pass/fail validation.  
   c. Document `featureId` naming conventions (e.g., `<feature-name>-<version>`) to ensure uniqueness and readability.

4. **Sync Feature Inventory with Existing Dev Tools**  
   Integrate the centralized inventory with our project management (Jira) and CI/CD systems:  
   - Link each `featureId` to its corresponding Jira ticket for end-to-end traceability from feature request to code.  
   - Add a CI step to run the inventory tool on every pull request: block merges if experimental code lacks a valid `featureId`, or if deprecated components from killed features are used in non-experimental code.

---

### Rationale
This decision directly addresses the core problem of ambiguous feature attribution by:  
- Creating an explicit, machine-traceable link between code and features via `featureId`.  
- Eliminating manual search for prolonged experiment code by using automated tooling to locate all relevant components.  
- Ensuring compliance with the ""Killing Feature"" rule by providing a complete, auditable list of components to deprecate for each killed feature.  
- Reducing human error in updates and deprecation workflows through automation and CI-enforced validation.

### Consequences
- **Positive**: Improved code maintainability, reduced time spent tracking experimental code components, enforceable compliance with experimental feature rules, enhanced traceability between features and code.  
- **Neutral**: Requires upfront effort to update existing experimental annotations and build/integrate tooling.  
- **Negative**: No breaking changes if phased rollout is followed, but teams must adapt to new annotation requirements during feature development.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","# Architectural Decision Record: Technical Handles for Payment & Shipping Method Identification

## Decision Statement
We will implement a mandatory, immutable, unique technical handle system for all payment and shipping method entities, alongside existing UUIDs, to enable direct, reliable identification by extension developers without redundant Admin API calls. This system includes:

1.  **Schema & Validation**: Add a `technical_handle` string field to `payment_method` and `shipping_method` database tables, with non-nullable constraints, unique indexes per entity type, and validation for a standardized pattern (`^[a-z0-9-]+$`).
2.  **Handle Assignment**:
    - System default methods: Auto-generate handles using a predictable, human-readable pattern (e.g., `payment-paypal-standard`, `shipping-dhl-parcel`).
    - User-created custom methods: Require a unique handle during creation (with auto-generated fallbacks from the method name if not provided, resolving conflicts automatically).
3.  **API & Webhook Integration**:
    - Expose `technical_handle` in all relevant API responses (Admin API, Store API, app webhooks).
    - Support `technical_handle` as a filter/query parameter in API endpoints for direct entity retrieval.
    - Include handles as primary identification fields in app webhook payloads, eliminating the need for extensions to call the Admin API to resolve UUIDs.
4.  **Extension Developer Support**:
    - Allow extensions to register payment/shipping methods with predefined handles during installation, enabling cross-extension reference via known identifiers.
    - Update extension configuration interfaces to support handle-based method selection alongside UUIDs.

## Rationale
- **Eliminates Redundant API Calls**: Extensions can use handles directly to reference payment/shipping methods without fetching UUID details via the Admin API, reducing latency and server load.
- **Improves Developer Experience**: Human-readable handles are easier to reference in code, configuration, and documentation than opaque UUIDs, lowering the risk of errors.
- **Environment Consistency**: Handles remain stable across staging, production, and local environments, avoiding UUID mismatch issues during data migration or extension deployment.
- **Conflict Prevention**: Unique constraints ensure no two entities of the same type share a handle, preventing accidental misreference by extensions.

## Trade-offs
1.  **Migration Effort**: Existing instances require a one-time migration to backfill handles for existing entities, with logic to resolve potential duplicates in custom method names.
2.  **UI Adjustment**: The Admin UI must be updated to include handle fields for custom method creation, with clear validation guidance for users.
3.  **Immutability Restriction**: Handles are immutable post-creation, which may feel restrictive for users rebranding methods, but this is acceptable as handles are technical identifiers (distinct from editable display names).

## Implementation Boundaries
- Handles are only mandatory for payment and shipping method entities initially; future expansion to other entities (e.g., tax rules) will be evaluated based on developer feedback.
- Auto-generated handles for custom methods will append a suffix (e.g., `shipping-fast-delivery-2`) if a duplicate name-based handle exists to ensure uniqueness.
- Extensions will still support UUIDs for backward compatibility, with handles as the preferred identification method for new development.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","# Architectural Decision Record: Storefront Styling Standardization with Bootstrap Utilities

## Decision

1. **Prioritize Bootstrap Utility Classes for Atomic Style Requirements**
   - For simple, atomic styles (e.g., spacing, borders, text alignment, basic colors, or background styling), replace all redundant custom SCSS rules with corresponding Bootstrap utility classes applied directly in HTML templates. Remove any custom SCSS that replicates functionality provided by Bootstrap’s built-in utilities (e.g., margin/padding rules like `margin-bottom: $spacer`).

2. **Mandate Semantic Custom Classes for Extensibility**
   - Always retain descriptive, component-specific custom class names on elements (even when no default custom SCSS is shipped) to preserve semantic clarity and support theming/extensibility. These classes (e.g., `shipping-modal-actions`, `shipping-abort`) should be used to group or target elements without relying on generic utility classes, enabling developers and themes to add custom styling without modifying core templates.

3. **Permit Custom SCSS for Complex Layouts Only**
   - Custom SCSS is allowed when:
     a. The desired layout or styling cannot be implemented efficiently or cleanly using Bootstrap’s default components and utilities, or
     b. Overuse of utility classes would result in cluttered, unreadable templates that harm long-term maintainability.
   - When writing custom SCSS for complex scenarios, leverage Bootstrap variables, mixins, and design system patterns to ensure consistency with the existing storefront style.

4. **Enforcement Guidelines**
   - All new UI implementations must follow the utility-first approach for atomic styles. Code reviews will flag redundant custom SCSS that duplicates Bootstrap utilities and verify the presence of semantic custom classes where needed for extensibility.
   - Any deviation from the utility-first approach (i.e., using custom SCSS for non-complex styles) must be documented in inline comments or associated documentation, justifying the rationale for the exception.

This decision balances reducing technical debt from redundant SCSS with maintaining flexibility for theming, while ensuring readability and maintainability across both simple and complex UI components.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision

We will implement a cohesive, multi-layered solution to resolve the current E2E test suite’s flaws and fully satisfy all stated requirements, consisting of the following interconnected architectural and process decisions:

---

### 1. Adopt Behavior-Driven Development (BDD) as the Core Test Paradigm, with Cucumber.js for Accessible Syntax
- **Action**: Standardize all new E2E tests on Gherkin (Given/When/Then) syntax via Cucumber.js. Collaborate with product owners, QA, and developers to write test features that directly map to product requirements (user stories, acceptance criteria). Retire legacy unstructured tests incrementally as BDD tests are validated.
- **Rationale**: Gherkin’s plain-language syntax is accessible to non-technical stakeholders, ensuring tests are derived from real product needs and fostering cross-team alignment. This addresses the requirement for readable, requirement-aligned tests while establishing a consistent strategy across the suite.

### 2. Migrate to Playwright as the Automation Execution Engine (Paired with Cucumber.js)
- **Action**: Replace the current ad-hoc Cypress implementation with Playwright, integrated with Cucumber.js for BDD workflow support. Leverage Playwright’s built-in auto-wait, flakiness mitigation (transient error retries), and cloud-native testing capabilities.
- **Rationale**: Playwright resolves the current suite’s inability to test cloud environments via native integration with Playwright Test Cloud, AWS, Azure, and GCP. Its robust reliability features (auto-wait, isolated browser contexts) directly reduce flakiness, while its speed and parallel execution support meet the ""fast tests"" requirement.

### 3. Enforce Deterministic Tests Across All Environments via API-First State Management
- **Action**: Implement API-driven test data setup/teardown for all tests (replacing UI-based workflow dependencies). Use environment-specific, runtime-injected configuration files (e.g., `.env.local`, `.env.cloud`) to parameterize endpoints, credentials, and test data sources. Ensure every test runs in a fully isolated state (no shared test data or cross-scenario dependencies).
- **Rationale**: Controlling test state via backend APIs eliminates coupling to pre-existing environment state, enabling consistent, deterministic execution across local, staging, and cloud environments. Isolation prevents flakiness caused by shared resources.

### 4. Establish a Test Strategy Aligned with the Test Pyramid
- **Action**: Prioritize test development and execution to optimize speed and reliability:
  - **Behavioral E2E Tests**: Cover only critical user journeys (e.g., checkout, account onboarding) to minimize UI-dependent flakiness.
  - **API Tests**: Validate backend logic and data integrity (faster than E2E, reduces reliance on UI interactions).
  - **Component Tests**: Use Playwright’s component testing capability to validate UI components in isolation where beneficial.
- **Rationale**: This strategy ensures the suite is efficient (faster execution via lighter test types) and reliable (reduces UI-related flakiness). It also provides clear guidelines for test creation, resolving the current lack of a consistent strategy.

### 5. Automate Cloud Environment Testing via CI/CD Integration
- **Action**: Configure CI/CD pipelines (e.g., GitHub Actions, GitLab CI) to trigger test runs automatically on cloud environment deployments. Inject cloud-specific configuration variables at runtime (no hardcoded environment details) and use Playwright Test Cloud for scalable, parallel test execution.
- **Rationale**: This enables seamless testing of cloud environments and ensures tests run consistently across the development lifecycle. Parallel execution reduces overall suite runtime, aligning with the ""fast, reliable tests"" requirement.

### 6. Enforce Maintenance & Quality Standards
- **Action**: Build a shared library of reusable Cucumber step definitions to avoid duplicate code. Establish a mandatory review process for all Gherkin features and step definitions to maintain consistency. Use Allure Reports to generate human-readable, requirement-linked test results accessible to non-technical stakeholders.
- **Rationale**: Reusable steps reduce maintenance overhead and ensure test consistency. Transparent reporting aligns all teams on test coverage and product quality, while code reviews prevent regression to flaky or environment-coupled test patterns.

---

This decision set directly addresses every flaw in the current suite (environment coupling, flakiness, lack of strategy, cloud testing limitation) and meets all specified requirements for deterministic, fast, behavior-aligned, and accessible tests.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision
We will implement three interconnected changes to resolve transactional inconsistencies and silent failure propagation in flow execution:

### 1. Centralized Transaction Coordination for Flow Actions
- Eliminate direct transaction management (start/commit/rollback) in individual flow actions. Instead, introduce a dedicated `FlowTransactionManager` service to handle all transactional operations for flows.
- This manager will:
  a. Detect and participate in any existing ambient (outer) transaction if one is active during flow execution, aligning flow action work with the outer transaction’s boundaries.
  b. Initiate a new transaction only if no ambient transaction exists, committing it on successful flow execution or rolling it back on failure.
- All flow actions will be refactored to rely on this manager for transactional guarantees instead of direct database transaction API calls.

### 2. Error Propagation and Flow Abort on Transaction Failure
- Modify the flow dispatcher to cease ignoring transaction-related errors (e.g., commit failures, connection marked as rollback-only). Instead:
  a. Log a detailed error message including the flow ID, failing action name, root cause exception, and context like affected entity IDs for observability.
  b. Immediately abort execution of subsequent flow actions to prevent further inconsistent state changes.
  c. Propagate the transaction failure exception back to the calling code, ensuring any ambient outer transaction is notified of the failure so it can explicitly initiate a rollback (instead of proceeding to commit blindly).

### 3. Guardrails Against Nested Transaction Conflicts
- Enforce a rule that flow actions cannot initiate independent transactions when an ambient transaction is active. The `FlowTransactionManager` will throw a clear `NestedTransactionNotAllowedException` if a flow action attempts to bypass the coordinator and use direct transaction APIs.
- Retain Shopware’s default savepoint-disabled database configuration but ensure all flow-related transactional work adheres to ambient transaction boundaries to avoid unreported rollback-only state propagation.

---

## Rationale
- **Eliminates nested transaction conflicts**: By aligning flow transactional work with existing ambient transactions or creating isolated transactions only when safe, we avoid the root cause of connection rollback-only state being hidden from outer calling code.
- **Prevents silent failures**: Propagating errors instead of ignoring them ensures the outer transaction (if present) is aware of flow execution failures, enabling intentional rollbacks rather than unexpected commit failures.
- **Maintains consistency**: Centralized coordination and guardrails prevent regressions where future flow actions might reintroduce manual transaction handling, ensuring uniform transaction behavior across all flow operations.

---

## Implications
- **Refactoring required**: Existing flow actions using manual transaction logic must be updated to use the `FlowTransactionManager`.
- **Calling code adjustments**: Code invoking flows will need to handle propagated exceptions to manage outer transaction rollbacks appropriately.
- **Improved observability**: Detailed error logs will simplify debugging transaction failures in flow executions.
- **No breaking changes for non-transactional flows**: Flows without transactional requirements will continue to execute as before, with no impact on their behavior.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","### Decision
1. **Immediate Retention of Compatibility Mode**: Continue to enable Vue 3’s Compatibility Mode by default in the administration application for the current major release cycle. This aligns with our goal of minimizing breaking changes for plugin developers and splitting the migration workload (core administration + plugin ecosystem) into two sequential major releases instead of a single high-impact update.
2. **Timeline for Disablement**: Target full disablement of Compatibility Mode for the subsequent major release. This timeline will be communicated publicly to plugin developers immediately, with regular reminders and updates to ensure sufficient time for their migration efforts.
3. **Parallel Migration Efforts**:
   - Prioritize refactoring the core administration codebase to eliminate Vue 2-specific patterns (e.g., deprecated lifecycle hooks, `Options API` exclusives) during the current major release, to ensure it is fully compatible with pure Vue 3 before Compatibility Mode is disabled.
   - Launch a dedicated plugin migration support program, including:
     - Automated scanning tools to detect Vue 2-dependent code in plugins and provide actionable migration guidance;
     - Step-by-step migration guides tailored to common plugin use cases;
     - A dedicated support channel (e.g., GitHub discussions, Slack thread) for resolving plugin migration issues.
4. **Validation and Enforcement**: Integrate CI/CD checks and linting rules to flag new Vue 2-specific code in both the core admin and plugin submissions, preventing regressions and accelerating migration progress. Runtime warnings will be retained for remaining Vue 2 patterns to guide incremental fixes.
5. **Post-Disablement Support**: After Compatibility Mode is turned off, maintain a short-term support window for critical plugin migration issues to smooth the transition to pure Vue 3.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Architectural Decision

We will implement a tiered, context-aligned strategy to resolve the Vuex 4.1.0 getter reactivity bug while avoiding module stability issues from downgrades:

### 1. Immediate Runtime Workaround
Implement a targeted, isolated runtime patch for Vuex 4.1.0 to restore getter reactivity. The patch will:
- Target the specific code path identified in the Vuex 4.1.0 bug report responsible for breaking reactivity (e.g., overriding the getter registration logic that fails to track dependencies correctly).
- Be encapsulated in a dedicated utility module to minimize codebase intrusion, with clear comments linking to the official Vuex issue and planned removal trigger.
- Be validated across all affected components to ensure it resolves the reactivity issue without introducing new bugs.

### 2. Medium-Term Official Fix Alignment
- Assign a team member to monitor the official Vuex repository for a patch release (≥4.1.1) that:
  1. Resolves the getter reactivity bug;
  2. Confirms no regression of module failure issues present in Vuex 4.0.2.
- Upon release of a verified stable version, immediately upgrade to this version and remove the runtime workaround, followed by full regression testing of state management flows.

### 3. Contingency Plan (if official patch is delayed >2 weeks)
- Fork the Vuex 4.1.0 repository, apply the upstream fix for the reactivity bug (leveraging existing PRs/issue resolutions from the Vuex community), and publish the forked package to our internal package registry with a clear version tag (e.g., `vuex@4.1.0-patched-1`).
- Update project dependencies to use the internal forked package, with documentation marking it as a temporary measure.
- Initiate a feasibility assessment for phased migration to Pinia (Vue’s official replacement for Vuex) as a long-term mitigation against legacy state management dependency risks, prioritizing low-complexity feature areas first.

Additionally, we will contribute to the Vuex community by validating the proposed fix in our codebase and providing feedback to maintainers to expedite the release of an official stable patch.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Architectural Decision Record: Overhaul Extension Mechanism to Event-Driven + Hook System

### Decision
Replace over-reliance on fine-grained Decorator, Adapter, and Factory patterns with a **PSR-14 Compliant Event-Driven Architecture (EDA) paired with a Standardized Hook System**, complemented by selective Dependency Injection (DI) Container service overriding for full component replacement. Retain Decorator/Factory patterns only for coarse-grained component creation where event-driven extension is not feasible (e.g., complex multi-variant object initialization).

### Rationale (Directly Addressing Context Challenges)
This decision resolves all three core pain points in the current architecture:

1. **Backward/Forward Compatibility**
   - PSR-14 events use typed, semantically versioned Event DTOs with optional properties. Adding new context to events does not break existing subscribers (who only access the properties they need), while deprecating legacy properties is done via explicit warnings (not breaking changes).
   - Service overriding relies on stable, documented service IDs. Changes to implementations are isolated to the DI layer, with deprecation notices for upcoming ID changes to enable smooth forward migration for third-party developers.

2. **Process Extension Limitations**
   - Emit events at granular points within core subprocesses (e.g., `order.calculate_taxes.before`, `user.authenticate.success`) without extracting subprocesses into separate interfaces/classes. Third-party developers can subscribe to these events to inject logic before, after, or around subprocess steps, even if the core process remains monolithic.
   - Filter hooks (integrated with event DTOs) enable dynamic alteration of process outputs without wrapping entire components in decorators.

3. **Code Proliferation**
   - Replaces hundreds of fine-grained interfaces, abstract classes, and factory implementations with a smaller set of typed Event DTOs and event dispatcher configurations. For example, a single `OrderProcessingEvent` eliminates the need for `OrderTaxCalculatorInterface`, `OrderShippingCalculatorInterface`, and their corresponding factories/decorators for extension purposes.
   - Event subscribers are simpler to implement than interface-based extensions (requiring only an `__invoke` method or event handler) and centralize extension points in a documented registry, reducing cognitive load for core and third-party developers.

### PHP-Specific Implementation Guidelines
1. **Adopt PSR-14**: Use interoperable event dispatchers like Symfony EventDispatcher or Laminas EventManager to ensure compatibility with third-party tools.
2. **Typed Event DTOs**: Define immutable Event DTOs with optional properties (via null coalescing or default values) and immutable update methods (e.g., `withNewTaxRate()`) to support filter hooks without mutating state.
3. **Centralized Event Registry**: Maintain a public, versioned registry of all core events, including purpose, DTO structure, and deprecation status, to guide third-party extensions.
4. **DI Service Overriding**: Restrict service overriding to components requiring full replacement (e.g., payment gateways) and document stable service IDs with clear deprecation timelines.
5. **Gradual Migration**: Phase out fine-grained Decorator/Adapter patterns incrementally by wrapping existing interface implementations in temporary event decorators during transition, ensuring backward compatibility while adopting the new extension mechanism.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision

### 1. Core Architectural Choice
Implement a **dual Jest runner configuration** that introduces a dedicated non-compat-mode runner alongside the existing compat-mode runner. This enables phased removal of compat mode for component tests while maintaining test stability during migration.

### 2. Non-Compat Runner Configuration
- **Create a distinct Jest config**: Add `jest.config.no-compat.js` that inherits shared settings (e.g., module resolvers, transformers) from the base `jest.config.js` and explicitly sets `testEnvironmentOptions: { compat: false }` (adjust property names to match your Jest version’s API for disabling compat mode).
- **Test Targeting Strategy**: Define a clear pattern to separate non-compat tests from compat-mode tests:
  - Option A: Place non-compat component tests in a subdirectory (e.g., `src/components/**/__tests__/no-compat`), configured as the target for the new runner.
  - Option B: Use a file suffix (e.g., `*.no-compat.test.js`) for non-compat tests; the new runner will filter for these files.
  The existing runner will continue targeting compat-mode tests using the original pattern/directory structure.

### 3. Phased Migration & Validation
Adopt a risk-mitigated migration process:
- **Initial Validation**: Migrate 1-2 stable, low-risk component tests to the non-compat runner, confirm they pass, and validate no regressions in component behavior.
- **Gradual Component Migration**: For each component, refactor its tests to remove compat-mode dependencies (e.g., deprecated API usage, polyfills) and re-run with the non-compat runner. A component is marked ""compat-free"" only after all its tests pass without compat mode.
- **Final Consolidation**: Once all component tests are migrated and validated, deprecate the compat-mode runner and consolidate all tests under the non-compat configuration.

### 4. Tooling & CI/CD Integration
- **Local Execution**: Add npm/yarn scripts to `package.json`:
  - `test:compat`: Runs existing compat-mode tests.
  - `test:no-compat`: Runs non-compat tests.
  - `test:all`: Executes both runners in parallel/sequentially for full validation.
- **CI/CD Pipeline Update**: Modify pipelines to run both runners on all PRs and main branch merges. Block PRs if non-compat tests fail to enforce migration quality; flag compat-mode test failures as usual.

### 5. Documentation & Governance
- Update project docs to explain:
  - The dual runner setup and its purpose.
  - Step-by-step instructions for migrating existing tests to non-compat mode.
  - Troubleshooting for common non-compat issues (e.g., missing polyfills, deprecated React APIs).
- Deprecate the compat-mode runner for new tests: Add lint rules or template guidance to encourage all new component tests to use the non-compat runner by default.

This decision addresses the context’s requirement by providing a structured, low-risk path to remove compat mode per component, ensuring tests are validated without compat mode while preserving existing test coverage during migration.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision

1. **Adopt a Tiered Health Check Framework**
Implement three hierarchical health check types to validate component and system health at increasing levels of granularity:
   - **Liveness Checks**: Lightweight, per-component heartbeat checks (e.g., process runtime validation, basic thread pool availability) to confirm the component is running and responsive.
   - **Readiness Checks**: Component-level validation that verifies the component can accept and process traffic (e.g., successful connection to critical dependencies like databases/upstream APIs, valid configuration loading, and empty error queues).
   - **Functional Health Checks**: End-to-end, workflow-centric tests that simulate critical user journeys (e.g., ""process a payment"" or ""retrieve user data"") to validate that interconnected components work together as intended to deliver core services. These checks will target high-priority functional paths identified as critical to business operations.

2. **Implement Gradated Health Statuses**
Replace binary ""healthy/unhealthy"" statusing with three gradations to accurately reflect the system’s operational state despite partial component impairment:
   - **Fully Healthy**: All components pass liveness, readiness, and functional checks; no critical workflows are impaired.
   - **Degraded**: One or more non-critical components fail readiness or liveness checks, but all critical functional workflows remain operational. Non-critical features may be unavailable, but core services are unaffected.
   - **Unhealthy**: Critical components fail checks, or one or more core functional workflows fail. This status indicates core service delivery is at risk or disrupted.

3. **Integrate Health Checks into Deployment and Runtime Workflows**
   - **CI/CD Pipeline**: Mandate functional health check execution in a production-mirrored staging environment as a pre-deployment gate. Block deployment if critical functional checks fail.
   - **Post-Rollout Verification**: Automatically run functional health checks immediately after deployment (e.g., during canary phases or post-blue/green shifts) to validate live system functionality. Trigger an automatic rollback if critical checks fail within a 5-minute window.
   - **Runtime Monitoring**: Continuously execute checks at configurable intervals (liveness: 10s, readiness: 30s, critical functional checks: 5min). Route health data to a centralized monitoring platform (e.g., Prometheus + Grafana, Datadog) for real-time visualization and historical trend tracking.

4. **Configure Targeted Alerting Based on Health Status**
   - **Fully Healthy → Degraded**: Send a low-priority (P3) alert to engineering teams for proactive investigation, with no traffic redirection.
   - **Degraded → Unhealthy**: Trigger a high-priority (P1) alert, and initiate automated failover to standby instances or rollback to the previous stable deployment if configured for the service.
   - **Failed Critical Functional Checks**: Send a medium-to-high priority (P2/P1) alert based on whether the failed workflow impacts core service delivery.

This decision directly addresses the context’s pain points: functional health checks catch interconnected workflow errors missed post-rollout, gradated statuses reflect the system’s ability to operate with partial component issues, and integrated deployment/runtime checks prevent undetected issues from reaching production or persisting in live environments.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","### Architectural Decision
1. **Formalize and retain the namespace restriction for major feature flag activation**: The `TestPreparationStartedSubscriber` will continue to restrict major feature flag enabling exclusively to tests within the `Shopware\Tests\Unit` namespace during unit test suite execution. This ensures flags are only applied in isolated, self-contained unit tests where their impact is predictable and limited to individual components.
2. **Replace hardcoded checks with a configurable allowlist mechanism**: Modify the subscriber to use an explicit, maintainable allowlist (stored as a test service parameter or config array) instead of hardcoding the `Shopware\Tests\Unit` namespace. This allows controlled expansion to other namespaces in the future if valid, use-case-driven requests are approved.
3. **Enforce non-compliance safeguards**: For tests outside the allowlist attempting to enable major feature flags:
   a. Block the flag activation to prevent unintended side effects in non-isolated test environments (e.g., integration/functional tests where major features may lack full dependency setup).
   b. Log a structured warning message including the offending test namespace, feature flag name, and a link to the allowlist request process to guide developers.
4. **Define a formal allowlist request workflow**: Add documentation to Shopware’s test guidelines outlining how to request a namespace be added to the allowlist, requiring justification such as:
   - Proof of test isolation (no cross-test dependencies or shared state).
   - A clear business or technical need for major feature flag access.
   - Validation that enabling the flag will not introduce flakiness or break existing test suites.

### Rationale
This decision balances safety and flexibility:
- Retaining the Unit namespace restriction prevents accidental flag activation in non-isolated tests, reducing flakiness and unexpected test failures.
- The allowlist mechanism supports controlled expansion without sacrificing stability, ensuring only trusted, isolated test namespaces can access major feature flags.
- Non-compliance logging and documentation reduce developer friction and provide clear paths for valid use cases to be addressed.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","# Architectural Decision Record: Automated Migration from Vue 2 Options API to Vue 3 Composition API

## Decision Statement

Adopt a structured, two-phase automated migration approach using official Vue tooling to convert Vue 2 Options API components to Vue 3 Composition API, complemented by targeted manual validation and refactoring. Specifically:
1.  Use the official `vue-codemod` repository’s options-api-to-composition-api codemod scripts as the primary automated conversion tool.
2.  Integrate `@vue/eslint-plugin` with Vue 3 Composition API rules to enforce idiomatic code standards and catch post-conversion issues.
3.  Conduct manual review and refactoring for complex components (e.g., those with mixins, advanced watchers, or state management integrations) that cannot be fully automated.

## Justification
This approach directly addresses the context’s constraints while maximizing the benefits of the Composition API:
- **Official Tooling Reliability**: The `vue-codemod` scripts are maintained by the core Vue team, ensuring alignment with Vue 3’s Composition API specifications. They automate 80-90% of repetitive conversion tasks (e.g., mapping data properties to `ref`/`reactive`, converting lifecycle hooks to Composition API equivalents) with minimal breaking changes.
- **Efficiency & Risk Mitigation**: Eliminates the time cost and human error of full manual conversion, allowing the team to focus on high-value refactoring rather than repetitive code changes.
- **Safety & Quality**: Post-conversion ESLint checks (using `plugin:vue/vue3-recommended`) flag edge cases the codemod may miss, such as missing reactive wrappers or incorrect watcher dependencies.
- **Scalability**: Works for large codebases and supports incremental migration (converting individual components or directories) to minimize disruption to ongoing development.
- **Alignment with Modernization Goals**: Lays the foundation for improved code organization, better TypeScript support, and enhanced reusability (e.g., replacing mixins with composables during manual refactoring phases).

## Alternatives Rejected
1. **Full Manual Conversion**: Rejected due to prohibitive time investment, elevated risk of human error, and inability to scale across a large codebase.
2. **Third-Party Non-Official Conversion Tools**: Rejected because they often produce non-idiomatic Composition API code, may not keep pace with Vue 3 updates, and introduce compatibility risks for future maintenance.
3. **`@vue/composition-api` for Vue 2**: Rejected because this tool only enables Composition API syntax in Vue 2, but does not convert existing Options API code to Vue 3-compatible Composition API—defeating the goal of modernizing the codebase for Vue 3.

## Key Implementation Guidelines
- **Pre-Migration Preparation**: First upgrade components to be compatible with Vue 2.7 (a transitional release supporting both Options and Composition API) to simplify the conversion path.
- **Automated Conversion Phase**: Run the `vue-codemod` script on targeted component directories incrementally, committing changes in small batches to simplify debugging.
- **Validation & Refactoring Phase**: 
  - Execute ESLint to fix conversion errors and enforce best practices.
  - Replace mixins with reusable composables for components relying on shared logic.
  - Add TypeScript type annotations where applicable to leverage the Composition API’s improved TS support.
- **Validation**: Write/run unit and integration tests for converted components to confirm functionality remains intact before merging changes.
- **Documentation**: Establish and document composable patterns and conventions to maintain consistency across the migrated codebase.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃÂÃÂ¡ÃÂÃÂ¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃÂÃÂ¡ÃÂÃÂ¯s templating and reactivity model.","## Decision

1. **Core Adoption**
Replace the TwigJs block inheritance system with the custom `sw-block` and `sw-block-parent` Vue.js components as the official, native mechanism for building, extending, and overriding reusable UI components. This transition leverages Vue.js’s reactive component-based architecture to deliver a more dynamic, maintainable, and modern frontend development experience while preserving and enhancing the block customization capabilities plugin developers rely on.

2. **Component Behavior Specifications**
Standardize the following behaviors for each component to replicate and improve upon TwigJs’s block functionality:
   - **`sw-block-parent`**:
     - Act as a container for declaring named, extensible content blocks via a registry of block names and their default scoped slot content.
     - Dynamically resolve and render block content by prioritizing `sw-block` overrides (from plugin extensions or nested components) over its own default content.
     - Integrate with Vue’s reactivity system to auto-re-render block content when default content or overrides are updated in real time.
   - **`sw-block`**:
     - Enable targeting of a specific block in an ancestor `sw-block-parent` using a required `name` prop (matching the parent’s block identifier).
     - Support three interaction modes to align with and extend TwigJs’s block features:
       - **Replace Mode (Default)**: Fully replace the parent block’s default content with the `sw-block`’s slot content (equivalent to Twig’s `{% block %}` override).
       - **Extend Mode**: Prepend or append content to the parent’s default block using an optional `extend` prop (`prepend`/`append`). For granular control, expose a `$parentContent` scoped slot prop to insert the parent’s default content anywhere within the `sw-block`’s slot (equivalent to Twig’s `{{ parent() }}`).
     - Pass scoped data from the `sw-block-parent` to the `sw-block`’s slot to enable dynamic, reactive data binding between parent and extended components.

3. **Migration & Compatibility**
   - Provide a gradual, documented migration path for existing TwigJs-based templates to Vue.js components using `sw-block`/`sw-block-parent`, including code examples, conversion guides, and linting tools to assist plugin developers.
   - Maintain a temporary compatibility layer during transition: wrap existing TwigJs block definitions in `sw-block-parent` containers and map Twig block overrides to `sw-block` components, ensuring existing plugin extensions function until developers can migrate to the native Vue.js approach.
   - Deprecate TwigJs’s block inheritance system for new component development, directing all future reusable UI work to use `sw-block`/`sw-block-parent`.

4. **Developer Experience**
   - Publish official documentation that includes side-by-side comparisons with TwigJs’s block system, common use cases (basic overrides, content extension, reactive data binding), and TypeScript type definitions for autocompletion and type checking.
   - Provide a sandbox environment for developers to test block overrides and extensions with `sw-block`/`sw-block-parent` without modifying production code.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃÂÃÂ¡ÃÂÃÂ¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃÂÃÂ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","### Decision
We will eliminate the inline asterisk `*` next to all product prices and implement a compliant, accessible, user-centric three-part pricing disclosure system to address the downsides of the current approach:

1. **Global Prominent Notice**:
   Deploy a non-intrusive, dismissible (cookie-persistent) sticky top banner across all Storefront pages. The banner will display localized text (e.g., ""All prices include VAT; shipping/delivery charges apply unless stated otherwise"") alongside a screen-reader labeled info circle icon. This icon will link directly to the full detailed disclaimer in the footer. This ensures users receive critical pricing context upfront before engaging with product listings.

2. **Product Page Inline Context**:
   On Product Detail Pages, place a brief, unobtrusive line of text directly beneath the main product price (e.g., ""Incl. VAT, plus shipping""—localized per region). This provides immediate, unambiguous pricing context where users are most focused, eliminating the need for ambiguous cross-referenced symbols.

3. **Footer Disclaimer Enhancement**:
   Retain the full detailed regulatory disclaimer in the footer but remove asterisk cross-referencing. Instead, link it to the global top banner and product inline notes to create a cohesive information trail. The footer text will be formatted to meet WCAG 2.1 AA accessibility standards (e.g., screen-reader friendly, high contrast).

Additionally, we will conduct regional compliance checks to ensure all disclosures align with local consumer protection laws (e.g., EU VAT regulations, FTC guidelines) and validate the system with accessibility testing to ensure it is usable by all users.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","### Architectural Decision Record: Accessible Document Generation for Shopware 6

---

#### Decision
We will replace the current DomPDF library with **WeasyPrint** as the primary PDF generation engine for Shopware 6’s document solution, and implement a layered accessibility framework to ensure full WCAG 2.1 AA compliance for all generated documents. 

To execute this, we will:
1. Migrate all out-of-the-box document templates (invoices, delivery notes, credit slips, etc.) to use semantic HTML5 (e.g., `<header>`, `<section>`, `<table>` with `<th>`/`<caption>`) and WCAG-aligned CSS that WeasyPrint can translate into valid tagged PDFs.
2. Integrate automated accessibility testing into the CI/CD pipeline using tools like the PDF Accessibility Checker (PAC) and Pa11y to validate compliance with core WCAG criteria (logical reading order, alt text for images, form field labeling, color contrast).
3. Maintain an optional DomPDF compatibility mode for merchants with custom templates, paired with clear deprecation notices and a step-by-step migration guide to WeasyPrint’s semantic template standards.
4. Publish a public accessibility best practices guide for developers creating custom document templates, including guidelines for semantic markup, avoiding inaccessible patterns, and testing workflows.

---

#### Rationale
- **Native Accessibility Support**: WeasyPrint natively generates tagged PDFs— a non-negotiable requirement for accessible documents, as tagged PDFs preserve semantic structure (headings, lists, tables) that screen readers and assistive technologies rely on. This eliminates the need for manual workarounds that would be required to patch DomPDF’s limitations.
- **Open-Source Alignment**: WeasyPrint is licensed under the BSD 3-Clause license, matching Shopware 6’s open-source ethos and avoiding proprietary lock-in or licensing costs for merchants.
- **Active Maintenance**: WeasyPrint has an active development community and regular updates, ensuring long-term support for emerging accessibility standards.
- **Minimal Disruption**: The compatibility mode allows merchants to transition gradually, avoiding immediate breaks to custom document setups while driving adoption of accessible practices.
- **Automated Validation**: CI/CD integration ensures ongoing compliance as templates are updated or new features are added, reducing the risk of accessibility regressions.

---

#### Rejected Alternatives
1. **Patching DomPDF**: DomPDF’s core architecture lacks native support for tagged PDFs, and retrofitting accessibility features would require significant, ongoing engineering effort that diverts resources from high-priority roadmap items.
2. **Commercial PDF Services**: Proprietary solutions like PDFlib or Adobe Acrobat API introduce licensing costs and third-party service dependencies, conflicting with Shopware’s open-source model and increasing operational overhead for self-hosted merchants.
3. **Headless Chrome**: While capable of generating tagged PDFs, it is more resource-intensive than WeasyPrint, requires additional system dependencies (Chrome/Chromium), and is less optimized for high-volume server-side document generation.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision

To address the complexity and maintainability issues caused by unrestricted rule class extensions, we implement the following structured changes to the rule system:

### 1. Standardize Rule Implementation Boundaries
- Mandate that all core and custom rule classes must implement a strict `RuleInterface` and extend a single abstract `AbstractRule` base class. This base class will encapsulate shared logic (e.g., condition evaluation, parameter validation, serialization) and enforce required methods (`evaluate()`, `getConstraints()`, `getConfig()`), eliminating arbitrary inheritance or unstructured extensions.
- Prohibit direct extension of non-core rule classes or unrestricted inheritance hierarchies; all rule logic must be encapsulated within classes adhering to this standard structure.

### 2. Centralize Rule Registration and Validation
- Introduce a core `RuleRegistry` service as the single source of truth for all rules. All rules (core and custom) must be registered via the registry using a standardized configuration (e.g., PHP annotations, XML definitions, or `RuleDefinition` DTOs) including metadata like unique identifiers, labels, and supported contexts.
- The registry will validate rule compliance (interface adherence, method signature correctness, parameter constraints) during registration, rejecting non-compliant rules and providing actionable error messages to developers.

### 3. Replace Unrestricted Extensions with Controlled Hooks
- Remove support for arbitrary modifications to rule classes via inheritance or monkey-patching. Instead, provide documented, maintainable extension points such as:
  - Event listeners for pre/post rule evaluation (e.g., `RuleEvaluatedEvent` to modify results or log data).
  - A Rule Condition Builder API to compose complex rules from existing standardized conditions, reducing the need for entirely custom rules.
  - Configurable condition adapters that map custom data sources to standard rule conditions without altering core logic.

### 4. Enforce Compliance via Automation and Migration
- Integrate Shopware-specific static analysis rules (e.g., PHPStan extensions) into CI/CD pipelines to detect non-compliant rule implementations early.
- Provide a migration toolkit for legacy rules: include automated refactoring scripts to convert unstructured rules to the new standard, and a deprecation timeline for unregistered/non-compliant rules (with runtime warnings for usage of deprecated rules).

### 5. Establish Governance for Rule Evolution
- Maintain formal guidelines for rule development (naming conventions, validation standards, performance best practices) and a review process for core rule changes to ensure alignment with the standardized structure.
- Create support channels for custom rule developers to resolve compliance issues and contribute feedback on extension point usability.

This decision balances necessary flexibility for customizations with strict structural guardrails, reducing technical debt, accelerating core system improvements, and simplifying long-term maintenance of the rule system.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","# Architectural Decision Record (ADR) - Decision

## Decision Statement
Remove the `src/helper/iterator.helper.js` utility class entirely from the Storefront codebase. Replace all instances of `Iterator.iterate()` usage with native JavaScript looping constructs semantically appropriate to the data type being iterated.

## Implementation Actions
To execute this decision, we will take the following steps:

1. **Systematic Replacement of Usage**:
   - For each occurrence of `Iterator.iterate()` in the codebase:
     - Identify the data type of the iterable (Array, NodeList, Map, FormData, etc.).
     - Replace the helper call with the corresponding native loop:
       - Arrays/NodeLists: Use `Array.prototype.forEach()`, `for...of`, or traditional `for` loops (aligned with team style guidelines for readability).
       - Maps: Use `Map.prototype.forEach()` or `for...of` over `map.entries()`, `map.keys()`, or `map.values()`.
       - FormData: Use `FormData.prototype.forEach()` or `for...of` over `formData.entries()`.
     - For cases where the helper included implicit null/undefined checks, add explicit defensive checks (e.g., `if (iterable) iterable.forEach(...)` or optional chaining `iterable?.forEach(...)`) to maintain equivalent behavior.

2. **Codebase Cleanup**:
   - Delete the `src/helper/iterator.helper.js` file once all usage is removed.
   - Remove all import statements referencing the iterator helper from files that previously used it.

3. **Standardization & Documentation**:
   - Update the Storefront coding guidelines to specify preferred native looping constructs for common data types (e.g., prefer `for...of` for NodeLists to avoid confusion with array-specific methods, use `forEach` for simple array iterations without index needs).
   - Communicate to the development team that all iterated data types in the Storefront are standard web/JavaScript types (e.g., NodeLists, Arrays), so no custom iterator is required.

## Expected Outcomes
This decision directly addresses all downsides outlined in the context:
- **Eliminates unnecessary abstraction**: Removes the redundant layer between developers and native looping, reducing code complexity.
- **Leverages web standards**: Developers can use familiar, documented native constructs instead of custom shopware syntax, lowering onboarding time and reducing confusion.
- **Ensures consistent usage**: Standardizes loop patterns across the codebase by replacing mixed helper/native usage with uniform native constructs.
- **Clarifies data type intent**: Explicit use of native loops for standard types dispels the misconception that iterated data are special custom objects requiring unique handling.
- **Reduces dependency bloat**: Removes extra import statements and the helper file itself, simplifying file structure and reducing maintenance overhead.

## Acceptance Criteria
- The `src/helper/iterator.helper.js` file no longer exists in the codebase.
- No files import or reference the iterator helper class.
- All previously iterated data types use appropriate native loops without relying on a third-party helper.
- No regressions in functionality are introduced (verified via unit tests and end-to-end testing of affected components).
- Development team members follow the updated coding guidelines for native loop usage.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision

1.  **Adopt Asynchronous, Decoupled Flow Execution**: When the `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` feature flag is enabled, execute all flows asynchronously, fully decoupled from the core business process, immediately after the business process completes successfully and commits its state changes. The core business process will no longer wait for flow execution to finish and will remain unaffected by flow-related failures or performance delays.

2.  **Replace Decorator-Based Execution with Dedicated Async Infrastructure**:
    - Retire the current Symfony `EventDispatcher` decorator for flow execution. Instead, implement a dedicated `FlowOrchestrator` service that listens for post-process completion events (e.g., `BusinessProcessCompletedEvent`) emitted after the triggering business process reaches a consistent, successful state.
    - Use Symfony Messenger (or an equivalent enterprise-grade message queue system) to enqueue flow execution jobs. This ensures reliable, non-blocking triggering of flows without tying them to the main process's execution thread.

3.  **Isolate Flow Error Handling and Monitoring**:
    - Implement flow-specific error handling that does not impact the core business process:
      - Fatal errors in flow execution will not roll back or cancel the completed business process.
      - Configure retry policies (e.g., exponential backoff with a configurable maximum retry count) for transient flow errors (e.g., temporary email server downtime).
      - Route permanently failed flow jobs to a dead-letter queue (DLQ) for manual review and reprocessing, with logs containing detailed context (flow ID, triggering event metadata, error stack trace).
    - Deploy a dedicated monitoring setup for flow execution, including a status dashboard (tracking pending, in-progress, succeeded, failed flows) and isolated log streams to simplify debugging without cluttering business process logs.

4.  **Maintain Backward Compatibility and Incremental Rollout**:
    - Preserve the existing synchronous flow execution logic for cases where the feature flag is disabled, ensuring no regression in existing behavior.
    - Gradually expand coverage of the async system to all flow types over time, using the feature flag to validate stability and gather feedback before full adoption.

5.  **Design for Future Expandability**:
    - Build the `FlowExecutor` service with modular interfaces to support future enhancements such as parallel flow execution, conditional flow scheduling, priority-based queueing, or third-party integration hooks—all without modifying core business process code.
    - Establish clear contracts for flow registration and event triggering to simplify adding new flow types or custom execution strategies.

This decision resolves all identified downsides: eliminates business process disruption from flow errors, removes performance bottlenecks from synchronous heavy operations (e.g., email sends), simplifies debugging via isolated execution contexts, and lays the groundwork for scalable flow capability expansion.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision

Adopt the **Modified Michael Nygard ADR Template** (Markdown-based) as the standard format and structure for all architectural decision records (ADRs) in the project. This includes standardized storage, naming conventions, and lifecycle management rules:

### 1. Core Template Structure (Mandatory for Critical Decisions)
All high-impact architectural decisions (e.g., cloud provider selection, microservices adoption) must use this comprehensive template:
| Field | Description |
|-------|-------------|
| Title | Descriptive, action-oriented title (e.g., ""20240520-Adopt-Event-Driven-Architecture-For-Order-Processing"") |
| Status | Lifecycle state (Proposed → Accepted → Superseded → Deprecated → Rejected) |
| Context & Problem Statement | Detailed background, constraints, and the specific problem being addressed |
| Decision Drivers | Key technical, business, or operational factors influencing the choice (e.g., scalability requirements, compliance rules) |
| Considered Options | Alternatives evaluated, with brief pros/cons for each |
| Decision | Explicit, unambiguous statement of the chosen solution |
| Consequences | Positive, negative, and neutral outcomes (including tradeoffs like increased complexity vs. improved scalability) |
| Stakeholders | List of individuals/teams affected by or involved in the decision |
| Links | References to related artifacts (design docs, Jira tickets, other ADRs) |

### 2. Lightweight Subset (Optional for Minor Decisions)
For low-impact decisions (e.g., selecting a logging library, defining API rate limits), use this condensed version to balance efficiency and clarity:
- Title (follows naming convention)
- Status
- Context (abbreviated)
- Decision
- Consequences

### 3. Storage & Naming Standards
- All ADRs are stored in the project’s Git repository under `docs/architecture/decisions/` (versioned alongside code).
- Naming convention: `YYYYMMDD-<lowercase-hyphenated-decision-title>.md` (e.g., `20240520-adopt-event-driven-order-processing.md`).
- Superseded decisions must include a `Superseded By` field linking to the replacement ADR; deprecated decisions are explicitly labeled and archived if redundant.

---

## Rationale for the Decision
This choice strikes the optimal balance between comprehensiveness, industry familiarity, and flexibility:
- **Industry Standard Foundation**: Building on Michael Nygard’s widely recognized template reduces onboarding time for new team members and aligns with common architectural documentation best practices.
- **Scale Adaptability**: The full template provides depth for critical decisions, while the lightweight subset avoids unnecessary verbosity for trivial choices.
- **Tooling Agnosticism**: Markdown and Git leverage existing team skills and infrastructure, eliminating dependencies on specialized ADR tools (e.g., Structurizr) that add learning curves and maintenance overhead.
- **Lifecycle Tracking**: Standardized status fields ensure the team maintains an accurate, up-to-date record of architectural intent even as the system evolves.

---

## Considered Alternatives & Rejections
1. **Original Michael Nygard Template**: Rejected due to missing critical fields (Stakeholders, Links) that are essential for tracking decision context and impact.
2. **Custom Project-Specific Template**: Rejected as it requires significant upfront work to define, creates onboarding overhead, and is unfamiliar to external contributors.
3. **Tool-Based ADR Solutions**: Rejected because they introduce unnecessary tooling dependencies and learning curves, without clear benefits over Git/Markdown for our team’s workflow.
4. **Ultra-Lightweight 4-Field Template**: Rejected as it fails to capture sufficient context or tradeoffs for high-impact decisions, leading to incomplete documentation for future reference.

---

## Implications
- **Team Alignment**: All architects, engineers, and lead contributors must adhere to the template and naming rules when creating ADRs.
- **Onboarding**: The template will be included in team onboarding materials to ensure consistent usage.
- **Maintenance**: Quarterly reviews will be conducted to update ADR statuses, supersede outdated decisions, and keep documentation accurate.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision

### Core Decision Statement
We will design and implement a **modular, layered TUF repository toolkit** that extends the existing modern Metadata API, while deprecating the legacy repository tools (repo.py, repository_tool.py, repository_lib.py) with a phased migration plan. This new toolkit will address all limitations of the legacy implementation by prioritizing concern separation, extensibility, and a minimal, coherent API.

### Detailed Implementation Decisions
To address the context-specific problems, the toolkit will follow these key design tenets:
1.  **Layered Architecture with Clear Concern Separation**:
    - **Core Library Layer**: A reusable, dependency-light layer that implements essential TUF repository logic (role lifecycle management, signing/verification workflows, delegation rule enforcement, metadata versioning) without making application-specific assumptions. This layer will depend solely on the Metadata API and abstract interfaces for external services (storage, signing).
    - **Utility Helper Layer**: Optional, opinionated components for common use cases (e.g., file-system storage adapters, bulk metadata sync, CLI-friendly error handling) that build on the Core Layer but are not required for its core functionality.
    - **Reference Application Layer**: A lightweight CLI tool (replacing legacy repo.py) built using the Core and Utility Layers to demonstrate end-to-end repository management, serving as a reference for users building custom TUF repository implementations.
2.  **Pluggable Interfaces & Dependency Injection**:
    - Define abstract contracts for `MetadataStore` (to handle metadata persistence) and `SigningProvider` (to handle key management and signing) to allow users to integrate custom backends (cloud storage, databases, HSMs) instead of hardcoding file-system or local key assumptions.
    - Eliminate all global state; all mutable state (e.g., loaded metadata, active signing keys) will be explicitly managed via injectable objects or context managers.
3.  **Lazy Metadata Loading by Default**:
    - The Core Layer will load metadata only when required for an operation (e.g., fetching a specific role’s metadata to sign an update) instead of bulk-loading all repository metadata upfront. Bulk loading will be offered as an optional utility for small-scale CLI use cases.
4.  **Intent-Driven, Minimal API**:
    - Split monolithic legacy methods into focused, intent-driven functions (e.g., `Role.sign_metadata()` instead of a single method handling initialization, signing, and storage).
    - Reduce argument bloat by grouping related configuration into dedicated dataclasses (e.g., `DelegationConfig` instead of passing 5+ delegation arguments to a single method).

### Migration & Deprecation Plan
- Mark legacy repository tools as deprecated in the next minor release, with clear warnings directing users to the new toolkit.
- Provide a migration guide mapping legacy workflows (e.g., repository initialization, target addition, role rotation) to equivalent operations in the new toolkit.
- Maintain backward-compatibility shims for critical legacy functionality during a 2-major-release transition period to minimize disruption for existing users, then remove legacy code in the next major release.

### Rationale Alignment
This decision directly addresses all legacy pain points:
- **Reduces Maintenance Overhead**: Modular layers break down the 7000+ lines of legacy code into smaller, focused components, simplifying testing and updates.
- **Improves API Coherence**: The Core Layer’s dependency-injected, state-explicit API eliminates globals and argument bloat, making it easier to integrate and extend.
- **Supports Diverse Use Cases**: By separating core logic from application-specific choices (like bulk loading), the toolkit works for both small CLI repositories (via Utility Layer helpers) and large-scale servers (via custom storage/lazy loading integrations).
- **Leverages Existing Investments**: Building on the modern Metadata API avoids redundant work and ensures consistency across individual metadata operations and full repository management.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision

### Core Architectural Choice
Migrate the entire Primer React component library from JavaScript to TypeScript, replacing manually maintained ambient type definitions (`index.d.ts`) with type definitions derived directly from component implementations. This aligns type accuracy with the source code, eliminates unsustainable manual maintenance overhead, and scales with the library’s growing size and complexity.

### Implementation & Execution Guidelines
1. **Gradual Migration Approach** (to minimize disruption):
   - All new components, feature additions, and bug fixes will be developed in TypeScript from inception.
   - Prioritize converting existing components to TypeScript based on:
     - Volume of TypeScript-related bug reports,
     - Consumer usage frequency,
     - Component complexity (resolving high-impact pain points first).
   - For un-migrated components, add TypeScript-compatible JSDoc annotations to improve temporary type accuracy and simplify eventual conversion to TypeScript.

2. **Phase Out Ambient Types**:
   - During migration, maintain the `index.d.ts` file only for components not yet converted, but mark it as deprecated in code comments and public documentation.
   - Remove `index.d.ts` entirely once all components are migrated to TypeScript; consumers will rely on TypeScript’s automatic type emission from the library’s compiled output.

3. **Type Quality Assurance**:
   - Enable strict TypeScript compiler options (e.g., `strict: true`, `noImplicitAny: true`, `strictNullChecks: true`) to enforce type safety and catch inconsistencies early in development.
   - Integrate TypeScript type checking into the continuous integration (CI) pipeline to block merges of code with type errors.
   - Add type-specific validation test cases for critical component props, state, and return types to ensure alignment with expected consumer behavior.

4. **Consumer Communication**:
   - Notify library consumers of the migration plan via release notes, official documentation, and GitHub discussions, highlighting improved type accuracy and long-term stability benefits.
   - Provide a backward-compatibility guarantee for most users: generated types will mirror existing ambient definitions where possible to avoid breaking changes. For edge cases, publish a migration guide for adjusting consumer workflows.

### Rationale Alignment
This decision directly resolves the root issue of disconnected manual type definitions by tying type information to component implementations. It eliminates the need for error-prone manual synchronization, reduces TypeScript-related bug reports, and creates a scalable foundation for the library’s future growth. The gradual migration balances immediate improvement with minimal disruption to both the development team and library consumers.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","# Architectural Decision Record: Component Creation Pattern for Primer React

## Decision
We will adopt the **Box-based component pattern** as the default approach for building new components in Primer React and its consuming applications. The direct styled-components pattern will be reserved for explicit edge cases where Box’s capabilities are insufficient to meet component requirements.

## Rationale
This decision prioritizes consistency, maintainability, and alignment with Primer’s design system principles:
1. **Design System Alignment**: Box is a core primitive of the Primer React design system, encapsulating standardized handling of responsive styles, theme tokens, and the `sx` prop. Using Box ensures all components leverage the same underlying styling logic, reducing inconsistencies in appearance and behavior across the codebase.
2. **Reduced Boilerplate**: The Box pattern eliminates redundant code for prop merging (e.g., combining default styles with user-provided `sx`), element type configuration (via the `as` prop), and attribute propagation. Component authors only need to define component-specific props and default styles, relying on Box to handle low-level integration.
3. **Seamless `sx` Prop Integration**: Box natively merges default component styles with user-provided `sx` values, ensuring predictable style precedence (user `sx` overrides default styles). The styled-components pattern requires manual inclusion of `${sx}` in the style template, which is error-prone and less intuitive for new contributors.
4. **Improved Maintainability**: Centralizing styling logic in Box means updates to the design system (e.g., token changes, responsive behavior tweaks) automatically propagate to all Box-based components, reducing the need for widespread refactoring.
5. **Simplified Prop Handling**: Box abstracts common prop concerns (e.g., passing attributes to the underlying DOM element, type safety for element-specific props) so component authors can focus on component-specific functionality rather than low-level styling mechanics.

## Exceptions
The direct styled-components pattern is permitted in the following scenarios:
1. **Advanced Styled-Components Features**: When a component requires features not natively supported by Box, such as complex keyframe animations, theme-dependent style interpolations that are difficult to express via `sx`, or dynamic style switching based on parent component state.
2. **Third-Party Component Wrapping**: When wrapping a third-party component that does not integrate well with Box’s `as` prop or requires direct styled-components-specific customization (e.g., overriding deeply nested styles of the third-party component).
3. **Performance-Critical Components**: In rare cases where the overhead of Box’s abstraction is measurable and impacts user experience (this must be validated via performance profiling before choosing styled-components over Box).

## Implications
1. **Refactoring Roadmap**: Existing components using the direct styled-components pattern will be gradually refactored to use Box, prioritizing high-usage components and those with frequent maintenance needs.
2. **Documentation Updates**: Primer React’s component creation guidelines will be revised to feature the Box pattern as the primary approach, including best practices for merging default styles with `sx`, handling component props, and leveraging Box’s `as` prop.
3. **Tooling Support**: Update TypeScript type definitions and lint rules to enforce the Box pattern by default, with exceptions requiring explicit approval or documentation.
4. **Capability Expansion**: Invest in enhancing Box’s feature set over time to reduce edge-case exceptions (e.g., adding built-in support for keyframes, theme-aware style helpers).","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","# Architectural Decision Record: Formalized Draft Component Lifecycle and Governance

## Decision

We will establish a formalized governance framework for the `@primer/react/drafts` subpath, defining clear criteria for component inclusion, a structured lifecycle for draft components, and standardized documentation practices to ensure consistency, user clarity, and smooth transitions between draft and stable states. The framework will address both parallel versioning of existing components and work-in-progress (WIP) components as described in the context:

### 1. Clear Criteria for Draft Component Inclusion
- **Parallel Versioning**: Drafts will exclusively host new major versions of stable main-bundle components (e.g., `UnderlineNav` v2) during active development. This allows side-by-side testing of the new version with the stable production version without disrupting existing user workflows.
- **Work-in-Progress (WIP) Components**: Drafts will host components (e.g., `TreeView`) that are not yet production-ready due to incomplete accessibility audits, unstable APIs, or pending product sign-off. These components are intended for early feedback and internal testing only.

### 2. Structured Draft Component Lifecycle
#### Entry into Drafts
- A component must be approved via a core design/engineering team sync to be added to drafts. For parallel versions, this requires a documented migration plan from the stable main-bundle version. For WIP components, this requires a formal roadmap outlining milestones for production readiness.

#### Maintenance in Drafts
- Draft components must adhere to the same code quality standards as main-bundle components (including TypeScript support, unit tests, and basic documentation) but may omit full WCAG 2.1 AA accessibility certification or final API sign-off during development.
- Parallel version drafts will include a clear deprecation timeline for the stable main-bundle version, to be communicated to users via release notes and documentation once the draft is promoted.

#### Promotion to Main Bundle
A draft component will be moved to the main `@primer/react` bundle only when all of the following are met:
1.  It has passed a full accessibility audit (WCAG 2.1 AA compliant).
2.  Its API is deemed stable after real-world testing in internal GitHub products and feedback from external early adopters.
3.  For parallel versions, a complete migration guide is published, and a 30-day notice period for deprecation of the old stable version is provided to users.
4.  Core design and engineering teams sign off on production readiness.

#### Deprecation/Removal from Drafts
- Once a draft component is promoted to the main bundle, the draft version will be marked as deprecated in documentation and removed in the next major release of `@primer/react`.
- Abandoned WIP components will be removed from drafts after a 30-day notice period, communicated via release notes and the library’s GitHub repository.

### 3. Standardized Documentation and Import Practices
- All draft components will remain exported from the `@primer/react/drafts` subpath to maintain a clear separation from stable main-bundle components.
- Documentation for draft components will include:
  - A prominent ""Draft Component"" banner at the top of the docs page.
  - For parallel versions: A side-by-side comparison with the stable main-bundle version, including key changes and migration steps.
  - For WIP components: Explicit advising against production use, plus contact information for the core team to facilitate collaboration and feedback requests.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","# Architectural Decision Record: Scalable Snapshot Testing for React Components

## Decision

We will adopt a structured, intent-driven approach to snapshot testing that addresses scalability challenges by implementing the following coordinated practices:

1. **Targeted, Behavior-Aligned Snapshots**:
   - Replace full-component-tree snapshots with targeted snapshots that capture only parts of the UI directly tied to the test’s behavioral intent. For example:
     - When testing a button’s active state, snapshot only the button element (not its entire parent container).
     - When testing a list component, snapshot a single list item template instead of the full list populated with large datasets.
   - Prohibit snapshotting entire deep/wide component trees unless explicitly justified for critical structural invariants (e.g., core layout component hierarchy).

2. **Intent-Centric Test Naming and Documentation**:
   - Mandate test names that clearly describe the specific behavior or state being validated (e.g., `renders with ""active"" class when isActive prop is true` instead of `renders correctly`).
   - For complex cases, add inline comments explaining the snapshot’s purpose (e.g., `// Ensures error state displays validation icon and message`).

3. **Noise-Reducing Custom Serializers**:
   - Integrate custom Jest serializers to filter out non-volatile, non-essential details from snapshots, including:
     - Anonymous function references (replace with placeholders like `[EventHandler]`).
     - Dynamic values (e.g., random IDs, timestamps) that don’t impact component behavior.
     - Props/attributes irrelevant to the test’s intent (configurable per test or component type).

4. **Hybrid Testing Strategy**:
   - Combine targeted snapshots with explicit assertions to balance coverage and clarity:
     - Use snapshots for validating structural UI changes (e.g., presence of a modal overlay, correct form field nesting).
     - Use explicit `expect` assertions for discrete values (e.g., `expect(button.textContent).toBe(""Submit"")`, `expect(input.disabled).toBe(true)`).

5. **Snapshot Governance in Code Reviews**:
   - Enforce review checks for all snapshot changes:
     - New snapshots must adhere to targeted, intent-driven guidelines.
     - Failed snapshot updates require a description of the intentional change (linked to a feature/bug fix) and verification that the updated snapshot aligns with the test’s original intent.
     - Reject snapshots that are overly large or lack clear behavioral purpose.

## Rationale

This decision directly mitigates the three core scalability challenges outlined:

- **Large Snapshots**: Targeted snapshots and noise filtering reduce size, making them easier to store and review without losing critical validation value.
- **Debugging Diffs**: Smaller, focused snapshots with minimal noise simplify identifying exactly what changed (e.g., a class name update instead of sifting through hundreds of irrelevant lines). Intent-centric naming helps developers quickly assess if changes are expected or regressions.
- **Unclear Test Intent**: Explicit test names, documentation, and hybrid assertions eliminate ambiguity about each test’s purpose. Governance ensures every snapshot serves a clear behavioral role, consistently answering: ""What is this testing?"" as the project scales.

This approach retains snapshot testing’s core benefit (catching unintended structural changes) while fixing its scaling weaknesses, ensuring long-term maintainability.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision

We will implement a tree-shakable, environment-aware warning system for `@primer/react` that eliminates all warning-related code from production bundles while preserving actionable developer warnings in non-production environments. The system includes the following components and enforced patterns:

### 1. Centralized Environment-Gated Warning Utility
Create a shared `warnDev` utility function that:
- Is gated behind a compile-time check for `process.env.NODE_ENV !== 'production'`
- Accepts either a static string or a function returning a string (to defer expensive message computation to development only)
- Prefixes all warnings with `[@primer/react]` for clear attribution
- Acts as a no-op in production (with zero runtime overhead)

Example implementation:
```typescript
export function warnDev(
  messageOrGetter: string | (() => string),
  ...args: unknown[]
): void {
  if (process.env.NODE_ENV !== 'production') {
    const message = typeof messageOrGetter === 'function' ? messageOrGetter() : messageOrGetter;
    console.warn(`[@primer/react] Warning: ${message}`, ...args);
  }
}
```

### 2. Enforced Usage Patterns for Warnings
All development warnings must follow these rules to ensure full tree-shaking effectiveness:
- **No direct `console.warn()` calls**: Use only the centralized `warnDev` utility for all developer-facing warnings.
- **Wrap conditional checks in environment gates**: Any logic used to trigger warnings (e.g., checking for deprecated props) must be wrapped in the same `process.env.NODE_ENV !== 'production'` condition. This ensures bundlers eliminate the entire conditional branch (including checks and warning calls) from production bundles.

Example of correct usage:
```typescript
// Warning logic is fully eliminated in production builds
if (process.env.NODE_ENV !== 'production' && props.deprecatedProp != null) {
  warnDev(() => `The ""deprecatedProp"" prop is deprecated. Use ""newProp"" instead.`);
}
```

### 3. Build-Time Optimization for Bundle Stripping
Configure the `@primer/react` build pipeline and document consumer requirements to guarantee full removal of warning code:
- **Library Distribution**: Ship CommonJS and ESM builds with intact `process.env.NODE_ENV` checks, allowing consumer bundlers (Webpack, Rollup, Vite) to substitute the variable during their build process.
- **Consumer Guidance**: Require consumers to use bundlers with compile-time environment replacement (e.g., Webpack’s `DefinePlugin`, Rollup’s `@rollup/plugin-replace`) to set `process.env.NODE_ENV = 'production'` in production builds. This lets bundlers evaluate environment conditions to `false` and tree-shake all warning-related code.
- **Side Effect Prohibition**: Explicitly forbid side effects (state changes, data mutations) inside warning conditional branches to avoid inconsistent behavior between development and production.

### 4. Testing & Validation
- **Unit Testing**: Write Jest tests to verify `warnDev` emits correct warnings in non-production environments and acts as a no-op in production. Mock `console.warn` to assert warning triggers for conditions like deprecated props.
- **Bundle Validation**: Add automated checks to confirm production bundles of `@primer/react` contain no traces of `console.warn`, `warnDev` runtime logic, or warning-related conditional branches.

## Rationale
This decision directly addresses the context’s core requirements:
- **No `console.warn()` in production**: The `warnDev` utility is a no-op in production, and environment-gated conditions ensure bundlers remove all warning calls.
- **Warning code removed from production**: Build-time evaluable conditions enable bundlers to tree-shake entire branches of warning-related code, including condition checks and message generation.
- **Consistency**: A centralized utility ensures uniform warning formatting and simplifies updates to warning behavior (e.g., adding telemetry for critical warnings in development).
- **Performance**: Deferring message computation via functions avoids unnecessary runtime overhead in development and guarantees zero overhead in production.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision

### 1. Adopt a Centralized, Persistent Live Region System
Implement a global, reusable live region solution for Primer React and GitHub products consisting of:
- A single, persistent, visually hidden live region component rendered once at the root of the application. This component will remain in the DOM at all times (avoiding dynamic injection or visibility toggling) and use standard visually-hidden CSS to ensure it’s registered by screen readers on page load.
- A set of abstracted React hooks and components that wrap low-level ARIA live region attributes, eliminating the need for contributors to manually manage inconsistent markup.

### 2. Standardize Use Case-to-Behavior Mappings
Define explicit mappings between common live region scenarios and compliant, tested behaviors to ensure cross-screen reader consistency:
| Use Case | Recommended Behavior |
|----------|----------------------|
| One-time programmatic announcements (e.g., query result counts, form submission confirmations) | Use `usePoliteAnnouncement` hook to update the global persistent live region. This uses `aria-live=""polite""` with `aria-atomic=""true""` to announce messages when the screen reader is idle, ensuring the full message is read. |
| Urgent, interruptive announcements (e.g., critical errors, time-sensitive alerts) | Use `useAssertiveAnnouncement` hook to update the global persistent live region. This uses `aria-live=""assertive""` with `aria-atomic=""true""` to interrupt the screen reader immediately, reserved only for high-priority, time-sensitive content per WCAG guidelines. |
| Dynamic content changes (e.g., incrementing button counts, loading spinner status updates) | Use the `<LiveText>` component, which applies context-specific ARIA attributes directly to statically rendered elements. Props will allow contributors to specify urgency (`polite`/`assertive`) and atomicity, ensuring only intended content changes trigger announcements. For dynamic elements that mount/unmount frequently, fall back to the global live region via the announcement hooks. |

### 3. Deprecate and Migrate Non-Compliant Implementations
- Deprecate direct, manual usage of live region roles (`alert`, `status`, `log`) and raw `aria-live`/`aria-atomic` attributes in components, as these have shown inconsistent behavior across screen readers.
- Add automated lint rules (via ESLint) to detect non-compliant live region usage and suggest the centralized pattern.
- Provide a migration guide for existing components to transition to the new hooks and `<LiveText>` component, with validation testing across major screen readers (NVDA, VoiceOver, JAWS) and browsers.

### 4. Enforce Validation and Testing
- Validate the centralized pattern against edge cases (rapid content changes, dynamic component mounting, visibility updates) to ensure consistent announcements across all supported screen reader-browser combinations.
- Include the live region system in Primer React’s accessibility regression test suite, using automated tools (e.g., axe-core) and manual screen reader testing to catch regressions.

## Rationale
This decision addresses the core issues in the context:
- **Persistent DOM presence**: Avoids screen reader registration failures caused by dynamic injection or visibility toggling of live regions, a key pain point noted in the referenced Tetra Logical article.
- **Abstracted APIs**: Reduces contributor error by eliminating the need to manually manage low-level ARIA attributes, ensuring consistent application of best practices.
- **Use Case Clarity**: Explicit mappings guide contributors to choose the correct behavior for their scenario, balancing accessibility with user experience.
- **Cross-Screen Reader Consistency**: Testing the centralized pattern across major tools ensures announcements work reliably, resolving the inconsistency of ad-hoc implementations.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"To provide a meaningful Architectural Decision, I’ll first define a realistic, common Context for a B2B SaaS application, then outline the corresponding Decision and Justification in standard ADR format:

---

## Context
We are building a B2B SaaS project management platform that handles:
1.  Sensitive user data (PII, client project contracts, billing records) requiring compliance with GDPR and HIPAA (for healthcare-sector clients).
2.  Unstructured/ semi-structured project metadata (custom task fields, client-specific workflow logs, collaborative comments) that varies widely across enterprise customers.
3.  A projected 12x increase in concurrent users (from 1k to 12k) and 8x growth in data volume by Q4 2025.
4.  Current state: A monolithic PostgreSQL database is experiencing write bottlenecks for unstructured metadata and read latency for user-facing billing queries, while compliance audits are hindered by mixed transactional and non-transactional data in a single store.

## Problem Statement
Select a data storage architecture that balances:
- Compliance with GDPR (data portability, purpose limitation) and HIPAA (audit trails, end-to-end encryption).
- Horizontal scalability for growing unstructured data and concurrent user requests.
- ACID guarantees for transactional user and billing data.
- Maintainable schema flexibility for client-specific project metadata.

## Options Considered
1.  Scale the existing monolithic PostgreSQL database via vertical scaling + read replicas.
2.  Migrate to a serverless unified database (AWS Aurora Serverless v2).
3.  Implement a polyglot persistence architecture with specialized databases for distinct data types.

## Decision
We will adopt a polyglot persistence architecture:
- **PostgreSQL (relational database):** Store transactional, compliance-critical data including user accounts, billing records, project contracts, and core task assignments. Deploy read replicas to offload read-heavy user queries.
- **MongoDB (document-oriented NoSQL database):** Store unstructured/semi-structured data including custom project metadata, workflow logs, collaborative comments, and audit trails for non-transactional user actions. Deploy a sharded cluster for horizontal scalability of write-heavy log data.
- **Add a centralized data pipeline (Apache Kafka) to sync critical compliance audit events from both databases to a dedicated compliance logging service for GDPR/HIPAA audits.**

## Justification
- **Compliance Alignment:** PostgreSQL’s robust ACID guarantees and built-in audit logging support meet HIPAA requirements for transactional data integrity. MongoDB’s flexible document model allows granular tagging of data for GDPR purpose limitation and data portability (easy export of client-specific metadata).
- **Scalability:** MongoDB’s sharded cluster scales horizontally to handle the projected 8x growth in unstructured metadata and concurrent write requests for logs. PostgreSQL read replicas reduce latency for read-heavy user billing queries without sacrificing transactional integrity.
- **Schema Flexibility:** MongoDB eliminates the need for rigid relational schemas for client-specific project fields, reducing development overhead for custom client onboarding.
- **Cost & Maintainability:** Avoids the diminishing returns of vertical scaling (Option 1) and retains fine-grained control over compliance workflows (unlike the black-box nature of serverless unified databases in Option 3). Each database is optimized for its specific use case, reducing technical debt from forcing all data into a one-size-fits-all schema.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","# Architectural Decision Record: Testing Infrastructure Optimization

## Decision
We will implement a unified testing stack that addresses both duplicate markup and visual debugging gaps by centering Storybook as the single source of truth for component test scenarios and integrating visual debugging capabilities for Jest tests. The specific actions are:

### 1. Reuse Storybook Stories as the Single Source of Truth for Component Test Setup
- **Tooling**: Adopt `@storybook/testing-react` (or framework-equivalent package) to import and reuse Storybook story definitions directly in Jest tests.
- **Implementation**:
  - For all component test scenarios, first define variants and interactive flows as parameterized Storybook stories (using Args and ArgTypes for dynamic configuration).
  - In Jest tests, use `composeStories` or `composeStory` utilities to import these stories, eliminating the need to rewrite component markup/setup code.
  - Replace manual interactive story testing with the official **Storybook Test Runner** (powered by Playwright) to automate validation of user flows (clicks, form inputs, state changes) directly within Storybook, reducing the need to reimplement these flows in Jest.

### 2. Enable Visual Debugging for Jest Tests
- **Tooling**: Integrate **Jest Preview** (or `@storybook/preview` for shared environment consistency) into the Jest testing pipeline.
- **Implementation**:
  - Configure Jest Preview to render components from Jest tests (or reused Storybook stories) automatically when tests are run in debug mode or fail. Share global styling, theme providers, and environment setup between Storybook and Jest Preview via a common `preview.config.js` file to ensure consistent renderings.
  - Add a custom Jest command (e.g., `npm run test:debug`) that launches the Jest Preview web server alongside Jest, allowing developers to visually inspect component states during test execution.

### 3. Standardize the Testing Workflow
Update the team’s testing process to align with this unified stack:
1. Define component requirements as Storybook stories (including edge cases and interactive flows).
2. Run the Storybook Test Runner to validate interactive scenarios automatically as part of pre-commit/CI checks.
3. Reuse story imports in Jest tests for unit-level assertions (e.g., prop validation, state logic).
4. Use Jest Preview to debug failing tests by visualizing the component’s state at the point of failure, instead of relying solely on CLI output.

---

## Rationale (Tied to Context Problems)
- **Eliminates duplicate markup**: Reusing Storybook story definitions in Jest tests removes redundant component setup code between stories and test files, directly solving the first context problem.
- **Improves Jest debugging visibility**: Jest Preview adds a visual layer to Jest test runs, letting developers inspect rendered components during debugging. Shared environment configuration with Storybook ensures consistency between test and story renderings, addressing the second context problem of low visibility into Jest tests.
- **Reduces manual effort**: The Storybook Test Runner automates interactive scenario testing, eliminating the need to manually validate stories before transferring flows to Jest.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Architectural Decision Record: Unified Workspace and Script Management for `primer/react`

---

### Decision

We will implement three core changes to standardize cross-workspace dependency management, improve script maintainability, and streamline contributor workflows:

1. **Leverage Native npm Workspaces as the Official Cross-Workspace Dependency Layer**
   - Update the root `package.json` to explicitly declare all project workspaces via the `workspaces` field (e.g., `[""packages/*"", ""docs"", ""examples/*"", ""tools/scripts""]`). This enables npm to automatically handle dependency resolution, symlinking, and installation across all workspaces without manual intervention.
   - Refactor all cross-workspace dependencies (e.g., `docs` → `@primer/react`, `examples/*` → `@primer/react`) to use npm’s official workspace protocol (e.g., `""@primer/react"": ""workspace:^""` instead of relative paths, file dependencies, or `npm link`). This ensures consistent, automatic linking of dependent workspaces during installation, eliminating error-prone manual steps.

2. **Refactor the `script` Directory into a Dedicated `tools/scripts` Workspace**
   - Move the existing `script` directory contents to a new `tools/scripts` workspace with its own `package.json`. This workspace will:
     - Host complex scripts with isolated dependencies (e.g., CLI utilities, build tools) instead of polluting the root `package.json`.
     - Expose executable scripts via the `bin` field in its `package.json`, making them accessible across all workspaces via npm’s workspace-aware script execution.
   - Modularize large, monolithic scripts into smaller, testable components within this workspace to improve maintainability.

3. **Standardize Installation and Contributor Workflows**
   - Deprecate manual per-workspace `npm install` steps and the existing `npm run setup` script. Replace these with the native root-level `npm install` command, which will install all workspace dependencies and resolve cross-workspace links in a single step.
   - Formalize a consistent workspace structure in project documentation:
     - `packages/`: Core library packages (e.g., `@primer/react`).
     - `docs/`: Documentation workspace.
     - `examples/`: Example project workspaces.
     - `tools/`: Utility workspaces (e.g., `tools/scripts`, `tools/linters`) for specialized tooling with isolated dependencies.

---

### Rationale (Supporting Context for Decision)
- **Native npm Workspaces**: Eliminates custom, error-prone dependency linking logic by leveraging npm’s built-in capabilities, ensuring compatibility with standard npm workflows and reducing maintenance overhead.
- **Dedicated Scripts Workspace**: Addresses the limitation of unstructured scripts by providing a maintainable home for complex tooling with its own dependency tree, avoiding bloat in the root `package.json`.
- **Streamlined Contributor Experience**: Reduces onboarding friction by replacing multi-step setup with a single root `npm install` command, aligning with industry-standard workspace practices.
- **Scalability**: Establishes a flexible structure to accommodate future workspaces (e.g., new utility tools, example projects) without disrupting existing workflows.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","### Architectural Decision
1. **Directory Structure Restructuring**: 
   Separate public and internal code within the `src` directory to establish a clear boundary:
   - Create a `src/public` subdirectory to contain all code intended for external consumer use (e.g., components, hooks, utility functions documented as part of the official API).
   - Move all internal-only code (e.g., private implementation helpers, shared logic not meant for public consumption, test utilities) into a `src/internal` subdirectory.

2. **Package.json Exports Update**:
   Replace the broad `./lib-esm/*` wildcard in the `exports` field with explicit entries targeting only the public code paths. This eliminates unintended exposure of internal code. Example updated configuration:
   ```json5
   {
     exports: {
       // Root entry points (keep as-is if needed)
       ""."": {
         import: ""./lib-esm/index.js"",
         require: ""./lib/cjs/index.js""
       },
       // Explicit public module paths
       ""./lib-esm/components/*"": {
         import: ""./lib-esm/components/*.js"",
         require: ""./lib/cjs/components/*.js""
       },
       ""./lib-esm/hooks/*"": {
         import: ""./lib-esm/hooks/*.js"",
         require: ""./lib/cjs/hooks/*.js""
       },
       ""./lib-esm/utils/*"": {
         import: ""./lib-esm/utils/*.js"",
         require: ""./lib/cjs/utils/*.js""
       }
       // Omit any entries for internal paths entirely
     }
   }
   ```

3. **Build Pipeline Adjustment**:
   Update the Rollup configuration to:
   - Only compile and bundle modules from `src/public` into the `lib-esm` and `lib/cjs` output directories.
   - Bundle dependencies from `src/internal` directly into the public modules that consume them, rather than emitting standalone files for internal code in the output directories.

4. **Type Safety Enhancement**:
   Modify `tsconfig.json` to exclude internal code from public type declarations:
   - Add `src/internal` to the `exclude` array to prevent generation of `.d.ts` files for internal code.
   - Use the `@internal` JSDoc tag for any remaining internal code in public directories, paired with TypeScript’s `stripInternal: true` compiler option to ensure these types are not exposed to consumers.

### Rationale (Supplementary)
This decision addresses the core issue of unintended public API exposure by combining structural clarity with explicit configuration. Separating public and internal code reduces maintainance overhead, while explicit exports and build rules ensure only intended code is accessible to consumers. Type safety adjustments further enforce the boundary by hiding internal types from downstream users.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision

### 1. Adopt a Static CSS Architecture to Eliminate Runtime Style Injection
- **For new components**: Mandate static CSS (utility classes, component-specific CSS modules, or a hybrid approach) instead of runtime style injection (e.g., `sx` prop, dynamic inline styles, component-level style generation). 
- **For existing components**: Execute an incremental migration prioritizing high-traffic, performance-critical application areas first. This will reduce initial page load times (aligning with lab benchmarks showing a 60% speedup using static CSS vs runtime injection) and eliminate the need for server-side style collection.

### 2. Standardize on Primer Primitives CSS Variables for Theming and Consistency
- Replace hard-coded styling values with Primer Primitives’ exposed CSS variables (for color, spacing, typography, border radius, etc.) across all components. This enables native browser handling of theming, enforces design system guardrails, and removes runtime overhead from dynamic value calculation.

### 3. Optimize Dynamic Styling with Conditional Static Classes
- For styles dependent on props or state, use conditional application of pre-defined static CSS classes (leveraging CSS variables) instead of runtime-generated style objects. This cuts dynamic update latency (matching application benchmarks showing a 60% speedup over dynamic `sx` prop usage) by relying on fast, native browser class attribute changes.

### 4. Simplify SSR Pipelines by Removing Double-Rendering Overhead
- Update server-side rendering workflows to eliminate the requirement for dual application renders to collect runtime styles. Instead, serve pre-built static CSS alongside rendered HTML, parallelizing style and content loading and reducing SSR latency by ~20% (per issues-app experiment data).

### 5. Enforce Guardrails to Maintain Compliance
- Implement automated checks to uphold the new architecture:
  - Lint rules to block runtime style injection APIs in new code and flag existing uses for migration.
  - Component templates and style guides that default to static CSS and Primer Primitives variables.
  - Validation tooling to ensure all styling uses approved Primitives variables, preventing unregulated custom values that break consistency or theming.

This decision directly resolves all outlined performance pain points while enabling robust theming and strengthening alignment with Primer design system standards.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision

1. **Deprecate Storybook Play Functions for Automated Interaction Tests**: We will immediately deprecate the use of Storybook `play` functions as a mechanism for authoring or executing automated component interaction tests. Existing tests relying on `play` functions for testing purposes will be migrated to our approved tooling over a 6-week transition period.
2. **Targeted Interaction Testing Strategy**: Align interaction testing with our existing Jest and Playwright stack by splitting component interaction tests into two purpose-built categories:
   a. **Isolated Component Interaction Tests**: For testing component behavior in isolation (e.g., state transitions, event handling, prop validation) without needing a full browser environment, we will author tests using **Jest + React Testing Library (RTL)**. These tests will integrate seamlessly with our unit/integration test suite, leveraging Jest’s mocking capabilities, coverage reporting, and robust assertion library, paired with RTL’s user-centric testing patterns.
   b. **Browser-Context Component Interaction Tests**: For interaction tests dependent on real browser features (e.g., responsive layout behavior, CSS animations, browser API integrations like `localStorage` or `fetch`), we will extend our existing **Playwright** setup to write tests against our Storybook stories. These tests will coexist with our Playwright-based visual regression tests, allowing us to validate both interaction behavior and visual consistency in a single browser workflow.
3. **Retain Play Functions for Documentation Only**: We will continue to use Storybook `play` functions exclusively for interactive documentation—demonstrating component usage flows in the Storybook UI for stakeholders and developers—but not for automated test execution.

### Transition Plan
- **Phase 1 (Weeks 1-2)**: Audit all existing Storybook stories to distinguish `play` functions used for documentation vs. automated testing, and flag critical tests for migration.
- **Phase 2 (Weeks 3-4)**: Migrate high-priority interaction tests to either Jest/RTL (isolated cases) or Playwright (browser-context cases), with peer reviews to ensure alignment with new standards.
- **Phase 3 (Weeks 5-6)**: Update internal testing documentation to reflect the new strategy, remove references to `play` functions for test authoring, and disable the Storybook test runner from our CI/CD pipeline.
- **Ongoing**: Enforce the new strategy via code reviews and linting rules to prevent new automated tests from being written using `play` functions.

---

### Rationale (Supporting the Decision)
This decision addresses key pain points of the previous play-function-based approach:
- **Tooling Consistency**: Eliminates cognitive overhead by unifying interaction testing with our existing Jest/Playwright stack, which developers are already proficient in.
- **Test Performance & Flexibility**: Isolated tests run faster in Jest/RTL, while browser-context tests leverage Playwright’s advanced browser automation capabilities. Both toolchains offer richer assertions, mocking, and reporting than Storybook’s native test runner.
- **Coverage Integration**: Jest/RTL tests integrate seamlessly with our existing code coverage pipeline, filling gaps left by play functions that did not support coverage tracking.
- **Maintainability**: Splitting tests by context ensures clear ownership and organization, making complex interaction scenarios easier to debug and scale.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Architectural Decision Record: Responsive Prop Resolution for SSR Layout Shift Prevention

### Decision
We will implement a **CSS-first, hydration-safe hybrid strategy** for handling responsive props to eliminate layout shifts during server-side rendering (SSR):

1. **Style/Layout Props**: For all props controlling visual or layout-related behavior (e.g., `gap`, `padding`, `gridColumns`, `width`), resolve responsive values using **CSS media queries** instead of JavaScript-based resolution:
   a. Maintain a single source of truth for breakpoints (shared between CSS and JS, e.g., `breakpoints.ts` exporting CSS variables and corresponding media query strings).
   b. Provide a utility function `generateResponsiveStyles<T>(propValue: T | ResponsiveProp<T>, cssProperty: string)` that converts responsive prop objects (e.g., `{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }`) into inline styles or scoped CSS rules using breakpoint-specific media queries.
   c. Components accepting style-related responsive props will use this utility during SSR to output all necessary responsive CSS rules, ensuring the browser applies the correct value immediately without JS hydration delays.

2. **Behavioral/Non-Style Props**: For props controlling dynamic behavior or non-layout content (e.g., `isExpanded`, `renderMobileContent`), enhance the existing `useResponsiveValue` hook with a hydration-resilient flow:
   a. On the server, render the configurable fallback breakpoint value (default: `regular`).
   b. On the client:
      i. Retain the server-rendered fallback value during hydration to avoid initial layout shifts.
      ii. After hydration completes (via `useLayoutEffect`), resolve the client’s actual viewport value using `matchMedia` and update the component state.
      iii. Require components using these props to reserve space for all possible values (e.g., `min-height`/`min-width`, CSS `clamp()`) during SSR to prevent shifts when updating post-hydration.

3. **Standardization & Paved Path**:
   a. Enforce a shared `ResponsiveProp<T>` type across all components to unify syntax for single values and responsive objects.
   b. Add lint rules to warn against using `useResponsiveValue` for style/layout props where CSS-based resolution is feasible.
   c. Update documentation to prioritize CSS-based responsive props for visual needs, with clear guidance on when to use the enhanced hook for behavioral cases.

### Rationale
- **CSS Media Queries Eliminate Root Cause**: Unlike JS-based resolution (which waits for hydration), CSS media queries are applied by the browser during initial page load, ensuring no delay in applying the correct layout and eliminating shifts.
- **Hybrid Approach Balances Flexibility & Performance**: Covers both visual (CSS-first) and behavioral (JS-enhanced) use cases without compromising on user experience or functionality.
- **Single Source of Truth Prevents Inconsistencies**: Shared breakpoints align CSS media queries and JS matchMedia logic, avoiding mismatched responsive behavior.
- **Hydration-Resilient Hook Minimizes Behavioral Shifts**: Retaining server fallback values during hydration and using `useLayoutEffect` ensures post-hydration updates are applied before the browser paints, reducing noticeable shifts.

### Implications
1. **Component Refactoring**: Existing components using style-related responsive props will need updates to use the CSS utility instead of `useResponsiveValue`.
2. **Developer Knowledge**: Teams will need to understand mapping prop values to CSS properties/media queries, though the utility will abstract most complexity.
3. **Edge Case Overhead**: For behavioral props, developers must implement space-reservation techniques to prevent post-hydration shifts, adding minor but necessary complexity.
4. **Testing Requirements**: Additional SSR tests will verify correct CSS output, and Cumulative Layout Shift (CLS) metrics will be monitored to confirm effectiveness.

### Rejected Alternatives
- **Server-Side Viewport Guessing**: Unreliable due to user-agent spoofing and inconsistent header support, and fails to handle post-initial window resizes.
- **JS-Only Progressive Enhancement**: Does not eliminate shifts entirely, only reduces perceived impact via transitions/animations.
- **Client-Side Only Rendering**: Harms SEO and initial load performance, contradicting SSR benefits.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","### Decision
We will implement a structured, backward-compatible deprecation strategy for the `title` prop on `ActionList.Group` in favor of `ActionList.GroupHeading`, prioritizing clear developer communication, accessibility preservation, and a low-friction migration path:

1. **Dual-Tier Deprecation Warnings**
   - **TypeScript Users**: Mark the `title` prop as deprecated in TypeScript definitions using the `@deprecated` JSDoc tag, with a message: `@deprecated Use <ActionList.GroupHeading> instead. See [link to migration docs] for examples and accessibility benefits.`
   - **Runtime Users**: Add a development-only console warning triggered when the `title` prop is used, mirroring the TypeScript message to ensure JavaScript developers receive explicit notification of the deprecation and recommended replacement.

2. **Backward Compatibility with Semantic Mapping**
   - Maintain functional parity for existing usage: When the `title` prop is provided, internally render an `ActionList.GroupHeading` component with the title value as its children, preserving the original implicit semantics (e.g., defaulting to `as=""h2""` to match the new component’s accessible default).
   - Resolve conflicts explicitly: If a developer provides both the `title` prop and an explicit `ActionList.GroupHeading` child, prioritize the explicit `GroupHeading` and emit an additional runtime warning stating the deprecated `title` prop is being ignored.

3. **Comprehensive Migration Guidance**
   - Update the `ActionList` component documentation to include:
     - Side-by-side code examples (old vs. new API) matching the user-provided patterns.
     - A dedicated deprecation callout highlighting benefits of the new approach: explicit heading level control via `as`, support for custom header content, and improved accessibility via explicit semantic markup.
     - A link to a detailed migration guide (if available) for edge cases (e.g., custom styling for titles, dynamic content).

4. **Semver-Compliant Removal Timeline**
   - Deprecate the `title` prop in a minor version release, keeping it functional for all subsequent minor versions in the current major release line.
   - Remove the `title` prop entirely in the next major version release, with advance notice in deprecation warnings and documentation to set clear expectations for developers.

5. **Accessibility Preservation**
   - Ensure the backward-compatible rendering of `GroupHeading` from the `title` prop maintains or enhances original accessibility: Map any implicit ARIA attributes or heading semantics from the old API to the new component’s props to preserve screen reader behavior.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","### Architectural Decision
1. **Adopt Standard Proxy Environment Variables**: The self-hosted runner will support proxy configuration using the de facto standard environment variables `http_proxy`, `https_proxy`, and `no_proxy` (lowercase) as the primary interface, with full backward compatibility for their uppercase equivalents (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`).
2. **Dual-Casing Propagation**: When proxy settings are configured (either via user-provided environment variables at runner startup or runtime), the runner will propagate both lowercase and uppercase variants of these variables to all child processes and tool execution contexts. This ensures compatibility with tools that rely on either casing convention.
3. **Precedence & Normalization**:
   - If both lowercase and uppercase variants of the same proxy variable are provided, the lowercase variant will take precedence (aligning with majority tooling support noted in the context). The runner will normalize values by setting the uppercase variant to match the lowercase value to eliminate configuration conflicts.
   - If only uppercase variants are provided, the runner will automatically populate the corresponding lowercase variants with identical values to ensure compatibility with the majority of tools that prefer lowercase.
4. **No-Proxy Respect**: The runner will preserve and propagate user-provided `no_proxy`/`NO_PROXY` values (comma-separated hostnames, IP ranges, or domain patterns) as-is, with no automatic modifications unless explicitly enabled via an optional runner configuration flag (e.g., auto-adding loopback addresses like `127.0.0.1`/`localhost` to avoid proxying local traffic).

### Rationale
- **Alignment with Industry Norms**: Directly supports the convention used by nearly all common tools (curl, wget, git, Docker, Python, R) referenced in the context, minimizing configuration friction for enterprise users.
- **Max Compatibility**: Propagating both casing variants ensures coverage for the subset of tools that rely on uppercase proxy variables, eliminating gaps in toolchain functionality.
- **Predictable Behavior**: Precedence for lowercase variants and value normalization prevents conflicting configurations and ensures consistent behavior across all integrated tools.
- **Enterprise Flexibility**: Preserving user-provided `no_proxy` values while offering optional auto-configuration for local traffic balances strict user control with practical defaults for common enterprise environments.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","**Decision**
Adopt an opt-in command echoing system with the following formalized rules to balance log clarity, troubleshooting needs, and user control:
1. **Default Echo State**: Command echoing is disabled by default. It is automatically enabled only if the `ACTIONS_STEP_DEBUG` secret is explicitly set to a truthy value (e.g., `true`). This ensures minimal log clutter for standard workflow executions while preserving debug visibility for users who opt into verbose logging.
2. **Error-Case Mandatory Echo**: Regardless of the current echo state, full command output will be echoed in user logs whenever a command fails to process (e.g., returns a non-zero exit code, encounters an execution exception, or fails validation). This guarantees users receive critical troubleshooting context for failed commands without requiring explicit debug enablement.
3. **Explicit Toggle Commands**: Introduce two dedicated workflow commands: `::echo::on` and `::echo::off`. These commands allow users to explicitly toggle the command echoing state at any point during workflow execution. The state set by these commands overrides the default `ACTIONS_STEP_DEBUG`-driven state and remains in effect for all subsequent commands until another toggle command is issued or the workflow step/context concludes.
4. **State Change Transparency**: When `::echo::on` or `::echo::off` is executed, add a low-priority, non-intrusive log entry (e.g., `Echo state updated to [on/off]`) to workflow logs. This maintains transparency about why command outputs are displayed or suppressed without cluttering primary user logs.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","# Architectural Decision Record: Extra Information in ""Set up job"" Step Logs

## Decision
Implement a modular, extensible metadata collection framework within the runner to inject structured, context-aware extra information into the ""Set up job"" step log, with specific handling for hosted and self-hosted runner environments. This decision includes the following sub-decisions to address the stated context:

### 1. Structured Metadata Schema & Formatting
Define a consistent, human-readable and machine-parsable format for extra log information, using a section-based structure with mandatory and optional fields:
- **Mandatory Fields**: OS name/version, kernel version (for Linux), OS build number (for Windows/macOS).
- **Hosted Image-Specific Fields**: Image ID, image build timestamp, pre-installed software versions (curated list of common tools like Python, Node.js, Docker), and image version tag.
- **Self-Hosted Optional Fields**: (If enabled) Host machine hostname (redactable), critical service versions, and runner agent version.
- All extra info will be appended to the ""Set up job"" log under a clear, demarcated header like `=== Job Context Metadata ===` for visibility.

### 2. Pluggable Metadata Collector Component
Build a lightweight, environment-aware collector to gather the required data:
- **Hosted Runners**: Use a pre-built manifest file (e.g., `/opt/runner/image-metadata.json` for Linux, `C:\runner\image-metadata.json` for Windows) included in every hosted image. This manifest is maintained by the hosted image engineering team and contains all pre-installed software versions, image lineage, and build details. The runner will read this file synchronously during the ""Set up job"" step to avoid runtime command overhead.
- **Self-Hosted Runners**: Add an optional configuration flag (`collect-host-metadata`) in the runner config file. When enabled, the collector will execute a curated allowlist of safe system commands (e.g., `lsb_release -a`, `choco list --local-only`, `sw_vers`) to gather metadata, with no arbitrary command execution allowed.

### 3. Integration into ""Set up Job"" Flow
Embed metadata collection into the early stages of the ""Set up job"" step:
1. Execute the metadata collector immediately after initializing the job context but before any job preparation tasks (e.g., fetching repo, setting up credentials).
2. Format the collected data into the defined section structure and append it to the ""Set up job"" log output.
3. For hosted runners, validate the manifest signature (using a public key embedded in the runner) to ensure the metadata has not been tampered with.

### 4. Configuration & Security Controls
- **Hosted Runners**: Metadata collection is mandatory to ensure consistent troubleshooting visibility for users.
- **Self-Hosted Runners**:
  - Metadata collection is disabled by default; admins can enable it via the config flag.
  - Sensitive fields (e.g., hostname, local IP) are redacted by default; admins can opt to include them via an additional flag (`include-sensitive-metadata`) with explicit consent.
  - The command allowlist for self-hosted runners is fixed in the runner codebase and updated via runner releases to prevent security risks from untrusted commands.

## Rationale
- **Modularity**: The pluggable collector design allows easy extension for future metadata types (e.g., cloud instance details for hosted runners) without modifying core runner logic.
- **Efficiency for Hosted Runners**: Using a pre-built manifest avoids runtime command execution, reducing job start latency and ensuring consistent, accurate software version info.
- **Flexibility for Self-Hosted Runners**: Optional controls balance transparency with security and performance, allowing admins to tailor metadata collection to their needs.
- **Consistency**: A standardized format makes logs easier to parse for both end-users (troubleshooting job failures) and automated tools (support bots, analytics pipelines).","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision
### 1. Explicit Approval
Approve the addition of `steps.<id>.outcome` and `steps.<id>.conclusion` fields to the steps context, as proposed. This feature will enable granular conditional execution of downstream steps based on the results of prior steps.

### 2. Field Definitions
To ensure clarity and consistent usage, the fields will be defined as follows:
- **`steps.<id>.outcome`**: A string representing the raw, unmodified execution result of the step. Valid values are:
  `success`, `failure`, `cancelled`, `skipped`. This reflects the actual outcome of the step’s execution, regardless of `continue-on-error` or other workflow configuration overrides.
- **`steps.<id>.conclusion`**: A normalized string representing the step’s effective status as it impacts workflow progression. Valid values are:
  `success`, `failure`, `cancelled`. This accounts for configuration like `continue-on-error`: if a step fails but `continue-on-error: true` is set, the conclusion will be `success` even if the outcome is `failure`.

### 3. Usage Guidelines
- Use `outcome` when reacting to the actual execution result of a step (e.g., running debug logs only if a test step failed, even if `continue-on-error` is enabled).
  Example: `if: steps.test.outcome == 'failure'`
- Use `conclusion` when basing workflow progression on the step’s approved status (e.g., proceeding to deployment only if the build step’s conclusion is `success`).
  Example: `if: steps.build.conclusion == 'success'`

### 4. Compatibility & Migration
- This change is fully backward-compatible: existing references to `steps.<id>.outputs` will continue to work without modification.
- No breaking changes are introduced; the fields are additive and will only be used explicitly by users who need conditional execution based on step results.

### 5. Implementation Mandates
- Ensure both fields are populated consistently for all step types (script steps, action steps, composite steps).
- Update official documentation to include:
  - Clear definitions of `outcome` and `conclusion`.
  - Practical examples of conditional execution using these fields.
  - Notes on how `continue-on-error` interacts with the `conclusion` field.
- Validate edge cases:
  - For skipped steps, `outcome` will be `skipped` and `conclusion` will not block workflow progression unless explicitly enforced via conditional logic.
  - For cancelled workflows, all in-progress steps will have `outcome` and `conclusion` set to `cancelled`.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision

### Core Decision
Implement a new command-line argument `--labels` (short form `-l`) for the runner configuration scripts (`config.sh`/`config.cmd`) to support specifying custom labels during automated self-hosted runner registration. This argument will accept a comma-separated list of labels, which will be merged with the built-in OS/architecture labels (e.g., `Linux`, `X64`) to form the final set of labels registered for the runner.

### Supporting Implementation Details
1. **Argument Syntax**:
   - The `--labels` flag takes a comma-separated string of label names (e.g., `./config.sh --url <repo-url> --token <token> --labels nodejs-18,ci-worker,docker`).
   - Short form `-l` is supported for brevity (e.g., `./config.cmd -u <org-url> -t <token> -l python3.10,gpu-enabled`).

2. **Label Handling**:
   - **Deduplication**: Automatically remove duplicate labels (including overlaps with built-in OS/architecture labels) to avoid redundant entries.
   - **Validation**: Reject labels containing invalid characters (spaces, commas, or special characters not permitted by GitHub Actions label rules) with a user-friendly error message.
   - **Merging**: Custom labels specified via the flag will be appended to the built-in labels (OS, architecture) unless explicitly excluded? No—built-in labels are always retained, as they are required for GitHub Actions to recognize the runner's environment.

3. **Persistence & Retention**:
   - The combined set of labels (built-in + custom) will be stored in the runner's `config.yaml` file under the `labels` key.
   - During runner restarts or reconfigurations:
     - If reconfiguring without the `--labels` flag, existing custom labels from `config.yaml` will be preserved.
     - If reconfiguring with a new `--labels` value, the new custom labels will replace the old ones (built-in labels remain unchanged).

4. **Backward Compatibility**:
   - Existing registration workflows that do not use the `--labels` flag will continue to function as before, only registering the default OS/architecture labels.

### Rationale for This Choice
This approach directly addresses the core context need of scripted automation for self-hosted runner configuration by:
- Providing an explicit, scriptable interface to set labels during initial setup (avoiding manual edits to config files).
- Aligning with common CLI patterns for list-based arguments (comma-separated values are widely recognizable in automation scripts).
- Ensuring persistence of labels across runner lifecycle events, which is critical for reliable operation.
- Maintaining backward compatibility with existing runner deployments.

This decision refines the direction hinted at in ADR275 by prioritizing CLI-driven automation as the primary user experience for script-based runner configuration.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","### Decision
We will implement a multi-layered fault tolerance strategy to handle transient faults with third-party authentication providers, consisting of the following core components tailored to auth-specific workflows:

1. **Idempotent Request Retry Policy**
   - For safe, idempotent authentication operations (e.g., JWT token validation, token refresh, provider metadata retrieval), deploy an exponential backoff retry mechanism with jitter. Configure a maximum of 3 retry attempts and a 10-second maximum total delay. Exclude non-idempotent operations (e.g., initial OAuth2 authorization code flow initiation) from retries to avoid duplicate state or provider-side inconsistencies.

2. **Circuit Breaker Pattern**
   - Integrate a circuit breaker (using a library like Resilience4j or cloud-native service mesh tooling) around all calls to third-party auth providers. Configure the breaker to trip after 5 consecutive failures, enter an open state for 30 seconds to reduce load on the provider, and allow probe requests to test for recovery. When tripped, the breaker will fail fast instead of initiating retries, preventing cascading failures in our system.

3. **Cached Public Key Fallback for JWT Validation**
   - For JWT-based auth flows (e.g., Google OAuth2), maintain a local cache of the provider’s public signing keys. Refresh these keys every 24 hours when the provider is healthy. When the provider is unreachable (circuit breaker open or retries exhausted), use cached keys to validate JWT signatures locally, ensuring uninterrupted authentication. Invalidate the cache immediately once the provider recovers to use the latest keys.

4. **Client-Side Rate Limiting**
   - Enforce provider-aligned rate limits on our system’s outgoing auth requests, tracking request counts per client ID/tenant using a distributed rate limiter (e.g., Redis sliding window counters). Reject requests that exceed the limit with a 429 Too Many Requests response to proactively avoid triggering provider-side rate limits.

5. **Monitoring & Alerting Pipeline**
   - Track metrics including auth provider success/failure rates, retry attempt counts, circuit breaker state transitions, fallback activation frequency, and rate limit violations. Set up alerts for:
     - Circuit breaker open states lasting >5 minutes
     - Retry failure rates exceeding 20%
     - Fallback activation (indicating sustained provider unreachability)
     - Exceedance of client-side rate limits
   - Log granular context (request ID, client ID, provider response code) for all auth provider interactions to support root-cause analysis.

### Rationale (Tied to Context)
- **Retry Mechanism**: Directly mitigates temporary outages or rate limits by giving the provider time to recover, with jitter preventing thundering herd issues.
- **Circuit Breaker**: Stops futile retries during sustained provider unavailability, reducing unnecessary load on the provider and protecting our system from cascading timeouts/failures.
- **Cached Key Fallback**: Ensures continuous authentication capability even when the provider is down—critical for maintaining system availability without compromising security (since JWT keys are rotated infrequently and refreshed when the provider is healthy).
- **Client-Side Rate Limiting**: Proactively avoids provider rate limits, a common cause of transient auth faults, by respecting provider constraints upfront.
- **Monitoring & Alerting**: Enables rapid detection of persistent issues beyond transient faults, allowing our team to intervene early (e.g., escalate to provider support) and maintain visibility into auth pipeline health.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","### Decision

1. **ADR Format & Template**: Adopt the Michael Nygard-style Architectural Decision Record (ADR) template as the standardized format for documenting all non-trivial project decisions. The template will include mandatory fields:
   - Context (the problem or scenario that prompted the decision)
   - Decision (the explicit choice made)
   - Status (Accepted, Superseded, Withdrawn, etc.)
   - Consequences (positive, negative, and neutral outcomes of the decision)
   - Participants (internal and external contributors involved in the decision)
   - Date (when the decision was finalized)
   This template is chosen for its widespread industry recognition, clarity, and compatibility with plaintext/Markdown, ensuring accessibility for all contributors.

2. **Storage & Version Control**: Host all ADRs in the project’s public repository under a dedicated `/docs/adr` directory, version-controlled alongside source code. Each ADR will be named using a sequential number + descriptive title (e.g., `0001-adopt-nygard-adr-format.md`) to maintain chronological order and ease of reference. Version control ensures ADRs are tracked in the project’s history, remain accessible to all, and do not become disconnected from the context that led to their creation.

3. **Open Collaborative Process for ADRs**:
   - **Initiation**: Any contributor (internal team member or external community member) may draft an ADR for decisions that affect project architecture, contributor workflows, or long-term product direction. Drafts must follow the standardized template.
   - **Review & Feedback**: All ADR drafts will be submitted as public pull requests (PRs) to the repository. A minimum 72-hour comment period is required to allow time for community input. The core team will explicitly invite feedback via project communication channels (e.g., GitHub Discussions, public Slack) to ensure outside contributors have opportunities to weigh in.
   - **Finalization**: Consensus (or a documented majority vote, with dissenting opinions noted in the ADR’s Consequences section) will be required to accept an ADR. Merged PRs signify the decision is finalized, and the ADR’s Status is set to ""Accepted.""

4. **Historical Maintenance**: When a decision is superseded by a new one, a new ADR will be created explicitly referencing the outdated ADR. The old ADR’s Status will be updated to ""Superseded"" with a link to the new decision, preserving the full historical context of how the project’s thinking evolved. This ensures the record remains durable and does not become ""out of date""—it instead captures the timeline of decision-making.

5. **Accessibility Guidelines**: All ADRs must be written in clear, plain language (minimizing unnecessary jargon) and include diagrams (stored in `/docs/adr/assets`) for complex architectural decisions to aid understanding for new members and non-technical contributors.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂÃÂ¡ÃÂÃÂª with all relevant discussion summarized ÃÂÃÂ¡ÃÂÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision

### 1. Standardize a Content-Addressable URI Schema for Extension Identification
Adopt a dedicated `ext://` URI schema to uniquely and unambiguously identify EDN extension files in decentralized storage. The schema enforces content-addressable identifiers (critical for integrity) and optional semantic versioning:
- **Format**: `ext://<protocol>/<content-identifier>?version=<semver>`
  - `<protocol>`: Mandatory (e.g., `ipfs`, `arweave`) to specify the decentralized storage layer.
  - `<content-identifier>`: Mandatory content hash/TX ID (e.g., IPFS CID `QmXYZ123...`) for tamper-evident verification.
  - `<semver>`: Optional (SemVer 2.0.0 compliant) to target a specific extension version.
- Rationale: Ensures extensions are traceable, verifiable, and consistent across decentralized networks, eliminating ambiguity in identification.

### 2. Implement a Modular Decentralized Resolution Layer
Build a pluggable resolution layer with protocol-specific adapters (IPFS, Arweave, Filecoin) to fetch EDN files:
- Each adapter includes retries (3 attempts) and 10-second timeouts per request to handle transient decentralized network issues.
- The layer prioritizes content-addressable protocols (e.g., IPFS) for built-in integrity checks, with support for non-content-addressable protocols via signature verification as a secondary option.
- Rationale: Enables future-proofing for new decentralized storage protocols while mitigating reliability risks common in peer-to-peer networks.

### 3. Enforce Strict EDN Validation & Security Guardrails
All fetched EDN files must pass three mandatory checks before processing:
- **Schema Validation**: Validate against a predefined EDN schema (e.g., using `clojure.spec` or `malli` for Clojure systems) to ensure required fields (extension ID, entry point, permission scopes) exist and data types are valid.
- **Integrity Verification**: For content-addressable protocols, cross-check the fetched file’s hash against the URI’s content identifier to prevent tampering. For non-content-addressable protocols, require a cryptographic signature (signed by the extension author’s public key) verified against a trusted registry.
- **Permission Scoping**: Require extensions to declare minimal, granular permissions (e.g., ""read user data"", ""access network"") in the EDN file. Reject extensions with overly broad permissions and prompt users to explicitly approve requested scopes before enabling the extension.
- Rationale: Mitigates security risks from untrusted decentralized sources and ensures compliance with least-privilege principles.

### 4. Add Caching & Fallback Mechanisms for Reliability
- **Local Caching**: Cache resolved EDN files locally with a 24-hour TTL, with invalidation triggers for explicit version updates, content hash changes, or user-initiated refreshes. Reduces redundant decentralized network calls and improves load times.
- **Curated Fallback Registry**: Maintain a project-curated centralized registry of popular extension EDN files. If decentralized resolution fails, offer users the option to fetch from the registry (with a clear warning) to ensure access to critical extensions during network outages. Users may opt out of fallback resolution via settings.
- Rationale: Balances decentralized autonomy with user experience reliability.

### 5. Support Semantic Versioning for Extension Updates
Enable extension authors to publish multiple versions to decentralized storage, with the resolution layer:
- Fetching exact version matches when specified in the URI’s `version` parameter.
- Retrieving the latest compatible version (per SemVer rules) if no version is specified, using protocol metadata (e.g., IPFS directory listings tagged with versions) or a companion metadata EDN file in the same storage location.
- Rationale: Facilitates controlled extension updates while maintaining compatibility with existing user setups.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","# Architectural Decision Record: Group Chat Scope for Q2 Beta

## Decision
Exclude private group chat functionality from the Q2 beta release scope. All ongoing development, bug fixing, and support for group chat will be paused immediately. Resources previously allocated to group chat will be redirected to prioritizing and stabilizing 1-1 chat, public chat, and other critical path features required for the beta launch. Existing group chat code will be disabled in the beta build to eliminate associated regressions and security risks, with a formal plan to revisit group chat as a post-beta feature once core chat functionality is validated and stable.

## Rationale
This decision is directly aligned with the context provided:
1.  **Tight Beta Timeline Compliance**: Focusing on 1-1 and public chat (deemed more vital for the MVP chat experience) ensures the team stays on the critical path and meets the Q2 launch deadline without scope creep.
2.  **Eliminate Wasted Effort**: Pausing group chat resolves recurring scope debates that have drained team resources, aligning everyone on clear, prioritized goals.
3.  **Mitigate Quality & Security Risks**: Disabling group chat removes unresolved regressions (tracked via the `group-chat` label) and avoids the elevated encryption/security complexity of private group chat, which could delay the beta or introduce unvalidated security vulnerabilities.
4.  **MVP Value Alignment**: Public chat better serves the MVP’s goal of enabling users to connect with strangers (an open, default experience) compared to private group chat, which is lower-priority for initial user engagement.

## Implications
1.  **Resource Reallocation**: Team members previously assigned to group chat will shift to 1-1 chat, public chat, or critical path tasks (e.g., bug fixes, security hardening) with immediate effect.
2.  **Code Management**: Existing group chat code will be safely disabled (not permanently removed) in the beta build to avoid introducing regressions. A technical spike will be conducted post-beta to assess code reuse and cleanup if group chat is reintroduced.
3.  **Stakeholder Communication**: Product documentation, user-facing messaging, and internal team alignments will be updated to reflect the absence of group chat in the beta.
4.  **Post-Beta Roadmap**: Group chat will be added to the post-beta feature backlog, with prioritization to be reassessed after the beta launch based on user feedback, core feature stability, and resource availability.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","### Architectural Decision
1. **Adopt Ethereum Discovery v5 (Discv5) Protocol**: Replace the hardcoded peer list in the messaging app with dynamic peer discovery using the Discv5 protocol (as defined in EIP-1459 and aligned with the Status.im Discv5 research implementation guidelines).
2. **Bootstrap with a Curated Static Node Set**: Maintain a small, trusted list of static bootstrap nodes (operated within our managed messaging cluster) to initialize the Discv5 discovery process. This avoids relying on untrusted third-party peers for initial connectivity while eliminating the need for hardcoding full active peer lists.
3. **Implement Dynamic Peer Pool Management**: Build app-level logic to maintain a configurable, healthy pool of messaging peers:
   - Periodically refresh peer candidates via Discv5’s distributed hash table (DHT).
   - Evict unresponsive, low-reliability, or non-cluster peers based on predefined metrics (uptime, message delivery success rate, protocol compliance).
   - Ensure the pool size stays within upper/lower bounds to balance connectivity and resource usage.
4. **Integrate Cluster-Specific Peer Prioritization**: Add a peer scoring mechanism (extending Discv5’s native scoring capabilities) to prioritize nodes from our managed messaging cluster. This will use cluster-specific identifiers and reputation signals to ensure the app prefers trusted, high-performance server peers over untrusted external nodes.
5. **Decouple Cluster Scaling from App Deployments**: Configure our messaging server cluster to auto-register new nodes in the Discv5 DHT upon deployment. This allows independent scaling of the cluster without requiring app updates or hardcoded peer list changes.
6. **Add Discv5-Specific Observability**: Integrate metrics for peer discovery success rate, peer pool health, protocol message latency, and scoring outcomes into existing monitoring tools to track reliability and troubleshoot connectivity issues proactively.

### Alignment with Context Objectives
This decision directly addresses the core pain points and goals outlined:
- Eliminates hardcoded peer limitations, enabling seamless scaling of the messaging cluster as new nodes are added.
- Improves high availability and reliability by allowing the app to fail over to healthy, dynamically discovered peers if existing nodes become unresponsive.
- Enhances security via curated bootstrap nodes and peer scoring, reducing exposure to untrusted or malicious peers and mitigating Sybil attack risks.
- Enables independent cluster management (scaling, updates) without app-level changes, aligning with DevOps best practices for maintainability.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","### Architectural Decision
1. **Standardize Mnemonic and Seed Derivation**:
   - Adopt **BIP-39** as the exclusive industry-standard framework for mnemonic phrase generation, validation, and seed derivation. This includes strict adherence to the official BIP-39 wordlist, mandatory checksum validation during mnemonic creation/import, and use of the specified PBKDF2-HMAC-SHA512 function with 2048 iterations for seed generation.
   - Implement **BIP-44** with the Ethereum coin type (`m/44'/60'/0'/0/{index}`) as the default derivation path for generating Ethereum addresses from mnemonic phrases. This path is the de facto standard across leading wallets (MetaMask, Trust Wallet, Ledger, etc.), ensuring address parity when importing/exporting mnemonics.

2. **Cross-Wallet Import/Export Workflow**:
   - For mnemonic imports: Automatically derive addresses using the BIP-44 Ethereum path first. Add a fallback option for users to manually select alternative standard derivation paths (e.g., BIP-49 for wrapped segwit, BIP-84 for native segwit, or legacy non-standard paths used by older wallet versions) if the default path does not yield the user’s expected address.
   - For new wallet creation: Generate mnemonics strictly following BIP-39 and derive addresses via the BIP-44 Ethereum path, ensuring newly created Status wallets can be seamlessly imported into any BIP-39/BIP-44 compliant wallet.

3. **Backward Compatibility for Existing Users**:
   - Maintain full support for the legacy key derivation pipeline used by existing Status wallets to avoid disrupting current users. Introduce an optional migration flow that allows users to switch their existing wallet to the standard BIP-39/BIP-44 pipeline, enabling cross-wallet compatibility with their existing mnemonic phrases.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","# Architectural Decision Record (ADR) Decision

## Decision: Dual-Track Ethereum Connectivity Strategy (Interim Stopgap + Accelerated ULC Adoption)

### 1. Immediate Interim Measures (0–3 Months)
a. **Reduce Infura Reliance via Multi-Upstream Node Support:**
   - Launch a default multi-upstream RPC node configuration that includes multiple trusted third-party providers (e.g., Infura, Alchemy) and allows users to add custom self-hosted/community nodes via a simplified UI. Implement automatic failover and request distribution across nodes to spread trust, enhance reliability, and mitigate single-point-of-failure risks.
   - Document and promote self-hosted node setup guides for advanced users to further reduce third-party dependency.

b. **Optimize LES for Production Usability:**
   - Enable CHT (Checkpoint Hash Trie) sync by default for LES to drastically reduce initial sync time and bandwidth usage, referencing the existing `Updating CHT BloomTrie` wiki documentation for implementation.
   - Deploy network-aware adaptive sync throttling: dynamically adjust LES sync speed and request frequency based on network type (e.g., throttle on mobile data to conserve battery/bandwidth; full speed on WiFi).
   - Add automatic disk pruning for LES with sensible default limits (e.g., 10GB) to prevent excessive disk space consumption.

c. **Quantify LES Performance Impact:**
   - Conduct a targeted performance audit using anonymized beta user telemetry to measure LES’s exact impact on sync latency, CPU/battery usage, and disk footprint. Use these insights to refine interim optimizations and prioritize ULC development tasks.

### 2. Medium-Term Transition (3–6 Months)
a. **Accelerate ULC Integration:**
   - Allocate 2–3 core engineering resources exclusively to ULC (Ultra Light Client) development, testing, and production integration. Prioritize:
     - Multi-node connectivity to enable cross-verification across untrusted nodes, enhancing decentralization and security as outlined in the context.
     - Compatibility with core app features (wallet, chat, dapp browsing) to ensure a seamless user experience during transition.
     - Performance benchmarking against optimized LES and Infura to validate improvements in sync speed, resource efficiency, and reliability.
   - Roll out ULC as an opt-in beta feature for power users first, then expand to a gradual full rollout once stability and performance meet production standards.

### 3. Long-Term Strategic Commitment (6+ Months)
a. **Phase Out Centralized Defaults:** Replace Infura as the default upstream provider with ULC once it is proven reliable and performant.
b. **Deprecate Unoptimized LES:** Retain the optimized LES configuration as a fallback option for users who prefer local light client operation, but sunset unoptimized LES setups.
c. **Iterate on ULC:** Establish an ongoing feedback loop with users to refine ULC’s energy efficiency, low-traffic network performance, and sync smoothness, addressing the remaining pain points listed in the context.

---

## Rationale
- **Decentralization & Security:** The interim multi-upstream node support reduces exclusive reliance on Infura, while ULC’s multi-node connectivity provides a trust-minimized, decentralized solution aligned with the team’s long-term goals.
- **Immediate User Relief:** Optimized LES configurations directly address the identified performance issues (slow sync, disk bloat, bandwidth/energy usage) without extensive overhauls, improving beta user experience while ULC matures.
- **Aligned with Consensus:** Accelerating ULC adoption reflects the team’s shared view that it is the most promising long-term path to avoid third-party node dependency.
- **User Choice:** Maintaining multiple connectivity options (optimized LES, multi-upstream RPC, ULC) empowers users to balance their priorities of decentralization, performance, and convenience.

---

## Trade-Offs
- **Short-Term Engineering Overhead:** The dual-track strategy (optimizing LES + building ULC) requires additional resources, but this is offset by reduced long-term technical debt and user dissatisfaction.
- **UI/UX Complexity:** Supporting multiple connectivity models adds minor UI complexity, which is mitigated by clear defaults and simplified setup workflows.
- **ULC Timeline Risk:** Accelerated development reduces but does not eliminate delays, but interim measures provide a robust fallback to ensure app reliability during transition.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","# Architectural Decision Record: Encapsulate Sensitive Data in a Dedicated Redacted Type

## Decision
We will introduce an immutable, type-safe wrapper class/struct (e.g., `SensitiveString`) to encapsulate all sensitive data (such as user passwords, API keys, or PII) that must not appear in logs or accidental print statements. This type will enforce safe handling of sensitive data by default, with explicit controls for intentional access to raw values.

### Key Requirements for the `SensitiveString` Type
1.  **Immutable Value Storage**: The underlying sensitive data is set at construction time and cannot be modified post-initialization to prevent accidental or malicious tampering.
2.  **Redacted String Representation**: Override default string conversion methods (e.g., `toString()` in Java/Kotlin, `__str__` in Python, `fmt.Stringer` in Go) to return a fixed redacted value (e.g., `[REDACTED]` or `[SENSITIVE_DATA]`). This ensures that if the type is included in log statements, debug prints, or error messages, no sensitive data is exposed.
3.  **Explicit Raw Value Access**: Provide a clearly named, documented method (e.g., `unsafeUnwrap()`, `getRawSensitiveValue()`) to retrieve the raw, unredacted value. This method will include prominent documentation warning about the security risks of logging or exposing the raw value, forcing developers to acknowledge intentional access to sensitive data.
4.  **Safe Serialization**: Ensure the type is handled securely during serialization (e.g., JSON, XML, database persistence):
    - Exclude the raw sensitive value from serialization by default (via annotations like `@JsonIgnore` in Jackson, or custom serializers).
    - If serialization is unavoidable, serialize only the redacted string representation.
5.  **Static Analysis Enforcement**: Add lint rules or static analysis checks to:
    - Flag instances where raw strings are used to hold sensitive data (e.g., password parameters in functions).
    - Warn against passing the result of `unsafeUnwrap()` directly to log methods or other output mechanisms.

## Rationale
This decision addresses the core problem of accidental exposure of sensitive data in logs by shifting from reactive manual redaction to proactive, type-enforced safety:
- **Centralized Control**: All handling of sensitive data is encapsulated in one type, eliminating the need to rely on developers remembering to redact values in every log statement.
- **Explicitness**: The type makes sensitive data handling obvious in code, improving readability and reducing the risk of accidental exposure via implicit string conversions.
- **Immutability**: Prevents unintended modifications to sensitive data that could lead to leaks or security breaches.
- **Compatibility**: Can be incrementally adopted in existing codebases by wrapping raw sensitive strings at input boundaries (e.g., user authentication endpoints) and propagating the `SensitiveString` type throughout the code instead of raw strings.

## Implications
1.  **Refactoring Effort**: Existing code that uses raw strings for sensitive data will need to be refactored to use the new `SensitiveString` type, starting with input layers and propagating outward.
2.  **Developer Awareness**: Teams will need to be trained on the new type, its purpose, and the safe use of the `unsafeUnwrap()` method.
3.  **Serialization Adjustments**: Existing serialization logic may need updates to handle the new type correctly (e.g., adding custom serializers/deserializers) to avoid unintended data leaks.
4.  **Static Analysis Integration**: Requires configuring or adding lint rules to enforce usage of the type, which may take initial setup time but reduces long-term risk.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision

1. **Deprecate and Retire the Jailed JavaScript Execution Environment**  
   Immediately mark the existing jailed JS extension API as deprecated, halt all further development, bug fixes, and maintenance of the jail system. This includes removing all documentation related to the jail API from public and internal resources.

2. **Migrate All Existing Command Logic to Native ClojureScript/Re-frame**  
   Refactor all command functionality currently split between the jailed environment and hardcoded app logic into unified, native implementations. This will:
   - Eliminate hardcoded ""magic"" logic scattered across the codebase.
   - Enable proper unit testing of command behavior (no reliance on asynchronous jail communication).
   - Simplify logic flow by removing RN Bridge ping-pong overhead and asynchronous call chains.

3. **Adopt Project Pluto as the Sole Extension Framework**  
   Prioritize development and integration of the Project Pluto extension framework as the official method for extending Status functionality. This framework will:
   - Replace all use cases previously intended for the jailed API (including command message customization).
   - Support additional host hooks (wallet assets/collectibles, chat contacts, etc.) to enable richer, more flexible extensions.
   - Use a declarative design aligned with Status’s existing ClojureScript/Re-frame architecture, ensuring better performance and maintainability.

4. **Transition and Communication Plan**  
   - For any potential (though currently non-existent) third-party extensions relying on the jailed API, communicate the deprecation timeline and provide guidance on migrating to Project Pluto once it reaches stable release.
   - Ensure Project Pluto’s documentation is comprehensive and accessible, highlighting its advantages over the deprecated jail system for both internal and future external extension developers.

5. **Remove Jail-Related Infrastructure**  
   Gradually strip all RN Bridge code, jail execution logic, and interop layers from the codebase once all existing command logic is fully migrated to native implementations. This will reduce resource consumption and simplify the codebase’s overall architecture.

## Rationale
This decision directly addresses all critical downsides of the existing jail system:
- Eliminates poor code quality and split logic by unifying command implementations in native code.
- Resolves performance bottlenecks caused by RN Bridge communication.
- Abandons a poorly maintained, underused API in favor of a future-proof, declarative extension model (Project Pluto) that supports richer extension scenarios.
- Aligns with real-world usage patterns (no active third-party jail-based extensions, while regular DApps rely on standard web3.js APIs).","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision

To implement the Tribute to Talk (TtT) anti-spam system aligned with the whitepaper’s requirements, the following architectural decisions are made:

1.  **Smart Contract Escrow Module**
    Deploy a dedicated SNT-compatible smart contract (integrated with existing Status SNT token infrastructure) to manage cold contact deposits in escrow:
    - Hold sender deposits securely until a recipient reply event is validated.
    - Automatically transfer escrowed SNT to the recipient upon proof of a reply.
    - Return deposits to the sender if the recipient does not reply within a user-configurable timeout (default: 30 days) or explicitly rejects the cold contact.

2.  **User-Controlled Preference System**
    Build a decentralized preference layer allowing users to:
    - Set a minimum required SNT stake for cold contacts (with a system-enforced floor to deter trivial, spam-friendly stakes).
    - Enable/disable TtT enforcement for all cold contact requests.
    - Whitelist trusted addresses (e.g., existing contacts, verified services) to bypass the stake requirement entirely.
    - Store preferences on IPFS with cryptographic links to the user’s Status wallet for integrity and ownership.

3.  **Message Routing & Stake Validation Pipeline**
    Modify the Status chat infrastructure’s message routing layer to include a mandatory TtT check for cold contacts:
    - Before delivering a cold message, validate that the sender has deposited at least the recipient’s minimum required SNT into the escrow contract.
    - Block the message and notify the sender of the required stake if validation fails.
    - If validation passes, escrow the funds, mark the message as ""TtT-attested,"" and deliver it to the recipient’s inbox with a clear indicator of the deposited stake.

4.  **Reply Attestation & Escrow Execution**
    Implement a secure, user-friendly workflow to trigger escrow actions:
    - When a recipient replies to a TtT-attested message, the client generates a signed cryptographic proof of the reply (tied to the original message ID and recipient wallet address).
    - Submit this proof to the smart contract, which verifies the signature and context before executing the SNT transfer to the recipient.
    - Use meta-transactions for reply attestations to eliminate gas costs for recipients when claiming stakes.

5.  **Gas Optimization & UX Feedback**
    Minimize on-chain transaction overhead and improve user experience:
    - Batch escrow return transactions for senders with multiple expired cold contacts to reduce gas fees.
    - Provide real-time notifications: alert senders when deposits are escrowed or returned, and recipients when stake transfers complete.

6.  **Lightweight Dispute Resolution**
    Establish a decentralized dispute process for edge cases (e.g., failed transfers, false reply attestations):
    - Allow users to submit disputes with supporting evidence (transaction records, signed proofs) to a rotating committee of SNT stakers.
    - Resolve disputes via majority vote, with committee members incentivized by SNT rewards for fair resolutions and penalized for malicious behavior.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision

### 1. Decision Statement
We will implement a dedicated **Area Owner** role for each of the five defined mobile app sections (Communities, Syncing & Onboarding, Activity Center & Shell, Wallet, Messaging). Each Area Owner will be a tenured member of the mobile development team with deep expertise in their assigned section, tasked with targeted oversight and quality-focused responsibilities to resolve the app’s lack of polish and fill existing oversight gaps. This role will operate under the direction of the existing mobile team lead to preserve centralized team alignment while delegating granular section accountability.

### 2. Core Responsibilities of Area Owners
To directly address the identified gaps in polish and oversight, each Area Owner will own:
- **Section Quality & Polish**: End-to-end accountability for their section’s UI/UX polish, functional consistency, edge-case handling, and performance. This includes proactively identifying and prioritizing tasks to fix visual inconsistencies, unrefined interactions, and overlooked user flows.
- **Design Alignment**: Serving as the primary liaison between the mobile team and design team for their section. Responsibilities include validating design specs for feasibility, ensuring pixel-perfect implementation of design standards, and resolving alignment conflicts to maintain a cohesive, polished user experience.
- **Oversight & Governance**: Leading section-specific code reviews to enforce consistent coding practices, design system adherence, and quality bars; conducting weekly section audits (UI walkthroughs, bug bashes) to catch unpolished areas; and documenting section-specific standards to guide all team members working on the section.
- **Planning & Execution**: Collaborating with the team lead to integrate polish-focused tasks into sprint backlogs, balancing feature development with quality improvements, and tracking progress on section-specific polish goals.
- **Knowledge Sharing**: Mentoring junior team members on section-specific best practices, maintaining up-to-date documentation, and facilitating cross-team knowledge transfers to reduce silos.

### 3. Reporting & Collaboration Structure
- All Area Owners will report directly to the existing mobile team lead, ensuring centralized strategic alignment while delegating granular oversight.
- Weekly syncs between the team lead and all Area Owners will align cross-section priorities, escalate blockers, and track polish progress.
- Biweekly cross-functional syncs (Area Owners + Design Team representatives) will resolve alignment gaps and validate ongoing polish improvements.

### 4. Selection Criteria for Area Owners
Candidates must demonstrate:
- Deep technical and functional expertise in their target app section.
- Strong attention to detail and a track record of delivering polished, high-quality work.
- Experience collaborating with design teams and understanding of UI/UX best practices.
- Ability to lead peer reviews and guide team members effectively.

### 5. Success Metrics
We will measure effectiveness over 3 sprints using:
- 30% reduction in design-related bugs (visual inconsistencies, misaligned components) in target sections.
- 90%+ approval rate from the design team on section-specific deliverables.
- 25% increase in completed polish tasks (defined by design/product) in target sections.
- Positive user feedback indicating improved perceived polish in assigned sections.

This decision directly addresses the lack of oversight by distributing granular section accountability, ensuring dedicated focus on polish, and strengthening dev-design alignment—all while preserving the team lead’s ability to manage overall team strategy.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","# Architectural Decision Record: Handling Product Option Values Collection in Sylius API Platform Implementation

## Decision

To balance efficiency and ease of use for the Product Option values collection, we adopt the following multi-faceted approach aligned with Sylius and API Platform best practices:

### 1. Data Loading & Query Optimization
- **Default Lazy Loading**: Configure Doctrine ORM to use `LAZY` loading for the `values` collection by default, avoiding unnecessary data fetching when values are not required (e.g., listing Product Options without their values).
- **Conditional JOIN FETCH**: Use API Platform’s normalization context to trigger a `JOIN FETCH` for the `values` collection only when the `product_option:read` serialization group is requested. This eliminates n+1 query issues when values are needed, while preserving lightweight requests when they are not.
  - Implementation: Define a custom Doctrine query extension or use API Platform’s native `doctrine.joins` context option to inject the `JOIN FETCH` clause for the `values` association when the relevant serialization group is active.

### 2. Resource Representation & Endpoint Design
- **Embedded Nested Collection for Read Operations**: Embed Product Option Values directly within the Product Option resource response when using the `product_option:read` group. This simplifies client consumption by providing all relevant data in a single request.
- **Dual Write Operations**:
  - **Nested Payloads**: Allow creating/updating Product Options with embedded values in a single request (via the Product Option’s main endpoint) to reduce round-trips and improve usability.
  - **Optional Sub-Resource Endpoints**: Expose `ProductOptionValue` as an API Platform sub-resource under `ProductOption` for granular operations (e.g., adding/removing a single value without modifying the entire Product Option). This caters to advanced use cases like bulk updates of large value collections.

### 3. Persistence Configuration
- **Doctrine Cascade & Orphan Removal**:
  - Set `cascade={""PERSIST"", ""MERGE""}` on the `values` association to automatically persist new values and merge updates when the parent Product Option is saved.
  - Enable `orphanRemoval=true` to ensure values removed from the collection are deleted from the database, maintaining data consistency during updates.
- **Nested Denormalization**: Configure API Platform’s denormalization context to support nested input for values, allowing clients to include value data directly in the Product Option payload for create/update operations.

### 4. Validation & Data Integrity
- **Nested Validation**: Apply `@Assert\Valid()` to the `values` collection to propagate validation constraints (e.g., `NotBlank` on value code/label) from the `ProductOptionValue` entity to the parent Product Option resource.
- **Uniqueness Constraint**: Implement a custom validation constraint to ensure all values under a single Product Option have unique codes/labels, preventing duplicate entries.

### 5. Performance & Scalability
- **Pagination for Sub-Resources**: Enable pagination on the `ProductOptionValue` sub-resource endpoint to avoid transferring large datasets in a single response.
- **HTTP Caching**: Configure cache headers (e.g., `Cache-Control: public, max-age=3600`) for Product Option responses including values, leveraging API Platform’s caching capabilities to reduce repeat database queries.

## Rationale
- **Efficiency**: Lazy loading + conditional `JOIN FETCH` minimizes database load by only fetching values when needed, while atomic nested requests reduce transaction overhead.
- **Ease of Use**: Embedded values simplify common read/write workflows (single request for full Product Option management), while sub-resources offer flexibility for edge cases.
- **Consistency**: Doctrine’s cascade operations and orphan removal ensure data integrity without manual boilerplate, aligning with Sylius’s domain model conventions.
- **Maintainability**: The approach leverages native API Platform and Doctrine features, avoiding custom workarounds and keeping the codebase aligned with framework standards.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision

We adopt a **Standardized Request-Driven Localization Strategy** for all locale-aware Sylius resources to resolve inconsistency, reduce frontend overhead, and improve user experience. The core mandates of this strategy are:

### 1. Centralized Locale Resolution with Priority Hierarchy
Implement a unified locale resolution mechanism that uses the following priority order to determine the target locale for each request:
   - **Authenticated User’s Stored Locale**: If the requester is an authenticated customer, use their saved `locale` attribute (validated against the store’s supported locales; ignored if invalid).
   - **Accept-Language HTTP Header**: Parse the header, match the highest-quality language tag to the store’s supported locales, and select the best-matching valid locale.
   - **Explicit `locale` Query Parameter**: Allow frontend developers to override automatic resolution with an optional query parameter (e.g., `?locale=fr_FR`), provided the value is in the store’s supported locales list.
   - **Store Default Locale**: Final fallback if no other valid option is found.

### 2. Standardized Response Structure for Localized Content
Replace the current inconsistent approach (returning all translations or partial localized fields) with a unified response format:
   - **Default Response**: Return only the resolved locale’s value for each locale-aware field (e.g., `name`, `description`) as a top-level attribute. This reduces payload size and eliminates frontend translation-filtering work.
   - **Opt-In Full Translations**: Add an `include_translations` query parameter (e.g., `?include_translations=true`) that appends a `translations` object to the response. This object maps supported locale codes to their full set of translated fields for cases where the frontend needs access to multiple languages.
   - **Fallback for Missing Translations**: If a field has no translation in the resolved locale, return the store’s default locale’s translation; if no default exists, return `null` (documented per resource field to avoid ambiguity).

### 3. Uniform Application Across All Endpoints
- **Mandate Consistency**: All existing and future endpoints returning locale-aware resources (Products, Payment Methods, Shipping Methods, etc.) must adhere to the resolution and response rules.
- **Reusable Backend Components**: Develop a shared `LocaleResolverInterface` for locale logic and a `LocalizedResourceNormalizer` for serialization. Integrate these into all resource controllers to eliminate duplication and ensure consistency.
- **Deprecation Plan**: Mark endpoints with inconsistent localization behavior as deprecated, with a migration deadline to the standardized approach by the next minor Sylius release.

### 4. Validation and Transparent Error Handling
- **Locale Validation**: Reject any invalid locale (via query parameter or user attribute) and fall back to the next valid option in the resolution hierarchy.
- **Documented Fallback Logic**: Explicitly document how fallback behavior applies to each locale-aware field in the OpenAPI/Swagger API reference.

### 5. Documentation and Frontend Guidance
- **API Reference Updates**: Document locale resolution rules, query parameters (`locale`, `include_translations`), response structure, and fallback behavior in the official API docs.
- **Frontend Best Practices**: Publish guidance recommending use of the `Accept-Language` header for most cases, the `locale` query parameter for explicit overrides (e.g., language pickers), and automatic handling of authenticated users’ stored locales.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision

1. **Adopt a lightweight Markdown-based Architectural Decision Record (ADR) process** for capturing all significant Terraform Controller design, implementation, and process decisions not covered by formal project proposals. This process prioritizes ease of authoring, readability, and discoverability over rigid bureaucracy.
2. **Repository Structure & Naming Conventions**:
   - Store all ADR files in a dedicated `docs/adr` directory at the root of the repository.
   - Use a sequential integer prefix (starting at 001) followed by a hyphenated short title as the filename (e.g., `001-lightweight-adr-process.md`). This chronological numbering ensures decision history is easy to track and reference.
3. **Standardized Minimal Template**: Each ADR must follow a simplified, consistent template with mandatory sections to balance completeness and low overhead:
   - **Title**: Concise, specific summary of the decision.
   - **Context**: Background problem, constraints, and key alternatives considered that led to the decision.
   - **Decision**: Explicit statement of the choice made (this section).
   - **Consequences**: Enumerated positive, negative, and neutral outcomes to clarify tradeoffs for future contributors.
   - **Status**: Current state (e.g., Proposed, Accepted, Rejected, Superseded, Deprecated).
   - **Supersedes/Superseded By**: Links to related ADRs if the decision replaces or is replaced by another.
4. **Discovery & Maintenance**:
   - Maintain an `index.md` file in the `docs/adr` directory that lists all ADRs with their title, status, and direct link to the full document. This index serves as the primary entry point for contributors to explore past decisions.
   - Require accepted ADRs to be merged into the repository concurrently with (or immediately after) the corresponding implementation PR to ensure documentation stays in sync with code.
5. **Process Integration**:
   - Contributors must draft an ADR for review (via GitHub issue or PR) before implementing significant changes outside formal proposal scope. The ADR must be accepted by core maintainers before the change is merged.
   - Encourage cross-referencing relevant ADRs in code comments, PR descriptions, and user-facing documentation where understanding the decision’s rationale is critical to using or modifying the codebase.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","### Decision
We will adopt a hybrid approach using **GitHub Webhooks as the primary mechanism for real-time pull request (PR) change detection**, supplemented by **periodic polling via the GitHub API as a secondary fallback** to ensure critical PR events are not missed. For restricted environments without public endpoints (e.g., air-gapped systems), polling will serve as the sole detection mechanism.

---

### Rationale
1. **Real-Time Developer Workflow Efficiency**: Webhooks deliver event notifications immediately when PR changes occur (e.g., new commits, label updates, merge requests). This enables faster feedback loops for CI/CD triggers, automated code reviews, or status updates—directly improving developer productivity and workflow responsiveness.
2. **Reduced API Rate Limit Risk**: Unlike polling, webhooks only send data when changes happen, eliminating redundant, repeated API calls. This minimizes exposure to GitHub’s rate limits, which is critical for teams managing large numbers of PRs or integrating with multiple repositories.
3. **Resilience to Delivery Failures**: Webhooks can fail due to network outages, service downtime, or exhausted GitHub retries. Polling acts as a safety net to catch missed critical events (e.g., PR merges, approval status changes) at tuned intervals (e.g., 15 minutes for high-priority events, hourly for low-impact changes).
4. **Flexibility for Isolated Environments**: For internal tools behind firewalls or air-gapped systems where public endpoints are unavailable, polling remains a self-contained, viable alternative without requiring external ingress.

---

### Key Tradeoffs & Mitigations
| Mechanism | Tradeoffs | Mitigations |
|-----------|-----------|-------------|
| Webhooks | Requires a publicly accessible endpoint; risk of spoofed events or duplicate deliveries | Use GitHub Webhook Relay services or reverse proxies to avoid direct exposure; validate all requests with GitHub’s secret signature; implement idempotent processing via GitHub’s `delivery_id` to skip duplicates |
| Polling | Risk of hitting API rate limits; delayed detection for non-critical events | Use conditional requests with `ETag`/`If-Modified-Since` headers to fetch only updated PRs; batch repository checks; tune intervals based on event criticality |

---

### Implementation Guidelines
- **Webhooks**: Subscribe to high-impact PR events (`pull_request.opened`, `pull_request.synchronize`, `pull_request.closed`, `pull_request.labeled`). Implement exponential backoff for failed event processing.
- **Polling**: Use the GitHub REST API’s `List pull requests` endpoint with conditional headers. For critical events (e.g., merges), poll every 15 minutes; for low-priority events (e.g., description edits), poll hourly.
- **Isolated Environments**: Deploy a dedicated polling service with managed API tokens and rate-limit monitoring to avoid throttling.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","# Architectural Decision: Balanced Cross-Namespace Reference Enforcement with Guardrails and Backward Compatibility

## Decision
To address the security risks of cross-namespace references while preserving compatibility with existing Flux-based deployments, we will implement the following multi-layered approach:

1.  **Default to Same-Namespace References for New Resources**
    - Enforce same-namespace validation by default for all three cross-namespace reference points (`Terraform.spec.sourceRef`, `Terraform.spec.dependsOn[]`, `.data.resources[]`) in new cluster installations. This aligns with Kubernetes' namespace isolation best practices and reduces the default attack surface.

2.  **Maintain Backward Compatibility for Existing Deployments**
    - Grandfather all existing resources with cross-namespace references to avoid breaking current installations.
    - Provide a cluster-wide configuration flag (`allowLegacyCrossNamespaceRefs`) to opt into allowing new cross-namespace references without explicit policy guardrails. This flag will default to `enabled` for existing clusters and `disabled` for new clusters.

3.  **Explicit Policy-Driven Opt-In for Cross-Namespace Access**
    - Introduce a namespace-scoped CRD `TerraformCrossNamespacePolicy` that empowers namespace administrators to define granular allowlists for cross-namespace references:
      - For `sourceRef`: Allow references to Flux sources in specified remote namespaces.
      - For `dependsOn[]`: Allow references to objects in specified remote namespaces, and/or grant specified remote namespaces permission to reference objects in the policy's namespace.
      - For `.data.resources[]`: Allow references to resources in specified remote namespaces.
    - Align with Flux's `SourceExport`/`SourceImport` mechanism: Tf-controller will natively support `sourceRef` pointing to local `SourceImport` resources (which reference remote `SourceExport` objects), leveraging Flux's established security model for centralized source sharing.

4.  **Multi-Layer Validation & Enforcement**
    - Deploy an admission webhook to reject any new cross-namespace reference that is not explicitly allowed by a `TerraformCrossNamespacePolicy` or (for legacy mode) the cluster-wide flag.
    - Implement runtime checks in the tf-controller to verify policy compliance before resolving any cross-namespace reference, even for resources created before policy enforcement was enabled.

5.  **Gradual Deprecation of Legacy Cross-Namespace References**
    - Mark arbitrary cross-namespace references (not using policy or Flux `SourceImport`/`SourceExport`) as deprecated in documentation, runtime logs, and CLI output.
    - In the next major version (v1.0+), set `allowLegacyCrossNamespaceRefs` to `false` by default for all clusters, requiring explicit policy opt-in for any cross-namespace access.

## Rationale
- **Security & Compliance**: Defaulting to same-namespace references upholds Kubernetes' namespace isolation guarantees and reduces unauthorized cross-namespace access and permission sprawl. Explicit policies ensure cross-namespace access is intentional and auditable.
- **Backward Compatibility**: Grandfathering existing resources and the legacy flag avoid breaking critical Flux-based deployments where users rely on centralized sources or cross-namespace dependencies.
- **Alignment with Flux Ecosystem**: Leveraging Flux's `SourceExport`/`SourceImport` aligns tf-controller with established community patterns, reducing user friction and reusing existing security tooling.
- **Gradual Migration Path**: The deprecation timeline gives users time to adopt secure, policy-driven patterns without immediate disruption.

## Implications
- **User Impact**: Existing users with legacy cross-namespace references will need to migrate to policy-based allowlists or Flux's `SourceImport`/`SourceExport` before the legacy flag is disabled. We will provide detailed migration guides and tooling to simplify this process.
- **Development Effort**: Requires implementing the `TerraformCrossNamespacePolicy` CRD, admission webhook validation, runtime policy checks, and integration with Flux's `SourceImport`/`SourceExport` resources.
- **Operational Effort**: Cluster administrators will need to configure the legacy flag appropriately and manage namespace-level policies for teams requiring cross-namespace access.
- **Security Improvement**: Reduces the risk of unintended cross-namespace access and permission escalation, making the tf-controller more secure by default while still supporting valid use cases like centralized source management.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","### Architectural Decision Record: Decision for Single TF Object Deletion
---

#### 1. Overarching Decision
We will implement a cached, Workspace BLOB-driven finalization workflow to resolve single TF object deletion, decoupling the process from live dependent resources (Source, Secrets, ConfigMaps) and ensuring reliable cleanup of provisioned infrastructure. This decision prioritizes single object deletion as the foundational fix for subsequent dependency and namespace deletion challenges.

---

#### 2. Specific Sub-Decisions & Implementation Details

##### 2.1 Workspace BLOB Dependency for Finalization
Modify the TF object finalization process to exclusively use the cached Workspace BLOB (stored in the controller’s internal storage) for executing Terraform destroy operations. This eliminates reliance on upstream dependent resources that may have been deleted or made unavailable before the TF object’s deletion is initiated.

##### 2.2 Mandatory, Lifecycle-Tied Workspace BLOB Caching Strategy
- **Cache Requirement**: For every TF object that completes a successful apply operation, persist its corresponding Workspace BLOB in the controller’s storage and retain it for the full lifecycle of the object.
- **Cache Freshness**: Automatically update the cached BLOB whenever the TF object’s configuration (Source, Secrets, ConfigMaps) changes and a new apply operation succeeds. If an apply fails, preserve the last valid BLOB to ensure deletion capability is not lost.
- **Cache Purge**: Only delete the cached BLOB from storage after:
  - The TF object’s finalization process completes successfully (Terraform destroy is done and the object is marked for deletion in Kubernetes), or
  - The object is deleted without ever having a successful apply (and no infrastructure was provisioned). For failed destroy attempts with transient errors, retain the BLOB to support configurable retries.

##### 2.3 Modified Finalization Workflow for Single TF Object Deletion
Update the TF object finalizer to execute the following steps:
1. **Retrieve Cached BLOB**: On detecting a deletion request, fetch the associated Workspace BLOB from storage. If no valid BLOB exists (e.g., object was never applied):
   - Log a clear user-facing warning about missing state and infrastructure cleanup requirements.
   - Remove the finalizer and allow Kubernetes to delete the object, with documentation advising manual cleanup of any provisioned resources.
2. **Reconstruct Workspace**: Unpack the BLOB to restore the complete workspace filesystem (including backend configuration, variable files, and references to the Terraform state).
3. **Execute Destroy**: Spin up a tf-runner instance with the reconstructed workspace and run `terraform destroy` using the embedded state or backend configuration from the BLOB.
4. **Finalize & Cleanup**: 
   - Upon successful destroy completion, remove the finalizer to allow Kubernetes to delete the TF object, then purge the cached BLOB.
   - For transient errors (e.g., tf-runner timeout), retry the destroy operation up to a configurable limit before falling back to the manual cleanup warning.

##### 2.4 Error Handling & Observability
- Add structured logging for all stages of the finalization process (BLOB retrieval, workspace reconstruction, destroy execution, cache purge) to aid in debugging.
- Expose metrics for:
  - BLOB cache hit/miss rates during deletion,
  - Finalization success/failure counts,
  - Fallback trigger events (for cases where no BLOB exists),
  - Retry attempt statistics for destroy operations.

---

#### 3. Rationale
This decision directly addresses the core barrier to single TF object deletion: the unavailability of dependent resources preventing clean infrastructure cleanup. By relying on the self-contained Workspace BLOB snapshot, we decouple deletion from upstream resources, ensuring the controller has all necessary data to execute Terraform destroy even if Sources, Secrets, or ConfigMaps are deleted first.

The mandatory caching strategy guarantees a valid workspace snapshot exists for all TF objects that have successfully provisioned infrastructure, which is critical for reliable cleanup. Freshness policies ensure the BLOB reflects the latest valid configuration used to create resources, so destroy operations target the exact state of the infrastructure.

Fallback mechanisms handle edge cases where no BLOB exists, avoiding stuck objects while clearly informing users of manual cleanup needs. This workflow provides a consistent, reliable path to single object deletion, which is a prerequisite for resolving resource dependency and namespace deletion challenges (categories 2 and 3 in the context). Additionally, this approach leverages the existing pipeline for generating Workspace BLOBs, minimizing architectural overhaul while maximizing effectiveness.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Architectural Decision Record: Decision

### Decision
We will replace the maintainer-only, slash-command-triggered E2E testing workflow with a hybrid, multi-target E2E testing framework that prioritizes automated feedback, cross-distribution validation, and local developer testing, while maintaining lightweight guardrails for critical changes. The specific implementation components are:

1. **Automated PR-Triggered Core E2E Tests**: For every pull request, automatically execute a core suite of E2E tests using lightweight, ephemeral environments (e.g., GitHub-hosted runners with `kind` clusters) without requiring maintainer approval. Since no sensitive repository secrets are needed for zarf-init package creation or core test execution, this eliminates manual bottlenecks while preserving security (no exposure of confidential data in untrusted PRs).

2. **Configurable Multi-Distribution Test Matrix**: Implement a scalable test matrix to validate against multiple Kubernetes distributions (kind, k3s, minikube, and managed EKS/GKE for cloud-specific cases) and Linux distributions (Ubuntu, CentOS Stream, Alpine). This matrix will run nightly to catch cross-distribution regressions, and can be manually triggered by maintainers via a slash command (retaining the existing `peter-evans/slash-command-dispatch` tooling for targeted full runs) for PRs impacting cross-platform compatibility. Use containerized runners and Terraform for infrastructure-as-code to provision consistent, disposable environments for each matrix combination.

3. **Local E2E Test Execution Mode**: Develop a self-contained local testing setup that enables developers to run a subset of core E2E tests on their machine without relying on EC2 instances. This mode will use Docker and `kind`/`k3d` to spin up a local Kubernetes cluster, automate dependency setup (e.g., optional local registry for non-core tests), and expose a CLI command (e.g., `zarf test e2e local`) to initiate runs. Document prerequisites and test coverage to guide developers in using this mode for rapid feedback during feature development.

4. **Guardrails for Critical Changes**: Retain branch protection rules requiring maintainer review before merging. Maintainers may trigger full matrix runs for high-impact PRs (e.g., changes to package initialization, cross-distribution logic) to ensure thorough validation, balancing the speed of automated core tests with rigorous checks for changes affecting broader compatibility.

This decision addresses all stated requirements: it enables cross-distribution testing (Kubernetes and Linux variants), provides local testing for faster developer feedback loops, and replaces the restrictive manual trigger workflow with a more agile system aligned with the new secret-free context of Zarf v0.15.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","# Architectural Decision Record: Zarf Image Injection Mechanism for Unmanaged K8s Clusters

## Decision

Zarf will implement a primary in-cluster temporary registry-based image injection approach with a direct node-level fallback, paired with cluster-wide TLS trust configuration, to enable image deployment in K8s clusters lacking native CLI image injection support:

1. **Primary Approach: Temporary In-Cluster Registry with Automated TLS Trust**
   - Deploy a minimal, self-contained, Zarf-built OCI registry as a single-replica StatefulSet in the cluster, using cluster-admin privileges.
   - Generate a short-lived self-signed root CA and server certificate for the registry, then use a privileged DaemonSet to inject this root CA into each node’s system root certificate store (e.g., `/etc/ssl/certs` on Linux nodes) and reload/restart the CRI (container runtime interface) if required to apply the trust update.
   - Push Zarf-packaged images to this temporary registry, configure workload manifests to reference the registry’s ClusterIP DNS name for image pulls, and tear down the registry (and optionally clean up the injected CA) once initial workload deployment is complete.

2. **Fallback Approach: Direct Node Image Injection with Bootstrapping Affinity**
   - For environments where the temporary registry is unfeasible (e.g., immutable nodes, strict resource limits, no permission to restart CRIs), use a privileged DaemonSet to run init containers on each node. These containers will load Zarf-packaged images directly into the local CRI using runtime-specific tools (e.g., `ctr` for containerd, `docker load` for Docker).
   - During bootstrapping, apply temporary node affinity rules to initial workload pods to schedule them on a single pre-injected node, reducing per-node modification overhead until all nodes have the required images loaded. Remove the affinity rules once bootstrapping is complete.

## Rationale

This decision addresses all constraints and requirements outlined in the context:

### Alignment with Dependency-Free Operation
- The primary temporary registry approach eliminates reliance on pre-existing external or in-cluster registries, as Zarf supplies the minimal registry image as part of its core packaging. This ensures compatibility with unknown, dependency-free environments.

### Resolution of TLS Trust Per-Node Challenges
- Injecting the self-signed CA into all nodes via a DaemonSet ensures consistent TLS trust across the cluster for the temporary registry, allowing any node’s CRI to pull images without certificate errors. This avoids the need for manual per-node CRI configuration tweaks and works with most common runtimes (containerd, CRI-O, Docker).

### Minimization of Node State Disruption
- The primary approach’s temporary registry and optional CA cleanup minimize permanent node modifications, adhering to best practices for cluster immutability. For the fallback, temporary affinity rules reduce the need for immediate full-node image loading, speeding up initial bootstrapping.

### Flexibility for Restricted Environments
- The fallback direct injection method caters to edge cases where registry deployment is blocked (e.g., air-gapped clusters with strict resource quotas, nodes with read-only file systems for root CA directories). It leverages cluster-admin privileges (a standard Zarf requirement) to access nodes and load images directly into the CRI’s local store.

## Tradeoffs
| Primary Approach (Temporary Registry) | Fallback Approach (Direct Node Injection) |
|----------------------------------------|-------------------------------------------|
| Pros: Aligns with standard K8s image pull workflows; minimal permanent node changes; scalable across large clusters. | Pros: No registry overhead; works in resource-constrained or immutable node environments; avoids TLS trust configuration. |
| Cons: Requires restarting CRIs (potentially disruptive in running clusters); uses cluster resources for the registry. | Cons: Modifies node-local image stores (intrusive); slower for large clusters (per-node image loading); requires managing runtime-specific tools. |","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision

1. **Mandate SBOM Coverage**: Require generation and active maintenance of Software Bill of Materials (SBOM) for all software (internal custom code, open-source components, third-party commercial software) deployed or scheduled for deployment on government-owned hardware, to fully comply with Executive Order 14028 (EO14028).

2. **Standardize Machine-Readable Formats**: Adopt SPDX v2.3+ (Software Package Data Exchange) and CycloneDX v1.4+ as the mandatory, interoperable SBOM formats. These formats align with EO14028’s requirements for machine-readability and compatibility with federal compliance tools and audit systems.

3. **Automate SBOM Generation**: Integrate SBOM generation into all CI/CD pipelines for internal software, ensuring an SBOM is automatically created and linked to every build. Approved tools for generation include:
   - Syft (for container and dependency scanning)
   - CycloneDX CLI (for direct project-level integration)
   - SPDX Tools (for format conversion and augmentation)
   Legacy software not in CI/CD will have SBOMs generated via ad-hoc runs of these tools and documented in the centralized repository.

4. **Centralized SBOM Repository & Lifecycle Management**: Deploy a secure, role-based access-controlled (RBAC) centralized SBOM repository (e.g., Dependency-Track, SPDX SBOM Store) to store and track all SBOMs. Each SBOM must be versioned, linked to its corresponding software artifact (binary, container image, source package), and updated whenever the software is patched, modified, or retired to maintain accuracy over its lifecycle.

5. **Third-Party Software SBOM Compliance**: For third-party software deployed on government hardware:
   - Require vendors to provide SBOMs in the mandated SPDX/CycloneDX formats as a pre-condition for procurement or deployment.
   - If a vendor does not provide a compliant SBOM, generate a derivative SBOM using dependency scanning tools (e.g., Grype, Dependency-Check), document limitations of the generated SBOM, and obtain explicit approval from the compliance team before deployment.

6. **Validation & Auditing**: Implement automated SBOM validation in CI/CD pipelines (using tools like sbom-validator, CycloneDX Validate) to enforce format correctness, completeness, and adherence to EO14028 requirements. Conduct quarterly compliance audits to verify all deployed software has an up-to-date SBOM, with gaps resolved within 30 days of detection.

7. **Auditor Access & Transparency**: Provide read-only access to the SBOM repository and supporting documentation (generation processes, validation logs) to authorized federal auditors and internal compliance teams to demonstrate ongoing adherence to EO14028.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision

Implement a dual approach combining a cluster-wide **Mutating Admission Webhook** (optional but default core Zarf component) and enhanced `zarf prepare` functionality to automate mutation of image paths, secret references, and Zarf Git Server URLs in Kubernetes manifests. This solution supports both direct Zarf/Helm deployments and GitOps workflows (Flux CD, Argo CD, etc.), eliminating manual templating for package authors while providing flexible options for diverse cluster environments.

---

### Key Implementation Details
#### 1. Mutating Admission Webhook
- **Deployment**: Deployed by default during `zarf init`, with an explicit `--disable-mutating-webhook` flag for opt-out.
- **Configuration**: Populated with cluster-specific values for the Zarf Registry and Git Server (obtained during initialization).
- **Mutation Logic**:
  - Intercepts `CREATE`/`UPDATE` events for relevant resources (Pods, Deployments, CRDs, Flux `GitRepository`, Argo CD `Application`, etc.).
  - Rewrites image paths to the Zarf Registry if the image is tracked in Zarf’s package metadata or verifiable via a lightweight registry API check.
  - Rewrites Git repository URLs to the Zarf Git Server if the original URL matches a source repository from a deployed Zarf package.
- **Granular Control**: Uses resource annotations (e.g., `zarf.dev/mutate: ""true""`/`""false""`) to opt-in/opt-out resources, with sensible defaults (all Zarf package resources are auto-opted in).

#### 2. Enhanced `zarf prepare` Command
- **Automatic Annotation Injection**: Adds Zarf mutation annotations to manifests during package creation or pre-deployment preparation, removing manual annotation work for authors.
- **GitOps-Ready Output**: Generates either fully pre-mutated manifests (with hardcoded Zarf Registry/Git Server paths) or kustomize overlays with mutation patches, suitable for direct commitment to GitOps repositories.
- **Package Build Integration**: Flags resources requiring mutation during package building, so `zarf prepare` applies transformations automatically without author intervention.

#### 3. GitOps Tool Integration
- **Pre-Configured Components**: Offer Zarf components for Flux CD and Argo CD to streamline setup:
  - For Flux: Configure `GitRepository` resources to pull from the Zarf Git Server and trust the Zarf Registry.
  - For Argo CD: Adjust repository credentials and ensure compatibility with the webhook’s mutation logic.
- **Prescriptive Workflows**: Document when to use the webhook (dynamic, in-cluster mutation) vs. pre-mutated manifests (static, repo-based mutation) based on environment constraints (e.g., air-gapped clusters, security policies).

---

### Rationale
- **Unified Mutation Across Deployment Methods**: Solves the core gap where Zarf’s mutation logic was unavailable to GitOps tools, covering both direct deployments and GitOps workflows.
- **Reduced Author Burden**: Eliminates manual templating or custom logic for Zarf Registry/Git Server references, as mutation is handled automatically via the webhook or with minimal effort using `zarf prepare`.
- **Environment Flexibility**: The dual approach caters to users who prefer cluster-wide automation (webhook) and those needing static, pre-validated manifests (prepare command) for compliance or air-gapped scenarios.
- **CRD and Git Server Coverage**: Explicitly addresses the previously unhandled case of mutating CRD references to the Zarf Git Server.

---

### Consequences
#### Positive
- Package authors no longer need custom templating for GitOps workflows.
- Zarf’s mutation logic is consistent across all deployment paths.
- Users gain granular control over mutated resources via annotations.
#### Negative
- The webhook adds minimal cluster-wide operational overhead (mitigated by cached Zarf metadata).
- Requires additional documentation to guide users on choosing between webhook and prepare-based mutation.
- Initial implementation effort to build/test the webhook, enhance `zarf prepare`, and integrate with major GitOps tools.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","# Architectural Decision Record: Simplify Temporary Registry Image Distribution Workflow
## Decision
Replace the `stage1` Rust reassembly binary and `stage2` Go registry management binary with a minimal, shell script-based injector workflow that directly loads the `registry:2` crane tarball into the `zarf-registry` pod’s container runtime (containerd) without relying on an intermediate Docker registry instance. 

Complement this with configmap optimizations:
1. Retain only split configmaps for the `registry:2` tarball (aligned with Kubernetes’ 1MiB per-configmap limit) and eliminate all configmaps associated with `stage1`/`stage2` binaries.
2. Use the existing Zarf injector pod base image (assumed to include `kubectl`, `crictl`, and `tar`) to execute the workflow, removing the need for custom compiled binaries.

### Core Workflow Implementation
The injector pod will perform the following single-step process:
1. Mount all tarball chunk configmaps into a directory and reassemble the full `registry:2` tarball using `cat` (no Rust reassembly binary required).
2. Use `kubectl exec` to transfer the reassembled tarball to the `zarf-registry` pod’s filesystem.
3. Within the `zarf-registry` pod, run `crictl load` to import the tarball directly into the node’s containerd runtime, making the `registry:2` image available to the pod without pulling from an external or intermediate registry.
4. Exit immediately upon successful load, leaving no temporary resources running.

## Rationale
This decision directly resolves all identified pain points in the current workflow:
1. **Eliminates Binary Maintenance**: Removes the need to track, cross-compile, and distribute the `stage1` (Rust) and `stage2` (Go) binaries, reducing the Zarf ecosystem’s managed binary count by one-third.
2. **Reduces Configmap Overhead**: Cuts the total number of init-related configmaps in half (by removing binary-related configmaps), speeding up cluster init time and reducing API server load.
3. **Cuts Build & CI Overhead**: Eliminates cross-compilation steps for `stage1`/`stage2`, simplifying CI pipelines and reducing build times.
4. **Eliminates Wasteful Resource Usage**: Removes the need to run a full-featured Docker registry to host a single image, reducing temporary CPU/memory consumption in the cluster.
5. **Simplifies Debugging**: Replaces opaque binary-driven logic with a transparent shell script, making it easier for the team to troubleshoot and modify the workflow.

## Tradeoffs
1. **Runtime Dependency**: Relies on the injector pod base image including `kubectl`, `crictl`, and `tar`. If missing, we will add a lightweight init container to the injector pod with these utilities (negligible overhead compared to the current setup).
2. **Security Permissions**: Requires the injector pod to have permissions to `exec` into the `zarf-registry` pod and interact with the node’s containerd. This exposure is minimal, as the injector pod is short-lived and performs only a single, targeted operation before being destroyed.
3. **Reassembly Latency**: Reassembling the tarball via `cat` adds a small amount of time, but this is offset by eliminating the resource-heavy steps of starting a registry, pushing the tarball, and switching to read-only mode.

## Next Steps
1. Modify the Zarf init package build process to split the `registry:2` tarball into configmap-sized chunks (excluding `stage1`/`stage2` binaries).
2. Develop and test the shell script for the injector pod to handle tarball reassembly and direct runtime loading.
3. Update the injector pod manifest to execute the new script instead of invoking `stage1`/`stage2`.
4. Validate compatibility across target Kubernetes distributions and runtimes (containerd, CRI-O).
5. Remove all references to `stage1` and `stage2` from Zarf’s codebase, CI pipelines, and documentation.
6. Monitor init performance post-deployment to confirm reduced configmap load time and resource usage.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","# Architectural Decision Record: Testing Strategy Enhancement for Zarf as a Library

## Decision
We will implement a hybrid testing strategy centered on two complementary pillars:

1. **Strategic, limited-scope unit testing** targeted at high-priority areas:
   a. All public library interfaces (functions, methods, and data structures exposed to downstream consumers) to enforce consistent contract behavior and prevent breaking changes.
   b. Isolated logic modules, edge-case handling, and utility functions that are impractical or impossible to validate with existing end-to-end (e2e) tests (e.g., complex data transformation logic, error recovery pathways requiring hard-to-reproduce conditions).
2. **Targeted partial e2e/integration testing** for cross-component interactions where unit tests alone cannot capture real-world usage patterns (e.g., interactions between Zarf’s library components and external systems like Kubernetes clusters or package registries, where over-simplified mocks would compromise test validity).

## Rationale
This decision directly addresses the core context and constraints outlined:
- **Supports Zarf’s evolution as a library**: Testing public interfaces ensures downstream products rely on stable, contract-compliant behavior, addressing increased scrutiny of Zarf’s exposed APIs and reducing the risk of breaking changes for consumers.
- **Fills gaps in e2e coverage**: Limited unit tests efficiently cover code sections that are hard to reach via e2e testing without imposing the maintenance burden of full system-wide unit tests.
- **Leverages existing team expertise**: Partial e2e/integration tests build on Zarf’s current testing workflow, reducing the learning curve while capturing context-rich interactions that unit tests cannot replicate.
- **Balances coverage and maintainability**: By focusing unit tests on interface contracts (not implementation details), we avoid brittle tests that break during refactoring. Partial e2e tests are kept focused on critical cross-component flows to minimize orchestration complexity.

## Mitigations for Potential Cons
### For Limited Unit Testing
- **Establish clear guidelines**: Document standards for writing unit tests (validate input/output, error conditions, and contract behavior; avoid testing internal private functions unless they contain isolated, complex logic not covered by public interfaces). Enforce these guidelines during PR reviews.
- **Reusable mock library**: Create a shared set of mocks for common dependencies (e.g., external APIs, filesystem access) to standardize test setup and reduce duplication.

### For Partial e2e/Integration Testing
- **Modular test harnesses**: Reuse existing e2e infrastructure (pre-configured test clusters, mock registries) to build lightweight, modular test suites that minimize orchestration overhead.
- **Priority-based test selection**: Only implement partial e2e tests for flows that are critical to library consumers and cannot be validated via unit tests, avoiding over-investment in non-critical scenarios.

## Action Items
1. **Publish testing guidelines within 2 weeks**: Define criteria for unit test targets and interface-focused test standards.
2. **Codebase audit**: Map all public library interfaces and identify hard-to-e2e-test logic modules to prioritize for unit test coverage.
3. **Phase in unit tests**: Start with high-priority public interfaces and isolated logic, integrating tests into the CI pipeline.
4. **Implement partial e2e tests**: Design modular tests for critical cross-component flows, reusing existing e2e infrastructure, and add to CI.
5. **Quarterly strategy reviews**: Adjust priorities, update guidelines, and address maintenance pain points to ensure the strategy remains effective as Zarf evolves.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","# Architectural Decision: Two-Tiered SBOM Generation for Non-Image Zarf Package Artifacts

## Decision
We will implement a two-tiered, user-centric approach to generating SBOMs for non-image artifacts in Zarf packages:

1. **Default Automated Coverage**: Enable out-of-the-box SBOM generation using Syft for all artifacts specified via the `files` and `dataInjections` fields in the Zarf package configuration. This will be mandatory and require no explicit user action.
2. **Optional Git Component Coverage**: Add a package-level configuration flag (e.g., `sbom.includeGitBundledArtifacts`) that, when enabled, triggers SBOM generation for only the subset of files from `git` components that are actually bundled into the package (excluding `.git` metadata and files not explicitly included via Zarf’s git component rules).
3. **Advanced Manual Workflow**: Document a supported, manual workflow for power users to run Syft directly on the Zarf build temporary directory, with clear caveats about potential noise from unbundled transient files.

## Rationale
This decision balances completeness, accuracy, and usability by addressing the gaps in each proposed option while mitigating their downsides:
- **Avoids Noise from Overly Broad Scans**: Rejects Option 1’s approach of scanning the entire build directory, which would introduce confusing false positives from unbundled git repo files (e.g., index files, transient dependencies not included in the final package).
- **Covers Common Use Cases by Default**: Adopts the core of Option 2, ensuring we capture non-image dependencies like k3s and the injector (used in Zarf’s init package) via `files` and `dataInjections`—the most common ways users add non-image artifacts. This guarantees actionable, accurate SBOMs for most users without extra effort.
- **Targeted Flexibility Without Complexity**: Instead of Option 3’s open-ended (and likely underused) flexibility, we provide a focused optional flag to cover git component artifacts. This addresses the limitation of Option 2 (missing git-based bundled files) while keeping implementation simple and the feature easy to discover for users who need it.
- **Empower Power Users**: The documented manual workflow serves advanced users who want full visibility into the build process, without adding complexity to the core tooling.

## Implications
### Implementation
- Modify Zarf’s build pipeline to isolate `files` and `dataInjections` artifacts and pass them to Syft for SBOM generation.
- For git components with the optional flag enabled, build a filtered subset of the repo (excluding `.git` and non-included paths) before running Syft on it.
- Extend the Zarf package schema with the `sbom.includeGitBundledArtifacts` flag and add corresponding CLI overrides (e.g., `--sbom-include-git`).

### Documentation
- Clearly explain default SBOM coverage (files, dataInjections) in user guides and schema references.
- Document when to enable the git component flag (e.g., when bundling artifacts from git repos) and how to interpret the resulting SBOM.
- Provide step-by-step instructions for the advanced manual Syft scan workflow, including caveats about transient file noise.

### User Experience
- Most users receive accurate, relevant SBOMs automatically, with no learning curve.
- Users with git-based non-image artifacts can easily extend coverage via a simple configuration toggle.
- Power users have a supported path to deep-dive scans without being overwhelmed by unnecessary complexity in the core tool.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","### Decision
We will implement a **Zarf Connected Mode** that enables declarative deployment of Zarf packages to Kubernetes clusters in connected environments without requiring prior installation of the Zarf init package. This mode will preserve Zarf’s core declarative deployment and upgrade capabilities while eliminating dependencies on init-package-specific infrastructure.

Key implementation details for this decision include:
1. **CLI and Package Specification Extensions**:
   - Add a `--connected` flag to the `zarf package deploy` command to explicitly opt into Connected Mode.
   - Allow package authors to declare compatibility with Connected Mode via a new `connectedCompatible` boolean field in the `zarf.yaml` metadata, which will trigger automatic validation of package dependencies.
2. **Infrastructure Bypasses for Connected Environments**:
   - In Connected Mode, Zarf will skip checks for and installation of init-package components (e.g., Zarf agent, embedded registry, init CRDs/controllers).
   - Container images referenced in packages will be pulled directly from their upstream public registries (instead of mirroring to a Zarf-managed registry) unless the package explicitly specifies a mirroring requirement.
   - Use native Kubernetes API primitives (instead of Zarf-specific custom resources) for deployment lifecycle management where possible, reducing reliance on init-provided tooling.
3. **Compatibility and Validation**:
   - Maintain strict backward compatibility: Packages that depend on init-package components (e.g., disconnected-focused packages using the Zarf registry) will fail gracefully in Connected Mode with a clear error message, prompting users to either switch to standard mode or adjust the package configuration.
   - Add validation logic to ensure Connected Mode deployments only use packages that do not require init-specific resources (e.g., checking for references to `zarf.dev/registry` annotations or init-only CRDs).
4. **Upgrade Support**:
   - Extend Connected Mode to support declarative upgrades of packages deployed in this mode, using the same direct upstream resource access and native Kubernetes primitives for incremental changes.

### Rationale
This decision directly addresses the context need to leverage Zarf’s declarative deployment and upgrade capabilities in connected environments without the overhead of the init package. It expands Zarf’s utility beyond disconnected use cases while preserving its core value proposition for both disconnected and connected users. By using upstream resources directly, it reduces cluster clutter and deployment time in connected scenarios, making Zarf a more flexible tool for modern Kubernetes workflows.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision

We will refactor and enhance the component `scripts` field to a structured, metadata-rich model that addresses current limitations while maintaining full backward compatibility with existing configurations. The key changes are as follows:

### 1. Adopt a Structured Script Object Model
Replace the current flat structure of string arrays (`before`, `after`, `prepare`) with a flexible object-based format where each script entry includes granular metadata. Legacy array syntax will be automatically migrated to the new model (e.g., `before` array entries become `phase: ""pre-install""` scripts with default settings) to avoid breaking existing packages.

### 2. Expand Lifecycle Phase Coverage
Extend script execution phases to cover the full component and package lifecycle, adding granular events beyond the existing `before`/`after`/`prepare` to support advanced use cases:
- **Package Creation**: `pre-create`, `post-create` (expanding the original `prepare` phase into pre/post create steps)
- **Component Installation**: `pre-install`, `post-install` (replacing the generic `before`/`after` phases with installation-specific labels)
- **Component Upgrade**: `pre-upgrade`, `post-upgrade`
- **Component Destruction**: `pre-destroy`, `post-destroy`

### 3. Add Core Metadata Attributes for Scripts
Each script entry will support the following configurable attributes:
| Attribute          | Purpose                                                                 |
|---------------------|-------------------------------------------------------------------------|
| `command`           | Required: The shell command or path to an embedded script to execute.   |
| `phase`             | Required: The lifecycle phase when the script runs (from the expanded list above). |
| `name`              | Optional: Human-readable identifier for logging, debugging, and dependency references. |
| `condition`         | Optional: Zarf template boolean expression to conditionally run the script (e.g., `{{ eq .OS ""linux"" }}`). |
| `errorHandling`     | Optional: Behavior on failure (`stop` (default), `continue`, `retry` with configurable count/delay). |
| `environment`       | Optional: Map of environment variables to inject into the script execution context. |
| `outputs`           | Optional: List of variables to capture from script stdout/stderr, exposed to other scripts/components via Zarf templates. |
| `priority`          | Optional: Integer to define execution order within the same phase (lower values run first). |
| `dependsOn`         | Optional: List of script `name`s that must complete successfully before this script runs. |

### 4. Enable Cross-Script and Component Data Sharing
Implement support for script outputs to be captured as Zarf variables, which can be referenced in subsequent scripts, component manifests, or other component configurations. This eliminates hard-coded dependencies and enables dynamic workflow automation.

### 5. Add Validation and Tooling
- Introduce a JSON Schema for the new `scripts` structure to validate configurations during `zarf package create` and `zarf package deploy`.
- Add CLI warnings for deprecated legacy syntax, with a migration guide to help users adopt the structured model.

This decision addresses the current limitations by empowering teams to build flexible, resilient, and context-aware workflows while preserving compatibility with existing packages and enabling gradual migration to the enhanced model.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","# Architectural Decision Record: Local Docker Daemon Image Import Strategy

## Decision Statement
Adopt **Crane’s memory-efficient streaming mode as the default method** for importing local Docker daemon images into Zarf, paired with an optional user-facing flag (`--fast-local-images`) to re-enable the original high-speed (but OOM-prone) Crane implementation for resource-rich environments. Additionally, prioritize ongoing optimizations to the memory-efficient path to minimize performance gaps while aligning with future OCI package schema requirements.

## Rationale
This decision directly addresses the critical OOM crash issue, balances performance and reliability, and aligns with long-term architectural goals, as supported by the evaluation data and constraints:

1.  **Resolve Critical OOM Showstopper**: Crane’s memory-efficient mode eliminates the crashing behavior for large images (e.g., 20GB single-layer images) by streaming tar data instead of loading entire layers into memory—fixing the core usability gap for local development with large workloads.
2.  **Superior Performance vs. OOM-Safe Alternatives**: Even in memory-efficient mode, Crane outperforms all other OOM-safe methods evaluated (e.g., Docker CLI, Skopeo). For a cached 20GB single-layer image, Crane’s memory-efficient path completes in 2m10s compared to Docker CLI’s 5m2s, and it is significantly faster than methods that were up to 3x slower than the original Crane implementation.
3.  **Align with Future OCI Schema Changes**: Crane produces OCI-compliant image data, avoiding the legacy Docker save format that would conflict with upcoming package schema changes (e.g., OCI imports, Issue #1319). This reduces technical debt and ensures compatibility with planned features.
4.  **Maintain User Choice for Performance**: The optional `--fast-local-images` flag empowers users with sufficient memory (e.g., 16GB+ RAM) to retain the original fast performance for small-to-medium images, balancing safety for resource-constrained systems and speed for power users.
5.  **Simplified Maintainability**: Leveraging Crane (a existing dependency in Zarf’s stack) avoids the overhead of adopting new tools like Skopeo or reimplementing image handling via the Docker Go Client, reducing long-term maintenance costs.

## Trade-offs Accepted
- **Default Performance Reduction**: The memory-efficient mode is slower than the original OOM-prone implementation but remains faster than all other OOM-safe alternatives and resolves a blocking user issue.
- **Opt-In OOM Risk**: The `--fast-local-images` flag retains the potential for OOM crashes, but this is explicitly documented as an advanced option for users who understand their environment’s capabilities.

## Mitigating Actions
1.  **Implement Default Memory-Efficient Path**: Configure Crane to use streaming mode for Docker daemon imports, ensuring OCI-compliant output and no in-memory tar layer accumulation.
2.  **Add Optional Speed Flag**: Introduce the `--fast-local-images` flag to re-enable the original high-speed Crane implementation, with clear documentation of its OOM risk and use cases (resource-rich environments, small images).
3.  **Optimize Memory-Efficient Path**:
    - Expand layer caching logic to minimize redundant data transfer for repeated imports.
    - Ensure full compatibility with the `skip-sbom` flag to cut processing time (as demonstrated in evaluation data, `skip-sbom` reduces duration by ~50% for cached workloads).
4.  **Document Trade-offs**: Update Zarf’s documentation to explain the default vs. fast mode trade-offs, including recommendations based on image size and available system memory.
5.  **Iterate with User Feedback**: Monitor post-implementation user feedback to refine heuristics (e.g., automatic switch to memory-efficient mode for images above a size threshold) if feasible in future releases.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","# Architectural Decision Record: Zarf + Big Bang Deployment Optimization

## Decision

We will design and implement a **Dynamic, Values-Aware Big Bang Zarf Package Framework** with the following interconnected components and workflows to resolve the Big Bang deployment pain points outlined:

### 1. Context-Aware Flux Prerequisite Automation
The Big Bang Zarf package will include Flux v2 (CRDs, controllers, and required RBAC) as a conditional required component:
- Zarf will automatically validate for a compatible Flux instance in the target cluster prior to Big Bang deployment.
- If no compatible Flux is detected, Zarf will install a Big Bang-certified Flux version as a mandatory prerequisite.
- Users may skip installation via a package flag (`--skip-flux`) if they can validate their existing Flux deployment meets Big Bang’s version and configuration requirements.

### 2. Values-Driven Artifact Filtering Pipeline
We will add a pre-deployment/pre-package-generation pipeline that:
- Accepts a user-provided `values.yaml` (or inline overrides) for Big Bang.
- Renders the full set of deployment resources using Big Bang’s Helm chart and Kustomize manifests (via `helm template` + `kustomize build` with the user’s values).
- Parses rendered manifests to extract:
  - Only the images referenced by enabled Big Bang components.
  - Only the git repositories specified in Flux GitRepository/HelmRepository CRDs for enabled components.
- Populates Zarf’s `zarf.yaml` manifest with this filtered list of images and repositories, eliminating the superset artifact bloat.

### 3. Standardized Values Injection & Manifest Customization
Implement a Zarf-native values merging system to simplify Big Bang configuration:
- Merge user-provided values with Big Bang’s default values, preserving user value precedence.
- Automatically update Big Bang’s base `kustomization.yaml` to reference the merged values file and exclude manifests for disabled components.
- Tie this injection step directly to the artifact filtering pipeline to ensure the filtered artifact list reflects the final, customized deployment configuration.

### 4. Dual-Mode Artifact Management (Airgapped/Online)
Optimize for both airgapped and online deployment scenarios:
- **Airgapped**: The Zarf package will pull and embed only the filtered images and git repository contents during package creation, reducing storage footprint by eliminating unneeded artifacts.
- **Online**: The package will skip embedding artifacts, instead using the filtered list to direct Flux (via Zarf) to pull only required resources from upstream, minimizing network overhead.

---

## Rationale (Tied to Context)
- Addresses Flux dependency: Automates prerequisite installation while allowing flexibility for existing Flux deployments, eliminating manual setup overhead.
- Eliminates storage bloat: Values-driven filtering ensures only artifacts required for the user’s specific deployment are included, resolving the superset image/repo problem.
- Simplifies values injection: Standardized merging and kustomization updates remove complexity from modifying Big Bang’s base structure, while tight coupling to filtering ensures artifact lists align with the final deployment config.
- Aligns with Zarf’s core mission: Dual-mode artifact management supports airgapped workflows (Zarf’s primary use case) while providing flexibility for online environments.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision

### 1. Native OCI Registry Integration for Scalable Package Distribution
Zarf will extend its core functionality to natively support publishing, pulling, and deploying Zarf packages as OCI-compliant artifacts to/from any standard OCI registry (e.g., Docker Hub, GitHub Container Registry, AWS ECR, Harbor).

- **OCI Artifact Encapsulation**: Zarf packages will be wrapped as OCI artifacts using a custom media type (`application/vnd.zarf.package.v1+tar.zst`) to preserve Zarf’s unique package structure while adhering to the OCI Distribution Spec. This ensures compatibility with all modern OCI registries without requiring registry-side modifications.
- **CLI Workflow Integration**:
  - Add `zarf package publish <local-package-path> <oci-registry-uri>`: Converts the local Zarf package into an OCI artifact, authenticates with the target registry (via Docker config files, environment variables, or interactive login), and pushes the artifact to the registry.
  - Add `zarf package pull <oci-registry-uri> <output-path>`: Retrieves the OCI-packaged Zarf package from the registry and converts it back to a standard local Zarf package file.
  - Enable direct registry deployments: Allow `zarf deploy <oci-registry-uri>` to pull and deploy the package in a single step, eliminating the need for intermediate local file storage.

### 2. Supply Chain Security: Native Signing and Verification
Zarf will integrate with industry-standard supply chain security tools to enable end-to-end package signing and verification, aligning with OCI’s existing security ecosystem:

- **Package Signing for Creators**: Provide native support for signing OCI-packaged Zarf packages using Cosign (the de facto standard for OCI artifact signing). Creators can sign during publication via:
  ```bash
  zarf package publish <package> <registry-uri> --sign <key-reference>
  ```
  `<key-reference>` supports local key pairs, cloud KMS services (AWS KMS, GCP KMS, Azure Key Vault), or HashiCorp Vault. Signatures are stored alongside the OCI artifact in the registry (via Cosign’s signature manifest format) or linked to a Rekor transparency log for immutable audit trails.
- **Package Verification for Deployers**: Implement built-in signature validation during package retrieval and deployment:
  - Add a `--verify` flag to `zarf package pull` and `zarf deploy` commands, which validates the artifact’s signature against trusted public keys, Rekor log entries, or pre-configured trust policies.
  - Introduce a Zarf trust store (via `zarf config` commands) to manage trusted signers, allowed keys, and registry-specific verification rules (e.g., requiring signatures for all packages from `internal-registry.example.com`).

### 3. Backward Compatibility and Registry Portability
- Maintain full backward compatibility with existing local package workflows (e.g., `zarf package create`, manual file transfers) to avoid disrupting current users.
- Ensure OCI-packaged Zarf packages work with any standard OCI registry, leveraging OCI’s cross-registry portability guarantees to avoid vendor lock-in.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","### Architectural Decision

To address the gaps in airgap development support and limited Git functionality, we implement two interconnected feature sets in Zarf, designed to preserve native developer workflows while enabling fully offline airgap operations:

---

#### 1. Expand Airgap Development Artifact Support for Language-Specific Libraries
- **Integrate a lightweight, multi-protocol package repository proxy**: Deploy a containerized, self-contained package proxy (using a stripped-down, open-source solution like Nexus Repository OSS or a purpose-built Go-based proxy) that supports major language package managers including npm, PyPI, Maven Central, RubyGems, Go Modules, and Cargo. This proxy will store cached artifacts within the Zarf cluster’s persistent storage.
- **Add a new `dev-repository` component type**: Extend the Zarf package schema to let users define:
  - Target package manager protocols and upstream repository URLs (public or private).
  - Explicit packages (with version constraints) or entire repository indexes to pull and cache during the packaging phase.
  - Authentication credentials for private upstream repositories (via Zarf’s existing secret management system).
- **Automatic environment configuration**: When deployed, Zarf will configure the airgap environment to redirect package manager requests to the internal proxy via cluster DNS overrides, shell environment variable injection, or IDE configuration snippets, eliminating manual tool setup for developers.

---

#### 2. Enable Native Git Client Compatibility Beyond Flux
- **Extend internal Git server with mirroring capabilities**: Upgrade Zarf’s existing Gitea-based internal Git server to support automated mirroring of arbitrary upstream Git repositories (HTTPS/SSH, public/private) during the packaging phase.
- **Add `git-mirror` package schema field**: Allow users to define upstream repos, authentication credentials (SSH keys, personal access tokens), and optional sync rules (e.g., branch filtering, periodic updates for temporary internet-connected airgap refreshes).
- **Transparent Git proxy for native workflows**: Deploy a sidecar Git proxy alongside the internal server that intercepts native Git client requests (clone, fetch, pull) via cluster DNS redirection for common Git domains (github.com, gitlab.com). The proxy routes requests to pre-mirrored internal repos, preserving standard Git command functionality without modifying developer workflows.
- **Backward compatibility**: Maintain full support for existing Flux `GitRepository` objects, enabling the same internal Git server to power both GitOps deployments and native development workflows.

---

#### Unifying Capability
Release a pre-configured ""Airgap Dev Environment Template"" Zarf package that combines the multi-protocol package proxy, Git mirroring, and common language artifact indexes. This template reduces setup time for teams, providing a turnkey solution for offline application development in airgap environments.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","### Decision
To transition Zarf to a reliable, mission-critical GA v1.x.x product, the following mandatory, interconnected actions will be implemented:

---

#### 1. **Strict Semantic Versioning (SemVer) 2.0.0 Compliance for v1.x.x Releases**
- Formalize v1.0.0 as the GA baseline, with a non-negotiable rule: all subsequent 1.x minor/patch releases must be fully backward-compatible (no breaking changes to APIs, CLI syntax, deployment manifests, or data formats). Breaking changes will only be allowed via major version increments (e.g., v2.0.0) following a public deprecation cycle.
- Enforce this policy via automated PR checks that scan for potential breaking changes (e.g., API schema modifications, CLI flag removals) and block merges unless explicitly approved by the architecture team as part of a major version roadmap.

#### 2. **Tiered Mission-Critical Testing Strategy**
- **Core Component Coverage**: Mandate 80%+ unit/integration test coverage for all core Zarf components (package building, air-gap utilities, cluster management) and block PR merges if coverage drops below this threshold.
- **E2E Mission Workflow Validation**: Implement a suite of E2E tests simulating real-world mission-critical scenarios (air-gapped deployments, cluster upgrades, disaster recovery, edge environment resilience) and run these tests on every PR, nightly, and for all release candidates.
- **Customer Environment Beta Program**: Launch a closed beta with 3-5 target mission-critical users to test pre-release builds in their production-like environments (e.g., classified networks, resource-constrained edge clusters). Require sign-off from at least two beta users before finalizing any GA release.

#### 3. **Deprecation Policy for Graceful Compatibility Management**
- Establish a minimum 2 minor version grace period for any feature, API, or CLI functionality slated for removal. During this period, Zarf will emit clear warnings in CLI output, logs, and documentation, along with step-by-step migration paths.
- Maintain a backward-compatibility layer for deprecated features throughout the grace period to ensure existing workflows function without modification.

#### 4. **Self-Service Tooling & Documentation to Reduce Expert Reliance**
- Build a `zarf support diagnose` CLI command that automatically collects system state, logs, and configuration data, runs pre-defined health checks, and provides actionable remediation steps for common mission-critical issues (e.g., air-gap sync failures, deployment timeouts).
- Develop dedicated mission-critical runbooks (deployment, upgrade, disaster recovery) with non-expert-friendly, step-by-step instructions, and integrate links to these runbooks directly into CLI error messages and official documentation.
- Launch a mission-critical user support channel (Slack dedicated channel + GitHub Discussions category) with a 4-hour initial response SLA for critical issues, staffed by Zarf core maintainers and subject matter experts.

#### 5. **GA v1.0.0 Feature Scope Freeze**
- Halt all new feature development for the v1.0.0 release candidate and prioritize resolving critical bugs, filling testing gaps, and validating backward compatibility. New features will be scheduled for minor v1.x releases post-GA, ensuring they do not compromise stability or compatibility.

These decisions directly resolve the context’s pain points: eliminating unplanned breaking changes, expanding testing coverage for mission-critical use cases, and empowering end-users to operate Zarf independently of deep lifecycle experts.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision

We will implement native, transparent support for ECR as an external registry within Zarf’s core functionality, abstracting ECR-specific requirements (token refresh and repository creation) from package creators and deployers, while building a reusable registry extension framework based on the `hooks` concept to support other registries in the future.

### Specific Implementation Choices:
1. **Automated ECR Authentication Token Management**:
   - During `zarf init` when ECR is selected as the external registry:
     - Zarf will create a Kubernetes CronJob configured to run every 11 hours (to avoid expiration gaps) that fetches new ECR authorization tokens using either the user’s initial IAM context or a dedicated cluster-bound IAM role (if specified).
     - The CronJob will update the cluster’s image pull secret with the new token, ensuring continuous validity for both image pulls and pushes without manual intervention.
   - For package deployment: Zarf will validate the token’s freshness prior to any image push operation and trigger an immediate refresh if it is within 1 hour of expiration to prevent deployment failures.

2. **Automated ECR Repository Creation**:
   - When deploying a package to an ECR-configured cluster, Zarf’s core image-handling logic will pre-scan all images included in the package.
   - For each image, Zarf will check if the corresponding ECR repository exists using the same IAM credentials from the init process. If the repository does not exist, Zarf will create it with configurable default settings (e.g., image scan enablement via `zarf init` flags) before initiating the image push.
   - This eliminates the need for pre-deployment repository creation by users or ECR-specific `actions` in packages.

3. **Registry Extension Framework Built on Hooks**:
   - Define a registry extension interface using Zarf’s existing `hooks` architecture, which allows registering handlers for registry-specific operations: auth token refresh, repository pre-creation, push validation, and cleanup.
   - This framework will enable third-party contributors to add support for other registries with unique quirks (e.g., GCR project permissions, ACR webhook requirements) without modifying Zarf core.
   - Ship the ECR extension as part of Zarf’s default distribution, making it available out-of-the-box for users.

4. **Package Agnosticity Guidance**:
   - Update Zarf documentation to explicitly guide package creators to avoid ECR-specific `actions` and rely on core Zarf support instead.
   - Provide migration steps for existing packages using ECR custom actions to remove those actions and ensure compatibility with Zarf’s native ECR handling.

### Rationale:
- This solution resolves both ECR-specific pain points without burdening users with pre-deployment work or requiring package creators to build registry-specific logic.
- By abstracting registry details into core Zarf functionality, packages remain system-agnostic, enabling deployment across multiple registry providers without modification.
- The hook-based extension framework aligns with Zarf’s goal of using hooks to extend core functionality (not work around it) and future-proofs the tool for supporting additional registries with unique requirements.
- It maintains consistency in ECR handling across all packages, reducing fragmentation and potential deployment errors.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","# Architectural Decision Record: Zarf Transformation Library Integration with Pepr

## Decision
Adopt **WebAssembly (WASM)** to compile Zarf’s battle-tested Go transformation library into a consumable module for Pepr’s TypeScript runtime. This approach will reuse the exact Zarf transformation logic directly within Pepr, eliminating duplicate code maintenance while preserving the reliability of Zarf’s hardened library.

## Rationale
1.  **Preserve Battle-Tested Code**: Compiling Zarf’s Go transformation library to WASM allows us to reuse the exact production-validated code without rewriting it to TypeScript, avoiding the risk of introducing bugs in a rewritten implementation and eliminating the need to sync two disparate codebases.
2.  **Low-Latency Execution**: WASM runs in-process within Pepr’s TypeScript runtime (e.g., Node.js/Deno), eliminating network overhead associated with gRPC/REST. This is critical for admission control workflows, where low-latency responses are required to avoid disrupting Kubernetes operations.
3.  **Minimal Operational Complexity**: Unlike gRPC/REST (which require separate sidecar services or network communication), WASM integrates as a self-contained module. This reduces operational overhead, removes moving parts, and simplifies deployment of Pepr.
4.  **Loose Coupling**: The WASM module acts as a decoupled dependency. Changes to Zarf’s transformation library can be propagated to Pepr via updated WASM builds without modifying core TypeScript logic, keeping the two codebases independent while ensuring consistency.

## Rejected Options & Justifications
- **Rewrite to TypeScript**: Rejected because it discards Zarf’s hardened, battle-tested code, introduces significant development effort, and creates a permanent requirement to maintain and sync two duplicate codebases (risking drift and inconsistencies over time).
- **gRPC**: Rejected due to operational complexity (requires deploying a sidecar service, managing network configuration, and service discovery) and network latency that would degrade admission control performance.
- **REST**: Rejected because it has higher latency than gRPC/WASM, relies on verbose JSON serialization, and shares the same operational overhead as gRPC without offsetting benefits.

## Tradeoffs
- **Initial Setup Complexity**: Compiling Go to WASM requires configuring build pipelines to handle type marshalling between Go and TypeScript (e.g., converting Kubernetes resource objects between Go structs and TypeScript interfaces).
- **Debugging Challenges**: Debugging WASM modules is less straightforward than native TypeScript or Go code, though modern tools (e.g., Chrome DevTools WASM debugging, Go’s WASM debugging support) mitigate this.
- **Limited Go Feature Support**: Some low-level Go system calls may not be compatible with WASM, but Zarf’s transformation library focuses on Kubernetes resource manipulation—use cases that align well with WASM’s constrained environment.

## Mitigation Strategies
1.  **Automated WASM Builds**: Extend Zarf’s CI/CD pipeline to automatically compile the transformation library to WASM whenever changes are merged. Publish the WASM module to a package registry (e.g., npm) for easy consumption by Pepr.
2.  **Type Wrapper Layer**: Implement a thin TypeScript wrapper around the WASM module to handle type conversion, error mapping, and function invocation. This will abstract WASM-specific details, making the Zarf transformations feel native to TypeScript developers.
3.  **Debugging Tooling Integration**: Document and standardize debugging workflows using WASM-compatible tools (e.g., Node.js `--experimental-wasm-debugging` flag, Go’s `tinygo` compiler for optimized WASM builds with better debug support).

## Follow-Up Actions
1.  Validate compatibility of Zarf’s transformation library with WASM compilation (e.g., test for unsupported Go features).
2.  Implement a prototype of the WASM module and TypeScript wrapper to verify end-to-end functionality in Pepr.
3.  Update Zarf’s CI/CD pipeline to automate WASM build and publication.
4.  Add documentation for developers on using the WASM-integrated transformations in Pepr.
5.  Conduct performance benchmarks to confirm latency meets admission control requirements compared to alternative approaches.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","# Architectural Decision Record: Unified Package Source Loading and Operation Consistency

## Decision

Unify Zarf’s package source handling by implementing a common abstraction layer, standardizing operation support across all source types, and refactoring core package commands to rely on this interface. This will ensure consistent behavior for all supported package sources across core operations (inspect, deploy, remove, list) where technically feasible, with clear error handling for logically/technically unsupported operations.

### Specific Sub-Decisions:

#### 1. Define a Core `PackageSource` Interface
Create a language-agnostic interface (e.g., Go interface for Zarf’s codebase) that all package sources must implement, with methods aligned to core operations:
- `LoadMetadata() (*PackageMetadata, error)`: Fetch minimal package metadata (name, version, components) without full extraction/deployment.
- `Inspect() (*FullPackageDetails, error)`: Return complete package details (images, variables, deployment logic, dependencies).
- `Deploy(options: DeployOptions) error`: Deploy the package to a target cluster.
- `Remove(options: RemoveOptions) error`: Clean up resources associated with the package (applicable only to in-cluster deployed packages or OCI registry packages).
- `List() ([]PackageReference, error)`: List available packages from the source (e.g., in-cluster Zarf CRDs, OCI registry tags).
- `SupportsOperation(op: OperationType) bool`: Explicitly indicate if the source supports a given operation.

Each source type (LocalTarball, SplitTarball, OCIPackage, InClusterPackage, RemoteHTTPSPackage) will implement this interface, encapsulating source-specific logic.

#### 2. Standardize Operation Support Matrix
Explicitly map valid operations to each source type (with clear rationale for exclusions):
| Source Type               | Inspect | Deploy | Remove | List |
|---------------------------|---------|--------|--------|------|
| Local Tarball (`.tar/.zst`) | Yes     | Yes    | No¹    | No   |
| Split Tarball (`.part...`) | Yes     | Yes    | No¹    | No   |
| OCI Package (`oci://`)     | Yes     | Yes    | Yes²   | Yes  |
| In-Cluster Deployed        | Yes     | Yes³   | Yes    | Yes  |
| Remote HTTPS URL           | Yes⁴    | Yes    | No⁵    | No   |

Notes:
1. Removing local/split tarballs is a filesystem operation outside Zarf’s scope; Zarf will return a guidance message directing users to delete files manually.
2. Remove for OCI packages deletes the manifest/tags from the registry (requires user confirmation and registry permissions).
3. Deploy for in-cluster packages redeploys from cached cluster state.
4. Inspect for HTTPS packages uses HTTP range requests (if supported) to fetch only `zarf.yaml`; falls back to temporary full download if range requests are unavailable.
5. Remote HTTPS packages are external resources; remove operations apply only to the deployed in-cluster instance.

#### 3. Refactor Core Commands to Use the Abstraction
Modify `zarf package inspect`, `deploy`, `remove`, and `list` commands to:
- Accept any valid source identifier (local path, OCI URL, in-cluster name, HTTPS URL).
- Dynamically resolve the correct `PackageSource` implementation via a factory function.
- Delegate all source-specific logic to the interface, eliminating duplicate code in command handlers.

#### 4. Implement Missing Functionality for Inconsistent Sources
- **Split Tarballs**: Add `Inspect` support by reading `zarf.yaml` from the first split part (no full reassembly required).
- **In-Cluster Packages**: Add `Inspect` support by fetching metadata from Zarf’s `ZarfPackage` CRDs and reconstructing package details from cluster state.
- **Remote HTTPS Packages**: Add `Inspect` support with range request optimization and temporary cleanup of downloaded artifacts.
- **OCI Packages**: Add `Remove` support to delete manifests/tags from the registry using OCI distribution APIs.

#### 5. Standardize Unsupported Operation Error Handling
When a user attempts an unsupported operation (e.g., `zarf package remove <https-url>`), return a standardized error message that:
- States the operation is not supported for the source type.
- Explains the rationale (e.g., ""Remote HTTPS packages are external resources; use `zarf package remove <in-cluster-name>` to clean up the deployment"").
- Suggests valid alternative actions (if applicable).

## Rationale
- **User Consistency**: Eliminates confusion by ensuring predictable behavior across all package sources and operations.
- **Maintainability**: Reduces code duplication and simplifies adding new sources (e.g., S3 storage) by only requiring interface implementation.
- **Usability**: Clear error messaging guides users to valid actions, reducing frustration from inconsistent support.
- **Extensibility**: The abstraction layer future-proofs Zarf for new package source types without rewriting core command logic.

## Trade-Offs
- **Development Effort**: Requires refactoring existing source-specific code and implementing missing functionality for under-supported sources.
- **Performance**: Remote HTTPS inspection may have higher latency if range requests are unsupported, but this is mitigated by temporary caching.
- **Complexity**: Adds a small amount of abstraction overhead, but this is offset by long-term maintainability gains.

## Non-Goals
- Supporting logically impossible operations (e.g., deleting an external HTTPS resource from Zarf’s control).
- Changing existing user-facing command syntax (core commands like `zarf package inspect <source>` remain unchanged).
- Adding new operations outside Zarf’s core package workflow scope.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","### Architectural Decision

To address the root causes of composability scaling, OCI skeleton integration gaps, and component filtering bugs, we will implement three core architectural changes aligned with a unified component composability model:

---

#### 1. **Centralized Import Resolution & Validation Engine**
Replace ad-hoc import chain handling with a dedicated, graph-driven engine that:
- Builds a **directed acyclic graph (DAG)** of component imports across all nested layers (resolving arbitrary depth of imports, not just first-level).
- Enforces consistent dependency resolution, cycle detection, and conflict resolution (e.g., overlapping component names across nested imports) for all nodes in the graph.
- Integrates automated test coverage for graph traversal: add tests that validate processing, validation, and test assertions across arbitrary import depths (not just first-layer imports) using synthetic multi-layer import scenarios.

This directly addresses the scaling and test coverage gaps in current import chain handling by ensuring uniform processing of all nested components and validating coverage at every layer.

---

#### 2. **Refactor OCI Skeletons into Core Composability**
Refactor OCI skeleton integration to be a first-class participant in the composability model instead of an afterthought:
- Move OCI skeleton parsing and component extraction into the centralized import resolution engine, so skeletons are converted into component nodes in the composability DAG (treated identically to local components).
- Align skeleton dependency handling, override logic, and component merging with the same rules used for local imports, eliminating bolt-on code paths that bypass composability checks.
- Update the package creation workflow to process OCI skeletons early in the component resolution phase (before filtering or packaging) to ensure consistency with other component sources.

This fixes the disconnect between OCI skeletons and core composability by embedding skeleton processing into the unified import resolution pipeline.

---

#### 3. **Unified Component Filtering Service**
Extract all component filtering logic (including the `only` flag) into a standalone, reusable service module:
- Mandate that all subsystems involving component selection (import resolution, package creation, OCI skeleton processing, and preview workflows) use this centralized service instead of implementing ad-hoc filtering.
- Define a standard filter specification format that supports nested component selection, override propagation, and exclusion rules, ensuring consistent behavior across all use cases.
- Add comprehensive unit tests for the filtering service covering edge cases: nested filter propagation across import layers, conflicting filter criteria, and filter application to both local and OCI-sourced components.

This eliminates create-time filter bugs by enforcing a single source of truth for component selection logic across all workflows.

---

### Guiding Principles
All changes will prioritize:
- **Single Source of Truth**: Core composability logic (imports, skeletons, filtering) is centralized to avoid duplication.
- **Backward Compatibility**: Existing package formats and workflows will be maintained via backward-compatible wrappers around the new engine/service.
- **Testability**: The DAG model and centralized services enable granular testing of composability rules across all edge cases.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision

We will implement a **developer-focused Dev Mode** by extending and exposing the existing YOLO mode via a dedicated CLI command and configurable flags, with enhancements to resolve iterative development pain points while preserving backward compatibility with production workflows. The decision includes the following key components:

### 1. CLI Exposure & Core Workflow
Introduce `zarf package dev` as a new top-level command, designed to replace sequential `zarf package create` + `zarf package deploy` runs for iterative changes. This command will activate an optimized development workflow with optional flags to tailor behavior:
   - `--watch`: Enables live reload, monitoring the package’s source directory (zarf.yaml, manifests, scripts, assets) for changes and automatically syncing updates to the cluster.
   - `--clean`: Targeted cleanup of only the resources deployed by the current Dev Mode session, eliminating the need for full cluster resets when fixing deployment errors.
   - `--use-existing-init`: Reuses a cluster with an existing `zarf init` installation (including its registry and agent) for development in partially disconnected environments.
   - `--connected`: Leverages YOLO mode’s original behavior for fully connected environments, skipping `zarf init` and reusing local container images directly without fetching or packaging them.

### 2. Optimized Dependency Handling
- **Connected Environments**: Skip fetching heavy dependencies (e.g., Docker images) by using local runtime assets directly, mirroring YOLO mode’s intent to avoid redundant downloads.
- **Partially Disconnected Environments**: Reuse cached dependencies from prior `zarf package create` runs (stored in the Zarf cache directory) unless their source references or content have changed, eliminating repeated re-downloads of large artifacts like binaries or images.
- **Incremental Sync**: Detect changes to individual components, manifests, scripts, or config values, and apply only those updates to the cluster (instead of redeploying the entire package) to minimize iteration time.

### 3. Environment Flexibility
- Support clusters both with and without `zarf init`: For clusters without init, use direct Kubernetes API access and local container runtimes; for initialized clusters, integrate with the existing Zarf registry/agent to maintain compatibility with air-gapped development workflows.
- Preserve YOLO mode’s original purpose while expanding it to cater to mixed connectivity scenarios, ensuring Dev Mode is usable by developers in fully connected, partially disconnected, or air-gapped (with cached assets) environments.

### 4. Backward Compatibility
Dev Mode will be fully opt-in, with no changes to existing `zarf package create` or `zarf package deploy` behavior for production package building and deployment. Documentation will be updated to position Dev Mode as the recommended workflow for iterative package development, guiding users toward this optimized path while retaining support for traditional workflows.

This decision directly addresses the core pain points: eliminating repeated full package rebuilds, replacing cluster resets with targeted cleanup, exposing YOLO-like functionality via the CLI, and speeding up iterative development cycles for both connected and partially disconnected environments.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","### Decision
1. **Migrate the documentation site from Docusaurus 2 to Astro (v3+)** as the primary static site generator (SSG) to resolve the core limitations of performance and dependency management.
2. **Optimize performance via Astro’s islands architecture**: Only hydrate interactive documentation components (e.g., search, tabbed content, live code previews) instead of entire pages, reducing client-side JavaScript payload size by 60-70% and improving core web vitals (LCP, FID, CLS) by prioritizing static content delivery.
3. **Streamline dependency management with Astro’s Vite-based toolchain**:
   - Use Vite’s native dependency resolution, tree-shaking, and caching to cut build times by 30-40% and minimize redundant dependencies in production bundles.
   - Adopt `pnpm` as the package manager alongside Vite to enforce strict dependency versioning, reduce disk space usage, and simplify conflict-free dependency updates.
4. **Reuse existing content with minimal refactoring**: Leverage Astro’s built-in MDX/Markdown support to port Docusaurus-flavored MDX files with only minor adjustments (e.g., replacing Docusaurus-specific components like `Tabs` with Astro’s community-maintained equivalents or custom island components).
5. **Implement incremental static regeneration (ISR)** for dynamic content (if applicable) to balance build performance and content freshness, avoiding full site rebuilds for small content updates.

### Rationale Tie-in
This decision directly addresses the stated limitations of Docusaurus 2:
- Performance is improved via partial hydration and reduced client-side JS, resolving slow load times for large documentation pages.
- Dependency management is simplified through Vite’s modern toolchain and `pnpm`, eliminating the monolithic dependency conflicts and bloated build artifacts common in Docusaurus 2.
- Content continuity is preserved, minimizing migration effort while enabling long-term scalability for interactive documentation features.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision

### Core Refactor Strategy: Decouple Logging/UI Concerns, Eliminate Global State, and Standardize Error Handling

We will overhaul Zarf’s logging and user feedback system to address all identified issues via the following interconnected changes:

---

#### 1. Abstract Logging & User UI with Dependency Injection
- Define two distinct, minimal interfaces to separate cross-cutting concerns:
  - **`Logger`**: A diagnostic logging interface focused on structured debug/error tracking, with methods `Trace()`, `Debug()`, `Info()`, `Warn()`, `Error()` that write to files/STDERR (independent of user-facing output).
  - **`UI`**: A user experience interface dedicated to CLI feedback, with methods `Print()`, `Error()`, `SpinnerStart()`, `SpinnerStop()`, `ProgressBarCreate()`, `ProgressBarUpdate()`, and `Success()` (powered by pterm under the hood).
- Inject `Logger` and `UI` instances into all components (CLI command handlers and package-level code) via constructor parameters or function arguments, replacing all references to global `message` or pterm state.
- Provide mock implementations (`MockLogger`, `MockUI`) for unit testing to capture output/state without process exits or side effects.

#### 2. Replace Immediate Fatal Exits with Error Propagation
- Remove all `message.Fatal` calls from package-level code; instead, propagate errors up the call stack to top-level CLI command handlers.
- Restrict process exit logic exclusively to CLI handlers: When an unrecoverable error is received, handlers will first execute registered clean-up tasks (e.g., temporary resource deletion, cluster teardown), then log the error via both `UI.Error()` (user-facing) and `Logger.Error()` (diagnostic), and finally exit with an appropriate non-zero status code.
- Add a guarded `UI.Fatal()` method only for top-level CLI use cases where clean-up is impossible, which triggers a final error log before exiting.

#### 3. Ensure Thread Safety for UI Components
- Update the `UI` interface implementation to make all progress bar/spinner methods thread-safe:
  - Add mutex guards around pterm component state modifications to prevent race conditions in concurrent contexts.
  - Support scoped `UI` instances (e.g., per goroutine) for independent concurrent tasks to avoid shared state conflicts.

#### 4. Deprecate Legacy Global State & Decouple Dependencies
- Introduce a temporary backward-compatibility layer for existing global `message` functions that delegates to a default `Logger`/`UI` instance; mark these global functions as deprecated and phase them out via incremental refactoring.
- Restrict all direct pterm dependencies to the `UI` interface implementation in the CLI layer; package-level code will only depend on the abstract `UI` interface, eliminating tight coupling to pterm.

#### 5. Add Non-Interactive Mode Support
- Implement a `SilentUI` variant of the `UI` interface that suppresses all terminal output (progress bars, spinners, styled messages) for non-interactive/scripting workflows. This implementation will track state for validation but produce no user-facing output, solving the challenge of disabling UI components in automated or multi-threaded scenarios.

---

This decision resolves all identified issues: eliminates test-blocking immediate exits, fixes thread safety risks from global state, reduces coupling between components, and enables flexible control over UI/logging behavior across use cases.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Architectural Decision Record: Schema Versioning and Legacy Feature Cleanup for Zarf v1

### Decision

1. **Adopt Explicit Semantic Schema Versioning**  
   Introduce a mandatory top-level `schemaVersion` field in the `zarf.yaml` file, starting with `1.0.0` for the Zarf v1 release. Apply semantic versioning rules to schema evolution:
   - Patch releases (e.g., 1.0.1, 1.1.2): Maintain full backward compatibility, fixing bugs or adding non-breaking validation improvements.
   - Minor releases (e.g., 1.1.0, 1.2.0): Add non-breaking new features with optional fields, no breaking changes to existing schema fields.
   - Major releases (e.g., 2.0.0): Introduce breaking changes only after following the formal deprecation policy outlined below.

2. **Clean Up V1 Schema by Removing Announced Deprecated Keys**  
   For the v1 launch, permanently remove all deprecated keys previously flagged for removal in v1:
   - `setVariable` (replaced by `setVariables`)
   - `scripts` (replaced by `actions`)
   - `group` (replaced by `flavor`; no auto-migration possible due to functional differences)
   - `cosignKeyPath` (replaced by create-time key path specification)
   These keys will no longer be accepted in `zarf.yaml` files targeting `schemaVersion: 1.x.x`, with strict schema validation enforcing this.

3. **Implement Targeted Backward Compatibility for Legacy Packages**  
   To support pre-v1 packages (without `schemaVersion` or using deprecated keys):
   - **Auto-Migration**: For compatible legacy keys (e.g., `setVariable` → single-item `setVariables`, `scripts` → `actions`), perform in-memory auto-migration during package parsing, with a CLI warning informing users of the migration and encouraging updates to the v1 schema.
   - **Manual Migration Enforcement**: For non-auto-migratable legacy keys (e.g., `group` → `flavor`), return a clear, actionable error directing users to the official migration guide, explaining functional differences and step-by-step updates to align with v1.

4. **Define a Formal Deprecation Policy for Long-Term Schema Stability**  
   - **V1 Lifetime Support**: Any feature deprecated in the v1 schema will remain fully functional for the entire duration of the v1 major version (until v2 is released).
   - **Deprecation Communication**: Deprecations will be documented in release notes, with CLI warnings triggered when using deprecated features (including the major version in which the feature will be removed).
   - **Major Version Transition**: For future major schema versions (e.g., v2), all deprecated features from the prior major version will be removed. Users will receive advance notice of at least 3 minor releases or 6 months (whichever is longer) before the major version launch.

5. **Enforce Schema Validation and Contract Clarity**  
   - Maintain machine-readable JSON Schema definitions for each schema version (starting with v1.0.0) to enable IDE tooling, linting, and external validation of `zarf.yaml` files.
   - For `schemaVersion: 1.x.x` packages, enable strict validation to reject invalid/removed fields immediately, providing precise error messages to guide users to correct their configurations.

### Rationale
This decision leverages the Zarf v1 release to establish a stable, user-centric schema contract while addressing technical debt from unmanaged deprecated features. Explicit versioning builds trust with users by committing to long-term support for v1 features, targeted backward compatibility minimizes disruption for existing users, and the formal deprecation policy ensures transparent communication of future changes. Strict validation in v1 eliminates ambiguity around supported fields and reduces runtime complexity from maintaining outdated code paths.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
