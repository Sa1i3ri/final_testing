context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision
1. **Primary Implementation: Native Lazy Loading**
   - Add the `loading=""lazy""` attribute to all offscreen images and thumbnails in the Storefront. This leverages built-in browser functionality, eliminating external dependencies, reducing bundle size, and simplifying ongoing maintenance.
   - Exclude above-the-fold images from lazy loading to prioritize immediate delivery of critical initial viewport content, ensuring optimal perceived performance for users on page load.

2. **Lightweight Fallback for Legacy Browsers**
   - Implement a minimal, self-contained JavaScript fallback for browsers that do not support native lazy loading (per https://caniuse.com/?search=loading):
     - The fallback will first detect native lazy loading support on page initialization.
     - For unsupported browsers, it will use the Intersection Observer API (with a tiny polyfill for extremely old environments if required) to load offscreen images when they approach the viewport.
   - Avoid integrating third-party libraries like lazysizes unless the lightweight fallback proves insufficient for edge cases or future advanced requirements.

3. **Validation and Documentation**
   - Test the implementation across target browsers to confirm native lazy loading behaves as expected and the fallback activates correctly in unsupported environments.
   - Document the implementation in the Storefront’s technical documentation, including browser support scope, fallback logic, and guidelines for adding new images with proper lazy loading attributes.

4. **Future Iteration Policy**
   - If specific unmet use cases emerge (e.g., advanced lazy loading of non-image media, custom loading thresholds, or responsive image handling beyond native capabilities), evaluate adding a lightweight third-party library only after verifying it delivers clear, necessary value that cannot be achieved with a custom solution.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision
1. **Deprecate the `autoload` parameter** for `OneToOneAssociationField` and `ManyToOneAssociationField` entities in Arachne. Existing usage of `autoload=true` will emit a deprecation warning in development environments and production logs, with a timeline for full removal in the next major release.
2. **Set default behavior to lazy loading** for all `OneToOneAssociationField` and `ManyToOneAssociationField` associations. This ensures associated data is only loaded when explicitly requested, eliminating unnecessary SQL joins, data transfer, and hydration overhead by default.
3. **Introduce explicit eager loading mechanisms** to enable intentional fetching of associated data:
   - Extend query builders with an `include` method that accepts association paths (e.g., `(-> query (include :user/profile) (include :post/comments))`) to specify which associations should be eagerly loaded in a given query.
   - Add optional schema metadata (e.g., `{:arachne.dbschema/eager-load-hint true}`) for association fields where eager loading is the common, intentional use case. This metadata can be leveraged via a `include-hinted` query method to load all associations marked with this hint in one call.
4. **Add static analysis tooling** to:
   - Flag usage of the deprecated `autoload` parameter in schema definitions.
   - Detect potential N+1 query issues caused by unoptimized lazy loading, providing suggestions to use explicit eager loading instead.
5. **Update core modules and documentation**:
   - Migrate all internal Arachne code using `autoload=true` to the new explicit eager loading APIs.
   - Revise guides and best practice documentation to emphasize intentional data fetching, explaining tradeoffs between lazy and eager loading for different use cases.

## Rationale
- Eliminates implicit, global eager loading that caused unnecessary performance overhead (excess joins, data transfer, payload bloat) for unused associated data.
- Promotes transparency: Developers can clearly see which associations are loaded in each query, aligning with Arachne's goal of application transparency.
- Maintains flexibility: Explicit APIs allow for targeted eager loading when needed, without forcing one-size-fits-all behavior on all instances of an association.
- Reduces technical debt: Deprecating `autoload` addresses a known performance weakness while providing a clear migration path for existing codebases.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision
We will implement a combined structure for ADRs that addresses all identified search and discovery use cases while retaining the benefits of area-based organization:

1. **Sequential Numbered Filenames with Headline Slugs**
   - Assign each new ADR a unique, zero-padded sequential numeric ID (e.g., 0001, 0002) to enable easy chronological ordering.
   - Use filename format: `[ID]-[kebab-case-slug-of-headline].md` (e.g., `0005-immutable-application-config.md`).
   - This supports:
     - Case 1 (search by headline): Repository/IDE searches for headline keywords will return clear, unique filenames, reducing ambiguous duplicate results.
     - Case 2 (latest ADRs): The highest numeric IDs correspond to the most recent ADRs, so users can browse any area folder or the central index to find new entries without accessing git history.

2. **Retain Area-Based Folder Structure**
   - Keep existing area-specific folders (e.g., `architecture/`, `modules/`, `config/`) and place each ADR in the folder matching its subject area.
   - This preserves support for Case 3 (browsing by specific area) while maintaining chronological ordering via numeric IDs within each folder.

3. **Central ADR Index File**
   - Create a root-level `ADR-INDEX.md` that lists every ADR with standardized metadata:
     ```markdown
     | ID   | Headline                                  | Area          | Status    | Date Updated | Link                                                                 |
     |------|-------------------------------------------|---------------|-----------|--------------|----------------------------------------------------------------------|
     | 0001 | Modular Application Component Model       | architecture  | Accepted  | 2024-01-15   | [0001-modular-component-model.md](architecture/0001-modular-component-model.md) |
     | 0005 | Immutable Application Config Data         | architecture  | Accepted  | 2024-05-20   | [0005-immutable-application-config.md](architecture/0005-immutable-application-config.md) |
     ```
   - Update this index manually (or via a simple script) whenever an ADR is added, updated, or superseded.
   - This serves as a single source of truth for all ADRs, enabling:
     - One-click access to any ADR from a single page.
     - Easy filtering/sorting by area, status, or date (via markdown table sorting tools or manual scanning).
     - Clear visibility into the full set of architectural decisions without navigating nested folders.

4. **ADR Frontmatter Metadata**
   - Add YAML frontmatter to each ADR file to store machine-readable metadata that aligns with the central index:
     ```yaml
     ---
     id: 0005
     title: Encode Application State as Immutable Config Data
     area: architecture
     status: Accepted
     date_updated: 2024-05-20
     supersedes: None
     superseded_by: None
     ---
     ```
   - This metadata can be used to automate updates to the central index via a simple Clojure or shell script (reducing manual effort) and provides at-a-glance context when opening an individual ADR.

5. **Superseded ADR Workflow**
   - When an existing ADR is superseded, create a new ADR with the next sequential ID.
   - Update the superseded ADR's frontmatter and index entry to set `superseded_by` to the new ADR's ID and link.
   - Update the new ADR's frontmatter and index entry to set `supersedes` to the old ADR's ID and link.
   - Do not remove or modify the content of the superseded ADR, only its metadata.

This structure balances all discovery needs: it retains area-based browsing, enables easy headline searches with unique filenames, provides instant access to the latest decisions via numeric IDs, and offers a centralized overview via the index file.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision
We will reject the proposed `sw-block` + Composition API hook-based extensibility approach due to its reliance on unsafe internal Vue logic and unresolvable data merging challenges. Instead, we will adopt a fully native Vue 3-based extensibility system that maintains the required plugin customization capabilities while adhering to official Vue APIs, enabling full ecosystem tooling support, simplifying Vue upgrades, and improving performance via precompilation.

The system will consist of three core components:

### 1. Template Extensibility (Native Vue Slots + Component Overrides)
- **Granular Template Modification**: Original admin components will expose explicit named and scoped slots at all key extensibility points (e.g., before/after core content, replaceable UI sections). Plugins can use these slots to inject or override specific template parts without replacing the entire component.
- **Full Template Replacement**: For complete template overhauls, plugins can register a custom component under the same name as the original component in the global Vue registry. The system will prioritize the plugin's component over the original, ensuring full swappability.

### 2. Logic Extensibility (Composables + Official Extension Patterns)
- **Reusable Core Logic**: All core component logic will be encapsulated into public, exported Composition API composables. Plugins can import these composables to reuse, wrap, or augment core functionality without relying on internal component state.
- **Granular Logic Hooks**: Original components will expose extension points via Vue's official `provide/inject` API. These hooks will allow plugins to modify specific state properties, computed values, or method outputs without full component replacement (e.g., `provide('modifyProductPrice', (originalPrice) => originalPrice * 1.1)`).
- **Full Logic Override**: Plugins can create wrapper components that inherit the original component's props/emits (via Vue's `extractDefineProps` or official utility functions), combine the original's exported composables with custom logic, and register the wrapper to replace the original component in the registry. This avoids unsafe data merging by leveraging native Vue component composition.

### 3. Standardized Tooling for Plugin Developers
- We will provide a set of official, maintained utilities to simplify plugin extensibility:
  - `overrideComponent`: A helper function to register a plugin component as an override for an original component, automatically inheriting props/emits and handling registry prioritization.
  - `useOriginalComposable`: A utility to safely import and extend an original component's core composable.
  - `injectExtensionHook`: A helper for plugins to access and use extension hooks exposed by original components.
- All public composables, slots, and extension points will be fully documented to ensure plugin developers have clear, stable interfaces to build against.

### 4. Migration Strategy
- We will migrate existing admin components incrementally to this new system, starting with the most frequently extended components.
- Maintain backward compatibility with the existing Component Factory system during migration via a compatibility layer that maps old extension patterns to the new native Vue system.
- Publish a deprecation timeline for the old system once critical components are migrated, with clear guides for plugin developers to upgrade their extensions to the new native approach.

This decision ensures that we meet all original goals: full plugin extensibility, native Vue ecosystem compatibility (enabling linting, static analysis, and precompilation), update-safe reliance on official Vue APIs, and simplified future Vue version upgrades.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision
Arachne will adopt a fragment-based changelog workflow to resolve the issues with the current single-file approach, aligned with trunk-based development practices. This workflow eliminates merge conflicts, removes release version uncertainty, and aligns changelog entries directly with code and Jira issue state. The detailed implementation rules are:

1. **Fragment Structure & Location**
   - All changelog entries are written as individual ""fragment"" files stored in a dedicated `changelog.d/` directory at the root of each Arachne module repository.
   - Each fragment filename follows the convention: `[JIRA-ISSUE-ID]-[short-descriptive-slug].md` (e.g., `ARACH-456-fix-db-connection-leak.md`). The Jira ID prefix ensures uniqueness and avoids filename collisions.
   - Fragments must include:
     - A 1-sentence summary of the change (user-facing language preferred)
     - Explicit linkage to the corresponding Jira issue (via ID and optional hyperlink)
     - A category tag aligned with the Keep a Changelog standard: `Added`, `Changed`, `Deprecated`, `Removed`, `Fixed`, or `Security`
     - For breaking changes or deprecations: Clear upgrade instructions for end users/module developers

2. **Fragment Submission Workflow**
   - Every code change that impacts end users or module developers (new features, bug fixes, breaking changes, deprecations) must include a corresponding fragment in the same pull request (PR) as the code. Trivial changes (comment typos, formatting adjustments) do not require a fragment.
   - PR reviewers will verify that the fragment is present, accurate, and follows the structure rules before approving the PR. This ensures changelog entries are always aligned with code state and linked to their Jira ticket, resolving mismatches between Jira and code.

3. **Release Preparation & Fragment Processing**
   - When preparing a new release:
     1. Collect all unprocessed fragments in `changelog.d/`.
     2. Merge their content into the main `CHANGELOG.md` file, organizing entries under the new release version (with release date) and their respective category sections.
     3. Move processed fragments to an archive directory (e.g., `changelog.d/archive/[release-version]/`) for traceability (do not delete fragments).
     4. Commit the updated `CHANGELOG.md` and archive changes as part of the release preparation PR.
   - This step eliminates release version uncertainty: developers do not need to guess the target version when writing fragments—version assignment happens during release preparation when the exact version is confirmed.

4. **Conflict Mitigation**
   - Since each change adds a unique fragment file (no overlapping edits to existing files), merge conflicts in changelog-related files are effectively eliminated. The only potential collision (duplicate filenames) is prevented by the mandatory Jira ID prefix, as each issue maps to exactly one code change.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision
Implement the Flow Builder DelayAction with a structured data retention, fresh resolution, and dynamic rule re-evaluation strategy to address stale event data and outdated rule issues:

1. **Post-Delay Data Freshness Handling**
   - Do not store full original event payloads in the delayed action queue. Instead, persist only:
     a. Unique, stable identifiers for all core business entities referenced in the event (e.g., order ID, customer UUID).
     b. A minimal context snapshot including the flow instance ID, original event type, delay configuration parameters, and timestamp of delay initiation.
   - When the delay elapses, before executing the action:
     i. Fetch the latest state of all referenced entities from the system of record (primary database, dedicated entity service API) using the stored identifiers.
     ii. Handle entity not found scenarios: Log a critical warning with flow/entity context, and either halt the flow branch or trigger a pre-configured fallback action defined in the flow's error handling rules.

2. **Dynamic Rule Re-Evaluation Post-Delay**
   - On delay expiration, retrieve the latest active rule set associated with the flow and action context:
     a. If the original rule set is no longer present (deleted/deprecated): Log an error, mark the flow step as failed, and route to a ""rule missing"" fallback action if configured.
     b. If the rule set has been updated: Re-evaluate the freshly fetched entity data against the current rule definitions using the same scope/matching logic as the original trigger. Only proceed with action execution if the updated rules still qualify the flow for that step; otherwise, branch to the appropriate alternative path in the flow.

3. **Durable Persistence & Idempotency**
   - Store delayed action metadata in a distributed, persistent queue (e.g., Kafka with log retention, Redis with RDB/AOF) to prevent state loss during system outages.
   - Assign each delayed action a unique execution ID to enable idempotent processing (avoid duplicate execution if the system retries the task post-recovery).

4. **Transparency & Auditing**
   - Maintain a complete audit trail for each delayed action, including:
     - Original delay initiation context (timestamp, event identifiers, rule set ID at trigger time).
     - Post-delay resolution details (timestamp, fresh entity data snapshot summary, rule set ID used for re-evaluation).
     - Outcome of execution (action run, fallback triggered, flow halted).
   - Expose this trail in Flow Builder's introspection tools to enable visibility into how delay handling impacts flow behavior.

This approach aligns with Arachne's goals of modularity (swappable data fetch services, rule engines), transparency (audit trails), and configurability (custom fallback actions for error cases).","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision
Integrate the mature Shopware App System plugin directly into the Shopware 6 core platform as a native, first-class feature.

## Rationale
This decision directly addresses the context goals and maturity of the app system:
1. **Ecosystem Stability Signal**: Integrating into core communicates to partners, app manufacturers, and end-users that the app system is a stable, long-term component of Shopware’s ecosystem, encouraging broader adoption and investment in app development for the platform.
2. **Streamlined Release & Upgrade Alignment**: Tying the app system to Shopware 6’s official release cycle and supported upgrade process eliminates fragmented versioning, ensuring consistent, predictable updates and reducing compatibility gaps between the app system and core platform.
3. **Reduced User Friction**: Removing the requirement to install a separate plugin lowers barriers for end-users to leverage app functionality, making the app ecosystem more accessible and integrated into the default Shopware experience.
4. **Engineering & Maintenance Efficiency**: Integrating into core simplifies cross-team collaboration, reduces plugin-specific overhead (e.g., separate dependency management), and aligns app system development with the broader platform roadmap.

## Key Implementation Steps
1. **Code Migration**: Port the existing app system plugin codebase into the Shopware 6 core, ensuring compliance with core coding standards, architectural patterns, and dependency management workflows.
2. **Data Migration Tooling**: Develop and validate automated migration scripts to transition app configurations, installations, and associated data from the plugin’s dedicated database tables to core platform tables (or adapt core tables to support app system data) without data loss or user disruption.
3. **Backward Compatibility Safeguards**:
   - Maintain full API and functionality parity with the mature plugin version, ensuring existing apps work seamlessly with the core-integrated system with no changes required for app developers.
   - Implement a deprecation path for the standalone plugin: mark it as obsolete, add in-app warnings for users with the plugin installed, and auto-disable the plugin when the core app system is active to prevent conflicts.
4. **Documentation & Guidance Update**: Revise official Shopware documentation to frame the app system as a core feature, updating installation, development, and upgrade guides to remove references to the standalone plugin and clarify core-based workflows.
5. **Cross-Environment Testing**: Conduct end-to-end testing across both cloud and on-premises environments to validate that the core-integrated app system matches the plugin’s performance, stability, and feature set.
6. **Stakeholder Communication**: Proactively announce the integration via release notes, blog posts, and community channels, highlighting benefits for users, app developers, and partners, and detailing the seamless transition process.

## Compatibility Considerations
- **End-User Transition**: For users upgrading to the Shopware 6 version with the core app system, the standalone plugin will be automatically detected, disabled, and migrated to the core feature with no manual intervention required.
- **App Developer Continuity**: No changes will be required for existing apps; the core integration will maintain 100% compatibility with the plugin’s API and extension points.
- **Cloud Environment Rollout**: Cloud deployments will receive the core-integrated app system automatically, eliminating manual plugin installation steps for cloud users.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision

1. **Standardize Core FlowStorer Interface**: Replace all per-storer specific interfaces with a single, universal `FlowStorer` interface that all storer implementations must adopt. This interface will include:
   - A `supports(eventData: EventData): boolean` method to determine if the storer can handle the given event data, eliminating the need for separate identification interfaces.
   - A `store(eventData: EventData): void` method for persisting data.
   - An optional `restore(): T` method (with a default no-op implementation) to support storers requiring restore logic, reducing boilerplate for scalar-focused storers that don’t need this capability.

2. **Implement a Generic Scalar FlowStorer**: Build a reusable `GenericScalarStorer<T>` that handles storage of all scalar types (strings, numbers, booleans, etc.). This storer will accept a target type (to validate event data compatibility) and a storage backend reference in its constructor. It will leverage the standard `FlowStorer` interface, eliminating the need for separate storer classes for individual scalar types.

3. **Establish a Centralized FlowStorer Registry**: Create a `FlowStorerRegistry` to manage all registered storer instances. This registry will:
   - Allow runtime registration/unregistration of storers, enabling plugins to easily add or remove storers.
   - Provide a `getApplicableStorers(eventData: EventData): List<FlowStorer>` method that filters registries using the `supports` method, simplifying event data routing to correct storers.
   - Include a plugin-friendly filtering mechanism (e.g., `excludeStorers(predicate: (FlowStorer) -> boolean)`) to let plugins bypass specific storers without needing to handle per-storer interface checks.

4. **Deprecate Legacy Storer Implementations**: Mark all existing per-scalar storer classes and their associated identification interfaces as deprecated. Provide a clear migration path to the generic scalar storer and standard interface, with plans to remove legacy implementations in a future major release.

### Rationale
- **Reduces Boilerplate**: The generic scalar storer eliminates redundant classes for individual scalar types, and the standard interface removes the overhead of maintaining separate identification interfaces per storer.
- **Simplifies Plugin Integration**: The centralized registry and predicate-based filtering let plugins modify storer behavior without deep knowledge of individual storer implementations.
- **Improves Maintainability**: A universal interface makes adding new storers straightforward, as developers only need to implement the core contract instead of creating custom identification logic.
- **Preserves Backward Compatibility**: The optional `restore` method maintains support for storers with restore logic while providing a default no-op for scalars, ensuring existing functionality works during migration.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision 
We will replace the custom sw-text-editor with a mature, actively maintained open-source WYSIWYG text editor that aligns with our core requirements for stability, maintainability, extensibility, and feature completeness. 

### Implementation & Selection Details
1. **Candidate Evaluation Criteria**: We will select the editor using these mandatory and priority metrics:
   - **Mandatory**:
     - Active maintenance (regular updates, active community/commercial backing, low backlog of critical bugs)
     - Robust core WYSIWYG functionality (text formatting, lists, links, image insertion) with minimal known issues
     - Well-documented API for integration with our administration stack and custom extension
     - Clean, modular codebase with comprehensive documentation to reduce long-term maintenance overhead
     - Compatibility with our existing frontend/backend infrastructure (e.g., content storage APIs, media library workflows)
   - **Priority**:
     - WCAG 2.1+ accessibility compliance
     - Lightweight footprint to avoid UI performance degradation
     - Optional markdown input/output support for power users
2. **Migration Phasing**:
   - Integrate the selected editor into a non-critical administration section for initial testing and user feedback
   - Migrate all existing sw-text-editor instances, ensuring backward compatibility for saved content (e.g., converting existing custom markup to the new editor's format)
   - Deprecate and remove the custom sw-text-editor codebase once all instances are migrated and validated

### Alternatives Considered
1. **Rewriting the existing sw-text-editor**: Rejected, as the current codebase’s architecture is fundamentally hard to maintain; a full rewrite would require significant time/resources without guaranteeing stability, duplicating effort solved by mature open-source tools.
2. **Building a minimal custom editor**: Rejected, as it would fail to meet the ""good feature set"" requirement without extensive ongoing development, leading to the same maintainability pain points as the current editor.

## Rationale
This decision directly addresses all stated goals:
- **Stability**: Mature open-source editors have undergone extensive real-world testing, eliminating most low-level WYSIWYG bugs present in the custom tool.
- **Maintainability**: Active community support and documentation reduce the need for our team to debug opaque custom code.
- **Extensibility**: Well-defined APIs allow us to tailor the editor to our specific needs (e.g., integrating with our media library) without rebuilding core functionality.
- **Modularity**: Most leading open-source editors use plugin-based architectures, making it easy to swap or add features as requirements evolve.

This approach minimizes technical debt, reduces development time, and delivers a reliable, feature-rich editing experience for administration users.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision 
1. **Formalize Default Salutation Value**: Establish ""not_specified"" as the official, inclusive default salutation for all Shopware 6 entities (primarily customers) where a salutation is not explicitly provided by a customer, administrator, or external data source.
2. **Universal Application of Default**:
   - Apply ""not_specified"" automatically during:
     - Front-end customer registration when no salutation is selected.
     - Admin panel customer creation/editing if no salutation is chosen.
     - Data imports (CSV, API) where the salutation field is empty or missing.
     - API requests (both read and write) that do not specify a salutation value.
3. **Data Migration for Existing Records**: Execute a one-time database migration to update all existing customer records with a null or empty salutation value to ""not_specified"". This migration will be backward-compatible, ensuring no breakage in dependent systems (e.g., order history, email templates) during and after execution.
4. **Inclusive UI/UX Adjustments**:
   - In the admin panel, make ""not_specified"" a visible, top-tier selectable option in salutation dropdowns, with clear labeling (e.g., ""Not Specified / Prefer Not to State"") to signal its inclusive intent.
   - In customer-facing front-end flows, set ""not_specified"" as the preselected salutation option to normalize its use, while still allowing customers to choose a specific salutation if desired.
5. **Content Rendering Rules**:
   - For customer-facing communication (order confirmations, transactional emails, account pages), if the salutation is ""not_specified"":
     - Omit gendered salutation text entirely.
     - Use a neutral, respectful fallback (e.g., ""Dear Customer"", ""Hello"") instead of defaulting to gendered alternatives.
6. **API Contract Updates**:
   - Revise Shopware 6’s OpenAPI specifications to explicitly list ""not_specified"" as the default value for all salutation fields in relevant endpoints.
   - Ensure API responses consistently return ""not_specified"" for entities where no salutation is set, rather than null or empty strings.
7. **Backward Compatibility Safeguards**:
   - Maintain support for existing salutation values (e.g., ""mr"", ""mrs"") alongside ""not_specified"".
   - Ensure third-party plugins and custom integrations can still read/write salutation values as before, with the new default being applied transparently when no value is provided.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision
1. **Immediate Removal of Current Autoprefixer**: Remove the `padaliyajay/php-autoprefixer` dependency and its integration into the `theme:compile` workflow. This resolves the significant performance bottleneck in the build process, eliminates a non-sustainable hardcoded solution, and removes unnecessary prefixes that no longer provide meaningful value for our supported browser matrix (aligned with Bootstrap v5’s native browser support).

2. **Validate Remaining Vendor Prefix Requirements**:
   - Conduct a one-time analysis of the compiled storefront CSS using a standards-compliant, `.browserslist`-aware tool (e.g., the official PostCSS Autoprefixer, via temporary CLI integration or a PHP wrapper) to confirm if any properties still require vendor prefixes for browsers specified in our `.browserslist` file.
   - If the analysis confirms no critical prefixes are needed, maintain the build workflow without any autoprefixer step.
   - If a small subset of properties requires prefixing, integrate PostCSS Autoprefixer (or a compatible PHP wrapper) into the workflow to dynamically apply only necessary prefixes based on our `.browserslist` configuration, replacing the hardcoded legacy solution.

3. **Periodic Compliance Review**:
   - Add a quarterly task to re-evaluate vendor prefix requirements, tied to updates to our `.browserslist` file and Bootstrap version, to ensure the build process stays optimized and aligned with supported browser capabilities over time.

4. **Documentation Update**:
   - Revise the project’s build process documentation to reflect the removal of the legacy autoprefixer and any new tooling (if integrated), including rationale tied to performance improvements and browser support alignment.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision
We will implement a multi-faceted approach to ensure pre-release versions of Arachne packages are correctly evaluated against known vulnerabilities while retaining the utility of pre-release testing.

### Specific Action Items:
1. **Enforce npm-compliant SemVer pre-release syntax for correct version ordering**:
   - All pre-release versions will strictly follow Semantic Versioning format: `<major>.<minor>.<patch>-<pre-release-tag>`, where the core version (before the hyphen) is numerically greater than any vulnerable stable version when the pre-release targets a line beyond the vulnerable version (e.g., `1.9.0-alpha.1` for a pre-release following the vulnerable `1.8.7`).
   - If npm’s version comparison logic exhibits a bug causing valid pre-releases to be misordered relative to stable versions, we will temporarily use a core version incremented beyond the vulnerable version (e.g., `1.8.8-alpha.1` instead of `1.9.0-alpha.1` for fixes backported to the 1.8 line) and file a bug report with npm to resolve the root issue.

2. **Add automated supplementary vulnerability checks to the release pipeline**:
   - Integrate a custom step into our pre-release publishing workflow that uses the npm Vulnerability API to explicitly compare the pre-release’s codebase against known vulnerabilities. This check will:
     - Verify the pre-release’s core version is newer than any vulnerable stable version for the package.
     - Confirm the pre-release includes patches for all critical vulnerabilities affecting prior stable versions.
   - Fail the release pipeline if the pre-release is found to be affected by unpatched vulnerabilities, or if npm’s native audit tool fails to recognize the pre-release as a safe version.

3. **Maintain transparency for pre-release vulnerability status**:
   - In pre-release release notes and package documentation, explicitly state:
     - The base stable version the pre-release is derived from.
     - Whether it includes fixes for known vulnerabilities (e.g., ""1.9.0-alpha.1: Pre-release of the 1.9 branch, includes patches for CVE-XXXX-XXXX present in 1.8.7"").
   - For pre-releases that are not affected by vulnerabilities in older stable versions, update the package’s npm metadata to flag this status, reducing user confusion.

4. **Address vulnerabilities affecting existing pre-releases**:
   - If a vulnerability is discovered that impacts a published pre-release, immediately publish an updated pre-release with the fix and deprecate the vulnerable pre-release via npm.
   - Reference the deprecated pre-release in the updated version’s release notes to guide users to upgrade.

### Rationale
- This approach resolves the core problem of missing npm vulnerability alerts by aligning pre-release syntax with npm’s version comparison logic, while adding guardrails to fill gaps in native tooling.
- It preserves the value of pre-releases for testing and community feedback without compromising security visibility.
- It adheres to industry-standard SemVer practices, making versioning predictable for contributors and users.
- The supplementary pipeline checks ensure we proactively catch issues npm’s native audit might miss, keeping Arachne packages secure at every stage of development.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision
1. **Simplify and Optimize the Delete Query**: Rewrite the existing delete condition to use a concise, index-friendly equivalent form:
   ```sql
   DELETE FROM cart
   WHERE COALESCE(updated_at, created_at) <= '2023-02-01'
   LIMIT 1000;
   ```
   This maintains the exact same logic as the original query but frames it as a single condition that can leverage a functional index.

2. **Create a Functional Expression Index**: Add a functional index on the `cart` table to enable fast lookups of outdated entries, avoiding full-table scans:
   ```sql
   -- For PostgreSQL
   CREATE INDEX idx_cart_outdated ON cart (COALESCE(updated_at, created_at));
   
   -- For MySQL 8.0+/MariaDB
   CREATE INDEX idx_cart_outdated ON cart ((COALESCE(updated_at, created_at)));
   ```
   This index directly supports the simplified query condition, allowing the database to quickly identify all rows eligible for deletion without scanning every entry in the table.

3. **Retain Batch Deletion**: Keep the `LIMIT 1000` clause in the delete query. This minimizes lock contention, reduces the duration of individual delete transactions, and prevents performance degradation on high-traffic shops.

4. **Validation**: After implementing the index and query change, run `EXPLAIN` on the updated delete query to verify that the new `idx_cart_outdated` index is being used (confirm `possible_keys` and `key` columns reference the index).

### Edge Case Handling
- If the target database does not support functional indexes (rare for modern SQL databases), use a composite index with a fallback condition:
  1. Create composite index:
     ```sql
     CREATE INDEX idx_cart_updated_created ON cart (updated_at, created_at);
     ```
  2. Split the delete query into two separate batch queries (to leverage the composite index efficiently):
     ```sql
     -- Delete entries with non-null updated_at first
     DELETE FROM cart
     WHERE updated_at <= '2023-02-01'
     LIMIT 1000;
     
     -- Delete entries with null updated_at and expired created_at
     DELETE FROM cart
     WHERE updated_at IS NULL AND created_at <= '2023-02-01'
     LIMIT 1000;
     ```
  This approach ensures the composite index is used for both sub-queries, avoiding full-table scans.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision 
We will implement an explicit **Experimental Feature Program** to enable early, user-feedback-driven releases of incomplete features, while preserving our core backwards compatibility promise for stable functionality. The program will include clear rules for feature designation, user expectations, lifecycle management, and supporting processes to address the context constraints.

### Key Program Components
1. **Explicit Experimental Designation**
   - Functionality ready for user feedback but not yet stable will be labeled as ""Experimental"" with no implied backwards compatibility guarantee. This designation allows for breaking changes, feature modifications, or full removal without advanced warning (beyond standard release notes).
   - Experimental features are excluded from our formal backwards compatibility policy, which remains in full effect for all non-experimental functionality.

2. **Clear User Visibility & Opt-In**
   - All experimental features must be clearly marked across all touchpoints:
     - **Documentation**: A dedicated section in our public docs will list experimental features, including their scope, intended feedback goals, known limitations, and explicit compatibility disclaimers.
     - **APIs**: Experimental endpoints/functions will use a distinct prefix (e.g., `experimental_`) or be tagged in API specs (e.g., OpenAPI `x-experimental` flag) to avoid confusion with stable APIs.
     - **UI**: Experimental UI components will display a prominent visual indicator (e.g., a ""Beta"" badge) to alert users of their status.
   - Experimental features will be opt-in only: users must explicitly enable them via a configuration setting, admin toggle, or explicit consent prompt—they will never be enabled by default.

3. **Lifecycle Management for Experimental Features**
   - **Proposal**: Teams must submit a brief proposal for any feature targeted for experimental release, outlining:
     - The core functionality to be shipped in the experimental iteration.
     - Specific feedback goals (e.g., ""validate demand for multi-account employee management"" or ""identify top requested customizations for advanced search"").
     - Success metrics for graduating to stable status (e.g., user adoption rate, positive feedback score, resolved critical pain points).
   - **Experimental Release & Feedback Collection**:
     - The feature is shipped with its experimental designation, paired with dedicated feedback channels (e.g., a tagged support ticket category, a dedicated forum thread, or a targeted user survey).
     - The team will actively collect and analyze feedback, iterating on the feature (including making breaking changes) in subsequent releases as needed.
   - **Graduation or Retirement**:
     - **Graduation**: When the feature is stabilized, feedback confirms its value, and it meets success metrics, it will transition to ""Stable"" status. From that point forward, full backwards compatibility guarantees apply.
     - **Retirement**: If feedback indicates the feature has limited value, does not align with user needs, or is not worth stabilizing, it will be removed in a future release. A notice of retirement will be included in release notes for at least one cycle before removal to give users time to adjust.

4. **Supporting Processes**
   - **Release Notes**: Each release will explicitly call out new experimental features, changes to existing experimental features, and retired experimental features.
   - **Iteration Transparency**: For ongoing experimental features, release notes will document any breaking changes made since the last iteration to help users stay informed.

### Alignment with Context Goals
- This program allows early releases of incremental feature functionality (e.g., B2B simple employee management or basic advanced search) without locking us into long-term backwards compatibility commitments.
- It enables direct user feedback on feature utility, extendability, and use cases early in development, reducing the risk of building features that do not meet real user needs.
- It preserves the integrity of our core backwards compatibility promise by isolating experimental functionality from stable, production-ready features.
- It delivers tangible business value to users sooner, even while features are still in development.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision
We will refactor Shopware 6's stock handling system to address flexibility, performance, and complexity issues with the following structured changes:

### 1. Introduce a Plugin-Friendly Abstraction Layer
- **Core Interfaces**: Define two primary extension points:
  - `StockProvider`: Standardizes methods for retrieving base stock, reserved stock, and computed available stock for products. Default implementations will use Shopware's internal database, but custom implementations (e.g., ERP integrations) can replace this via the DI container without modifying core code.
  - `StockMutationHandler`: Encapsulates all logic for modifying base stock and managing stock reservations. Plugins can decorate or replace this service to customize how stock is increased/decreased during order lifecycle events, product restocks, or other operations.
- **Extension Hooks**: Expose dedicated domain events for critical stock operations (e.g., `StockReservationCreatedEvent`, `StockDeductedEvent`, `StockReservationReleasedEvent`) to allow plugins to intercept and extend stock workflows without overriding core services.

### 2. Simplify the Stock Data Model
- **Eliminate Redundant Fields**: Remove the `availableStock` field from product stock entities, as it is a computed value that can be derived from base stock and active reservations.
- **Dedicated Reservation Tracking**: Introduce a new `ProductStockReservation` entity to explicitly track stock reserved for orders. This entity will include:
  - Links to product ID, order ID, and order state.
  - Reserved quantity and timestamp of reservation.
- **Computed Available Stock**: Define available stock as `baseStock - sum of active reservations` (active reservations are those linked to orders in non-completed, non-canceled states).

### 3. Optimize Performance for Large Catalogs
- **Database Indexing**: Add composite indexes on the `ProductStockReservation` entity for `product_id`, `order_state`, and `order_id` to speed up aggregation queries for reserved stock.
- **Caching Layer**: Implement a cache for frequently accessed stock values (e.g., available stock for top-selling products) with automatic invalidation when base stock or reservations are modified.
- **Batch Processing**: Support bulk updates for stock reservations during mass order state changes (e.g., bulk completing orders) to reduce database round trips and improve throughput.

### 4. Standardize Order Lifecycle Stock Logic
- **Event-Driven Mutation**: Bind core stock operations to Shopware's state machine events:
  - Create a reservation when an order transitions to a ""reserve stock"" state (e.g., ""open"").
  - Deduct reserved stock from base stock when an order is marked as ""completed"".
  - Release the reservation (return reserved quantity to available stock) when an order is canceled or refunded.
- **Predictable Extension Points**: Document these event bindings clearly, allowing plugins to adjust when stock is reserved/deducted by modifying state machine transitions or subscribing to the associated events.

### Migration & Backward Compatibility
- **Data Migration**: Provide a one-time migration script to:
  1. Calculate initial active reservations from existing open orders.
  2. Validate that `availableStock` equals `baseStock - sum of initial reservations` (flag mismatches for manual review).
  3. Drop the `availableStock` field after validation.
- **Compatibility Layer**: Temporarily retain a deprecated `availableStock` getter on product entities that computes the value on demand for existing plugins, with a warning directing developers to switch to the `StockProvider` interface.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision
1. We will extend the existing exception-to-log-level mapping configuration (already utilized in the cloud product) to target `ShopwareHttpException` instances with HTTP status codes in the 400–499 range, mapping them to the `notice` log level instead of the default `error` level.
2. This mapping will be added to the core Shopware configuration to ensure consistent behavior across both core and cloud deployments. Environment-specific overrides for individual exception classes or status code ranges will remain supported for edge cases.
3. The implementation will leverage the `symfony/monolog-bridge` exception handler’s built-in capabilities to customize log levels based on exception type and attributes (specifically the associated HTTP status code).
4. Default `error`-level logging will be preserved for all other exceptions, including `ShopwareHttpException` instances with 5xx status codes (as these indicate server-side failures requiring analysis), to avoid missing critical runtime issues.
5. We will document this behavior in the API development guidelines: advising developers that throwing a `ShopwareHttpException` with a 4xx status code is the standard pattern for handling client-side invalid requests, and that these events will be captured as notices (for audit/light debugging) without polluting error logs with non-critical noise.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision 
Arachne will adopt **UUIDv7** as the default primary key type for all new database entities across its core modules and official extensions. Existing entities using UUIDv4 will retain full backward compatibility, and developers may explicitly opt to use UUIDv4 for new entities if specific requirements demand it.

### Supporting Rationale
1.  **Index Performance Optimization**: UUIDv7's time-ordered prefix eliminates the severe B-tree index fragmentation caused by UUIDv4's fully random values. This results in more compact indexes, reduced new page allocations during writes, improved cache locality, and better long-term read/write performance for large datasets—addressing the key performance flaw noted in the context.
2.  **Preserve UUID Core Benefits**: UUIDv7 maintains all critical advantages of UUIDs: decentralized ID generation (no need for centralized sequencing), seamless integration across disparate data sources, and collision resistance—without sacrificing the performance gains of time-ordered identifiers.
3.  **Alignment with Best Practices**: Time-ordered unique identifiers are a widely accepted best practice for B-tree indexed databases, as they minimize index churn and streamline storage operations over an application's lifecycle.

### Additional Implementation Details
- **Backward Compatibility**: Arachne will continue to support UUIDv4 as an explicit option for primary keys. Developers can specify UUIDv4 in entity schemas for use cases where time-based metadata in identifiers (e.g., privacy-sensitive systems) is undesirable.
- **Migration Tooling**: For existing applications wishing to transition from UUIDv4 to UUIDv7, Arachne will provide optional migration modules and documentation to guide updating entity schemas, generating new UUIDv7 values for existing records, and adjusting constraints with minimal downtime.
- **Specification Compliance**: Core persistence modules will use a RFC 9562-compliant UUIDv7 generation library to ensure consistent, standard-conforming identifiers across all supported databases.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision
Arachne will implement an extensible, schema-driven system for defining and discovering flow builder events, aligned with its core modular, data-driven, and introspectable goals:

1. **Schema Extension for Custom Events**: Modules and applications will be able to define custom flow event types by extending the Arachne flow configuration schema. Each event type is represented as a distinct entity in the schema with:
   - A unique, namespaced identifier (to avoid conflicts across modules)
   - Human-readable metadata (display name, description) for use in the flow builder UI
   - A set of required/optional configuration parameters (schema-validated) specific to the event's behavior

2. **Dynamic Event Discovery**: The flow builder (both backend logic and UI) will dynamically fetch all registered event types directly from the application's active config schema, rather than relying on a hardcoded list. This ensures all custom events defined by enabled modules are automatically available in the builder without manual updates to core flow builder code.

3. **Modular Registration Workflow**: Custom event definitions must be packaged as part of an Arachne module, with their schema extensions documented and approved via standard ADR processes where applicable. This maintains modularity, allowing event definitions to be swapped or disabled alongside their parent modules with minimal effort.

4. **Data-Driven Event Configuration**: When a custom event is added to a flow, all its configuration details will be stored as structured data in the application's core config value, adhering to the ""everything is data"" principle. This avoids entangling event state with the Clojure runtime and ensures full introspectability of flow behavior.

5. **Introspectability for Custom Events**: The same schema and config introspection tools used for core Arachne entities will be compatible with custom flow events, enabling users to inspect event definitions, parameters, and their usage in flows at any time.

If a custom event's implementation requires executable logic, that logic will be defined as a namespaced function (registered via the module's schema) that accepts the event's config data as input—separating the event's declarative definition from its imperative runtime behavior.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision
We will implement a short-lived, token-based authentication flow for direct app-to-App Server communication, leveraging the existing Shopware Backend-to-App Server shared secret trust relationship to enable secure origin verification and reliable customer context propagation.

### Detailed Implementation Flow
1. **Token Issuance**:
   - When an embedded app loads in the Shopware Storefront, the Storefront first requests an access token from the Shopware Backend. This request includes the target app’s unique identifier and the current customer’s session context (if authenticated).
   - The Shopware Backend generates a JSON Web Token (JWT) with the following claims:
     - `exp`: Expiration timestamp (short TTL, e.g., 15 minutes, to limit exposure if tokens are leaked)
     - `app_id`: The unique identifier of the app the token is intended for
     - `shop_id`: The Shopware shop instance associated with the request
     - `customer_id`: (Optional) The authenticated customer’s identifier (only included if the customer is logged in)
     - `iat`: Issued-at timestamp
   - The JWT is signed using the existing shop-to-app-server shared secret for the relevant shop/app pair.
   - The Shopware Backend returns the signed JWT to the Storefront, which forwards it to the embedded app.

2. **Token Usage & Verification**:
   - When the app sends a request to the App Server, it includes the JWT in an `Authorization: Bearer <token>` header.
   - The App Server validates the token by:
     - Verifying the signature using the corresponding shop-to-app-server shared secret
     - Checking that the token has not expired (`exp` claim is in the future)
     - Ensuring the `app_id` claim matches the app making the request
   - If validation passes, the App Server uses the token’s claims (e.g., `customer_id`, `shop_id`) to securely identify the context of the request and process it appropriately. For unauthenticated customers, the token will omit the `customer_id` claim, and the App Server will treat the request as anonymous.

3. **Token Revocation (Optional)**:
   - For critical security events (e.g., customer logout, app uninstallation), the Shopware Backend will send a revocation webhook to the App Server listing invalid token IDs (`jti` claim, to be added if revocation is required). The App Server will maintain a short-lived cache of revoked tokens to handle these edge cases, complementing the short TTL of tokens.

### Rationale
- Addresses the core security gap: The App Server can now verify the origin and context of app requests via a trusted token signed by the Shopware Backend, eliminating blind trust of client requests.
- Leverages existing infrastructure: Uses the already established shop-to-app-server shared secret, avoiding the need to manage new authentication credentials or key stores.
- Ensures reliable customer context: The signed token provides authenticated customer identifiers directly from the trusted Shopware Backend, eliminating the need for the App Server to rely on untrusted client-provided customer data.
- Aligns with session security: Short token TTL limits the potential impact of token leaks, while optional revocation handles immediate security events.
- Maintains modularity: The flow integrates seamlessly with existing app embedding patterns in Shopware, requiring minimal changes to app developers’ workflows beyond handling the token in request headers.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision 
Implement a non-executing Flow Builder Preview feature designed to visualize the expected execution path of a flow, highlight decision branch logic, and surface potential configuration issues—all without triggering real-world side effects or modifying data. This feature will focus on proactive clarity to help users validate flow behavior before deploying, reducing time spent debugging unexpected outcomes.

Key implementation details for the feature:
1. **Non-Execution Simulation Logic**:
   - The preview will use a stateless, in-memory simulation of all flow steps and conditions that never invokes side-effecting actions (e.g., sending emails, writing to the database, or updating customer data).
   - For each step, it will calculate the intended outcome based on configured parameters and simulated context, rather than executing the step’s actual business logic.

2. **Context Simulation & Customization**:
   - Users will be able to either select predefined context templates (e.g., ""New Order with Total > $100"", ""Customer with VIP Tag"") or manually input custom context data (e.g., order status, customer segment) to test flow behavior under specific conditions.
   - Context inputs will be validated against step requirements (e.g., ensuring a ""Order Total"" value is provided for a condition that relies on it) and surface errors if critical data is missing.

3. **Visual Path Highlighting & Annotations**:
   - In the Flow Builder UI, the preview will dynamically highlight the exact sequence of steps that the flow would follow based on the simulated context, including clear indicators for conditional branch selections (e.g., a green checkmark on the ""Yes"" branch of an ""Order Total > $100"" condition).
   - Each step in the highlighted path will display a human-readable annotation explaining its intended behavior without execution, e.g., ""Would add 'VIP Loyalty' discount to order"" or ""Would skip email notification (customer opted out)"".
   - For steps not included in the execution path, the UI will gray them out with a note explaining why they are skipped (e.g., ""Condition 'Order is Digital' failed"").

4. **Proactive Validation Integration**:
   - The preview will run real-time validation checks to flag potential configuration issues upfront, such as:
     - Disconnected flow paths (steps that can never be reached due to logic gaps).
     - Missing required configuration for a step (e.g., an email template not assigned to a send-email step).
     - Invalid condition logic (e.g., comparing incompatible data types like a string order total to a numeric threshold).
   - Validation errors will be displayed inline with the relevant step, with actionable guidance to fix the issue.

5. **Preview Persistence & Sharing**:
   - Users can save their preview configurations (simulated context + flow version) for future reference. Saved previews will be linked to the flow they were created for and retain the exact state of the flow at the time of preview.
   - A shareable link or export option will be provided to allow users to send preview results to Shopware support or team members, accelerating debugging workflows when issues do arise post-deployment.

6. **UI Accessibility & Usability**:
   - The preview feature will be accessible via a prominent, one-click toggle in the Flow Builder’s main toolbar, with a clear label indicating when preview mode is active.
   - All visual cues (path highlighting, annotations, validation errors) will be compatible with screen readers and high-contrast modes to ensure accessibility for all users.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃÂÃÂ¡ÃÂÃÂ°realÃÂÃÂ¡ÃÂÃÂ± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision 
### 1. Formalize Separation of Responsibilities
Clarify and enforce distinct, non-overlapping responsibilities for the two core interfaces:
- **`PathnameStrategyInterface`**: Retain exclusive responsibility for defining the structure of storage paths and their constituent components (hash, cache buster, filename) for media entities and thumbnails. It will not contain any logic related to URL construction, presentation, or web routing.
- **`UrlGeneratorInterface`**: Hold sole responsibility for translating path components provided by the active `PathnameStrategyInterface` into valid absolute/relative URLs for media entities and thumbnails. It will encapsulate all URL-specific logic (e.g., base URL resolution, route formatting, protocol handling) while relying entirely on the path strategy for storage-aligned path structure.

### 2. Standardize Dependency Integration
- The default `UrlGeneratorInterface` implementation will be dependency-injected with the active `PathnameStrategyInterface` instance. This ensures that URL generation always aligns with the same path logic used for media storage, eliminating inconsistencies between stored file paths and served URLs.
- All media URL generation (including for entities, thumbnails, and dynamic contexts) must go through `UrlGeneratorInterface` instead of directly calling `PathnameStrategyInterface` methods, ensuring consistent URL formatting across the application.

### 3. Optimize URL Generation Triggering
- Retain the `media.loaded` event subscriber as the primary trigger for precomputing media URLs at entity load time to optimize frontend and Store API performance.
- Expose `UrlGeneratorInterface` as a public service, allowing direct injection and use in custom contexts (e.g., custom controllers, third-party plugins, batch processing tasks) where media URLs need to be generated dynamically outside of the `media.loaded` event lifecycle.

### 4. Preserve Extensibility & Backward Compatibility
- No breaking changes will be made to the existing `PathnameStrategyInterface` or `UrlGeneratorInterface` contracts. All existing third-party implementations of either interface will remain compatible.
- Third-party developers will be able to swap custom `PathnameStrategyInterface` implementations without modifying URL generation code, as the default `UrlGeneratorInterface` will automatically adapt to the active strategy’s output. Custom `UrlGeneratorInterface` implementations can leverage any valid `PathnameStrategyInterface` instance via dependency injection.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision
Implement a dedicated, versioned one-time data migration framework integrated with Shopware's official upgrade pipeline to decouple one-time data updates from recurring indexer operations. This framework will ensure migration logic runs exactly once per installation, eliminating the risk of unintended re-execution and system errors.

### Key Details of the Framework:
1. **Dedicated Migration Tracking**:
   - Create a new database table `data_migration_execution` to track completed migrations, storing the migration's unique identifier, execution timestamp, status, and Shopware version context.
   - Each migration will have a unique, version-aligned ID (e.g., `SW650_MEDIA_PATH_MIGRATION`) to align with Shopware release cycles and avoid conflicts.

2. **Migration Execution Flow**:
   - Migrations will be triggered automatically during the official Shopware upgrade process (both CLI `bin/console system:update` and Admin UI upgrade flows), immediately after schema migrations complete.
   - The framework will first check the `data_migration_execution` table; if a migration is already marked as completed, it will be skipped entirely.

3. **Migration Structure**:
   - Each data migration will be implemented as a standalone class (following a defined interface `Shopware\Core\DataMigration\DataMigrationInterface`) with mandatory methods:
     - `getId(): string`: Returns the unique migration ID.
     - `isApplicable(): bool`: Optional method to check if the migration is needed for the current installation (e.g., verifying existing media path format).
     - `execute(): void`: Contains the one-time data transformation logic.
     - `rollback(): void`: Optional rollback logic for failed migrations (for safe upgrade rollbacks).

4. **Deprecation of Indexer-Based Migrations**:
   - Existing indexer-extended data migrations will be refactored into the new framework and marked as deprecated.
   - A temporary compatibility layer will be added to skip deprecated indexer migration logic if the corresponding dedicated migration is already completed.

5. **Error Handling & Logging**:
   - If a data migration fails, the upgrade process will halt (configurable via CLI flag) and log detailed error context to both the Shopware system logs and the `data_migration_execution` table.
   - Administrators will be able to re-run failed migrations via the CLI (e.g., `bin/console data-migration:re-run <migration-id>`) after resolving the root cause.

### Rationale:
- Eliminates the core risk of unintended re-execution of one-time logic by decoupling it from recurring indexer runs.
- Aligns with Shopware's existing upgrade patterns, ensuring users don't need additional manual steps to trigger data migrations.
- Provides clear tracking and auditability of data changes across version upgrades.
- Reduces system errors for sensitive data transformations (like media path updates) that cannot safely run multiple times.
- Maintains compatibility with existing workflows while establishing a sustainable pattern for future cross-version data updates.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision
We will implement a **centralized, traceable experimental feature management system** that explicitly links all experimental code artifacts to their parent feature, and automates key maintenance workflows to address code fragmentation and compliance gaps. This system will consist of three core components:

### 1. Centralized Experimental Feature Registry
Create a single, version-controlled registry (e.g., a YAML/JSON file in the root of the codebase or a dedicated config class) that serves as the source of truth for all experimental features. Each feature entry will include:
   - A unique, immutable feature ID (human-readable, e.g., `payment-gateway-v2`).
   - Current status (active, prolonged, killed/deprecated).
   - Target stable version (or extended stable version for prolonged experiments).
   - A curated list of code artifacts (classes, methods, modules, configuration files, etc.) associated with the feature.
   - Optional: Rationale for the experiment, owner, and timeline.

### 2. Enhanced `@experimental` Annotation
Update the existing `@experimental` annotation to:
   - Require a `featureId` parameter that references a valid ID in the centralized registry (no anonymous experimental code allowed).
   - Optional: Allow linking multiple feature IDs only for code that is explicitly shared between independent experiments (discouraged unless necessary to avoid cross-feature coupling).

### 3. Automated Tooling for Compliance and Maintenance
Build or integrate tooling to streamline routine experimental feature tasks:
   - **Validation Linter**: Enforce that every `@experimental` annotation has a valid `featureId` present in the registry, and that all entries in the registry map to existing code artifacts. Fail CI checks for orphaned annotations or stale registry entries.
   - **Batch Update CLI**: A command-line tool that:
     - Queries the registry for a given feature ID and updates the `stableVersion` field in all linked `@experimental` annotations when an experiment is prolonged.
     - Identifies all code linked to a ""killed"" feature in the registry and auto-generates deprecation annotations (or development tasks) to mark those components as deprecated, ensuring compliance with the ""Killing Feature"" rule.
   - **IDE Integration**: Add support to display feature details (status, target stable version, owner) when hovering over an `@experimental` annotated element, improving developer visibility.

### 4. Updated Process Workflows
- **New Feature Creation**: Developers must first add a valid entry to the centralized registry before tagging any code with `@experimental`.
- **Experiment Extension**: When extending an experiment, update the feature's `stableVersion` in the registry, then run the batch update tool to propagate the change to all linked annotations.
- **Feature Killing**: When marking a feature as killed in the registry, run the deprecation tool to identify and tag all associated code. The registry entry will remain until the next major version, at which point both the entry and linked deprecated code can be removed.
- **Audits**: Monthly automated audits to sync the registry with code (remove entries where all linked code has been deleted, flag orphaned annotations).

This system will eliminate the core problem of fragmented experimental code by creating explicit, traceable links between features and their components, automate routine maintenance tasks, and ensure compliance with existing experimental feature rules.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision
1. We will implement a **stable, developer-controlled technical key system** as the primary unique identifier for payment and shipping methods in Arachne:
   - Extension developers must specify a unique, environment-agnostic technical key when registering payment or shipping methods (via app manifests, extension registration APIs, or configuration files). This key must be unique across all payment/shipping methods in an instance; duplicate keys will trigger immediate registration failure with a clear validation error.
   - For existing payment/shipping methods registered without a technical key, the platform will auto-generate a stable, immutable technical key (derived from the method's name and creation timestamp) to maintain backward compatibility.

2. The Shopware platform will expose the technical key in all critical contexts to eliminate reliance on Admin API ID resolution:
   - Include the technical key in order webhook payloads, API responses for payment/shipping method endpoints, and app context variables accessible to templates and logic.
   - Allow app servers to reference payment/shipping methods directly by their technical key in rule conditions, payment processing triggers, and other app logic without requiring prior Admin API calls to resolve internal IDs.

3. We will deprecate the requirement to use Shopware's internal auto-incremented IDs for extension-specific payment/shipping method identification. Documentation will be updated to prioritize technical keys as the preferred identifier for extension developers.

## Rationale
- Eliminates redundant API calls: App servers no longer need to make repeated Admin API requests to resolve method IDs, reducing latency, network overhead, and potential rate-limiting issues.
- Environment stability: Technical keys are consistent across development, staging, and production environments, unlike Shopware's internal IDs which vary between instances, simplifying cross-environment extension testing and deployment.
- Developer control: Gives extension developers explicit ownership of their method identifiers, reducing coupling to Shopware's internal data structures and aligning with Arachne's modularity goals.
- Transparency: Exposing technical keys in all relevant payloads ensures full visibility into payment/shipping method associations, supporting Arachne's transparency and introspectability objectives.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision 
The Storefront team will adopt the following approach to SCSS and markup styling to balance maintainability, modularity, and extensibility:

1. **Utility Class-First for Atomic Styles**
   - For simple, atomic style requirements (including spacing, border, text alignment, basic color, and display properties), use Bootstrap's built-in utility classes directly in HTML markup instead of writing custom SCSS rules. This eliminates redundant custom CSS and leverages standardized, well-tested Bootstrap utilities.
   - Example: Replace custom SCSS rules for margin/padding with `mb-3`, `p-2`, or equivalent Bootstrap spacing utilities in the HTML.

2. **Retain Semantic Custom Classes**
   - Even when using utility classes, ensure elements have semantic, component-specific custom classes (e.g., `shipping-modal-actions`, `register-login-collapse-toogle`) present in the markup. These classes do not need default SCSS rules but serve three key purposes:
     - Enabling theme developers to target specific components for custom styling without relying on fragile utility class combinations.
     - Improving markup readability by clarifying the purpose of elements.
     - Providing a stable target for testing or JavaScript interactions.

3. **Custom SCSS for Complex Use Cases**
   - Reserve custom SCSS for scenarios where Bootstrap utilities/components are insufficient or would lead to unmaintainable markup:
     - Complex, non-atomic layouts (e.g., nested conditional flex/grid behavior, custom responsive breakpoints not covered by Bootstrap).
     - Component-specific state styles (e.g., hover/active states that require coordinated changes across multiple properties).
     - Reusable component patterns that cannot be efficiently composed from Bootstrap's built-in components.

4. **Refactoring Existing Code**
   - Gradually refactor existing custom SCSS rules that replicate Bootstrap utility functionality to use the corresponding utility classes. During refactoring, preserve all existing semantic custom classes to avoid breaking theme or integration dependencies.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision
We will overhaul our end-to-end (E2E) test suite with a Behavior-Driven Development (BDD) approach, retaining and optimizing Cypress as the core framework, to address flakiness, environment coupling, and strategy gaps while meeting all stated requirements:

1.  **Align Tests with Product Requirements via BDD**
    - Author all new tests using Gherkin syntax, which maps directly to product user stories and acceptance criteria. This makes tests readable and comprehensible to non-technical stakeholders (product managers, QA, designers) and ensures tests validate application behavior rather than implementation details.
    - Integrate Cypress with its official Cucumber Preprocessor to link Gherkin scenarios to reusable Cypress step definitions, reducing redundancy and maintaining consistency across tests.

2.  **Eliminate Environment Coupling & Ensure Determinism**
    - Centralize all environment-specific configuration (base URLs, API endpoints, authentication credentials, third-party service keys) into environment variables and environment-specific Cypress config files (e.g., `cypress.cloud.config.js`). No hardcoded environment values will be allowed in test logic.
    - Implement strict test isolation:
      - Use API-based setup/teardown (instead of slow UI interactions) to reset application state before and after each test (e.g., creating test users via admin APIs, rolling back database changes).
      - Use Cypress's built-in mocks/stubs for non-critical external dependencies (e.g., payment gateways, analytics services) to eliminate reliance on external service availability and state.

3.  **Standardize Test Architecture for Reliability & Maintainability**
    - Adopt a Component Object Model (COM) to encapsulate UI element selectors and interactions. This replaces brittle, scattered selectors with reusable, maintainable components, reducing flakiness from UI changes.
    - Enforce a consistent test structure: each test file corresponds to a single feature/user story, with clear separation of setup, core test steps, and teardown logic.
    - Establish a mandatory peer review process for all new tests, including sign-off from both technical teams and relevant non-technical stakeholders (where applicable) to confirm alignment with requirements and adherence to standards.

4.  **Enable Cloud Environment Testing**
    - Integrate Cypress Cloud with our CI/CD pipeline to trigger automated test runs against cloud environments on every deployment. Use Cypress Cloud's parallelization feature to speed up test execution and reduce pipeline latency.
    - Validate cross-environment compatibility by running a critical path test suite against local, staging, and cloud environments during framework setup, ensuring test logic works seamlessly across all targets.

5.  **Phase Out Legacy Tests Gradually**
    - Deprecate existing unstandardized, flaky tests incrementally. For critical functionality, rewrite tests using the new BDD/COM approach before decommissioning legacy versions to maintain full test coverage during the transition.
    - Use Cypress's built-in coverage tracking tools to verify no critical product requirements are untested during the migration.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision
1. **Unify Transaction Context Alignment for Flow Actions**:
   - All flow actions interacting with the database will reuse the current active transaction context if one exists, eliminating nested transaction attempts by default. For flow actions that require independent transactional behavior (e.g., operations that must commit regardless of the outer transaction outcome), add an explicit `:isolate-transaction` flag in the flow action definition. When enabled, the flow dispatcher will spawn a separate, isolated transaction for the action, which will not share a connection with the outer transaction.
   - For Shopware environments with default no-savepoint connections, enforce a runtime guard that blocks non-isolated flow actions from starting new transactions, logging a clear warning if an action was previously configured to do so.

2. **Stop Silencing Transaction Errors & Propagate to Callers**:
   - Modify the flow dispatcher to abort subsequent flow action execution in the same flow when a transaction-related error (commit/rollback failure, rollback-only connection state) occurs.
   - Propagate all critical transaction errors up to the calling code, rather than ignoring them. This ensures the outer transaction context is immediately aware of the connection's rollback-only state, preventing unexpected commit failures later.

3. **Enhance Error Transparency & Logging**:
   - Replace vague error logs with detailed messages that include:
     - The ID/name of the failed flow action
     - The specific transaction operation that triggered the failure
     - An explicit note if the database connection is now in a rollback-only state
   - Add dedicated logging tags/metrics for transaction-related flow errors to simplify monitoring and debugging.

4. **Document Transaction Boundary Rules**:
   - Update flow action development guidelines to explicitly outline:
     - When to reuse the outer transaction context (default behavior)
     - When to use the `:isolate-transaction` flag
     - How transaction errors will propagate to calling code for handling

## Rationale
- Aligning flow actions with the outer transaction context eliminates the root cause of rollback-only connection state conflicts in Shopware's default configuration, avoiding unintended nested transaction failures.
- Propagating errors ensures calling code can take immediate corrective action (e.g., rollback the outer transaction, notify stakeholders) instead of encountering silent, hard-to-diagnose commit failures later.
- Isolated transactions preserve critical functionality for actions that need independent commit behavior without risking interference with outer transactions.
- Enhanced logging and clear guidelines improve developer awareness of transaction behavior, reducing future misconfiguration issues.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision
1. **Retain Vue 3 Compatibility Mode Enabled for the Current Major Release**
   The Arachne administration interface will keep Vue 3's compatibility mode active during the current major product release. This prioritizes minimizing immediate breaking changes for plugin developers, allowing them to migrate their plugins to Vue 3 on a timeline aligned with their own roadmaps rather than being forced to complete migration concurrently with the core administration transition.

2. **Two-Phase Migration Roadmap**
   - **Phase 1 (Current Major):** 
     The core administration codebase will be fully adapted to use native Vue 3 APIs while compatibility mode remains enabled. To support plugin developers, we will:
     - Publish a clear public timeline stating that compatibility mode will be disabled in the next major release.
     - Provide targeted migration guides tailored to common plugin patterns used in the administration, including examples of replacing Vue 2-specific features with Vue 3 equivalents.
     - Offer an early-access preview environment where compatibility mode is disabled, allowing plugin developers to test their updates ahead of the next major release.
     - Establish dedicated support channels (e.g., a migration-focused Slack channel or GitHub discussion board) to address blockers and answer questions.
   - **Phase 2 (Next Major Release):**
     Disable Vue 3 compatibility mode in the administration interface. Before finalizing this release:
     - Conduct a full audit of all officially supported plugins to verify they function correctly without compatibility mode.
     - Send final reminders to community plugin developers about the upcoming compatibility mode deprecation, with clear instructions on reporting unresolved issues.
     - Block release deployment until core administration and all critical plugins are confirmed to work reliably with native Vue 3 APIs.

3. **Post-Phase 2 Incompatibility Handling**
   Plugins not migrated to native Vue 3 APIs by the next major release will be marked as incompatible with the latest administration version in official documentation. Users will be advised to either migrate their plugins, use maintained forks, or revert to a legacy version of the administration if they depend on unupdated plugins.

This decision balances the goal of adopting modern Vue 3 capabilities with the practical need to support our distributed plugin ecosystem, splitting migration work into manageable sequential phases to minimize disruption for all stakeholders.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision
We will implement a targeted local workaround to mitigate the Vuex 4.1.0 getter reactivity bug, and prioritize migrating our application's state management layer to Pinia (the official Vue-recommended successor to Vuex) over the next two sprints.

## Rationale
1. **Immediate Problem Resolution**: Downgrading to Vuex 4.0.2 is blocked by critical module failure bugs, and waiting for an official Vuex 4.x fix introduces uncertainty (no clear timeline) while leaving production users exposed to reactivity issues. A local workaround—such as wrapping affected getters in Vue's native `reactive` utility or adjusting state access patterns to preserve reactivity—addresses the immediate bug with minimal disruption to existing functionality.
2. **Long-Term Architectural Stability**: Pinia is the officially supported state management library for Vue 3, built natively on Vue's reactive system. It eliminates the class of reactivity bugs present in legacy Vuex versions by design, while also offering simpler, more modular patterns that align with Arachne's goal of modularity. Migrating to Pinia reduces technical debt associated with relying on a legacy library with unresolved bugs and positions the application for future Vue ecosystem updates.
3. **Avoiding Unnecessary Technical Debt**: Forking Vuex to patch the bug would require ongoing maintenance of a custom dependency, tracking upstream changes, and resolving merge conflicts—costs that are not justified given the availability of a supported, superior alternative in Pinia.

## Action Items
1. Develop and test a local workaround for the Vuex 4.1.0 getter reactivity bug; validate that it resolves the issue without introducing new regressions.
2. Map existing Vuex modules to equivalent Pinia stores, draft a migration guide for developers, and conduct incremental migration of non-critical features first to de-risk the process.
3. Complete full Pinia migration and validate all state-dependent functionality; once confirmed stable, remove Vuex dependency and temporary workarounds.
4. Document the migration in an ADR to preserve context for future contributors.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision
To address the core challenges of compatibility, limited subprocess extension, and code proliferation, we will implement a combined architecture centered on an **Event-Driven Hook System**, paired with lean Dependency Injection (DI) composition, and establish strict compatibility governance:

1. **Adopt an Event-Driven Hook System as the primary extension mechanism**
   - Define standardized, named hooks at every critical point in core processes (including nested subprocesses) — categorized as pre-execution, post-execution, or around-execution. Hooks will use immutable, typed payload objects to pass context (inputs, intermediate results, outputs) between core code and extension listeners.
   - Third-party developers can attach listeners to these hooks to modify payload data, add side effects, or short-circuit processes (where permitted) without needing to extend or decorate core classes. This eliminates the need to extract subprocesses into separate interfaces/classes to support extensions.

2. **Replace excessive Factory/Adapter usage with DI composition and stable, minimal interfaces**
   - Leverage PHP’s native autowiring capabilities for constructor injection to eliminate manual factory boilerplate for most service dependencies.
   - Define only broad, stable interfaces for swappable core components (e.g., `PaymentProcessorInterface` instead of 10 fine-grained interfaces for payment subprocesses). Third-party developers can replace these components by overriding DI container bindings, rather than using adapter patterns to wrap existing implementations.

3. **Establish a compatibility governance framework**
   - Version all hooks and stable interfaces to signal breaking changes. For non-breaking updates, add new hooks instead of modifying existing payload structures or interface signatures.
   - Maintain backward compatibility for deprecated hooks/interfaces for a minimum of two major release cycles, with clear migration paths documented in official guides.
   - For breaking changes, require a mandatory deprecation period with runtime warnings before removal to give extension developers time to adapt.

4. **Implement a declarative extension registration system**
   - Create a centralized registry that allows third-party extensions to declaratively register hook listeners, component overrides, and new custom hooks via a simple configuration file (YAML/PHP array) instead of procedural setup code. This reduces the need for factory classes to instantiate extension components.

### Rationale
- **Backward/Forward Compatibility**: The hook system decouples extensions from core code implementation details. Typed payloads and versioning ensure extensions work across minor releases, and deprecation policies minimize breaking changes.
- **Process Extension Flexibility**: Hooks embedded directly in subprocesses eliminate the need to extract every extendable step into a separate class. Extensions can target specific subprocesses without altering core code structure.
- **Reduced Code Proliferation**: Minimal stable interfaces and autowired DI eliminate the flood of fine-grained interfaces, abstract classes, and factory boilerplate that comes with over-reliance on decoration and adapter patterns.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision
We will implement a targeted, backward-compatible Jest test setup to disable compat mode for individual components while maintaining confidence in test validity during transition:

1. **Define a dedicated non-compat Jest configuration**:
   - Create a root-level `jest.no-compat.config.js` that explicitly disables compatibility features:
     - Configure Babel to exclude legacy browser/ runtime transforms (e.g., set `@babel/preset-env` targets to modern environments or remove compat-specific plugins).
     - Disable Jest-specific compat shims (e.g., unset global variables used for compat checks, remove mock stubs for legacy APIs).
     - Isolate test state from compat-mode tests (e.g., use separate cache directories to prevent cross-contamination).

2. **Add a dedicated test execution script**:
   - Update `package.json` with a new script: `""test:component:no-compat"": ""jest --config jest.no-compat.config.js""`, allowing developers to run non-compat tests locally and integrate with CI pipelines.

3. **Per-component test targeting**:
   - Use a clear file naming convention to identify tests that should run without compat mode: rename component test files to `[ComponentName].no-compat.test.js` (or store them in a `__tests__/no-compat` subdirectory per component).
   - Configure `jest.no-compat.config.js` to exclusively match these files/paths, while the existing `jest.config.js` continues to run remaining compat-mode tests.

4. **CI validation and transition guardrails**:
   - Add both test scripts (`test:component` and `test:component:no-compat`) to the CI pipeline to ensure all tests pass in their respective modes during migration.
   - For each component migrated to non-compat mode:
     - Verify non-compat tests pass without relying on compat shims.
     - Confirm the component behaves correctly in production builds via end-to-end tests or manual verification.
     - Remove the component’s old compat-mode test files once validation is complete.

5. **Long-term consolidation**:
   - Once all components are migrated to non-compat mode, merge the two Jest configurations into a single `jest.config.js` with compat mode disabled globally.
   - Deprecate the `test:component:no-compat` script and update documentation to reflect the unified test setup.

6. **Contributor documentation**:
   - Add a section to the component testing guide explaining the migration process: naming conventions, local test execution steps, and validation checks to ensure components work without compat mode.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision 
Implement a tiered, component-aware health checking system integrated into Arachne’s data-driven, modular architecture to prevent post-rollout undetected failures and enable proactive monitoring of system and component health. The system will adhere to the following principles:

1. **Dual-Category Health Checks**  
   Split checks into two critical phases to catch issues at every stage of the application lifecycle:
   - **Pre-Deployment Validation Checks**: Automatically run during application build/rollout workflows before deployment. These checks validate component configuration correctness, dependency availability (e.g., database connection, cache accessibility), and core functional readiness (e.g., ability to process a minimal valid request). Critical checks will block deployment until resolved; non-critical checks will surface warnings with required risk acceptance documentation.
   - **Runtime Health Checks**: Continuously execute in live environments to monitor long-term component health. These will track two states:
     - Liveness: Whether the component is running and responsive.
     - Readiness: Whether the component can reliably handle incoming requests (e.g., no backlog, sufficient resource availability).

2. **Component-Level, Schema-Aligned Check Definitions**  
   Each modular component (e.g., database connector, API handler, cache client) will expose standardized health check metadata and functions registered in the Arachne application configuration data structure. This adheres to Arachne’s ""everything is data"" philosophy, ensuring checks are transparent, introspectable, and swappable alongside the components they monitor. Checks will return granular statuses (healthy, degraded, unhealthy) with context-specific details (e.g., error messages, latency metrics, connection counts) to explain failures or partial functionality.

3. **Centralized Aggregation and Reporting**  
   A core health service will auto-discover and aggregate results from all registered component checks:
   - For pre-deployment: Provide a single pass/fail summary with structured failure details to simplify debugging rollout issues.
   - For runtime: Expose a dedicated HTTP endpoint for health status queries, and support integration with external monitoring tools (e.g., Prometheus, Datadog) via standard metrics formats. Aggregated reports will distinguish between overall system health and per-component status, ensuring degraded components are not masked by an otherwise functional system.

4. **Degraded State Proactivity**  
   Explicitly support reporting of ""degraded"" component states (e.g., high latency, partial request failure rates) to trigger proactive alerts. This allows teams to remediate issues before they escalate to full system failures, addressing the gap where a system may appear healthy despite underperforming components.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision
1. **Replace Hardcoded Namespace with Configurable Allowed List**: Modify the `TestPreparationStartedSubscriber` to replace the hardcoded `Shopware\Tests\Unit` namespace check with a configurable list of allowed namespaces. The default list will retain `Shopware\Tests\Unit` to preserve backward compatibility with existing core unit tests.
2. **Introduce Configuration for Extending Permissions**: Add a PHPUnit configuration parameter (e.g., `allowed_feature_flag_namespaces`) that allows developers to extend the list of permitted namespaces. This enables plugin authors and internal teams to enable major feature flags in their custom unit test namespaces (e.g., `Acme\Plugin\Tests\Unit`).
3. **Enforce Test Suite Boundaries**: Implement validation logic to reject namespaces associated with non-unit test suites (e.g., those containing `Integration` or `Functional` keywords). This prevents accidental enabling of major feature flags in test contexts where they could conflict with integration guarantees or existing test setup logic.
4. **Preserve Explicit Enablement Requirement**: Retain the rule that major feature flags must be explicitly enabled via test annotations or dedicated test configuration. The expanded namespace list only grants permission to use this enablement mechanism, it does not auto-enable any flags.
5. **Update Documentation**: Add clear documentation to Shopware's test guides explaining how to configure additional allowed namespaces, including use cases for plugin unit tests and the rationale behind test suite boundary restrictions.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision
We will implement a phased, tool-assisted migration workflow to convert our Vue 2 Options API components to the Vue 3 Composition API, centered on official, framework-maintained automation tools paired with structured validation and refinement processes.

### Key Components of the Approach
1. **Primary Automated Transformation Tool**: Adopt the `@vue/codemod` utility’s `options-api-to-composition-api` and `vue-2-to-3` transform scripts as the foundation for bulk code conversion. These official Vue core team tools ensure alignment with framework best practices and reduce manual error risk for standard components.
2. **Intermediate Compatibility Step**: First upgrade the application to Vue 2.7 (the final stable Vue 2 release) before initiating migration. Vue 2.7 natively supports the Composition API, enabling us to test transformed components in a familiar environment before full Vue 3 adoption, minimizing disruption to critical workflows.
3. **Phased Migration Workflow**:
   a. **Pre-Migration Preparation**:
      - Audit components to categorize by complexity (simple UI components vs. logic-heavy components with custom mixins/options).
      - Run all unit/integration and visual regression tests to establish a functional baseline.
      - Update dependencies to resolve conflicts with Vue 2.7 and codemod tools.
   b. **Automated Transformation**:
      - Execute codemod scripts on low-complexity, high-usage components first to validate tool effectiveness early.
      - Log all transformation warnings/errors for targeted cleanup.
   c. **Post-Transformation Cleanup & Refinement**:
      - Resolve codemod-generated issues (e.g., unresolved references, incorrect reactive state handling).
      - Refactor auto-generated code for readability: extract repetitive logic into reusable composables, simplify reactive patterns, and align with project style guidelines.
      - Update TypeScript type definitions to leverage Composition API-specific typing improvements.
   d. **Validation & Sign-Off**:
      - Re-run baseline tests to confirm functionality remains intact.
      - Conduct code reviews to ensure adherence to Composition API best practices.
      - Perform UAT for high-impact components to validate visual/behavioral consistency.
4. **Edge Case Handling**: For components with complex custom options or non-standard mixins that cannot be reliably auto-transformed:
   - Document tailored manual migration patterns.
   - Assign dedicated developers to these migrations, with peer reviews to ensure quality.
5. **Progress Tracking**: Maintain a shared dashboard to track completed components, unresolved issues, and team progress for full transparency.

### Rationale
- **Efficiency**: Automated transformation reduces manual effort by 70-80% for standard components, accelerating the migration timeline.
- **Risk Mitigation**: Official tools ensure transformed code follows framework standards, minimizing new bugs. The Vue 2.7 intermediate step allows incremental testing without full Vue 3 adoption upfront.
- **Long-Term Maintainability**: The Composition API improves code organization, reusability, and TypeScript support, aligning with our goal of modernizing the codebase for future scalability.
- **Team Alignment**: Phased rollout and shared tracking ensure the distributed team maintains a clear understanding of progress and best practices.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃÂÃÂ¡ÃÂÃÂ¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃÂÃÂ¡ÃÂÃÂ¯s templating and reactivity model.","## Decision
Replace the existing TwigJS-based block inheritance system with a native Vue.js component-based block system, using `sw-block` and `sw-block-parent` as the core implementation components.

## Rationale
1. Aligns with modern frontend best practices: Vue.js’s reactive, component-first architecture is better suited for building dynamic, interactive UIs than TwigJS’s static template inheritance model, which lacks native support for reactive state-driven content changes.
2. Preserves and enhances plugin developer flexibility: The new component system retains the core capability for plugin developers to override or extend predefined content blocks, while adding support for advanced dynamic customization (e.g., conditional block overrides based on reactive application state) that was not feasible with TwigJS.
3. Simplifies the frontend tech stack: Eliminates the need to mix TwigJS template logic with Vue.js components, reducing cognitive overhead for developers and streamlining the application’s rendering pipeline.

## Implementation Guidelines
1. Component Specifications:
   - `sw-block-parent`: A wrapper component that declares a named block scope. It accepts a required `name` prop to uniquely identify the block, and renders its default slot as the base content when no override is present.
   - `sw-block`: A component that targets and customizes a matching `sw-block-parent`. It requires a `name` prop (matching the parent’s block name) and an optional boolean `extend` prop (default: `false`). If `extend` is `true`, it appends its slot content to the parent’s base content; if `false`, it replaces the parent’s base content entirely.
2. Migration Strategy:
   - Phase out all existing TwigJS block declarations and overrides by converting them to `sw-block-parent` and `sw-block` pairs.
   - For a transitional period (if needed to avoid breaking existing plugin code), implement a thin compatibility layer that maps TwigJS block syntax to the new Vue components. Deprecate this layer clearly and remove it once all plugins have migrated.
3. Documentation:
   - Update official plugin development documentation to replace references to TwigJS block inheritance with guides for using `sw-block` and `sw-block-parent`.
   - Highlight new capabilities enabled by the Vue.js system, such as reactive block customization and dynamic content injection based on application state.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃÂÃÂ¡ÃÂÃÂ¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃÂÃÂ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision
We will implement the following changes to address the downsides of universal price asterisks while maintaining full transparency around tax and shipping costs in the default Storefront:
1. **Remove universal price asterisks**: Eliminate the `*` suffix from all standard price displays to reduce UI clutter and improve readability of price values at a glance.
2. **Revise footer disclosure**: Update the footer notice to explicitly state its applicability to all listed prices without relying on a superscript link. Example revised text: ""All listed prices include VAT. Shipping costs and additional delivery charges apply unless explicitly noted otherwise.""
3. **Add contextual accessibility-focused cues**:
   - Place a small, unobtrusive info icon (🛈) in a consistent, prominent location on product list pages (e.g., top of the product grid) and product detail pages (e.g., adjacent to the primary product price).
   - The icon will have a descriptive ARIA label for screen readers (e.g., ""View tax and shipping details"") and trigger a tooltip or lightweight modal with the full, detailed disclosure text when clicked or hovered.
4. **Targeted indicators for non-standard prices**: For any price that deviates from the standard (e.g., prices excluding VAT, items with free shipping), use a clear, direct indicator (e.g., a small ""Excl. VAT"" tag or inline note) adjacent to that specific price, rather than relying on a generic footer link.

### Rationale
- **Reduces UI clutter**: Removing the universal asterisk cleans up price displays, making them faster for users to scan and process.
- **Improves accessibility**: The info icon with explicit labels and contextual disclosure resolves issues where screen readers may fail to correctly associate a distant superscript with footer text.
- **Preserves transparency**: The revised footer notice and contextual cues ensure users still have full access to critical tax and shipping details, while making the connection between the disclosure and prices more intuitive.
- **Clarifies exceptions**: Non-standard prices get targeted, immediate indicators, so users instantly understand when a price differs from the default, avoiding ambiguity from generic footnotes.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision
1. **Replace DomPDF with an Accessibility-First PDF Generator**: Adopt a PDF generation library that natively supports WCAG 2.1 AA compliance, prioritizing solutions like WeasyPrint or TCPDF with accessibility extensions. These tools offer out-of-the-box support for converting semantic HTML to tagged PDFs (a core WCAG requirement), preserving logical reading order, rendering image alt text, semantic table structures, and interactive accessible form elements.
   - Rationale: DomPDF’s architecture lacks robust support for tagged PDFs and core accessibility features; retrofitting it to meet WCAG standards would require costly, ongoing maintenance that is less efficient than adopting a purpose-built solution.

2. **Implement an Automated Accessibility Validation Pipeline**: Integrate tools like PDF Accessibility Checker (PAC 3) or axe-pdf into the document generation workflow. This pipeline will run:
   - As part of development tests to catch regressions in default templates
   - As a pre-delivery check in production to scan customer-facing documents for WCAG violations before they are sent
   - Rationale: Proactive validation ensures consistent compliance and reduces the risk of delivering non-accessible documents to users.

3. **Establish Accessible Document Template Standards**: Mandate WCAG-aligned HTML best practices for all default and custom document templates, including:
   - Semantic heading hierarchies (`<h1>` to `<h6>` instead of styled `<div>`s)
   - Required `alt` attributes for all images (with empty `alt=""""` for decorative images)
   - Semantic table markup (using `<th>` with `scope` attributes for headers)
   - Logical reading order that matches visual layout
   - For third-party template developers, provide a validation tool and clear documentation to enforce these standards.

4. **Add Admin Tooling for Accessibility Guidance**: Enhance the Shopware administration with in-app prompts and validation to highlight accessibility gaps when merchants edit document templates (e.g., missing image alt text, invalid heading nesting). Include a one-click accessibility check for custom templates to surface violations and recommended fixes.

5. **Ensure Backward Compatibility**: Maintain support for existing non-compliant templates via a fallback mode that generates the most accessible PDF possible (e.g., inferring basic semantic structure where missing) while notifying merchants of needed template updates. Provide a migration guide with automated refactoring snippets for common template patterns to ease transition.

## Rationale
This decision directly addresses the context of WCAG compliance by:
- Resolving the core limitation of DomPDF’s lack of accessibility support
- Enforcing end-to-end compliance through automated checks and template standards, reducing legal and usability risks
- Empowering merchants and third-party developers to maintain accessible documents via tooling and documentation
- Minimizing disruption to existing users through backward-compatible fallback mechanisms.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision
We will transition the Shopware rule system to an explicit, interface-based extension model with restricted core class access to resolve complexity and maintainability issues, following these structured steps:

1. **Lock Down Core Rule Class Inheritance**:
   - Modify core rule classes (e.g., `Rule`, `AbstractCondition`, `AbstractAction`) to prevent direct inheritance by third-party plugins, using access modifiers (e.g., marking classes as `final` or package-private) and runtime validation to enforce this restriction.
   - Introduce controlled abstract base classes (e.g., `PluginCondition`, `PluginAction`) that expose only non-breaking, contract-aligned extension paths for custom rule functionality.

2. **Establish Centralized, Schema-Validated Extension Points**:
   - Build a `RuleTypeRegistry` service as the sole entry point for third-party plugins to register custom rule conditions, actions, and types. Registration requires implementing core-defined interfaces (e.g., `ConditionInterface`, `ActionInterface`) and providing structured metadata (label, configuration schema, supported data contexts).
   - Integrate a schema validation layer into the registry to reject non-compliant custom rule registrations early, ensuring alignment with core data formats and runtime expectations.

3. **Deprecate Legacy Extensions with a Clear Migration Path**:
   - Mark all direct core rule class extensions as deprecated, with runtime warnings and detailed documentation updates for affected plugin developers.
   - Provide a migration toolkit (code generators, step-by-step guides) and a 2-major release deprecation window to help developers convert legacy extensions to the new interface-based model.

4. **Govern Core Rule System Evolutions**:
   - Implement a change impact assessment process for all core rule system updates. Any modification to core interfaces or abstract base classes must include a review of extension point compatibility, with backward compatibility guarantees maintained for compliant plugins.
   - Publish a dedicated extension point changelog to keep third-party developers informed of upcoming core system adjustments.

## Rationale
This decision directly addresses the root issues outlined in the context and RFC:
- **Reduces System Complexity**: Eliminating arbitrary core class extensions removes fragmented, untested code paths that hinder maintenance and slow core improvements.
- **Preserves Necessary Flexibility**: Explicit extension points and interfaces allow third-party developers to build custom rule functionality while aligning with core system design principles.
- **Enables Predictable Evolution**: The registry and validation layer ensure core updates can be made without breaking compliant plugins, reducing regression risk and enabling easier implementation of performance optimizations and new features.
- **Improves Transparency**: Standardized metadata and registration processes make custom rule implementations more discoverable and auditable for both core maintainers and end users.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision
1. **Deprecate the `Iterator` helper class** in `src/helper/iterator.helper.js` immediately. Mark the class with a clear deprecation warning in comments and console logs (for runtime detection) to notify developers of its pending removal.
2. **Migrate all existing usages** of `Iterator.iterate()` to native web-standard iteration methods, with guidance tailored to each data type:
   - For Arrays, NodeLists, and other array-like objects: Use `Array.prototype.forEach()` (or `Array.from()` first if needed to convert non-array iterables) or `for...of` loops.
   - For Maps and Sets: Use their native `.forEach()` methods or `for...of` loops to access key-value pairs directly.
   - For FormData: Use `for...of` loops or the native `.entries()`, `.keys()`, or `.values()` iterators.
3. **Standardize on native iteration patterns** in all new code. Update team guidelines and documentation to explicitly prioritize native web standards over custom iteration abstractions for common data types.
4. **Enforce consistency via code reviews**: Reject any new code that uses the deprecated `Iterator` helper, and guide contributors to use the appropriate native method instead.
5. **Remove the `Iterator` helper class entirely** from the codebase once all existing usages have been migrated. Set a clear timeline for this (e.g., completion by the next minor release, with a hard cut-off in the next major release if migration is not complete).
6. **For edge cases requiring custom iteration**: If a specific, non-standard iteration pattern is needed (a rare exception per the context), require explicit team approval and implement a targeted, single-purpose helper instead of reviving the general-purpose `Iterator` class. Document the rationale for this helper in code comments to avoid future confusion.

This decision directly addresses all identified downsides: it eliminates unnecessary abstraction, empowers developers to use appropriate native methods, aligns with widely understood web standards, resolves usage inconsistency, clarifies the standard nature of underlying data types, and removes redundant import dependencies.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision
We will decouple flow execution from the synchronous business process request/response cycle and implement asynchronous, isolated flow execution to resolve the identified performance, error resilience, and debugging issues. The specific changes are as follows:

1. **Asynchronous Flow Execution**: All flows will be executed asynchronously outside the core business process using a dedicated message queue system. This ensures:
   - Business processes complete independently of flow execution, eliminating flow errors as a cause of business process cancellation.
   - Resource-intensive operations (e.g., sending emails, external API calls) do not degrade user-facing business process performance.

2. **Dedicated Flow Execution Infrastructure**:
   - Replace the current Symfony EventDispatcher decorator with a purpose-built flow event listener. This listener will submit flow execution tasks to the message queue instead of running flows inline, simplifying stack traces and improving debugging clarity.
   - Implement a dedicated flow worker service to consume tasks from the queue, handling flow execution in an isolated environment. This environment will be designed to support future capability expansion (e.g., custom flow timeouts, parallel execution, conditional branching).

3. **Robust Error Handling for Flows**:
   - Track flow execution status (success, failed, retried, permanently failed) in a persistent store for audit and debugging purposes.
   - Implement configurable retry policies for transient errors (e.g., temporary email service outages) to reduce manual intervention needs.
   - Generate flow-specific error logs with full context (triggering event, flow configuration, error stack trace) to simplify troubleshooting without bloating business process logs.
   - Add alerting for permanently failed flows to notify relevant stakeholders of unresolvable issues.

4. **Rollout and Compatibility**:
   - Continue to gate this implementation behind the existing `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` feature flag for experimental validation.
   - Once validated (via metrics showing reduced business process failure rates, improved performance, and positive debugging experience feedback), deprecate the flag and make asynchronous execution the default behavior.
   - Add an explicit opt-in configuration for synchronous execution of specific flows only for exceptional cases where immediate execution is mandatory (e.g., critical post-process validation required before user feedback), to preserve the core benefits of asynchronous execution for most use cases.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision 
1. **ADR Format & Storage**
   - All architectural decisions will be written in Markdown format to ensure cross-platform readability and compatibility with Git hosting platform rendering (GitHub, GitLab, etc.).
   - Store cross-cutting, project-wide ADRs in a dedicated `docs/architecture/decisions` directory at the root of the Arachne monorepo. Module-specific ADRs may be stored in a parallel `docs/architecture/decisions` directory within their respective module's repository, with a reference link added to the root monorepo's ADR index for discoverability.
   - Use a sequential 4-digit numbering scheme for file names: `ADR-<4-digit-number>-<kebab-case-short-title>.md` (e.g., `ADR-0004-modular-component-contracts.md`). This ensures chronological ordering and easy reference to the sequence of decisions.

2. **Standard Template for ADRs**
   Every ADR must include the following ordered sections to ensure consistency and completeness:
   - **Title**: A concise, descriptive summary of the decision (matches the kebab-case title in the filename).
   - **Context**: A detailed explanation of the problem, constraints, project core goals, and existing system state that necessitates this decision. This section must provide sufficient background for future contributors (or revisitors) to understand the trade-offs without prior context.
   - **Alternatives Considered**: A comprehensive list of plausible alternative approaches (at least two, unless no reasonable alternatives exist). For each alternative, document key pros and cons to demonstrate thorough evaluation of all options.
   - **Decision**: An explicit, unambiguous statement of the chosen approach. Avoid vague language (e.g., ""We will define explicit component contracts for modular integration"" instead of ""We might improve modularity with contracts"").
   - **Rationale**: A clear link between the chosen decision and Arachne's core goals (modularity, transparency, introspectability, etc.). Explain why this alternative was selected over others, addressing how it mitigates constraints and achieves stated objectives.
   - **Status**: A single standardized tag from the following list:
     - Proposed: Under review by the steering group and contributors.
     - Accepted: Consensus reached; implementation may proceed.
     - Rejected: Consensus reached not to adopt the proposal.
     - Superseded: A new decision has replaced this one; include a direct link to the superseding ADR.
     - Deprecated: The decision is no longer in active use but has not been formally superseded (e.g., due to phased migration).
   - **Consequences**: A balanced overview of expected positive outcomes (benefits) and negative trade-offs (risks, additional maintenance work, limitations) of the decision. This sets expectations and aids future evaluation of the decision's validity.
   - **Related ADRs**: Links to other ADRs that are dependent on, inform, or are related to this decision (e.g., ""Supersedes ADR-0002"", ""Depends on ADR-0003"").

3. **Maintenance Conventions**
   - When a decision is revisited and superseded, create a new ADR for the updated decision. Update the original ADR's Status section to ""Superseded by ADR-XXXX"" with a link to the new record. Do not modify or delete the original ADR content to preserve historical context.
   - Each ADR must undergo steering group review before transitioning from ""Proposed"" to ""Accepted"" or ""Rejected"", aligning with the collaborative decision-making process established earlier.
   - Keep each ADR focused on one architectural decision; avoid combining multiple unrelated decisions into a single record to ensure clarity and maintainability.

This decision directly supports Arachne's goals: modularity (via module-specific ADR storage), transparency (via detailed alternatives and rationale), introspectability (via context and consequence sections), and avoids runtime entanglement (ADRs are static, runtime-independent files). It also ensures decisions are revisitable and collaborative, as required by the project's core principles.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision
We will address the gap between the modern Metadata API and repository implementation needs by building a **modular, layered repository framework** that integrates tightly with the Metadata API, while deprecating the monolithic legacy repository code (repo.py, repository_tool.py, repository_lib.py) once the new framework is stable. The framework will be structured into two distinct components:

### 1. Core Repository Library
This will be a lightweight, focused library that provides reusable building blocks for TUF repository operations without making application-specific decisions. Key design principles for this library:
- **Separation of concerns**: Avoid mixing library logic with application-level choices (e.g., do not mandate loading all metadata files at once; instead, expose granular methods to fetch, validate, and manipulate individual metadata objects as needed, letting applications implement caching/loading strategies suitable for their scale).
- **Coherent, minimal API**: Eliminate global state, reduce method argument counts by using structured options objects where appropriate, and align API patterns with the modern Metadata API’s design.
- **Flexibility**: Support diverse use cases (small CLI tools, large-scale servers, embedded systems) by abstracting I/O operations (e.g., metadata storage, network fetching) into pluggable interfaces that applications can implement.

### 2. Companion Tooling & Reference Implementations
To provide the necessary context and guidance for users implementing TUF repositories, we will build optional, modular tools on top of the core library:
- A reference CLI tool that demonstrates best practices for common repository tasks (e.g., signing metadata, rotating keys, publishing updates) using the core library.
- Example application blueprints (e.g., a basic server implementation, a desktop repo management tool) that show how to integrate the core library into real-world use cases.
- Documentation that maps common repository implementation questions to core library components and tooling examples.

### 3. Deprecation & Migration Path
- Maintain the legacy repository code in a deprecated state with clear warnings directing users to the new framework.
- Provide a migration guide that outlines how to transition legacy repository workflows to the new core library and tooling.
- Remove the legacy repository code in the next major release once adoption of the new framework is sufficient and critical bugs in the legacy code are no longer being fixed.

This approach will resolve the legacy implementation’s maintenance burden, eliminate incoherent API design, avoid mixing library and application concerns, and fill the gap in guidance for users leveraging the modern Metadata API to build TUF repositories.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision
We will migrate the entire Primer React codebase to TypeScript, replacing the manual ambient `index.d.ts` file with type definitions embedded directly in the component implementation code. This will eliminate the need for manual syncing of types and implementation, ensuring type definitions are always accurate and up-to-date as the library evolves.

To execute this transition sustainably:
1.  **Incremental Migration**: Convert components to TypeScript in phases, prioritizing components with the highest volume of TypeScript-related bug reports and high-traffic user-facing components first. This minimizes disruption to ongoing feature development and allows us to quickly resolve the most pressing type accuracy issues.
2.  **Phase Out Ambient Declarations**: For each component converted to TypeScript, remove its corresponding entry from the ambient `index.d.ts` file to avoid conflicting type definitions.
3.  **Enforce Type Safety in CI**: Add TypeScript compiler checks to the continuous integration pipeline to ensure all new components and code changes include valid, accurate type annotations. This prevents regressions in type quality as the library grows.
4.  **Maintain Backward Compatibility**: Ensure all existing public APIs remain functional during and after the migration. Use TypeScript's compatibility features (e.g., optional properties, type widening) to avoid breaking existing JavaScript and TypeScript consumers of the library.
5.  **Documentation Sync**: Update contributing guidelines to require TypeScript for all new components and significant modifications to existing components, ensuring future development maintains the type-safe standard.
6.  **Finalize Transition**: Once all components are converted to TypeScript, fully remove the manual `index.d.ts` file. Publish the library with TypeScript types generated directly from the source code, leveraging the TypeScript compiler's ability to emit type definition files alongside JavaScript outputs.

If a full immediate migration is not feasible due to resource constraints, we will first adopt JSDoc type annotations in existing JavaScript components to enable TypeScript to infer accurate types from the implementation. This intermediate step reduces manual type maintenance while we work toward a complete TypeScript migration.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision
For new Primer React components and component refactors, we will prioritize using the Box-based pattern for component creation over the styled-components pattern, unless a specific, documented exception applies.

## Rationale
1. **Design System Alignment**: The Box component is purpose-built to integrate natively with Primer’s design tokens, system props, and `sx` utility. This ensures components adhere to consistent design language and token usage out of the box, reducing the risk of diverging from established design standards— a common pitfall with styled-components, which can easily introduce custom CSS that ignores pre-approved tokens.
2. **Predictable Prop Handling**: The Box-based pattern simplifies merging component core styles with user-provided `sx` props (via utilities like `merge`), ensuring user overrides work consistently without requiring custom prop merging logic. In contrast, styled-components often need additional boilerplate to properly handle `sx` or system props, leading to inconsistent behavior across components.
3. **Reduced Cognitive Overhead**: Adopting a single primary pattern streamlines contributor onboarding and maintenance. Team members only need to learn one approach for building components, lowering the risk of fragmented code patterns across the codebase.
4. **Leaner Implementation**: Box abstracts low-level concerns like element rendering, system prop mapping, and style normalization, eliminating redundant boilerplate that is often required in styled-components implementations (e.g., redefining prop type definitions for style-related props).
5. **Ecosystem Compatibility**: Box’s object-style syntax aligns with broader Primer React conventions and existing utility functions, ensuring components integrate seamlessly with other library primitives and tools.

## Exceptions
The styled-components pattern may be used in rare, documented cases where:
- The component requires complex CSS features that cannot be efficiently expressed via Box’s `sx` utility (e.g., advanced nested selectors, component-scoped CSS variables not part of core design tokens, or keyframe animations that rely on styled-components’ built-in helpers).
- Wrapping non-HTML elements (e.g., third-party components) in a styled container where Box’s `as` prop is insufficient to achieve the desired styling outcome.

In all exception cases, the rationale for using styled-components must be documented in the component’s code comments or associated PR description, and the implementation should still leverage Primer’s design tokens wherever possible.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision
We formalize the `/drafts` export path in `@primer/react` for two distinct, well-defined use cases, with clear governance and lifecycle rules:

### 1. Primary Use Cases for `/drafts`
The `/drafts` path will be used exclusively for:
#### a. Parallel Component Versioning
When developing a major, breaking iteration of an existing root-exported component (e.g., a full redesign of `UnderlineNav` with incompatible API changes), the new iteration will be hosted in `/drafts` alongside the stable root version. This allows developers to test and adopt the new version incrementally while the stable version remains available for production use.

#### b. Pre-Release Work-in-Progress (WIP) Components
Components that are feature-incomplete, have unvalidated APIs, or require further stakeholder collaboration before general production adoption will be hosted in `/drafts`. These components are not intended for widespread production use without explicit alignment with the Primer React team.

### 2. Governance & Lifecycle Rules
#### For Parallel Component Versions:
- Draft versions must include documentation explicitly noting they are a replacement for the root component, including key API differences and migration guidance.
- Draft versions must meet the same code quality standards (testing, linting, type safety) as root components to ensure stability for early adopters.
- Once the draft version is validated via feedback, accessibility audits, and compatibility testing, it will replace the root-exported component. The old root version may be moved to a `/legacy` path if long-term support is required, otherwise it will be deprecated and removed in a future release.
- Post-promotion, the draft entry for the component will be deprecated with a runtime warning directing users to the root export, then removed in the next minor release.

#### For Pre-Release WIP Components:
- WIP components must include documentation stating their pre-release status, missing features, and a note that adoption requires collaboration with the Primer React team.
- WIP components must have core functionality tested but may defer full accessibility audits or edge-case testing until they are prepared for promotion.
- Once a WIP component is feature-complete, audited, and approved for general use, it will be moved to the root export path.
- Abandoned WIP components will be removed from `/drafts` with a clear commit message documenting the rationale for discontinuance.

### 3. Import & Usage Consistency
- All draft components must follow the established import pattern: `import {ComponentName} from '@primer/react/drafts'`. No draft components will be exported from the root package path.
- The package’s public documentation will clearly distinguish between root (stable/production-ready) and draft components, linking to relevant context for draft use cases.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision
To address the challenges of scaling snapshot testing while retaining its value for validating component behavior, we will adopt the following structured approach to snapshot testing in React projects:

1.  **Write targeted, intent-driven snapshot tests**
    - Each snapshot test must have a descriptive name that explicitly states the behavior it validates (e.g., `renders active state with highlighted class` instead of `renders correctly`).
    - Avoid snapshotting entire component trees unless the full structure is the explicit focus of the test. Instead:
      - Snapshot only specific critical elements or attributes (e.g., a button's disabled state, a dynamic class name, or a conditional text block) using `expect(element).toMatchSnapshot()` on the targeted portion of the rendered output.
      - For complex components, split tests into smaller, focused tests that validate one behavior each rather than a single test covering all possible states.

2.  **Clean up snapshots to reduce noise**
    - Configure custom Jest snapshot serializers to exclude non-deterministic or irrelevant values from snapshots, such as auto-generated keys, function references (like `[Function]`), dynamic timestamps, or third-party library-specific attributes that don't affect core component behavior.
    - Use `toJSON()` options (where supported by renderers) to prune unnecessary nodes or attributes before generating a snapshot.

3.  **Combine snapshots with explicit assertions for critical behavior**
    - For high-stakes component properties (e.g., accessibility attributes like `aria-label`, required form field attributes, or navigation links), supplement snapshot tests with explicit property-based assertions (e.g., `expect(renderedElement).toHaveAttribute('aria-role', 'dialog')`). This clarifies test intent and ensures critical requirements are not missed if snapshot diffs become difficult to parse.

4.  **Enforce snapshot quality standards**
    - Set a reasonable size limit for individual snapshots (e.g., no more than 50 lines of output) and reject PRs with oversized snapshots that do not clearly align with a specific test intent.
    - Require that all snapshot updates include a brief explanation in the PR description of why the change is intentional (e.g., ""Updated snapshot to reflect new primary button styling per design system v2"").
    - For all snapshot diffs in PRs, require a code review step where reviewers verify that the changes align with the intended component modifications and no unintended regressions are present.

5.  **Use inline snapshots for simple cases**
    - For small, straightforward component states (e.g., a static text label, a basic button in default state), use Jest's inline snapshot feature (`expect(element).toMatchInlineSnapshot()`) to embed the snapshot directly in the test file. This keeps the expected behavior alongside the test logic, making intent immediately clear without switching between test and snapshot files.

6.  **Revisit and refactor stale snapshots**
    - Periodically audit existing snapshot tests to remove redundant or outdated tests. If a component's purpose has changed, update or replace its snapshot tests to reflect the current intended behavior.
    - When a component is split into sub-components, migrate relevant snapshot tests to the sub-component level to maintain focused, modular test coverage.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision
1. **Centralized Tree-Shakable Warning Utility**: Implement a shared, environment-aware warning utility in `@primer/react` to handle all developer-facing warnings (deprecation notices, unsupported feature alerts, invalid prop checks, etc.). The utility will use statically analyzable environment checks to ensure warning code is completely eliminated from production builds.

   Example implementation:
   ```javascript
   // src/utils/warn.ts
   export function warn(message: string, ...args: unknown[]) {
     // Static check for development environment (recognized by bundlers like Webpack/Rollup/Vite)
     if (process.env.NODE_ENV !== 'production') {
       // eslint-disable-next-line no-console
       console.warn(`[Primer React Warning]: ${message}`, ...args);
     }
   }
   ```
   This pattern ensures bundlers can tree-shake the entire warning logic (including the `console.warn` call) when building for production, as the `process.env.NODE_ENV === 'production'` check is a static condition that evaluates to true, eliminating the dead code.

2. **Enforce Utility Usage**: Mandate that all warnings must use this centralized utility instead of direct `console.warn()` calls. This ensures consistent warning formatting, centralized control over environment behavior, and prevents accidental inclusion of warning code in production.

3. **Guard Runtime Warning Checks**: For warnings dependent on runtime logic (e.g., detecting use of a deprecated prop), wrap the entire runtime check in the same static environment guard to eliminate the check code from production builds, avoiding unnecessary runtime overhead:
   ```javascript
   import { warn } from '../utils/warn';

   function ExampleComponent(props) {
     if (process.env.NODE_ENV !== 'production') {
       // Runtime check only runs in development
       if (props.deprecatedProp != null) {
         warn('`deprecatedProp` is deprecated. Use `newProp` instead.');
       }
     }
     // Component logic
   }
   ```

4. **Linting Enforcement**: Add an ESLint rule to block direct `console.warn()` calls in the codebase, with an exception only for the centralized warning utility. This catches accidental direct uses during development and CI checks:
   ```javascript
   // .eslintrc.js
   module.exports = {
     rules: {
       'no-console': ['error', { allow: ['error'] }], // Allow console.error, block warn/info
       // Optional: Custom rule to allow console.warn only in src/utils/warn.ts
     }
   };
   ```

5. **Documentation & Validation**:
   - Add a section to the contributor guide outlining the warning pattern, including utility usage, guard clauses, and validation steps for tree-shaking.
   - Include automated checks in CI to verify that production build artifacts contain no trace of warning logic (e.g., static code analysis of minified bundles to confirm absence of `console.warn` or warning messages).
   - When deprecating features, reference this warning utility in the deprecation documentation to ensure consistent communication with developers.

6. **Error vs. Warning Distinction**: Reserve `console.error()` for critical runtime issues that require immediate attention even in production (e.g., invalid configurations that break core functionality). These should be used sparingly and not wrapped in environment guards, as they indicate production-failing states.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision
1. **Adopt a centralized, standardized live region system** for Primer React to resolve inconsistent screen reader announcements caused by ad-hoc implementations, dynamic DOM manipulation, and incorrect ARIA attribute combinations. This system will become the single source of truth for all accessibility announcements across the library.

2. **Implement a singleton, persistently rendered live region root component** that remains in the DOM at all times (not dynamically injected/removed). This component will manage validated ARIA attribute combinations tailored to common use cases, eliminating issues where screen readers fail to detect dynamically added live regions. It will support four preconfigured modes aligned with WCAG guidelines:
   - `polite`: Non-urgent announcements that wait for user interaction to finish (default)
   - `assertive`: Urgent updates that interrupt user interaction (reserved for critical events)
   - `status`: Short, non-intrusive status updates (maps to the `status` ARIA role with `aria-live=""polite"" aria-atomic=""true""`)
   - `alert`: Time-sensitive, critical alerts (maps to the `alert` ARIA role with `aria-live=""assertive"" aria-atomic=""true""`)

3. **Expose a centralized announcement API** (e.g., `import { announce } from '@primer/react'`) for programmatic announcements. The API will accept:
   - A mandatory message string or sanitized React node
   - An optional options object to specify mode (`mode?: 'polite' | 'assertive' | 'status' | 'alert'`) and whether to clear pending messages (`clearPrevious?: boolean`).
   This API will queue or replace messages in the singleton live region component, ensuring reliable detection by screen readers.

4. **Define standardized props for component-integrated live regions**: For components with dynamic content (e.g., buttons with updating counts), provide prevalidated props like `liveRegionProps` or `isLiveRegion` that inject tested ARIA attribute combinations. This abstracts low-level ARIA logic from component developers, reducing errors from incorrect attribute usage.

5. **Deprecate ad-hoc live region implementations**: Mark existing manual uses of `aria-live`, `alert`, or `status` roles in individual components as deprecated, and provide migration paths to the centralized system. Update contributing guidelines to require use of the system for all new live region functionality.

6. **Enforce cross-screen-reader testing**: Pre-validate the centralized system across major screen readers (VoiceOver, NVDA, JAWS) and browsers to ensure consistent announcements. Require that all live region-dependent components are tested against this validated system to maintain accessibility compliance.

## Rationale
- **Consistency**: Eliminates screen reader-specific failures caused by untested ad-hoc implementations.
- **Developer Experience**: Simplifies accessibility announcements with a straightforward API, removing the need to memorize correct ARIA attribute combinations or revalidate basic functionality.
- **Accessibility Compliance**: Aligns with WCAG 2.1+ success criteria (e.g., 4.1.3 Status Messages) and addresses common live region pitfalls documented in accessibility best practices.
- **Maintainability**: A single source of truth makes it easy to update live region behavior as screen reader support evolves, without modifying every component that uses announcements.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"It appears the provided Context section is incomplete and does not include specific details about the architectural problem, constraints, goals, tradeoffs, or challenges facing Arachne that require a decision.

To formulate a meaningful architecture decision for Arachne, please share context such as:
- Specific functional/non-functional requirements that need addressing
- Conflicting priorities or tradeoffs to balance
- Current pain points with existing architecture
- Desired capabilities or constraints unique to Arachne's use case
- Comparisons of potential approaches under consideration

Reference the prior Context examples (e.g., around modularity, transparency, avoiding runtime entanglement, or ADR workflow) for guidance on the level of detail needed to drive a targeted, well-justified architecture decision. Once you provide the full relevant context, I can draft the corresponding decision aligned with Arachne's established architectural principles.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision 
We will unify our component testing workflow around Storybook as the canonical source of truth for component scenarios, and integrate visual debugging tools for Jest to resolve our current testing setbacks. The specific steps are:

1. **Eliminate duplicate markup by reusing Storybook stories in Jest tests**
   - Adopt `@storybook/testing-library` and `@storybook/jest` to import existing Storybook stories directly into our Jest test files. This allows us to run automated assertions and interactive tests (e.g., simulating clicks, form inputs) against the exact component configurations, args, and states defined in stories—no need to reconstruct component scenarios separately in Jest.
   - For complex interactive workflows, enhance stories with dynamic args, controls, and play functions to capture all testable scenarios (base states, edge cases, user flows) in one place. Tests will reference these stories instead of duplicating markup.

2. **Add visual debugging capabilities for Jest tests**
   - Integrate Jest Preview into our testing setup. This tool will automatically launch a browser window displaying the rendered component whenever a Jest test fails, providing immediate visual context for debugging UI-related issues (e.g., broken layouts, incorrect state rendering).
   - Link all Jest tests to their corresponding Storybook stories via inline comments or test descriptions. This gives contributors visibility into the visual behavior of test scenarios and lets them inspect components in Storybook to better understand or enhance existing tests.

3. **Update our component testing workflow**
   - Require that all new components have comprehensive Storybook stories covering all testable scenarios before automated Jest tests are written. Stories will serve as the single source of truth for component behavior.
   - When updating component behavior, modify stories first to reflect changes, then update linked Jest tests to align with the revised story configurations.

4. **Maintain existing testing guardrails**
   - Keep our existing Chromatic visual regression testing pipeline intact, as it complements this workflow by catching unintended UI changes across all story states.
   - Retain Jest for unit testing non-component logic (e.g., utility functions) where visual context is not needed.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision
1. Adopt **npm Workspaces** as the official, standardized tool for managing the primer/react monorepo structure, inter-workspace dependencies, and unified dependency installation.
2. Restructure the repository to formalize all existing workspaces in the root `package.json`:
   - Add a `workspaces` field listing all current workspaces (e.g., `""workspaces"": [""packages/react"", ""docs"", ""examples/*"", ""packages/scripts""]`, adjusted to match the repo’s actual directory structure).
   - Deprecate the custom `npm run setup` script; contributors will instead run `npm install` from the root, which automatically installs all dependencies across all workspaces and handles inter-workspace linking natively.
3. Standardize inter-workspace dependencies using npm’s official syntax:
   - For any workspace that depends on another (e.g., `docs` relying on `@primer/react`), update the dependent’s `package.json` to declare the dependency as `""@primer/react"": ""workspace:*""` instead of relative paths, file dependencies, or `npm link`. This ensures consistent, automatic linking between workspaces during development and builds.
4. Refactor complex scripts into a dedicated workspace:
   - Move scripts from the `script/` directory that require their own dependencies or significant complexity into a new `packages/scripts` workspace. This workspace will have its own `package.json` to manage script-specific dependencies independently, avoiding clutter in the root or other workspace dependency lists.
   - Keep simple, dependency-free utility scripts in the root `package.json`’s `scripts` field or a slimmed-down `script/` directory if they do not warrant a full workspace.
5. Update contributing documentation to reflect the new workflow:
   - Instruct contributors to run `npm install` from the repository root instead of per-workspace installs or the old setup script.
   - Document how to declare and update inter-workspace dependencies using the `workspace:*` syntax for future maintainers and contributors.

### Rationale
- npm Workspaces is a native, well-supported tool that eliminates ad-hoc dependency linking methods (relative paths, `npm link`), reducing friction and potential configuration errors.
- Unified root installation simplifies onboarding and streamlines dependency management for contributors.
- A dedicated scripts workspace enables clean separation of concerns, keeping script dependencies isolated from application code and preventing dependency bloat in other workspaces.
- Standardized dependency declarations ensure consistency across the repo and align with established npm monorepo best practices.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision
We will restructure the @primer/react codebase and update our build and package configuration to explicitly separate public API surfaces from internal implementation code, preventing unintended exposure of non-public modules to consumers.

### Specific Action Items:
1. **Codebase Restructuring**
   - Move all internal implementation code (components, utilities, hooks, etc. not intended for public use) into a dedicated `src/internal` directory. This directory will be treated as non-public and excluded from our public API exports.
   - Keep only modules intended for public consumption in the top-level `src` directory (or a `src/public` subdirectory, for grouped public components) to make the public API structure immediately visible to contributors.

2. **Package.json Exports Refinement**
   - Replace the broad `./lib-esm/*` wildcard in the `exports` field with explicit, targeted paths to public modules. For example:
     ```json5
     {
       ""exports"": {
         ""./Button"": ""./lib-esm/Button.js"",
         ""./Alert"": ""./lib-esm/Alert.js"",
         ""./hooks/useTooltip"": ""./lib-esm/hooks/useTooltip.js""
         // ... additional public entry points
       }
     }
     ```
   - Ensure no paths pointing to the compiled `internal` directory are included in `exports`, so external consumers cannot import internal modules via valid package entry points.

3. **Build Configuration Update**
   - Adjust the Rollup configuration to compile both public and internal code as needed for the package's functionality, but place compiled internal code in a `lib-esm/internal` subdirectory that is not exposed via `exports`.
   - Add a build step check to verify that no internal modules are accidentally included in public export paths.

4. **Enforcement & Documentation**
   - Add an automated lint rule to flag imports of `src/internal` modules from within public API code, preventing accidental leakage of implementation details into public components.
   - Update contributing documentation to clearly define:
     - The purpose of the `src/internal` directory
     - How to add new public API components without exposing internal logic
     - The process for reviewing changes to the public API to ensure alignment with intended usage","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision
Shift Primer React's styling architecture from runtime style injection to a static, CSS variable-powered system to resolve performance bottlenecks, enable robust theming, and enforce stronger design system guardrails.

### Specific Implementation & Governance Decisions
1. **Static CSS Foundation**
   - Precompile all core component, utility, and base styles into static CSS files, using `primer/primitives` as the single source of truth for design values (colors, spacing, typography, etc.).
   - Serve these static files directly in application builds to enable parallel browser downloading and caching, eliminating runtime injection overhead to cut initial page load times.

2. **Dynamic Styling via CSS Variables**
   - Replace runtime-generated inline styles/CSS rules with CSS variables mapped directly to `primer/primitives` values. All dynamic component variations (state changes, size options, theme modes) will be controlled by toggling pre-defined CSS classes that reference these variables.
   - This eliminates the linear performance cost of runtime style updates, as changes only require class swaps rather than generating new styles.

3. **Component API Refinement**
   - Deprecate the runtime-heavy `sx` prop for all core Primer React components, replacing it with explicit, design-system-aligned props (e.g., `size=""sm""`, `variant=""default""`) that map to static CSS classes.
   - Maintain a limited, documented escape hatch for edge-case dynamic styles (unique layout constraints not covered by design system utilities) but enforce guardrails: require a code comment justifying usage, and restrict to non-core components only.

4. **SSR Performance Optimization**
   - Eliminate the double-rendering requirement in SSR workflows by removing runtime style collection. Applications will serve pre-compiled static CSS alongside server-rendered HTML, cutting SSR latency by removing the additional rendering cycle needed to gather runtime styles.

5. **Design System Guardrails**
   - Enforce all styling to consume `primer/primitives` exclusively through CSS variables (no direct value imports in JavaScript) to ensure consistent adherence to design system standards, simplify theme updates, and reduce value duplication.

6. **Phased Transition Plan**
   - **Phase 1**: Release static CSS-enabled versions of high-impact components (Box, IconButton, Card) alongside existing runtime versions, with automated codemods to assist application migration.
   - **Phase 2**: Mark runtime style injection APIs as deprecated, monitor adoption of static alternatives, and provide ongoing support for migration.
   - **Phase 3**: Fully retire runtime style injection from core components after a 6-month deprecation period, retaining legacy wrappers only for critical applications with extended support needs.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision
1.  Repeal the recommendation from ADR 009 to use Storybook play functions as the primary method for authoring user interaction tests.
2.  Standardize on **Playwright** as the single tool for all browser-based interaction tests, including those validating component behavior tied to Storybook stories.
3.  Define a unified workflow for Storybook-aligned interaction tests:
    - For each Storybook story requiring interaction validation, author a corresponding Playwright test that navigates directly to the story's URL (using Storybook's built-in URL structure or story ID).
    - Reuse Storybook story args and parameters in Playwright tests where possible (e.g., via shared test data files) to avoid duplication of component setup logic between documentation and tests.
    - Store Playwright interaction tests alongside existing Playwright visual regression tests in a dedicated directory structure (e.g., `tests/playwright/interactions`) for consistency.
4.  Retain Jest as the tool for unit and integration tests that validate component or logic behavior in isolation (without a full browser context).
5.  Continue using Playwright for all Visual Regression Tests, unifying both interaction and visual validation under a single browser testing framework.

## Rationale
This decision addresses key pain points associated with relying on Storybook play functions:
- Eliminates flakiness common to play functions, which lack Playwright's built-in wait-for-element and retry logic for stable test execution.
- Leverages Playwright's robust debugging tools (step-through execution, devtools integration, video recording) which are far more comprehensive than Storybook's limited play function debugging capabilities.
- Ensures consistency in test syntax, assertion libraries, and reporting across all test types, reducing cognitive load for the team.
- Avoids siloing test logic between Storybook (play functions) and dedicated test suites, making it easier to reuse, maintain, and update test coverage as components evolve.
- Unifies browser-based testing (interaction + visual regression) under one tool streamlining CI/CD pipeline configuration and test execution overhead.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision
We will implement a dual-pattern approach for responsive props to eliminate server-side rendering (SSR) layout shifts while maintaining flexibility for all use cases:

1.  **CSS Media Query Resolution (Preferred for Layout/Visual Props)**: For any responsive prop that impacts layout or visual styles (e.g., `gap`, `padding`, `width`), component authors must use native CSS media queries instead of client-side JavaScript resolution. This ensures the browser applies the correct style immediately on page load (before JS hydration) to avoid layout shifts.
2.  **JS-Based Resolution (For Non-Visual Logic)**: For responsive props that control non-visual behavior (e.g., toggling a feature, showing/hiding a non-critical UI element via logic), we will retain the `useResponsiveValue` hook but add guardrails to prevent misuse for visual/layout props.

To standardize this approach, we will provide component authors with:
- A shared `ResponsiveValue<T>` TypeScript type that enforces consistent shape for responsive prop inputs (either a single value or a partial/full map of breakpoints to values).
- A `generateResponsiveStyles` utility function that converts a responsive prop value into CSS media query rules aligned with our named breakpoints (narrow, regular, wide). This utility will output either inline style objects (for components using inline styles) or CSS class definitions (for components using styled systems).
- A `useSSRWarningResponsiveValue` wrapper around the existing hook that logs a clear warning if used with props marked as visual/layout, guiding authors to use CSS-based resolution instead.
- Updated component documentation that explicitly differentiates between visual/layout and logic-focused responsive props, with code examples for each resolution pattern.

### Example Implementation
For a `Stack` component's `gap` prop:
```tsx
// Component code using generateResponsiveStyles
import { generateResponsiveStyles, ResponsiveValue } from '@arachne/utils';

type StackProps = {
  gap: ResponsiveValue<'condensed' | 'normal' | 'spacious'>;
};

const Stack = ({ gap }: StackProps) => {
  const gapStyles = generateResponsiveStyles(gap, (value) => ({
    '--stack-gap': `var(--spacing-${value})`,
  }));

  return <div style={{ gap: 'var(--stack-gap)', ...gapStyles }} className=""stack"" />;
};

// The generateResponsiveStyles utility outputs:
{
  '@media (min-width: var(--breakpoint-regular))': { '--stack-gap': 'var(--spacing-normal)' },
  '@media (min-width: var(--breakpoint-wide))': { '--stack-gap': 'var(--spacing-spacious)' },
}
```

## Rationale
- **CSS media queries eliminate layout shifts**: Browsers process CSS before executing JavaScript, so responsive styles are applied immediately when the page loads. This matches the client's viewport size without waiting for hydration, eliminating server-client layout mismatches.
- **Leverages native browser performance**: CSS-based responsive design is more efficient than JS-based resolution, as it offloads work to the browser's optimized layout engine.
- **Clear separation of concerns**: Distinguishing between visual/layout and logic-focused props ensures we prioritize user experience where it matters most (layout stability) while retaining flexibility for valid non-visual use cases.
- **Consistent author experience**: Shared types and utilities reduce boilerplate for component authors and ensure uniform implementation across the library.

## Consequences
### Positive
- Eliminates layout shifts for all responsive layout/visual props during SSR, improving user experience and Core Web Vitals (CLS).
- Aligns with web standards and leverages native browser capabilities for better performance.
- Clear guidelines reduce confusion for component authors and prevent accidental misuse of JS-based resolution for visual props.

### Negative
- Component authors must implement two distinct resolution patterns, adding minor complexity to component code.
- CSS bundle sizes may increase slightly due to breakpoint-specific styles, but this is offset by the critical improvement in layout stability.
- JS-based resolution is still available but restricted to non-visual use cases, requiring strict documentation enforcement.

## Alternatives Considered
1. **Server-Side User-Agent Sniffing**:
   - Rejected: User agents are unreliable (easily spoofed, many devices have unknown agents), and mapping agents to breakpoints is error-prone. Layout shifts may still occur if the agent hint does not match the actual viewport size.
2. **Client-Side Only Rendering**:
   - Rejected: Negates the benefits of SSR (SEO, initial load performance, perceived speed), which is a core requirement for Arachne applications.
3. **Dynamic SSR with Client Hints**:
   - Rejected: Requires server infrastructure to handle per-request viewport hints, adding deployment complexity and limiting compatibility with all hosting environments.
4. **Ignoring Layout Shifts**:
   - Rejected: Layout shifts harm user experience and violate Core Web Vitals, which are critical for accessibility and search ranking.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision 
To handle prop deprecations (e.g., the `title` prop on `ActionList.Group` in favor of `ActionList.GroupHeading`) while meeting usability, backward compatibility, and migration clarity goals, we will follow this standardized process:

### 1. Deprecation Visibility & Communication
- **Type-level Warnings**: Mark deprecated props with the `@deprecated` JSDoc tag in TypeScript definitions, paired with a message that specifies the replacement API (e.g., `@deprecated Use ActionList.GroupHeading instead. See [link to migration guide] for details`). This ensures authors receive in-IDE warnings during development.
- **Runtime Warnings**: Add a console.warn call that triggers when the deprecated prop is used. The warning will include:
  - The name of the deprecated prop and component.
  - A direct reference to the replacement API.
  - A link to a dedicated migration section in the project’s documentation for full context.
  - The version in which the deprecated prop will be removed.

### 2. Temporary Backward Compatibility Shim
- Implement a backward compatibility layer to maintain existing functionality for deprecated props during the transition period. For example:
  - When the `title` prop is passed to `ActionList.Group`, internally render an `ActionList.GroupHeading` component with the `title` value as its children, and default to a sensible heading level (e.g., `h2`) if not specified.
  - Allow passing additional props to the shimmed component (if applicable) to support partial customization until authors fully migrate.

### 3. Explicit Migration Guidance
- Update component documentation to:
  - Display the deprecated prop with a ""Deprecated"" badge and link to the replacement.
  - Include side-by-side before/after code snippets (like the example provided in context) to demonstrate the migration path.
  - Explain the rationale for the change (e.g., improved accessibility via customizable heading levels, greater flexibility for adding additional content to group headers).
- Publish a project-level migration guide that aggregates all active deprecations and their steps, linked from runtime and IDE warnings.

### 4. Deprecation Timeline
- For all deprecated props:
  - Keep the compatibility shim and warnings active for a minimum of two minor versions (or one major version, whichever is longer) to give authors sufficient time to migrate.
  - Announce the upcoming removal in release notes for each version leading up to the removal.
  - Once the timeline expires, remove the deprecated prop, shim, and all associated warnings from the codebase.

### 5. Standardization for Future Deprecations
- Apply this same process to all future prop or API deprecations in Arachne components to ensure consistency, predictable migration paths, and minimal disruption for authors.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision
1. Arachne self-hosted runners will support proxy configuration via the industry-standard environment variables, prioritizing widespread compatibility with common tools and enterprise practices:
   - Primary supported variables (lowercase, aligned with majority tooling): `http_proxy`, `https_proxy`, `no_proxy`
   - Compatibility aliases (uppercase): `HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY` (lowercase variables take explicit precedence if both case variants are set to resolve ambiguity)

2. The runner will propagate these proxy environment variables to the job execution environment automatically. This ensures dependent tools (curl, git, docker, etc.) used within jobs inherit the proxy configuration without requiring per-job setup.

3. No hardcoded proxy configuration will be built into the runner; all proxy settings must be sourced from the environment to maintain flexibility for diverse enterprise proxy setups.

4. The following variable formats will be enforced and documented:
   - `http_proxy`/`https_proxy`: Must be a valid URL including protocol (e.g., `http://proxy.example.com:8080`) and optional authentication (e.g., `http://user:pass@proxy.example.com:8080`)
   - `no_proxy`: Comma-separated list of values, including hostnames (e.g., `localhost`), domains (e.g., `.example.com`), CIDR ranges (e.g., `192.168.1.0/24`), and loopback addresses (e.g., `127.0.0.1`)

5. Clear documentation will be provided outlining the proxy configuration process, variable precedence rules, format requirements, and examples for common enterprise proxy scenarios. This documentation will also note that the runner’s proxy handling aligns with the behavior of widely adopted tools like curl, git, and docker to reduce user friction.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision
Implement a conditional command echoing system with the following structured rules to balance log cleanliness and troubleshooting accessibility:

1. **Default State**: Command echoing is disabled by default to avoid cluttering user logs. The only exception is when the `ACTIONS_STEP_DEBUG` secret is enabled; in this case, command echoing defaults to on to support detailed debugging workflows.
2. **Error Context Preservation**: Regardless of the current echoing state, full command outputs will be automatically echoed in user logs if an error occurs during command processing. This ensures users receive sufficient context to diagnose and resolve failures without requiring explicit debug mode activation.
3. **User-Controlled Toggling**: Introduce explicit control commands `::echo::on` and `::echo::off` that let users manually toggle command echoing at any point in their workflow. These commands override both the default state and the `ACTIONS_STEP_DEBUG`-driven default, giving users fine-grained control over log verbosity.

Document these rules in official user-facing documentation to clarify when command output appears in logs and how to adjust the behavior to meet specific workflow needs.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision
1. **Implement a standardized metadata injection mechanism in the runner**: Add an opt-in, hook-based system to the runner’s ""Set up job"" workflow that enables hosted image maintainers, extension developers, or self-hosted runner administrators to register scripts or data sources for collecting environment-specific extra information (e.g., OS version, pre-installed software versions, image build timestamp, hardware specs).
2. **Structured, backward-compatible log formatting**: The collected extra information will be injected into the ""Set up job"" log step under a distinct, clearly labeled section (e.g., `### Hosted Environment Metadata`) to separate it from core setup operations. Data will be formatted as key-value pairs or a bulleted list for human readability, and enclosed in a machine-parseable block (e.g., wrapped in `<!-- Arachne: Environment Metadata -->` comments) to avoid breaking existing log parsing tools.
3. **Opt-in configuration**: The feature will be disabled by default to preserve backward compatibility. Hosted image providers or runner admins can enable it via a dedicated configuration flag (e.g., `runner.environment_metadata.enabled=true`) and specify the path to their metadata collection script or data file.
4. **Reference sample for hosted images**: Provide a pre-built sample metadata collection script for hosted image maintainers that captures common useful details (OS distribution/version, Node.js/Java/Docker versions, image creation date) to lower adoption barriers.
5. **Graceful fallback**: If no metadata is configured or the collection script fails, the runner will skip injecting extra content and proceed with the original ""Set up job"" log output to avoid disrupting workflows.

## Rationale
- This approach directly addresses the need to surface transparent, actionable environment details to users without cluttering core job setup logs.
- The hook-based design ensures modularity: different environments can provide tailored metadata without modifying core runner code, aligning with Arachne’s modularity goals.
- Structured formatting balances human readability and machine compatibility, preserving functionality for existing log analysis tools while enhancing user visibility into their job execution context.
- Opt-in behavior minimizes disruption to existing workflows, ensuring only environments that need this feature enable it.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision
Accept the proposal to add `steps.<id>.outcome` and `steps.<id>.conclusion` fields to the steps context to enable outcome-based conditional execution of downstream steps and improve pipeline transparency.

### Specification of New Fields
#### 1. `steps.<id>.outcome`
- **Type**: String (strict enum with allowed values: `""success""`, `""failure""`, `""cancelled""`, `""skipped""`)
- **Purpose**: A machine-readable, standardized status of the step’s execution result, designed explicitly for use in conditional logic (e.g., triggering downstream steps only if a prior step succeeded).
- **Population Rules**:
  - Populated for all steps, regardless of whether they were executed, cancelled, or skipped, to ensure consistent context for downstream evaluations.
  - Enforced at the step execution layer to prevent invalid or inconsistent values (rejects any value not in the enum set).

#### 2. `steps.<id>.conclusion`
- **Type**: Optional string (nullable)
- **Purpose**: A human-readable summary of why the step ended with its reported outcome, intended for debugging and pipeline visibility.
- **Population Rules**:
  - Defaults to a generic message tied to the `outcome` if no explicit conclusion is provided (e.g., ""Step completed successfully"" for `""success""`, ""Step was skipped"" for `""skipped""`).
  - May be overwritten by steps to include granular context (e.g., ""Failed: Timeout after 300s connecting to database"", ""Skipped: No changes detected in source code"").

### Implementation Guidelines
1. **Backward Compatibility**: The new fields are additive and will not modify or break existing usage of `steps.<id>.outputs`. No changes to existing pipelines are required to adopt these fields.
2. **Conditional Execution Integration**: Update the pipeline’s conditional expression parser to support referencing `steps.<id>.outcome` in `when` clauses (e.g., `when: steps.test.outcome == ""success""` to trigger a deploy step only after passing tests).
3. **Edge Case Handling**:
   - For cancelled steps: `outcome` is set to `""cancelled""`, with `conclusion` defaulting to ""Step was cancelled"" or including a trigger context (e.g., ""Cancelled by user request"").
   - For skipped steps: `outcome` is set to `""skipped""`, with `conclusion` reflecting the reason for skipping (e.g., ""Skipped due to conditional check failure"").

### Rationale
These changes directly address the need for flexible pipeline workflows by allowing downstream steps to react intentionally to prior step results. The standardized `outcome` field ensures reliable conditional logic, while `conclusion` improves pipeline transparency by making execution context easily accessible for human debugging. This aligns with best practices for modular, expressive pipeline design without introducing breaking changes to existing systems.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision
Extend the self-hosted runner registration and reconfiguration workflows to support defining custom labels via command-line arguments, enabling automated, script-driven setup of runner labels alongside the built-in OS/arch labels.

### Detailed Implementation & Rationale
To address the automation and modular labeling needs outlined in the context (Issue #262, ADR275 revision), we will implement the following:

1.  **CLI Argument Design**
    - Add an optional `--labels` flag (short form `-l`) to the `config.sh`/`config.cmd` registration command.
    - Support two input formats for flexibility in scripts:
      - A comma-separated list of labels: `./config.sh --labels label1,label2,prod-tier`
      - Multiple instances of the flag: `./config.sh -l label1 -l label2 -l prod-tier`
    - Both formats will merge into a single set of unique labels to avoid duplicates.

2.  **Label Merging & Persistence**
    - Combine user-provided custom labels with the built-in OS/arch labels during registration (e.g., `linux,x64` + `prod-tier,java17` = full label set).
    - Store the complete label set (built-in + custom) in the runner's local `.runner` configuration file to retain labels across restarts and reconfigurations.
    - During registration, ensure no duplicate labels are sent to the GitHub Actions API (ignore custom labels that match built-in labels like `linux` or `x64`).

3.  **Backward Compatibility**
    - Preserve default behavior: if no `--labels` flag is provided, runners will continue to register only with built-in OS/arch labels, maintaining full backward compatibility with existing automation scripts.

4.  **Validation**
    - Enforce GitHub's label constraints (no whitespace, invalid special characters) during registration; return a clear error message if invalid labels are provided to prevent invalid state.

5.  **Reconfiguration Support**
    - Extend the `reconfigure` subcommand to accept the same `--labels` flag. When used, this will replace the existing set of custom labels (built-in labels remain unchanged) to support dynamic label updates without full re-registration.

### Rationale
This design directly addresses the core need for scriptable label configuration, eliminating manual post-registration label edits. It aligns with automation best practices for self-hosted runner fleets, maintains backward compatibility, and adheres to GitHub's label standards to ensure seamless integration with the Actions service. The revision builds on ADR275 by prioritizing simplicity, flexibility, and robustness for automated workflows.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision 
Implement a layered, configurable fault tolerance strategy for communication with third-party authentication providers to handle transient faults, aligned with Arachne's modular, transparent, and data-driven design principles:

### 1. Retry with Context-Aware Backoff
- Apply automatic retries exclusively to confirmed transient fault types, including:
  - HTTP 5xx status codes (server-side errors)
  - Connection timeouts or reset errors
  - Rate-limit responses (HTTP 429) that include a `Retry-After` header
- Use exponential backoff with jitter to avoid thundering herd issues, falling back to provider-specific `Retry-After` values when present instead of fixed intervals.
- Limit maximum retry attempts to 3–5 configurable attempts per request to prevent infinite loops and reduce load on both the application and provider.
- Expose retry behavior configuration via Arachne's data-driven config schema, allowing per-provider customization of retry counts and backoff parameters.

### 2. Circuit Breaker Pattern for Sustained Failures
- Implement a per-provider circuit breaker that tracks request failure rates over a sliding window (e.g., 50% failure rate across 10 requests in 1 minute).
- Define three circuit states:
  - **Closed**: Normal operation, all requests are sent to the provider.
  - **Open**: Triggered when failure thresholds are met; immediately return a fallback response instead of sending requests to the provider to prevent cascading failures.
  - **Half-Open**: Periodically send a test request to the provider after a configurable delay (e.g., 30 seconds) to check for recovery before re-opening the circuit fully.
- Circuit breaker state transitions and thresholds will be configurable via Arachne's config system.

### 3. Graceful Fallback & User Transparency
- For requests that fail after retries or when the circuit is open:
  - Return a user-friendly, localized error message indicating temporary authentication issues (configurable per application).
  - Offer an optional fallback authentication mechanism (e.g., local password authentication, if configured in the application) as a configurable feature for critical use cases.
- Ensure all failure details are logged with context (request ID, provider name, error type, retry attempts) for debugging, while avoiding exposure of sensitive provider data to end users.

### 4. Monitoring & Observability
- Track and expose metrics for provider communication (failure rates, retry counts, circuit breaker state transitions, rate limit hits) via Arachne's existing monitoring endpoints (e.g., Prometheus-compatible metrics).
- Trigger alerts on critical events such as circuit breaker state changes, sustained high failure rates (>10 minutes), or repeated rate limit violations.
- Include links to related fault tolerance config entries in metric metadata to maintain alignment with Arachne's transparent, introspectable design goal.

### 5. Modular Integration
- Package the fault tolerance logic as a reusable, swappable Arachne module that can be added to any component interacting with third-party services, not just authentication providers.
- Avoid tight coupling to specific HTTP clients or authentication protocols by abstracting fault tolerance behavior behind a data-driven interface, consistent with Arachne's modularity principle.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision
We will implement a formal, public Architecture Decision Record (ADR) practice for all significant project decisions to foster transparency, inclusive contribution, and enduring, actionable documentation.

### Key Details of the Practice:
1. **Scope of Decisions to Document**:
   All decisions that impact project direction, core architecture, critical feature design, contributor workflows, or community governance will be captured as ADRs. Small, routine implementation choices (e.g., variable naming, minor bug fix approaches) do not require an ADR unless they set a precedent for future work.

2. **Standardized ADR Template**:
   All ADRs will use a consistent, minimal template including:
   - Unique identifier (sequential number + descriptive title, e.g., `0001-public-adr-practice.md`)
   - Context: Background and problem driving the decision
   - Decision: Explicit statement of the chosen path
   - Alternatives Considered: List of rejected options with brief rationale for exclusion
   - Consequences: Expected positive/negative outcomes of the decision
   - Status: Proposed → Under Review → Accepted/Rejected/Deferred
   - Date: When the final decision was made
   - Contributors: Names of those involved in drafting or reviewing the ADR

3. **Inclusive Workflow**:
   - **Proposal**: Any team member or external contributor may draft and submit a proposed ADR via the project's public channels (e.g., GitHub Discussions, dedicated ADR repo pull request, or public community forum).
   - **Public Review**: All proposed ADRs will be open to feedback from the entire community for a minimum of 7 calendar days. Feedback will be documented directly in the ADR's discussion thread or PR comments.
   - **Revision & Alignment**: The proposer will revise the ADR to incorporate constructive feedback, addressing unresolved concerns explicitly. If consensus is hard to reach, the core team will facilitate a recorded public sync discussion to surface and resolve key disagreements.
   - **Final Decision**: The core team will make a consensus-based decision (accepted, rejected, or deferred) after the review period, documenting the rationale in the ADR's status section.
   - **Archiving**: All ADRs (regardless of status) will be stored in a dedicated, public directory in the project's main repository or a standalone ADR repository for permanent reference.

4. **Revisiting Decisions**:
   If changing project context, new information, or community feedback warrants re-evaluating an accepted decision, a new ADR will be created that references the original record. The new ADR will follow the same proposal and review workflow, and once accepted, the original ADR will be updated to note it has been superseded, with a link to the new decision.

5. **Immutability of Accepted ADRs**:
   Once an ADR is marked ""Accepted"", its core content (Context, Decision, Alternatives, Consequences) will not be modified. Only minor clarifications or links to superseded records may be added to maintain accuracy without altering the historical record of the decision.

This practice ensures our decision-making process is open, accountable, and preserves a clear trail of our project's evolution for current and future contributors.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂÃÂ¡ÃÂÃÂª with all relevant discussion summarized ÃÂÃÂ¡ÃÂÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision
1. **Standardize Extension URI Schema**
   Define a dedicated, self-describing URI schema for Arachne extensions to uniquely identify them in decentralized storage. The URI structure will include mandatory metadata and storage protocol hints to ensure consistency and resolvability:
   `arachne-ext://<author-identifier>/<extension-name>/<semantic-version>[?storage=<decentralized-protocol>&cid=<content-identifier>][#<content-hash>]`
   - Mandatory fields: Author identifier (to avoid namespace collisions), extension name, semantic version (for version pinning and dependency resolution).
   - Optional fields: Decentralized storage protocol (e.g., `ipfs`, `s3`, `dat`), content identifier specific to the protocol (e.g., IPFS CID), and a top-level content hash (SHA-256) for integrity verification regardless of storage protocol.

2. **Core Extension Resolver Component**
   Implement a modular, swappable core component responsible for resolving extension URIs to their EDN content. The resolver will:
   - Support multiple decentralized storage protocols via pluggable driver modules (aligning with Arachne's modularity goal).
   - Cache resolved extension EDN locally (with TTL or content-hash-based invalidation) to reduce redundant decentralized storage calls.
   - Enforce integrity checks: Verify the fetched EDN against any specified content hash or protocol-native content identifier before processing.
   - Resolve transitive extension dependencies recursively (as declared in each extension's EDN metadata).

3. **Standardized Extension EDN Schema**
   Define a formal schema (using Arachne's core config schema system) for extension EDN files to ensure compatibility with Arachne's ""everything is data"" model. The schema will require:
   - **Metadata**: Extension name, semantic version, author, license, and explicit dependencies on other extensions/Arachne core modules.
   - **Schema Contributions**: New entity types, properties, or validation rules to extend Arachne's core config data model.
   - **Default Config Entities**: Pre-defined, overridable entities (e.g., HTTP middleware, database connection pools) that the extension provides as defaults.
   - **Code References**: Qualified symbols pointing to executable code (e.g., handler functions) where dynamic logic is necessary, avoiding inline code to keep the EDN purely declarative data.

4. **Extension Integration Workflow**
   - When an Arachne application config references an extension URI, the resolver fetches and validates the EDN, then resolves all transitive dependencies.
   - The extension's schema contributions are merged into the application's core config schema first, ensuring type safety for subsequent entity merges.
   - Default entities from the extension are merged into the application's config data structure, with application-specific config entities taking priority to support overrides (aligning with Arachne's configurable defaults goal).
   - Only after all extensions are resolved, merged, and validated does the application initialize its runtime components from the combined config.

5. **Introspection & Transparency Support**
   - Extend Arachne's introspection tools to surface extension-derived config entities and schema contributions. This will allow users to trace the origin of any config entity back to the extension (or override) that defined it.
   - Maintain an audit trail of resolved extensions in the application config, including their URIs, content hashes, and dependency trees, for full transparency into the application's composition.

6. **Requirement for Revisions**
   Any changes to the extension URI schema, resolver behavior, or EDN schema must be documented via a formal ADR, following the established Arachne decision-making workflow, and reviewed by the steering group before implementation.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision
Deprioritize private group chat functionality for the Q2 Status Mobile beta release, and refocus associated resources on core critical path work.

### Specific Actions to Implement this Decision:
1. **Scope Clarification**: Remove all private group chat-related tasks, bugs, and features from the Q2 beta milestone. Public chat and 1-1 chat will remain the exclusive focus of chat functionality for the beta.
2. **Resource Reallocation**: Immediately reassign engineering time previously dedicated to private group chat to resolving critical regressions, security, and usability issues in 1-1 and public chat—features identified as vital to the MVP beta experience.
3. **Issue Management**: Label all existing private group chat GitHub issues with a ""post-beta"" tag and move them to a backlog milestone for future consideration. Pause all active discussion and work on these issues until after the beta launch.
4. **Alignment Documentation**: Share this decision with the entire team to resolve recurring scope debates and ensure all contributors are aligned on the critical path for the Q2 launch.

### Rationale Alignment:
This decision directly addresses the context requirements:
- It maintains a tight scope to meet the Q2 beta deadline.
- Eliminates wasted effort from recurring scope discussions about group chat.
- Avoids the security and complexity overhead of stabilizing private group chat encryption/features for the beta.
- Focuses resources on the most high-impact chat experiences (1-1 and public) that support the MVP's goal of enabling meaningful user interactions.

Private group chat functionality will be revisited post-beta, once core chat features are stable and meet beta quality standards. At that point, the team can reassess scope, security requirements, and resource allocation for adding this feature.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision
We will replace the current hardcoded peer list system with dynamic peer discovery using the Discovery v5 protocol to enable scalable, resilient, and secure peer management in the application.

### Key Implementation Guidelines
1. **Bootstrap from Trusted Seed Nodes**:
   - Maintain a small, curated set of trusted seed nodes (instead of a full peer list) to initialize the Discovery v5 protocol. These seeds will act as the entry point for the app to join the peer network.
   - Seed nodes will be configurable (via environment variables or app settings) to allow operational flexibility without code changes.

2. **Dynamic Peer Pool Management**:
   - Implement an automatic peer discovery and maintenance workflow:
     - The app will continuously discover new peers via the Discovery v5 protocol to expand its pool.
     - Integrate peer health checks (e.g., latency, uptime, message success rate) to prune unresponsive or underperforming peers from the pool.
     - Set configurable limits on the minimum and maximum number of concurrent peers to balance resource usage and network resilience.

3. **Phase Out Hardcoded Peers**:
   - First run a transitional phase where both hardcoded peers and Discovery v5-discovered peers are used to validate protocol reliability and avoid service disruption.
   - Once Discovery v5 is proven stable in production, deprecate and remove all hardcoded peer references.

4. **Security and Reliability Enhancements**:
   - Enforce peer validation checks (consistent with Discovery v5 specifications) to mitigate risks from malicious or spoofed peers.
   - Add monitoring for peer discovery metrics (e.g., number of discovered peers, seed node uptime, peer churn rate) to track protocol performance and identify operational issues proactively.

### Rationale Alignment
This decision directly addresses the context goals:
- Eliminates the scaling limitation of hardcoded peers by allowing the app to dynamically adapt to user load by discovering new server peers.
- Enables independent scaling of the server peer cluster without requiring app code changes or redeploys.
- Improves high availability and reliability by automatically replacing failing peers and distributing traffic across a larger, dynamic pool.
- Enhances security by reducing reliance on static peer lists that can become stale or compromised, and leveraging Discovery v5's built-in protections against peer spoofing.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision
1. **Adopt Standardized Key Derivation**: Align Status's 12-word mnemonic-based key generation with the industry-standard BIP-44 derivation path for Ethereum: `m/44'/60'/0'/0/0`. This path is universally supported by major wallets (e.g., MetaMask, Trust Wallet, Coinbase Wallet) and will enable full cross-compatibility: mnemonics created in Status can be imported into these apps, and vice versa, to access the same Ethereum address.
2. **Preserve Backward Compatibility**:
   - Existing Status wallets created with the legacy derivation path will remain fully functional and accessible without any user action required.
   - Implement automatic detection logic for mnemonic imports: check both the standard BIP-44 path and the legacy Status path. If one path yields an address with existing transaction history or associated assets, default to that path. If both paths have no activity, prioritize the standard BIP-44 path. If both paths show activity, present a clear UI selection prompt with explanatory text to help users choose the correct address.
3. **Update User-Centric Flows**:
   - Modify the wallet creation flow to explicitly communicate that new mnemonics are compatible with other standard Ethereum wallets, reinforcing the benefit of cross-app portability.
   - Enhance the mnemonic import flow to include a visible, optional toggle for switching between derivation paths when automatic detection is inconclusive, with plain-language descriptions of each option.
4. **Transparent Documentation**:
   - Update Status's user documentation to clearly outline both the standard BIP-44 derivation path (for new cross-compatible wallets) and the legacy derivation path (for older wallets).
   - Add troubleshooting guidance for users encountering cross-wallet import issues, including how to verify derivation paths across apps.
5. **Security Assurance**: Conduct a targeted security audit of the updated key generation and import logic to confirm no vulnerabilities are introduced, particularly around path handling, mnemonic parsing, and backward compatibility checks.

This decision directly fulfills the cross-wallet compatibility goal while protecting existing users, maintaining transparency, and adhering to industry best practices for secure key management.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision
We will execute a two-pronged strategy to address node connectivity-related performance, decentralization, and security gaps in the Status beta, balancing short-term mitigation of current pain points with a focused long-term transition to a superior peer-to-peer architecture:

### 1. Short-Term Mitigations (Next 2 Sprints)
a. **Deploy CHT BloomTrie for LES**: Ship the completed CHT implementation to resolve critical LES performance issues (slow hot sync, excessive disk/bandwidth usage) for users running embedded LES nodes. This will reduce reliance on Infura by making LES a more viable, stable option.
b. **Expand User Node Configuration Controls**: Launch enhanced UI settings that allow users to seamlessly switch between three node setups (embedded LES, user-specified upstream nodes, Infura) with clear, plain-language indicators of the trust model, resource usage, and decentralization implications of each choice.
c. **Time-Boxed LES Impact Assessment**: Conduct a 1-sprint analysis to quantify how LES with CHT performs (sync speed, battery/disk/bandwidth) compared to Infura/upstream nodes in real-world mobile scenarios. This will validate if low-effort LES tweaks (e.g., adaptive sync throttling on mobile data) are worth implementing to further improve viability before ULC is ready.

### 2. Long-Term Transition to ULC (Ongoing Until ULC Production Readiness)
a. **Prioritize ULC Integration**: Allocate core engineering resources to accelerate ULC’s development and stabilization, as it is the consensus long-term solution for decentralized, secure node connectivity (supports multi-node peering to eliminate single points of trust).
b. **Phased ULC Rollout**:
   i. **Closed Beta**: Test ULC with a select user group to validate performance, edge cases (e.g., low-network environments), and decentralization benefits.
   ii. **Optional Public Beta**: Release ULC as an optional node layer for all users, with automatic fallback to existing LES/upstream/Infura setups for reliability.
   iii. **Default Configuration**: Promote ULC to the default node layer once it consistently meets or exceeds the performance of existing setups, maintaining backward compatibility for legacy configurations for a minimum of 3 subsequent releases.
c. **LES Maintenance Policy**: Limit LES support to critical bug fixes only (no new features or performance optimizations) to avoid diverting resources from ULC, aligning with the consensus that ULC is the more promising path.
d. **LES Deprecation**: Once ULC is default, deprecate LES support in a future major release with clear advance notice to users to transition their setups.

### 3. Decentralization & Security Guardrails
a. Ensure all node configuration options include explicit UI warnings about the trust tradeoffs of centralized services (e.g., Infura) versus peer-to-peer models (LES, ULC).
b. For ULC, enable multi-peer connectivity by default upon public release to maximize decentralization and reduce reliance on any single node from day one.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision
All sensitive data values (e.g., passwords, API keys, authentication tokens) within Arachne applications must be wrapped in an opaque, redacting core type (`SensitiveData`) at the point of ingestion into the application (e.g., reading from config files, environment variables, user input). This type will enforce automatic redaction in logging, print statements, and other string contexts, while providing a controlled way to access the actual sensitive value when necessary.

### Implementation Details
1. **Core Opaque Type**: Define a `arachne.core.types/SensitiveData` record (or deftype) in the core Arachne library that holds the raw sensitive value in a private field.
2. **Redaction Protocols**:
   - Override the `toString` method to return `[SENSITIVE DATA REDACTED]` instead of the raw value, preventing exposure in logs, `println`, or implicit string conversion.
   - Implement Clojure's `pr`/`prn` protocols to also return the redacted string, ensuring print-readable serialization does not leak sensitive data.
3. **Controlled Access**: Provide a single, explicitly documented accessor function `arachne.core.utils/sensitive-value` that retrieves the raw value from the `SensitiveData` type. This function will include warnings in its docstring about safe handling of the returned value (e.g., avoiding logging, immediate use, then discarding).
4. **Ingestion Enforcement**: Require that all sensitive data entry points (configuration parsers, environment variable readers, user input handlers) automatically wrap raw sensitive values in `SensitiveData` before passing them to internal application code.
5. **API Compatibility**: Update internal Arachne APIs that handle sensitive data to accept and return `SensitiveData` instead of raw strings, eliminating opportunities for accidental unwrapping or exposure.

### Alternatives Considered
1. **Custom Log Formatters**: Rejecting this approach because it relies on anticipating all locations where sensitive data might appear in logs and requires per-logger configuration. It does not prevent exposure in non-log contexts (e.g., print statements, debugging output).
2. **Metadata-Marked Strings**: Rejecting this because metadata can be easily lost during string manipulation, and many core Clojure functions do not respect metadata for printing/serialization.
3. **Macro-Wrapped Operations**: Rejecting this due to increased complexity in developer workflow and lack of type-level safeguards against accidental exposure.

### Rationale
This approach provides type-level protection against accidental sensitive data exposure, aligns with Clojure's data-oriented philosophy, and minimizes reliance on developer discipline alone. It is lightweight, easy to adopt, and ensures that sensitive data is safe by default while still allowing controlled access when required for valid use cases.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision 
1. **Deprecate and phase out the existing JavaScript jail-based status extension system**: No new features, bug fixes (except for immediate critical stability issues during transition), or enhancements will be made to the jail system.
2. **Migrate all existing command logic entirely to the main app codebase**:
   - Refactor all command functionality currently split between jail scripts and hardcoded app logic into unified, testable code within the core Status application. This will eliminate the need for asynchronous RN Bridge communication between the app and jail, resolve performance bottlenecks, and remove ""magic"" hardcoded code that reduces maintainability.
   - Remove all remaining dependencies on the jail runtime from production application code, including any residual command message logic tied to the jail API.
3. **Shift all extension development resources to Project Pluto**:
   - Focus all future extensibility efforts on the declarative Project Pluto extension system, which addresses the limitations of the jail system by supporting multi-point integration (chat commands, wallet assets, contacts, etc.) and aligns with modern, maintainable extension patterns.
   - Document the deprecation of the jail-based system for any potential existing users (noting the lack of active 3rd-party adoption) and redirect extension developers to Project Pluto as the supported path for extending Status functionality once it reaches a production-ready state.
4. **Retire the jail runtime entirely**: Once all existing command logic is migrated to the main app and Project Pluto is ready for extension use, remove the jail execution environment and associated API bindings from the Status codebase to reduce technical debt and simplify the application architecture.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision
We will implement Tribute to Talk (TtT) as per the whitepaper’s core economic anti-spam design, with the following concrete rules and implementation guardrails to ensure fairness, user control, and alignment with anti-spam goals:

1. **Stake Configuration & User Control**
   - Each user can set a minimum SNT deposit requirement for direct cold contacts (defined as messages from users not in their existing contacts list and not pre-approved for communication).
   - Users may adjust their minimum deposit amount at any time; changes will apply only to new cold contact requests initiated after the update (pending requests remain bound to the original requirement).
   - Users can toggle TtT on/off entirely: when disabled, cold contacts require no deposit, and all existing pending deposits follow refund rules outlined below.

2. **Deposit Escrow Workflow**
   - When a sender initiates a cold contact to a TtT-enabled recipient, they must deposit the recipient’s minimum required SNT into a temporary, on-chain escrow contract.
   - The cold contact request will only be delivered to the recipient once the deposit is confirmed on-chain.
   - Escrowed funds remain held until a resolution event (recipient reply, request expiration, sender cancellation, or spam flagging) occurs.

3. **Deposit Resolution Rules**
   - **Forfeiture to Recipient**: If the recipient replies to the cold contact within a configurable grace period (default: 7 days), the escrowed SNT will be transferred directly to the recipient’s wallet. This compensates the recipient for their attention and incentivizes engagement with legitimate requests.
   - **Full Refund to Sender**: The sender receives a complete refund in the following scenarios:
     - The recipient does not reply within the grace period.
     - The recipient explicitly rejects the cold contact request before replying.
     - The sender cancels the request before the recipient reads or acts on it.
     - The recipient deletes their account/loses wallet access before resolving the request (refund triggers after grace period expiration).
   - **Burn for Malicious Requests**: If the recipient flags the cold contact as spam, the escrowed SNT will be burned (not transferred to the recipient or refunded to the sender) to disincentivize abusive spam behavior.

4. **Transparency & Audibility**
   - All TtT transactions (deposits, transfers, refunds, burns) will be recorded on-chain with metadata linking them to the corresponding contact request, enabling full auditability.
   - Users will have a dedicated interface to view their TtT history, including pending deposits, received funds, refunded amounts, and burned spam-related deposits.

5. **Edge Case Safeguards**
   - If the sender’s wallet loses sufficient funds to cover the deposit after initiation but before on-chain confirmation, the contact request is automatically canceled with no funds deducted.
   - If the recipient’s minimum deposit requirement increases while a request is pending, the original deposit amount remains valid (no additional funds are required from the sender to complete the request).

This implementation balances the whitepaper’s economic incentives with practical user experience considerations, ensuring fairness for both senders and recipients while effectively discouraging abusive cold contact spam.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision
1. We will formalize a new role, **Mobile Section Domain Owner**, for each of the five defined mobile app sections: communities, syncing & onboarding, activity center & shell, wallet, and messaging.
2. Core Responsibilities for Domain Owners will be focused directly on resolving the identified oversight and polish gaps:
   - **End-to-End Section Ownership**: Serve as the single point of accountability for the UI/UX polish, functional consistency, and quality standards of their assigned section. This includes owning the section’s backlog of polish-related tasks, prioritizing them alongside the team lead and product stakeholders.
   - **Design Alignment Liaison**: Act as the primary mobile team contact for the design team for their section, ensuring implementation fully adheres to design system guidelines, pixel-perfect UI specifications, and user experience intent. Conduct regular syncs with design to clarify requirements and validate iterative changes.
   - **Oversight & Quality Gates**: Lead section-specific code reviews, usability testing sessions, and pre-deployment quality checks to catch gaps in polish, functionality, or consistency that might otherwise slip through general team reviews.
   - **Knowledge & Collaboration**: Document section-specific implementation patterns, known issues, and polish standards to maintain shared team transparency. Participate in cross-section syncs with other Domain Owners and the team lead to resolve dependencies and ensure uniform app-wide quality.
3. Role Alignment & Governance:
   - All Domain Owners will report to the existing mobile team lead, who retains overall team strategic direction, cross-section coordination, and escalation resolution.
   - A weekly 30-minute sync will be held with all Domain Owners and the team lead to align priorities, share best practices for polish and oversight, and address cross-section dependencies.
4. Selection Criteria:
   - Candidates will be chosen from current mobile team members based on deep familiarity with their assigned section’s functionality, technical implementation, and user workflows; strong attention to detail; and a track record of collaborating effectively with design stakeholders.
5. Implementation Timeline:
   - Nominate and confirm Domain Owners for all sections within 1 week.
   - Conduct a joint kickoff with the design team within 2 weeks to align each Domain Owner with their section’s polish requirements and backlog.
   - Launch regular section-specific quality checks within 3 weeks to start addressing existing polish gaps.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision
We will implement a hybrid, standards-aligned approach for handling Product Option values collections in the Sylius API Platform integration, balancing efficiency and ease of use:
1. **Dedicated Separate Resource (Default)**: Expose Product Option Values as a standalone API resource at `/api/v2/product-option-values` with a bidirectional relationship to Product Options. This follows REST and JSON:API best practices, keeps base Product Option payloads lean, and supports independent pagination, filtering, and sorting of values for performance-critical use cases.
2. **Optional Embedding**: Allow clients to embed Product Option Values directly in the Product Option response via the standard JSON:API `include` query parameter. This caters to common use cases (e.g., product configuration UIs) where clients need all values upfront without making multiple round-trip requests.
3. **Compound Document Support**: Enable creating/updating a Product Option along with its values in a single request using JSON:API compound documents, leveraging API Platform’s nested resource processing and Sylius’s existing ORM cascade persistence rules (e.g., `@OneToMany(cascade={""persist""})`) to avoid custom persistence logic.
4. **Performance Guardrails**:
   - Use API Platform’s `SerializerContextBuilder` to dynamically adjust Doctrine fetch modes: enable eager loading of values only when the `include` parameter is present, preventing N+1 query issues.
   - Enforce validation rules to ensure Product Option Values are always associated with a parent Product Option, eliminating orphaned records in the database.

### Alternatives Considered
1. **Full Embedding by Default**: Rejected because it bloats base Product Option payloads (especially for options with large value sets), increases bandwidth usage, and blocks independent filtering/sorting of values.
2. **Separate Resources Only**: Rejected because it forces clients to make multiple requests to retrieve a Product Option and its values, adding complexity for common end-user workflows.

### Rationale
- **Efficiency**: The default separate resource approach minimizes payload size and leverages lazy loading for unembedded requests, while on-demand eager loading eliminates performance bottlenecks when embedding is needed.
- **Ease of Use**: Optional embedding and compound document support give clients flexibility to match their data fetching strategy to their use case, reducing implementation overhead for frontend teams.
- **Sylius Compatibility**: Aligns with Sylius’s core domain model (Product Option ↔ Product Option Value association) and reuses existing ORM mappings to avoid custom, maintenance-heavy logic.
- **Standards Compliance**: Follows JSON:API specifications and API Platform best practices, ensuring consistency with the rest of the Sylius API ecosystem.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision
We will implement a standardized, server-side localization strategy for all customer-facing Sylius API endpoints, with optional explicit access to full translation sets, to ensure consistency, ease of use, and optimal customer experience:

1. **Default Behavior for Customer-Facing Endpoints**
   - By default, all locale-aware fields (product names/descriptions, payment/shipping method names, etc.) will return only the localized value matching the active context locale.
   - The active context locale will be determined using a strict priority chain:
     1. Authenticated customer's saved locale (if present and enabled for the store)
     2. Language specified in the `Accept-Language` request header (if the language is enabled for the store; partial locale matches like `fr` will map to `fr_FR` if available)
     3. Store's configured default locale
   - For any field with no translation available in the active locale, the system will fall back to the store's default locale translation to avoid empty content.

2. **Optional Access to Full Translation Sets**
   - Add a global, optional query parameter `include_translations=true` that, when included in a request, returns all available translations for locale-aware fields in a nested, consistent structure (e.g., `{""name"": ""T-Shirt"", ""translations"": {""en_US"": {""name"": ""T-Shirt""}, ""de_DE"": {""name"": ""T-Shirt""}}}`).
   - This parameter will be supported across all endpoints with locale-aware resources to maintain consistency.

3. **Consistent Response Structure**
   - Standardize the format of locale-aware fields in responses:
     - Default (no parameter): Flat field with the localized value (e.g., `""name"": ""T-Shirt""`)
     - With `include_translations=true`: Flat localized value (for backward compatibility) plus a nested `translations` object containing all locale-specific values for the field
   - Update all existing inconsistent endpoints to adhere to this structure, with a deprecation period for endpoints using the old mixed behavior (if needed, version the API to support both temporarily while migrating users).

4. **Fallback Documentation & Validation**
   - Explicitly document the locale priority chain, fallback behavior, and `include_translations` parameter usage in the official Sylius API documentation, including examples of requests and responses.
   - Add validation to ensure that store locales are properly configured, and log warnings for resources missing translations in both the active customer locale and store default locale.

5. **Admin-Facing Endpoints**
   - Admin endpoints will continue to return full translation sets by default, as admins require access to all locale content for management purposes. An optional `locale` query parameter will be supported to filter translations to a specific locale if needed.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision
We will implement a lightweight, file-based system to capture all non-proposal architectural, process, and implementation decisions for the Terraform Controller project, following these structured guidelines:

### 1. Storage & Discoverability
- Create a root-level `decisions/` directory in the project repository dedicated exclusively to decision records, ensuring easy access for all contributors.
- Add a `README.md` in the `decisions/` directory that:
  - Explains the purpose of the records (preserving context for future contributors)
  - Includes a copy of the standard decision record template
  - Maintains a chronological table of contents linking to each decision (with a 1-sentence summary of each for quick scanning)

### 2. Record Template & Naming
- Each decision will be documented in a separate Markdown file using a minimal, consistent template:
  ```markdown
  # [Clear Decision Title]
  **Date**: [YYYY-MM-DD]
  **Status**: [Accepted | Superseded]

  ## Context
  Brief background on the problem, constraints, or requirements that drove the decision.

  ## Decision
  Exact, unambiguous statement of the choice made.

  ## Rationale
  Key tradeoffs considered, why this choice was selected over reasonable alternatives, and how it aligns with project goals.

  ## Superseded By (if applicable)
  Link to the newer decision record that replaces this one (only for ""Superseded"" status)
  ```
- Use the naming convention `[sequential-number]-[kebab-case-title].md` (e.g., `0001-terraform-version-pinning-strategy.md`) to ensure chronological ordering and clear referenceability.

### 3. Capture & Maintenance Workflow
- Any team member may draft a record when a non-proposal decision is finalized (via sync meetings, async discussions, or ad-hoc alignment).
- The record must be committed to the `decisions/` directory within 3 business days of the decision being made to preserve accurate context.
- When revising a decision:
  1. Create a new record documenting the revised choice, with explicit reference to the original record.
  2. Update the original record’s status to ""Superseded"" and add a link to the new record in the ""Superseded By"" section.
  3. Never delete or rewrite original records—preserve them as historical context for past decision-making.

### 4. Visibility & Alignment
- Post a brief announcement in the project’s core communication channel (e.g., Slack, Discord) whenever a new decision record is merged into the main branch, linking to the file for awareness.
- Team leads will conduct a quarterly review of the `decisions/` directory to ensure records are complete, up-to-date, and aligned with project evolution.

This system balances lightweight simplicity with critical transparency, ensuring future contributors can quickly understand the ""why"" behind non-proposal decisions without imposing unnecessary process overhead.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision
We will use GitHub webhooks as the primary mechanism to detect pull request changes, with periodic polling as a secondary fallback to handle missed events or webhook delivery failures.

### Rationale
1. **Real-time responsiveness & efficiency**: Webhooks provide immediate notifications when pull request changes occur (e.g., new commits, comments, merges), enabling timely triggers for CI/CD workflows, feedback loops, or validation processes. Unlike polling, they avoid unnecessary GitHub API calls, reducing the risk of hitting rate limits and minimizing operational overhead—aligning with resource-efficient practice.
2. **Reliability via fallback polling**: Webhooks can fail to deliver due to network outages, endpoint downtime, or GitHub service interruptions. Periodic polling (configured with a reasonable default interval, e.g., 15 minutes) acts as a safety net to reconcile PR state and catch missed events, ensuring no critical changes go undetected.
3. **Security & robustness for webhooks**:
   - We will verify webhook requests using GitHub's HMAC signature to ensure authenticity and integrity of events.
   - We will implement idempotency checks using the `X-GitHub-Delivery` header to avoid processing duplicate events.
   - We will use a retry queue for failed event processing to handle transient errors gracefully.
4. **Optimized polling implementation**:
   - Polling will fetch only changes since the last run (using GitHub API `since` parameters) to minimize API call volume.
   - We will respect GitHub rate limits by monitoring response headers and adjusting poll frequency or pausing execution if limits are approaching.
5. **Configurable alignment with Arachne goals**: Both mechanisms will be configurable (via environment variables or a centralized config structure consistent with Arachne's modular, configurable ethos), allowing operators to enable/disable fallback polling, adjust intervals, and set webhook secrets/endpoints without code changes.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision
We will retain support for cross-namespace references in tf-controller (for `sourceRef`, `dependsOn`, and branch planner `.data.resources[]`) to maintain compatibility with existing Flux deployments that rely on centralized resource definitions. To mitigate Kubernetes security and namespace isolation risks associated with cross-namespace references, we will implement the following layered safeguards:

1. **Default-Disabled Opt-In Model**:
   - Cross-namespace references will be disabled by default for all new installations.
   - Users may enable cross-namespace access via three granular, explicit levels:
     1. **Cluster-wide**: A cluster-scoped `TfControllerConfiguration` custom resource with an `allowCrossNamespaceRefs` flag (with prominent security warnings in its schema and documentation).
     2. **Namespace-specific**: A namespace annotation (e.g., `tf-controller.fluxcd.io/allow-cross-namespace: ""true""`) to permit cross-namespace references only for `Terraform` resources in that namespace.
     3. **Per-resource**: A `spec.allowCrossNamespaceRefs` boolean field on individual `Terraform` objects, which overrides namespace/cluster settings if enabled.

2. **RBAC Hardening**:
   - The default tf-controller RBAC will restrict access to resources within the same namespace only.
   - Users enabling cross-namespace references must explicitly grant additional RBAC permissions (e.g., a `ClusterRole` with `get`, `list`, `watch` permissions for target resource types in specific namespaces) to the controller service account.
   - Admission webhooks will validate that the controller has the required permissions for any cross-namespace reference before allowing the `Terraform` resource to be created or updated.

3. **Admission Validation**:
   - Implement validation webhooks that reject cross-namespace references unless the relevant opt-in (cluster/namespace/per-resource) is active.
   - For each cross-namespace reference, the webhook will verify the existence of the referenced resource and confirm the controller has been granted access to it via RBAC.

4. **Guidance and Documentation**:
   - Promote same-namespace references as the secure, recommended default in all official examples and documentation.
   - Clearly articulate the security tradeoffs of cross-namespace references (broken isolation, permission sprawl) and provide best practices for safe usage (e.g., dedicated source namespaces with minimal RBAC access, restricted target namespace allowlists).
   - Explore and document alternative patterns to cross-namespace references, such as replicating sources to application namespaces using Flux sync tools, to guide users toward more secure architectures over time.

This approach balances compatibility with existing deployments while addressing Kubernetes security and isolation best practices, giving users granular control over the tradeoffs between flexibility and security.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision
1. **Implement a persistent, resource-scoped Workspace BLOB cache** within the TF-Controller’s storage layer to enable reliable single TF object deletion, which is a prerequisite for resolving resource dependency and namespace deletion issues.
2. **Modify the TF object finalization process** to prioritize using cached Workspace BLOBs over fetching data from original associated resources (Source, Secrets, ConfigMaps) when available.
3. **Define explicit lifecycle rules** for cached BLOBs to balance deletion reliability and storage efficiency.

### Detailed Implementation Specifications
#### Cache Storage & Identification
- Use the TF-Controller’s existing configured storage (e.g., Kubernetes Secrets/ConfigMaps with a dedicated storage class, or cluster persistent volume) to host cached BLOBs.
- Each cached BLOB will be uniquely identified by a composite key: the TF object’s UID + a hash of its latest applied spec state. This ensures that only valid, up-to-date BLOBs are used for deletion.

#### Cache Population
- After a successful apply or plan operation, immediately store a copy of the generated Workspace BLOB in the cache. This guarantees the cache holds a valid snapshot of all resources needed to destroy the infrastructure.
- Automatically invalidate the existing cached BLOB for a TF object whenever its spec is modified (e.g., new source revision, updated secrets/config maps). A new valid BLOB will be cached after the next successful apply/plan operation.

#### Cache Utilization in Deletion Finalization
- When a TF object enters the deletion phase, the controller first checks the cache for a valid BLOB matching the object’s UID:
  - If found: Initialize the tf-runner workspace using the cached BLOB to execute the Terraform destroy command. This eliminates dependencies on original Source, Secrets, or ConfigMaps (even if they have been deleted or modified).
  - If not found: Fall back to attempting to fetch original resources. If this fallback fails, log a user-friendly error message and provide a mechanism to manually trigger deletion by uploading a valid Workspace BLOB (e.g., via a controller API or annotation).

#### Cache Eviction & Cleanup
- **Automatic Eviction**: Delete a cached BLOB immediately after the TF object is fully deleted (finalizers removed) to free storage.
- **TTL-Based Eviction**: Add a configurable default TTL (e.g., 90 days) for cached BLOBs of inactive TF objects (no spec changes or operations in the period). Allow users to override this TTL via controller configuration to suit their storage needs.
- **Manual Cleanup**: Expose a controller API or CLI command to manually evict specific cached BLOBs or bulk-cleanup expired entries.

#### Metrics & Observability
- Track cache hit/miss rates during deletion operations, storage usage of cached BLOBs, and deletion success rates via controller metrics. This will enable monitoring of the solution’s effectiveness and help identify edge cases (e.g., frequent cache misses leading to deletion failures).

### Rationale
This decision directly addresses the root cause of single object deletion failures by decoupling the deletion process from the original associated resources. By using cached Workspace BLOBs—self-contained snapshots of all data needed to destroy infrastructure—the controller can reliably execute deletions even when Source, Secrets, or ConfigMaps are no longer available. Establishing this reliable single object deletion foundation unblocks the subsequent resolution of resource dependency and namespace deletion issues in the required order.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision
Adopt a hybrid E2E testing strategy combining automated PR-triggered baseline tests, configurable extended matrix testing, and a portable local testing workflow to meet all stated goals while balancing speed, coverage, and resource efficiency. The specific components are:

1. **Automated Baseline E2E Tests for All PRs**
   - Enable automated, unblocked E2E test runs for every contributor PR (no manual maintainer trigger required) since no sensitive secrets are needed for the current test suite.
   - The baseline suite will use lightweight, resource-efficient environments (e.g., Kind on Ubuntu LTS) to deliver fast feedback within standard PR checks, and will block PR merging if failed to preserve core functionality integrity.

2. **Extended Matrix Testing for Distribution Coverage**
   - Implement a CI matrix (via GitHub Actions) that supports testing across:
     - Kubernetes distributions: Lightweight options (Kind, K3d) as mandatory baseline checks, plus cloud/heavyweight distros (EKS, OpenShift) as optional extended checks.
     - Linux distributions: Ubuntu LTS, Debian, AlmaLinux (RHEL-compatible) to validate compatibility with target deployment environments.
   - Split matrix runs into tiers:
     - Mandatory tier: Runs automatically on all PRs to ensure core compatibility with common environments.
     - Optional tier: Triggered manually by maintainers via slash commands (retaining `peter-evans/slash-command-dispatch` for consistency) or scheduled weekly to avoid excessive resource consumption on routine PRs.
   - Use reusable CI workflows to share test logic between baseline and extended runs, reducing duplication and maintenance overhead.

3. **Portable Local E2E Testing Workflow**
   - Document and distribute a scripted local test setup using lightweight K8s clusters (Kind or K3d) that eliminates reliance on EC2 instances.
   - Ensure test framework portability: The same test code and assertions will run in both local and CI environments, so validation done locally translates directly to CI results.
   - Provide helper tooling to:
     - Spin up/tear down local K8s clusters with one command.
     - Build Zarf binaries and init packages locally for test execution.
     - Filter test runs to specific subsets (e.g., core init flow, feature-specific tests) for faster feedback during feature development.

4. **Security and Resource Guardrails**
   - Isolate any future test suites requiring sensitive secrets into separate, maintainer-only triggered workflows to preserve security.
   - Implement CI resource limits and caching (for container images, built binaries) to optimize run times and reduce unnecessary resource usage.
   - Retain audit logs for all test runs (automated and manual) for accountability and debugging purposes.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision
Zarf will implement a generic, cluster-agnostic image injection mechanism based on an ephemeral in-cluster OCI registry paired with a privileged daemon set to establish node-level TLS trust. This approach will meet the requirement of working across all Kubernetes distributions without relying on cluster-specific CLI tools or pre-existing registries.

### Detailed Implementation Steps
1. **Temporary Registry Deployment**:
   - Zarf will generate a unique self-signed TLS certificate authority (CA) and registry certificate on-the-fly for each injection operation.
   - It will deploy an ephemeral, stateful OCI registry pod into the target cluster, configured to use the generated TLS certificate for secure image transfers. The registry will be exposed to all cluster nodes via a ClusterIP service.

2. **Node Trust Establishment**:
   - Zarf will deploy a privileged daemon set to all cluster nodes. This daemon set will:
     a. Install the temporary registry's CA certificate into the node's system root CA store (compatible with common node OSes like Ubuntu, RHEL, and CoreOS).
     b. Apply CRI-specific configuration (where feasible, e.g., containerd config.toml, cri-o registries.conf) to explicitly trust the temporary registry, falling back to system CA installation if CRI-specific config is not accessible or supported.
     c. Verify that the node's CRI recognizes the trusted registry before marking the daemon set pod as ready.

3. **Image Push & Manifest Rewriting**:
   - Once the registry and daemon set are fully reconciled, Zarf will push all required images from its local cache to the in-cluster registry.
   - Zarf will automatically rewrite all workload manifests (Helm charts, raw YAML, etc.) to reference images from the temporary registry instead of their original remote locations.

4. **Cleanup & Retention Options**:
   - After successful deployment and validation of all workloads, Zarf will offer users two options:
     a. **Retain**: Keep the temporary registry for future image updates or additions (useful for air-gapped environments without ongoing external access).
     b. **Teardown**: Delete the temporary registry, and use the daemon set to remove the injected CA certificate from all nodes (restoring nodes to their pre-injection state where possible).

5. **Privilege-Restricted Cluster Fallback**:
   - For clusters with strict RBAC or pod security policies that block privileged pods, Zarf will provide an alternative workflow:
     a. Generate and present the temporary registry's CA certificate to the user.
     b. Provide step-by-step instructions for manual installation of the CA certificate on all cluster nodes.
     c. Proceed with registry deployment and image push once the user confirms manual trust setup is complete.

### Rationale
- **Generality**: This approach works across all Kubernetes distributions that support daemon sets and standard CRIs (containerd, cri-o, Docker), avoiding reliance on distro-specific CLI tools (e.g., `kind load`, `microk8s ctr`).
- **No Pre-Assumptions**: It does not require a pre-existing registry, making it suitable for unknown, air-gapped, or minimal environments.
- **Secure by Default**: Using TLS for registry communication avoids exposing unencrypted image transfers, with the daemon set ensuring consistent trust across all nodes without manual per-node configuration.
- **Minimal Footprint**: The ephemeral registry and daemon set are designed to be cleaned up easily, leaving minimal residual state on the cluster unless explicitly retained.
- **Fallback for Restricted Environments**: The manual trust setup option ensures compatibility with clusters that cannot run privileged workloads, balancing automation with flexibility.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision
Arachne will implement full support for generating and maintaining Software Bills of Materials (SBOMs) compliant with Executive Order (EO) 14028 and related federal requirements. The specific approach is as follows:

1. **SBOM Standard Alignment**
   - Prioritize support for SPDX 2.2+ (the primary standard recognized by U.S. federal agencies) and optionally CycloneDX 1.4+ for broader compatibility. Generated SBOMs will include all mandatory fields specified in the National Telecommunications and Information Administration (NTIA) SBOM Minimum Elements.

2. **Automatic, Modular SBOM Generation**
   - Integrate SBOM generation directly into Arachne’s toolchain (for both deps.edn and Leiningen-based workflows). The tooling will automatically aggregate dependency metadata from all loaded Arachne modules, transitive dependencies, and application-specific code to produce a single, unified SBOM for the complete application.
   - Each Arachne module must expose machine-readable metadata (via its configuration schema) including: module name, version, unique identifier (e.g., Maven coordinates, GitHub repo URL with commit hash), license, and provenance details (build timestamp, source repository). This ensures modular swaps (per Arachne’s modularity goal) automatically update the SBOM without manual effort.

3. **Transparency and Introspection**
   - Expose the full SBOM as an immutable data structure within the Arachne application’s core configuration, enabling runtime introspection and programmatic access to component details (aligning with Arachne’s transparency and ""everything is data"" principles).

4. **Distribution and Maintenance**
   - Package the SBOM alongside production application artifacts (e.g., as an SPDX JSON file in the same JAR, Docker image, or deployment bundle).
   - For versioned Arachne applications, provide tooling to update the SBOM automatically when modules or dependencies are modified, and track changes to SBOM content via audit trails linked to application configuration updates.

5. **Provenance Traceability**
   - Include detailed provenance data for all components: source control URLs, commit hashes, build environment details, and supply chain verification status (where applicable). This addresses EO14028’s requirements for secure supply chain traceability.

This decision aligns with Arachne’s existing goals: modularity (swappable modules update the SBOM automatically), transparency (SBOM is part of the introspectable application data), and data-driven design (SBOM metadata is encoded in module configs rather than runtime state). It ensures compliance with federal requirements while maintaining Arachne’s core architectural principles.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision
Zarf will implement a cluster-local Mutating Admission Webhook (MAW) as the primary method for resource mutation, paired with package lifecycle updates to eliminate manual author intervention and ensure consistent behavior across all deployment workflows:

1. **Cluster Mutating Admission Webhook Deployment**:
   - The webhook will be installed automatically during `zarf init` or cluster bootstrap, configured with live endpoints for Zarf's internal Registry and Gitea Git Server (populated directly from Zarf's cluster state).
   - It will intercept create/update operations for relevant resources (Pods, Deployments, StatefulSets, Flux `GitRepository` CRDs, ArgoCD `Application` CRDs, and secret-dependent resources) to:
     - Rewrite container image paths from external registries to point to Zarf's internal Registry.
     - Update Git repository URLs in GitOps resources to use Zarf's internal Gitea server.
     - Inject or update secret references for image pulls and Git access to use Zarf-managed secrets.
   - An opt-out mechanism via a Kubernetes annotation (e.g., `zarf.dev/skip-mutation: ""true""`) will be provided for resources that intentionally target external, non-Zarf-managed services.
   - TLS communication with the Kubernetes API server will be handled automatically by Zarf during cluster setup, using short-lived, self-signed certificates rotated as part of Zarf's cluster maintenance.

2. **Package Lifecycle & Tooling Updates**:
   - Deprecate the `zarf prepare` command, as the webhook will handle all required mutations dynamically.
   - Update `zarf package create` to include optional validation for hardcoded resource references that may conflict with Zarf's internal services, providing authors with actionable warnings instead of requiring manual changes.
   - All core Zarf packages (Registry, Gitea) will be pre-configured to work seamlessly with the webhook, setting a precedent for third-party package authors.

3. **Backward Compatibility**:
   - Maintain Helm post-rendering as an optional fallback for environments where admission webhooks are disabled, but mark it as deprecated in favor of the webhook.
   - Provide a migration guide for existing packages to remove manual mutation logic and adopt webhook-based automation.

## Rationale
- **Workflow Consistency**: The webhook intercepts all resource apply operations (Zarf CLI, Flux, ArgoCD, etc.), ensuring uniform mutation behavior between manual and GitOps-driven deployments.
- **Reduced Author Burden**: Eliminates the need for package authors to implement custom templating or run `zarf prepare`, allowing them to focus on core package functionality instead of Zarf-specific compatibility.
- **Dynamic Adaptability**: Unlike pre-mutated manifests, the webhook uses live cluster configuration for mutation targets, so changes to Zarf's internal service endpoints (e.g., during cluster reconfiguration) do not require manual package or manifest updates.
- **Air-Gap Alignment**: The webhook operates entirely within the cluster, adhering to Zarf's air-gapped design principles and avoiding external dependencies for mutation logic.

## Alternatives Considered
1. **Enhanced `zarf prepare` Pre-Mutation**:
   - **Pros**: No cluster-level components required; mutations baked into manifests at build time.
   - **Cons**: Rigid to cluster configuration changes (pre-mutated manifests break if Zarf service endpoints shift); still requires author action during package creation.
   - **Rejection Reason**: Fails to address dynamic cluster changes and does not fully eliminate author friction.

2. **GitOps Tool-Specific Integration**:
   - **Pros**: Leverages existing tooling ecosystems.
   - **Cons**: Requires custom logic for each GitOps tool (Flux, ArgoCD, etc.), increasing maintenance overhead; forces users to reconfigure GitOps pipelines to integrate with Zarf.
   - **Rejection Reason**: Creates inconsistent user experiences and adds significant ongoing maintenance work.

3. **Status Quo with Manual Author Mutation**:
   - **Pros**: No immediate development effort.
   - **Cons**: Continues to shift mutation burden to authors; creates inconsistencies between deployment workflows; increases support complexity due to varying author-implemented logic.
   - **Rejection Reason**: Conflicts with Zarf's goals of ease of use and consistent air-gapped operation.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision

We will eliminate the `stage1` Rust binary and `stage2` Go binary entirely, replacing their workflow with a simplified, resource-efficient process that directly populates the in-cluster registry’s storage without temporary registry infrastructure. The specific changes are:

1. **Precompute OCI Registry Storage Structure**: During Zarf’s build process, use tools like `crane` or `oras` to convert the `registry:2` image into the native OCI storage format (blobs + manifests) that the official Docker registry uses. Compress this complete storage structure into a single archive (e.g., tar.gz) and package it as a single configmap in the Zarf init package.
2. **Lightweight Injector Script**: Replace the injector pod’s existing binary-based logic with a minimal shell script (compatible with the base container image used for the injector) that performs the following steps:
   a. Mount the persistent volume claim (PVC) allocated for the `zarf-registry` deployment.
   b. Unpack the precomputed OCI storage archive directly into the registry’s expected data path on the PVC.
   c. Set appropriate file permissions on the unpacked content to match the `registry:2` container’s runtime requirements.
3. **Simplify Init Workflow**: Update Zarf’s init process to skip assembling `stage1`/`stage2` binaries entirely. The injector pod will only load the single storage archive configmap, execute the shell script, and terminate once the PVC is populated.

### Rationale
This decision directly resolves all identified overheads from the current approach:
- **Reduced Binary Count & Build Complexity**: Eliminates two cross-platform binaries, removing the need for multi-architecture compilation and cutting CI/CD build time and maintenance burden.
- **Fewer Configmaps**: Consolidates multiple split configmaps into one, speeding up cluster init by reducing resource creation and data transfer overhead.
- **Eliminates Wasteful Registry Usage**: Avoids running a full temporary registry to host a single image by writing the final storage structure directly to the target PVC, reducing CPU/memory consumption in the injector pod.
- **Preserves Core Functionality**: Maintains the end goal of provisioning a functional `zarf-registry` while streamlining the intermediate steps to align with the minimal required work.

### Transition Notes
- Integrate the OCI storage precomputation step into Zarf’s existing build pipeline to ensure the archive is always in sync with the target `registry:2` image version.
- Verify the injector’s base container image includes standard tools (e.g., `tar`, `gzip`) to unpack the archive; if not, use a minimal, pre-equipped base image.
- Remove all references to `stage1` and `stage2` from Zarf’s codebase, configuration files, and documentation.
- Add validation steps to the injector script to confirm the storage structure is unpacked correctly before terminating, ensuring the `zarf-registry` deployment initializes without errors.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision 
We will implement a hybrid testing strategy combining targeted contract-focused unit tests and refined integration/partial end-to-end tests, tailored to address Zarf’s shift to a library, cover hard-to-test code sections, and balance test efficacy with maintenance burden.

### Rationale
This approach mitigates the drawbacks of standalone options while leveraging their strengths:
- It prioritizes stability for downstream library consumers by validating public interface contracts, directly addressing the increased scrutiny on Zarf’s interfaces.
- Targeted unit tests cover code sections not feasible to validate via end-to-end testing (e.g., utility functions, edge-case error handling) without incurring the full maintenance overhead of universal unit testing.
- Integration/partial end-to-end tests align with Zarf’s existing testing practices, reducing the learning curve for the team while validating complex component interactions that unit tests cannot capture in isolation.

### Implementation Details
#### 1. Targeted Unit Testing
- Restrict unit tests to two high-value use cases:
  a. **Public Interface Contract Validation**: Tests for Zarf’s public library APIs will focus solely on input/output behavior (not internal implementation details) to ensure stable, predictable behavior for downstream consumers. These tests will validate compliance with documented interface expectations, including error conditions and edge cases.
  b. **Hard-to-E2E Code Coverage**: Unit tests will cover code sections that cannot be reliably tested via end-to-end or integration means (e.g., complex data transformation logic, utility functions, or error paths that are difficult to trigger in full environments).
- Enforce a strict rule: unit tests must not rely on private functions, internal state, or implementation-specific logic to ensure they remain valid as the codebase evolves.

#### 2. Refined Integration/Partial End-to-End Testing
- Expand existing end-to-end practices to include partial e2e/integration tests for components with complex cross-module or external dependencies. These tests will replicate realistic interaction patterns but use lightweight, purpose-built mocks (instead of full environment orchestration) to keep maintenance manageable.
- Focus these tests on validating that components work together as expected, rather than testing individual implementation details.

### Governance
- **PR Requirements**: All changes to public interfaces or critical untested code must include corresponding contract-focused tests (unit or integration) as part of the PR review process. Maintainers will validate that unit tests adhere to contract-first principles and avoid implementation coupling.
- **Test Reviews**: Conduct quarterly audits of existing unit tests to retire those that have become tied to implementation details (and thus lost long-term value), replacing them with contract-focused or integration tests where appropriate.
- **Coverage Tracking**: Maintain a code coverage target prioritizing public interfaces and hard-to-test sections, using coverage reports to identify gaps in the testing strategy.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision
We will implement a tiered approach to SBOM generation for non-image artifacts in Zarf packages, balancing completeness, low noise, and user flexibility:

### 1. Default Scanning Scope (Mandatory)
By default, generate SBOMs for all explicit artifacts that are part of the final Zarf package, excluding non-package build artifacts:
- **Files**: Scan all files specified in the package's `files` array (local paths, remote URLs, and extracted archives).
- **Data Injections**: Scan the content of all `dataInjections` entries in the package.
- **Git Repos**: For `git` entries, scan only the files that are included in the final package (i.e., after applying `sparseCheckout` rules, excluding `.git` metadata directories and index files that are not part of the deployed artifact set).
- All non-image SBOM results will be merged with image SBOMs into a single, comprehensive package-level SBOM, with clear labels distinguishing artifact types (e.g., `type: file`, `type: git-content`, `type: image`).

This addresses the core requirement of covering critical non-image components (like k3s and the injector in the init package) while avoiding noise from transient build temp files that are not present in the final package.

### 2. Optional Advanced User Configuration
Add a package-level configuration field (e.g., `sbom.extraScanPaths`) to allow users to specify additional paths within the Zarf build temp directory to include in the SBOM scan. This field accepts an array of glob patterns targeting files or directories not covered by the default scope (e.g., custom artifacts generated during the build process).

This caters to advanced use cases (e.g., custom git repo artifacts that fall outside default inclusion rules) without forcing complexity on most users.

### Workflow Alignment
- When generating an SBOM for a package, Zarf will first run the default scope scan, then apply any user-specified `extraScanPaths` if provided.
- All SBOM data (default and optional) will be included in the final package's SBOM output (whether as a separate file or embedded in the package manifest).

## Rationale
- Combines the strengths of options 2 and 3 from the context: It defaults to a low-noise, package-accurate scan that covers the most common non-image artifacts, while offering optional flexibility for advanced users.
- Eliminates the noise of option 1 (full temp dir scan) by only targeting artifacts that are actually part of the final package.
- Ensures critical non-image components (like the init package's k3s and injector) are always included in SBOMs by default, avoiding the risk of users forgetting to enable scanning.
- Implementation is feasible: Zarf already tracks which artifacts are included in the final package, so it can target those paths directly, and adding an optional config field is a minimal, non-disruptive extension.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision
We will implement a ""Connected Mode"" for Zarf that enables declarative deployment and management of Zarf packages without requiring a pre-existing Zarf init package, while maintaining full backward compatibility with the existing disconnected environment workflow.

Key details of this implementation include:
1. **Core Mode Flag & Behavior**:
   - Add a `--connected` flag to the `zarf deploy` and `zarf upgrade` commands. When this flag is set, Zarf will skip all checks for the Zarf init package, Zarf agent, and associated cluster resources (e.g., `zarf-state` secret, local registry) that are only required for disconnected environments.
   - Default behavior remains unchanged: without the `--connected` flag, Zarf will continue enforcing the init package requirement to preserve compatibility with existing disconnected workflows.
2. **Connected Mode Artifact Handling**:
   - In connected mode, Zarf will pull container images and other artifacts directly from their original public (or user-configured external) registries/repos instead of relying on a local Zarf-managed registry.
   - Package manifests can optionally specify override external registries for artifacts in connected mode, giving users control over source locations even in connected environments.
3. **Declarative Support**:
   - Extend Zarf package YAML manifests to include an optional `deployment.mode` field (values: `connected` or `disconnected`). When set, this field will auto-enable the corresponding mode during deployment, enabling fully declarative workflows without CLI flags.
   - Ensure all declarative validation logic adjusts dynamically based on the selected mode: skipping init-specific resource checks in connected mode, and validating artifact accessibility from external sources instead of the local registry.
4. **Backward Compatibility & Migration**:
   - Existing Zarf packages will work seamlessly in connected mode without modification (assuming their artifacts are publicly accessible).
   - Add support for migrating a deployment from connected to disconnected mode (and vice versa) via `zarf upgrade` with the appropriate mode flag, including automated cleanup or provisioning of mode-specific resources as needed.
5. **Documentation & Tooling**:
   - Update official documentation to outline connected mode use cases, limitations (e.g., no offline artifact support, dependency on external network connectivity), and step-by-step guides for declarative deployments in connected environments.
   - Add a `zarf package validate --connected` command to pre-check package compatibility and artifact accessibility for connected mode deployments.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision
Refactor the Zarf `scripts` noun system into a flexible, extensible framework that supports advanced lifecycle automation use cases while maintaining full backward compatibility with existing package configurations. The specific changes will include:

1. **Granular, Explicit Execution Triggers**
   - Replace the limited `before`/`after`/`prepare` top-level script sections with a structured trigger system aligned to all major Zarf package and component lifecycle events. Supported triggers will include:
     - Package-level: `package:create:before`/`after`, `package:deploy:before`/`after`, `package:destroy:before`/`after`
     - Component-level: `component:deploy:before`/`after`, `component:destroy:before`/`after`
   - Automatically map existing top-level `before`/`after` scripts to `package:deploy:before`/`after` and existing `prepare` scripts to `package:create:before` to preserve backward compatibility.

2. **Advanced Script Control & Metadata**
   - Allow both simple command strings (for backward compatibility) and rich object definitions for complex scripts. Object definitions will support:
     - `name`: A human-readable identifier for debugging and logging
     - `command`: The executable/script to run (required for object format)
     - `type`: Explicit runtime specification (e.g., `bash`, `powershell`, `python`) with fallback to the system default shell if unset
     - `condition`: A Zarf template expression that must evaluate to `true` for the script to execute
     - `continueOnError`: A boolean flag to control whether a failed script halts the lifecycle phase (defaults to `false` for consistency with existing behavior)
     - `dependsOn`: A list of script names or trigger paths to enforce execution order dependencies
   - Example of mixed-format usage:
     ```yaml
     scripts:
       package:deploy:before:
         - echo ""Simple backward-compatible script""
         - name: ""Conditional advanced setup""
           command: ""./scripts/advanced-setup.sh""
           type: ""bash""
           condition: ""{{ .Values.enableAdvancedFeatures }}""
           continueOnError: false
     ```

3. **Component-Specific Script Scope**
   - Enable defining scripts directly within individual component blocks (in addition to root-level package scripts) to support per-component lifecycle logic that executes only when the component is deployed or destroyed, rather than applying to the entire package unconditionally.

## Rationale
This decision addresses the limitations of the original `scripts` system by:
- Providing granular control over when scripts execute across the full package/component lifecycle
- Supporting conditional execution and dependency ordering to enable complex workflow automation
- Adding cross-platform runtime support via explicit script type specification
- Preserving backward compatibility to avoid breaking existing packages
- Allowing component-specific logic to keep package configurations modular and maintainable

The structured trigger system and rich script metadata aligns with team needs for more flexible automation while retaining simplicity for basic use cases.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision
1. **Default Memory-Safe Implementation**: Adopt Crane's memory-efficient tar stream handling mode as the default for loading local Docker images. This resolves the OOM crash issue while retaining full compatibility with future OCI package schema changes (critical for upcoming OCI import plans).
2. **Performance Flexibility**: Add an optional `--fast-local-loading` CLI flag that re-enables the original naive Crane mode. This flag will be intended for users working with smaller images or systems with ample memory who prioritize raw performance over memory safety, and will include a clear warning in documentation and CLI output about potential OOM risks with large images.
3. **Deprecation Strategy**: Phase out support for direct Docker CLI/image format usage entirely. Docker's legacy output format conflicts with long-term OCI schema plans, and even in optimized scenarios, Docker-based loading is slower than the memory-efficient Crane approach.

### Rationale
- **Memory Safety First**: The default switch to Crane's memory-efficient mode fixes the critical OOM crash issue for users working with large images, ensuring Zarf is reliable across all local development scenarios.
- **Performance Balance**: Even in memory-efficient mode, Crane outperforms Docker CLI-based loading in all benchmarked scenarios (e.g., 20GB cached image takes 2m10s with Crane vs.5m2s with Docker; Big Bang Core cached+skip-sbom takes56s vs.1m51s with Docker), avoiding the 3x slowdown of other alternative methods.
- **Future Compatibility**: Crane’s OCI-compliant output aligns directly with planned package schema changes for OCI imports, eliminating technical debt from legacy Docker format dependencies.
- **User Choice**: The optional fast flag preserves usability for users who don’t encounter memory constraints, allowing them to retain the original performance benefits when safe to do so.

### Alternatives Considered
- **Keep Naive Crane Mode**: Rejected, as it causes unrecoverable OOM crashes with large images, breaking core usability for a subset of users.
- **Switch to Docker CLI/Go Client**: Rejected, due to legacy format incompatibility with future OCI plans and significantly slower performance than memory-efficient Crane.
- **Adopt Skopeo**: Rejected, as it offers no clear advantages over Crane and underperforms in key benchmark scenarios compared to the memory-efficient Crane implementation.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision
We will implement a Zarf-specific Big Bang package workflow with four core components to address the complexities of Big Bang deployment:

### 1. Flux Dependency Management
The Zarf Big Bang package will include an explicit, version-compatible dependency on a curated Zarf Flux package. 
- During deployment, Zarf will automatically install and validate Flux (including required CRDs) before initiating Big Bang setup.
- The package will include logic to verify Flux's health and configuration state to ensure it meets Big Bang's requirements, halting deployment with actionable errors if preconditions are not met.

### 2. Dynamic Image and Repository Pruning
To eliminate bloat from unused assets, we will add an automated pre-deployment (or package customization) step that:
- Accepts a user-provided `values.yaml` file and merges it with Big Bang's default values to resolve enabled components and sub-components.
- Leverages Big Bang's component metadata (mapped in a maintained, versioned component-to-asset registry bundled with the Zarf package) to cross-reference enabled components with their required images and Git repositories.
- Filters Big Bang's `package-images.yaml` and repository manifests to retain only the assets needed for the enabled components.
- Updates relevant deployment manifests (e.g., `kustomization.yaml`) to use the filtered asset lists instead of the full superset.

### 3. Standardized Values Injection
We will build a Zarf pre-deployment hook to simplify and standardize values injection:
- The hook will take the user's merged values file and inject it into Big Bang's deployment pipeline in a way that aligns with Flux's reconciliation workflow (e.g., updating Flux `HelmRelease` manifests with the merged values).
- It will ensure values are applied consistently across all relevant Big Bang manifests, eliminating manual edits to `kustomization.yaml` or other base files.

### 4. Dual Package Customization Options
To accommodate different user needs, we will support two paths for package creation/deployment:
- **Pre-Customized Packages**: Provide a CLI tool that lets users generate a trimmed Zarf Big Bang package upfront using their specific `values.yaml`. This tool runs the image/repo pruning during package creation, resulting in a minimal, user-specific package that reduces storage and deployment time.
- **Dynamic Deployment Pruning**: For users who prefer a single universal package, include the pruning and values injection logic as part of the Zarf deployment flow. This runs dynamically at deploy time to adapt the universal package to the user's provided `values.yaml`.

All components of this workflow will be documented in Zarf's Big Bang package documentation, including examples of how to use the CLI tool, inject values, and verify the pruned asset list.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision
Zarf will implement native support for publishing, pulling, and verifying Zarf packages using OCI-compliant registries, with integrated supply chain security via standard OCI artifact signing mechanisms, to meet both scalability and trust goals.

### Specific Implementation Details:
1.  **Native OCI Registry Integration (Primary Goal)**
    - Zarf will natively support pushing and pulling Zarf packages to/from any OCI-compliant container registry (e.g., Docker Hub, GitHub Container Registry, AWS ECR, Harbor) via new commands: `zarf package publish` (for uploading packages) and `zarf package pull` (for retrieving packages).
    - Zarf packages will be encapsulated as OCI artifacts using a custom media type (`application/vnd.zarf.package.v1+tar`) to distinguish them from container images while adhering to OCI specification constraints. This preserves the full structure of existing Zarf packages, ensuring backward compatibility with local package workflows.
    - OCI artifacts will use standard registry features (namespaces, tags, content digests) for versioning and organization, allowing users to reference packages via familiar formats like `registry.example.com/team/app:v1.0.0` or immutable digests for deterministic deployments.

2.  **Supply Chain Security (Secondary Goal)**
    - Zarf will integrate with Sigstore Cosign (an industry-standard OCI artifact signing tool) to enable package creators to sign packages during publish, and deployers to verify package integrity and origin before installation.
    - **Signing Flow**: The `zarf package publish` command will include an optional `--sign` flag that triggers Cosign to sign the OCI manifest of the Zarf package. Creators can use either traditional asymmetric key pairs (stored securely, e.g., in a secrets manager) or Sigstore's keyless signing flow (leveraging OIDC identity providers like GitHub, GitLab, or Google) for simplified, audit-compliant signing.
    - **Verification Flow**: 
      - The `zarf package deploy` command will automatically attempt to verify signatures for packages pulled from registries by default (with an explicit `--skip-verification` flag for trusted, isolated environments).
      - A standalone `zarf package verify` command will be added to validate the signature and integrity of any registry-hosted or local package reference.
    - Signatures and associated attestations will be stored alongside the package in the OCI registry (per OCI Distribution Spec extensions), ensuring all trust artifacts are colocated with the package for easy access and auditability.

3.  **Compatibility Guarantees**
    - Existing Zarf packages will not require modification to be published to OCI registries.
    - Packages pulled from registries will behave identically to locally built packages, with no changes to deployment or runtime behavior beyond the initial verification step.
    - Zarf will maintain support for local package workflows (building, deploying from local files) alongside OCI registry workflows for users who prefer or require offline/air-gapped operations.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision
Zarf will be extended with two targeted capabilities to address airgap development gaps and limited git functionality, while aligning with its core airgap-first and modular design principles:

### 1. Extensible Language Artifact Proxy/Cache System
Implement a plugin-based, optional proxy and cache layer to support common language package managers (including npm, PyPI, Maven Central, Go Modules, and Rust Crates) for airgap development:
- Integrate with Zarf’s existing package workflow: Allow users to either pre-fetch specific artifact repositories or auto-discover project dependencies during package builds for offline use.
- Transparent mirroring: In airgap environments, intercept client requests for artifacts, serving cached versions if available; when online, fetch and cache upstream artifacts on-demand for future offline use.
- Compatibility: Store cached artifacts in Zarf’s content-addressable package archive to leverage its existing airgap transport and consistency guarantees.
- Opt-in activation: Enable via package configuration or CLI flags to avoid unnecessary overhead in production-focused deployments.

### 2. Enhanced Native Git Support
Expand Zarf’s git handling to support native client operations and generic git use cases, while maintaining backward compatibility with Flux:
- Upgrade the built-in Zarf git server to fully support native git client commands (clone, pull, push, fetch) for public and private repositories, not just Flux `GitRepository` objects.
- Add a git proxy mode: Transparently intercept upstream git requests, caching repo content when online and serving cached content to native clients in airgap environments without Flux-specific setup.
- Flux compatibility: Allow Flux `GitRepository` resources to reference the enhanced Zarf git server, and support automatic conversion of pre-fetched git repos into Flux-compatible artifacts when required.

### 3. Unified UX & Integration
- Add dedicated CLI commands (`zarf artifact init`, `zarf artifact prefetch`, `zarf git proxy`) to manage proxy/cache instances, pre-fetch dependencies, and configure client environments.
- Auto-generate client configuration snippets (e.g., `.npmrc`, `pip.conf`, `.gitconfig`) when deploying packages with these components to minimize manual developer setup.
- Introduce introspection features (e.g., `zarf artifact list`, `zarf git cache list`) to display cached content, versions, and expiration status for transparency.
- Ensure all new components remain modular, letting users enable only the capabilities needed for their specific use case (development vs. production).","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision
Zarf will adopt a structured, multi-phase approach to achieve a stable, Generally Available (GA) v1.0.0 release suitable for mission-critical operations, with ongoing stability guarantees for all v1.x.x minor and patch releases. The key components of this approach are:

### 1. Strict Semantic Versioning (SemVer 2.0.0) Enforcement
- For the v1.x.x release line:
  - Patch releases (v1.x.y) will contain only backward-compatible bug fixes and critical security patches; no new features or behavioral changes.
  - Minor releases (v1.y.0) will include backward-compatible new features, improvements, and deprecations; no breaking changes to stable APIs, CLI commands, or workflows.
  - Breaking changes to stable functionality will be restricted to major version increments (e.g., v2.0.0) only.

### 2. Formal Deprecation Policy
- Any stable API, CLI command, or feature targeted for removal must go through a transparent two-step process:
  1. **Deprecation Announcement**: Included in a minor v1.x.0 release, with:
     - Visible warnings in CLI output, logs, and documentation.
     - A clear timeline (minimum 3-month window) for removal.
     - Guidance on migration to replacement functionality.
  2. **Removal**: Only allowed in the next major version (v2.0.0), after the deprecation window has elapsed.
- No stable functionality will be removed or broken in any v1.x.x release.

### 3. Enhanced Testing Rigor for Mission-Critical Reliability
- Expand test coverage to prioritize mission-critical workflows:
  - Mandate 90%+ end-to-end (E2E) coverage for air-gapped deployments, cluster lifecycle management, data persistence, failure recovery, and edge case scenarios (e.g., partial network outages, resource constraints).
  - Implement a dedicated regression test suite that runs on every PR and nightly; failures in critical test cases will block merges and releases.
  - Establish a ""production-like integration testing"" program with early adopter users, incorporating their real-world workflows into automated tests and validating releases in their pre-production environments before general availability.
  - Conduct quarterly security scans and vulnerability audits for all dependencies and core code, with high-severity issues resolved within 7 days of discovery.

### 4. GA Readiness Gates for v1.0.0
- v1.0.0 will be marked GA only when all of the following criteria are met:
  - No open critical or high-severity bugs in stable functionality (validated by maintainers and user feedback).
  - All stable APIs, CLI commands, and core workflows are fully documented (user guides, API references, troubleshooting docs) with examples for mission-critical use cases.
  - A formal support process is in place, including:
    - Publicly documented SLAs for critical bug fixes (48-hour response time for security/critical issues).
    - Dedicated communication channels (e.g., Slack, GitHub Discussions) for GA users.
  - All experimental features are clearly labeled, excluded from stability guarantees, and marked prominently in documentation.

### 5. v1.x.x Post-GA Lifecycle Management
- Maintain a dedicated v1.x.x support branch for 12 months following the release of v2.0.0, providing critical bug fixes and security patches during this period.
- Publish detailed release notes for every v1.x.x release, highlighting changes, fixed issues, deprecations (if any), and migration guidance.
- Maintain a public roadmap for v1.x.x that communicates upcoming features, maintenance plans, and deprecation timelines to keep users informed and aligned.

### 6. Experimental Feature Guardrails
- New features that are not yet stable will be labeled ""experimental"" and excluded from GA stability guarantees. These features may change without deprecation, and are clearly marked in CLI output, logs, and documentation.
- Experimental features can only graduate to ""stable"" status after:
  - 3+ months of public availability and user feedback.
  - Passing extended E2E and regression testing.
  - Approval from the Zarf maintainer team and early adopter users.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision
1. **Implement Native, Transparent ECR Support as Core Zarf Functionality**
   - Zarf will add first-class support for ECR as an external registry, eliminating the need for package-specific workarounds (e.g., custom actions) or pre-deployment user effort. This ensures packages remain system-agnostic and usable across any supported registry provider without modification.

2. **Automated ECR Auth Token Refresh**
   - During `zarf init` when ECR is selected as the external registry, Zarf will deploy a managed Kubernetes CronJob (or equivalent controller) that:
     - Runs on a recurring schedule (e.g., every 11 hours, before the 12-hour token expiration window)
     - Uses the AWS credentials provided during init (or instance IAM roles, if running on AWS infrastructure) to generate a fresh ECR authentication token
     - Updates the cluster's image pull/push secret with the new token automatically
   - This token refresh is fully transparent to users and package creators, requiring no manual intervention after initial setup.

3. **Auto-Creation of ECR Image Repositories**
   - When deploying a package to a cluster configured with ECR as its external registry, Zarf will:
     - Pre-scan the package to extract all unique image repository names from the package's manifest
     - Validate which of these repositories already exist in the target ECR registry
     - Automatically create any missing repositories using the user's AWS credentials (same as init)
     - Proceed with image pushes only after all required repositories are created
   - This process runs silently as part of the standard `zarf package deploy` workflow, with no changes needed to the package itself.

4. **Abstract Registry Operations via a Core Interface**
   - Define a generic registry operations interface within Zarf that encapsulates provider-specific logic (like ECR token refresh and repo creation, Docker Hub rate limiting handling, etc.)
   - Each supported registry provider (ECR, GCR, Docker Hub, etc.) will implement this interface, ensuring consistent behavior across all providers while accounting for unique provider constraints
   - This abstraction ensures package creators never need to write provider-specific code, and Zarf can easily add support for new registries in the future.

5. **Discourage Registry-Specific Action Workarounds**
   - Update Zarf documentation to explicitly guide package creators to rely on core Zarf registry support instead of implementing registry-specific logic in `actions`
   - Add validation warnings for packages that include obvious registry workarounds in actions (e.g., ECR repo creation scripts) and direct users to the native functionality

6. **Fallback Compatibility**
   - Ensure all native ECR handling logic is conditional: it only activates if the cluster was initialized with ECR as the external registry. For other registries, Zarf will continue to use its existing standard workflow, maintaining full backward compatibility for system-agnostic packages.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision 
We will compile Zarf's battle-tested Go transformation library into a WebAssembly (WASM) module, and integrate this module directly into Pepr's TypeScript codebase. This approach decouples transformations from Zarf's Agent while reusing the hardened, reliable transformation logic, eliminating the need to maintain duplicate TypeScript and Go implementations.

### Rationale for Rejecting Alternative Options
1. **Rewrite in TypeScript**: Rejected outright. This would discard Zarf's battle-tested transformation logic, introduce significant risk of regressions and bugs, and require ongoing parallel maintenance of two separate libraries—directly contradicting our core goal of avoiding disparate, unsynchronized codebases.
2. **REST API**: Rejected due to unnecessary overhead and latency. Admission control workflows require low-latency processing, and HTTP/REST adds network and serialization overhead. Additionally, REST lacks the type safety of structured alternatives, increasing the risk of integration errors between Go and TypeScript. Managing a separate REST service for transformations also adds operational complexity (e.g., service discovery, availability monitoring).
3. **gRPC**: Rejected as overkill for this use case. While gRPC offers type safety and efficient binary serialization, it requires standing up and managing a dedicated gRPC service, introducing inter-process communication complexity and operational overhead. For admission control, which benefits from embedded, low-latency processing, this extra layer of service infrastructure is unnecessary.

### Implementation Details
- The Zarf team will extract its core transformation logic into a standalone Go package that can be compiled to a WASM module without dependencies on Zarf Agent-specific code.
- The compiled WASM module will expose a well-defined interface for invoking transformations, with input/output data structures serialized to JSON (or another lightweight, cross-language format) to ensure compatibility with TypeScript.
- Pepr will load and execute the WASM module directly within its TypeScript runtime, eliminating network calls and keeping transformation processing embedded in the admission control pipeline.
- To keep the transformation logic synchronized, we will establish a pipeline that automatically rebuilds and publishes the WASM module whenever changes are merged to Zarf's transformation library, with Pepr configured to pull the latest validated module version on releases.

### Status
Accepted","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision
1. **Standardize Core Action Support Across Package Sources**
   Zarf will implement consistent support for all core package management actions (deploy, inspect, remove, list) for every officially supported package source, where technically feasible:
   - **Split Tarballs**: Add inspect functionality by transparently reassembling split parts temporarily (without requiring permanent disk reassembly for inspection) and remove functionality that deletes all associated split files for a package.
   - **In-cluster Packages**: Implement inspect functionality to fetch package metadata and content directly from the cluster's internal storage, aligning with existing remove capabilities.
   - **HTTPS URLs**: Add inspect functionality with optional user confirmation for large packages (to avoid unintended large downloads) and remove functionality that clears the local cached copy of the package. For remote HTTPS sources that cannot be deleted, the CLI will output a clear note explaining this limitation instead of an error.

2. **Implement a Unified Package Source Abstraction Layer**
   Create a shared, modular interface for package source handling that abstracts low-level differences between source types (e.g., split tar reassembly, OCI registry API calls, cluster storage fetching). All core actions will interact with this interface, reducing code duplication and simplifying the addition of new sources while ensuring consistent behavior.

3. **Clarify Limitations via Documentation and CLI Feedback**
   - Update Zarf's official documentation and CLI help text to explicitly list any remaining action limitations per source type (e.g., ""Remote HTTPS packages cannot be deleted from their original host; this action only clears your local cache"").
   - For actions that are fundamentally impossible for a given source type (e.g., removing a public remote HTTPS package), the CLI will return a user-friendly explanatory message instead of a generic error.

4. **Add Consistency Validation Tests**
   Introduce a suite of integration tests that validate each core action against every supported package source to catch regressions in consistency as new features are developed or existing ones are modified. This ensures that all sources maintain parity with core actions over time.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision 
Refactor Zarf's package composition system to resolve scalability, consistency, and reliability issues by centralizing core composition logic, formalizing import chain validation, and aligning OCI skeleton handling with a unified composition workflow. The specific changes are:

### 1. Address Import Chain Scalability and Test Coverage Gaps
- **Centralize Import Chain Processing**: Extract import resolution logic into a reusable, isolated core library component that supports arbitrary depth of nested imports (not just first-layer imports). This library will handle dependency graph resolution, cyclic import detection, and conflict resolution for cross-package component references.
- **Expand Test Coverage**: Implement an end-to-end test suite that validates import chains with 3+ nested layers, including edge cases like cyclic dependencies, overlapping component names, and cross-package resource references. Add unit tests for the core import library to cover individual resolution steps.
- **Improve Debugability**: Enhance error messaging to explicitly identify problematic layers in broken import chains (e.g., ""Conflict in component 'foo' at import layer 2: conflicting resource definitions from package 'bar'"") and add an optional `zarf package inspect imports` command to visualize import chain hierarchies for complex packages.

### 2. Align OCI Skeleton Handling with Core Composition
- **Integrate OCI Skeletons into Composition Pipeline**: Redesign OCI skeleton processing to be a native step in the core import and composition workflow (replacing the bolted-on post-processing approach). Imported OCI skeletons will be merged with the base package’s skeletons in parallel with component merging.
- **Define Standard Merging Rules**: Establish clear conflict resolution and merging rules for OCI skeleton assets:
  - Base package skeletons take precedence over imported skeletons unless explicitly overridden via a new `--override-skeletons` flag.
  - Image lists and repository configurations from imported packages are merged deduplicatively (using image digests to avoid duplicates).
- **Validate Skeleton Consistency**: Add validation checks to ensure merged OCI skeletons do not contain invalid configurations (e.g., conflicting repo URLs for the same image, missing required skeleton fields) before package creation proceeds.

### 3. Centralize Component Filtering Logic
- **Unify Filtering in Core Composition**: Move all component filtering logic (including the `only` flag) into the core composition library, ensuring filters are applied uniformly across all stages of package creation:
  1. Resolve all nested imports to form a complete set of components and skeletons.
  2. Apply filters to exclude non-matching components and their dependent assets (including associated OCI skeleton entries).
  3. Validate that filtering does not break required dependencies (e.g., throw an explicit error if an included component relies on a filtered-out component).
- **Deprecate Decentralized Filtering Code**: Remove any duplicate filtering logic from import handlers or post-processing steps to eliminate conflicting behavior and reduce bug surface area.

## Rationale
- **Import Chain Scalability**: Centralizing and testing deep import resolution addresses gaps in current test coverage and enables support for large, modular package ecosystems without reliability regressions.
- **OCI Skeleton Consistency**: Integrating skeletons into the core pipeline ensures composability works consistently for both components and OCI assets, eliminating bugs from bolted-on post-processing.
- **Filter Reliability**: Centralizing filtering logic ensures create-time filters behave predictably across all package types and import depths, reducing user-facing bugs and maintenance overhead.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision
We will introduce a dedicated `zarf package dev` CLI command (with complementary configuration and flag options) to streamline the package development lifecycle, building on and extending the existing YOLO mode capabilities to eliminate tedious, repetitive steps in the current workflow.

### Key Implementation Details:
1. **CLI Command & Flags**:
   - Add `zarf package dev` as a new top-level command that combines optimized build and deployment steps for local development.
   - Support targeting a local directory (containing a `zarf.yaml`) instead of a pre-built tarball, eliminating the need to run `zarf package create` for every iteration.
   - Include optional flags to customize behavior:
     - `--no-cache`: Force a full rebuild (including re-fetching dependencies) for scenarios where fresh assets are required.
     - `--prune`: Automatically remove resources from previous dev deployments that are no longer defined in the current `zarf.yaml` (avoids manual cluster resets).
     - `--use-initialized`: Opt into deploying to an already `zarf init`-ialized cluster (for testing interactions with Zarf's core services like the registry or git server) instead of the default YOLO-style uninitialized cluster.

2. **Optimized Dependency Handling**:
   - Reuse cached dependencies (Docker images, Helm charts, binary assets) that already exist on the local machine or in the target cluster, skipping redundant fetching/repackaging of large assets.
   - Maintain the option to selectively skip heavy dependency fetching (like YOLO mode) for fast iteration on configuration changes that don't affect asset content.

3. **Incremental Deployment & Feedback**:
   - Apply only changed components or manifest values from the `zarf.yaml` to the cluster, rather than redeploying the entire package from scratch.
   - Provide targeted, real-time error feedback tied directly to the relevant section of the `zarf.yaml` or deployment step, with actionable guidance to resolve issues without full rebuilds.

4. **Backward Compatibility & Guardrails**:
   - Keep the existing `zarf package create` and `zarf package deploy` workflows intact for air-gapped and production use cases.
   - Explicitly label `zarf package dev` as a development-only tool (via CLI warnings and documentation) and block its use in environments marked as production.
   - Retain the underlying YOLO mode logic but refactor it to support the new dev mode's flexible cluster targeting and caching features.

5. **Package-Specific Dev Configuration**:
   - Add an optional `dev` section to the `zarf.yaml` to let package authors define default dev mode behavior, such as:
     - Which dependencies to prioritize caching
     - Default pruning behavior
     - Skip conditions for non-critical components during rapid iteration","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision
We will migrate the Arachne documentation site from Docusaurus 2 to **Next.js using the Nextra documentation framework** to address the performance and dependency management limitations of the current setup.

### Rationale
1. **Performance Improvements**:
   - Nextra leverages Next.js’s optimized static site generation (SSG) and incremental static regeneration (ISR) to deliver fast initial load times, automatic code splitting, and optimized asset bundling. This directly resolves observed performance bottlenecks in the Docusaurus 2 implementation, such as slow page loads for large documentation pages and unoptimized client-side JavaScript.
   - We will validate performance gains post-migration using Lighthouse audits to ensure alignment with web performance best practices (e.g., Core Web Vitals).

2. **Simplified Dependency Management**:
   - Next.js and Nextra use a standard, widely adopted npm/yarn/pnpm dependency model with a smaller, more maintainable core dependency footprint compared to Docusaurus 2. This reduces the risk of dependency conflicts, simplifies regular updates, and aligns with modern JavaScript tooling conventions, lowering the barrier for contributors to maintain the docs site.

### Migration & Maintenance Plan
1. **Content Preservation**: Convert existing Markdown/MDX content to Nextra-compatible format, preserving the existing navigation structure, internal links, and styling to minimize disruption to end users.
2. **Backward Compatibility**: Implement permanent redirects for all existing documentation URLs using Next.js’s built-in redirect functionality to prevent broken links and preserve SEO value.
3. **Incremental Rollout**: Test the migrated site in a staging environment to validate functionality, performance, and accessibility before deploying to production.
4. **Long-Term Maintenance**: Update contribution guidelines to reflect the new tooling stack, and establish a quarterly schedule for dependency updates and performance audits to keep the site optimized.
5. **Decision Revisitation**: If future limitations emerge with Nextra/Next.js, we will evaluate alternative frameworks (e.g., Astro Docs, MkDocs Material) by creating a new Architecture Decision Record that references this one.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision
1. **Refactor Logging to Separate Error Reporting from Process Termination**
   - Deprecate all logging functions that trigger immediate process exits (e.g., `message.Fatal`). Instead, these functions will return structured error values containing the fatal log context, shifting responsibility for termination to top-level CLI logic.
   - Restrict `os.Exit` calls exclusively to the CLI's root entry points. These entry points will first execute all required cleanup routines (e.g., temporary resource teardown, context cancellation) before exiting, after logging the error using the appropriate UI format.

2. **Eliminate Global State for UI Logging Components**
   - Replace shared global progress bar and spinner instances with explicit, scoped instances. These instances will be initialized at the CLI command level and passed to dependent functions either directly as parameters or via a context object.
   - Ensure thread safety for UI components by binding each instance to a single execution context; for cross-thread use cases, add synchronization primitives or allow creation of independent per-thread instances.

3. **Decouple Logging via an Abstraction Layer**
   - Define a core `Logger` interface that encapsulates all required capabilities: leveled logging (debug, info, warn, error), fatal error context propagation, and UI interactions (progress bar control, spinner management).
   - Implement two concrete backends for this interface:
     - **CLI Logger**: Uses pterm to maintain the existing user-friendly CLI output format, preserving the current interactive UX.
     - **Test Logger**: A mockable, in-memory implementation that captures all log entries and UI events without terminal output. This will enable straightforward assertion of log content and UI behavior in unit tests.

4. **Mandate Dependency Injection of Logging Instances**
   - Modify all CLI and package-level functions that use logging or UI components to accept a `Logger` instance as a required parameter (or retrieve it from a passed context for deeply nested code).
   - Remove all direct dependencies on global logging state or concrete in-house logging types from non-logging code to reduce coupling.

5. **Standardize Error Handling and Logging Patterns**
   - Document a team-wide error flow pattern: functions return contextual errors (using a structured error library like `pkg/errors` or `fmt.Errorf` with `%w` wrapping), and top-level CLI code handles logging these errors and triggering termination.
   - Add static analysis checks or linting rules to prevent direct `os.Exit` calls in non-top-level code and enforce the injection of `Logger` instances in dependent functions.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision
1. **Formalize Explicit Schema Versioning Starting with Zarf v1.0.0**
   - Introduce a mandatory `schemaVersion` field in `zarf.yaml` (e.g., `schemaVersion: ""1.0.0""`) to explicitly declare the schema the configuration adheres to. Schema versions will follow semantic versioning (MAJOR.MINOR.PATCH) aligned with Zarf’s product release cycle.
   - Establish the v1 schema as a long-term support contract: all features in the v1 schema will be supported through the entire v1 major release series, with deprecations only being removed in the next major version (e.g., v2).

2. **Clean Up Pre-v1 Deprecations in v1.0.0**
   - Remove all pre-v1 deprecated keys marked for elimination in v1, including `setVariable`, `scripts`, `group`, and `cosignKeyPath`. These keys will no longer be recognized or processed by Zarf v1.0.0+.
   - Provide targeted migration support:
     - For fully migratable keys (e.g., `setVariable` → `setVariables`), release a `zarf migrate` command that automatically converts pre-v1 `zarf.yaml` files to the v1 schema.
     - For non-migratable keys with functional differences (e.g., `group` → `flavor`), publish detailed manual migration guides explaining how to adjust configurations to preserve intended behavior.

3. **Define Post-v1 Schema Evolution Rules**
   - **Major Versions (e.g., v2.0.0)**: Reserved for breaking changes incompatible with prior major versions. All deprecated features from the previous major version will be removed entirely. A new major schema version will require explicit user migration via tooling and documentation.
   - **Minor Versions (e.g., v1.1.0)**: May introduce new features or deprecate existing ones, but will maintain backward compatibility. Deprecated features will remain functional for all subsequent minor releases in the same major series. Zarf binaries will automatically migrate configurations from older minor schema versions within the same major version during both package creation and deployment, eliminating the need to retain deprecated fields in packaged artifacts.
   - **Patch Versions (e.g., v1.1.1)**: Will only include fixes to schema validation rules or minor clarifications that do not alter valid configuration structures.

4. **Backward Compatibility Guarantees**
   - Zarf binaries will support all schema versions within their own major release series. For example, Zarf v1.5.0 will process any `zarf.yaml` with a schema version from v1.0.0 to v1.5.0, auto-migrating to the current schema on-the-fly.
   - Zarf binaries will not support schema versions from different major releases. Users must run `zarf migrate` to convert configurations to the target major schema version before processing.

5. **Standardize Deprecation Process for Post-v1 Features**
   - When deprecating a feature in a minor release, Zarf will emit clear warnings during configuration processing, including links to migration documentation and a notice that the feature will be removed in the next major version.
   - Deprecated features will remain functional through all minor releases of the current major version to avoid breaking existing workflows prematurely.

6. **Artifact Packaging Update**
   - When creating a package with a Zarf binary from a minor release, the packaged `zarf.yaml` will use the latest minor schema version of the current major release. Same-major Zarf binaries will handle auto-migration from this schema version to their own during deployment, eliminating the need to retain deprecated fields in packaged artifacts.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
