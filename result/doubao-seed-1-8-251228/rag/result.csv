context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision 
## Decision

* Implement native lazy loading for all offscreen images and thumbnails in the Storefront by adding the `loading=""lazy""` attribute to their `<img>` elements.
* Exclude above-the-fold images (e.g., main product image on product detail pages, hero banner images, site logo) from lazy loading to ensure their immediate rendering for optimal initial page load performance.
* Skip integration of third-party lazy loading libraries (like lazysizes) to minimize bundle size and external dependencies, relying instead on the widely supported native browser functionality.
* For legacy browsers lacking native lazy loading support, retain the existing default image loading behavior (immediate load) to avoid introducing unnecessary complexity or content accessibility regressions.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision 
## Decision

1. **Deprecate the `autoload` parameter** for `OneToOneAssociationField` and `ManyToOneAssociationField` associations:
   - A deprecation warning will be logged whenever `autoload=true` is set on these fields, guiding developers to migrate to explicit eager loading methods.
   - The parameter will be marked as deprecated in all relevant documentation and entity definition examples.

2. **Remove automatic eager loading via `autoload=true`** in the next major Shopware version. This eliminates the implicit, forced loading of associations that may not be needed in the current context.

3. **Explicit Eager Loading Recommendations**:
   For cases where eager loading of an association is required, developers should use the following transparent, context-aware approaches:
   - **API Consumers**: Use the `includes` query parameter to specify which associations to load (e.g., `?includes[product]=manufacturer`), ensuring only necessary data is included in API responses.
   - **Backend PHP Logic**: Call `Criteria::addAssociation()` or `Criteria::addAssociations()` in code to explicitly define associations to load for a given query.
   - **Admin UI Extensions**: Utilize component-specific association configuration to load required data only when the component renders or processes it.

4. **Performance Best Practice Guidance**:
   - We encourage default use of lazy loading (the out-of-the-box behavior for these associations) for most cases.
   - Eager loading should only be used when the association data is guaranteed to be consumed in the current execution path, as this avoids unnecessary SQL joins, data transfer, hydration overhead, and bloated API payloads.

### Rationale
- Implicit automatic loading via `autoload=true` leads to unintended performance regressions by fetching and processing unused data across all queries involving the entity.
- Explicit loading practices align with modern database performance best practices, giving developers full control over data retrieval based on the specific needs of each context.
- Deprecating and removing the `autoload` parameter eliminates accidental performance issues caused by forgotten implicit associations, making application behavior more predictable and maintainable.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision 
## Decision

We implement a hybrid system combining flat file storage, embedded metadata, and auto-generated index files to support all target search and discovery use cases:

1. **Flat Root Directory Storage**: Relocate all ADRs to a single root directory (e.g., `/docs/adr/`) using the existing date-based filename format (`YYYYMMDD-adr-title.md`). This eliminates duplicate headline matches across folders and ensures chronological sorting directly in the filesystem.
2. **Mandatory Embedded Metadata**: Require all new ADRs to include YAML frontmatter at the top with standardized fields:
   - `area`: The functional/technical area the ADR belongs to (e.g., ""architecture-framework"", ""data-persistence"", ""deployment-strategy"")
   - `created_date`: ISO 8601 date string (matching the filename's date prefix for consistency)
   - `status`: Current decision status (e.g., ""Accepted"", ""Superseded"", ""Draft"", ""Deprecated"")
3. **Auto-Generated Indexes**:
   a. **Machine-Readable Index**: Maintain an `adr-index.json` file, updated automatically via a pre-commit hook or CI pipeline, that maps every ADR to its metadata, file path, and content snippet. This supports programmatic filtering and searching.
   b. **Human-Readable Summary**: Generate an `adr-summary.md` file alongside the index, featuring two key sections:
      - **Latest ADRs**: Chronologically sorted list of the 10 most recent decisions (with status, area, and direct link to the full document)
      - **ADRs by Area**: Grouped lists of decisions organized by their `area` tag, sorted chronologically within each group
4. **Backward Compatibility**: Migrate existing folder-structured ADRs to the flat directory, retroactively add required metadata frontmatter to each, and run the index generator once to populate initial indexes.

### Rationale
This approach addresses all identified use cases effectively:
- **Known Headline Search**: Flat storage ensures a single, concise match per headline (no duplicate results across folders), making direct file access or filesystem search fast and unambiguous.
- **Latest ADR Access**: The auto-generated summary provides immediate visibility to recent decisions without needing to navigate Git history or sort through folder structures.
- **Area-Specific Discovery**: The grouped section in the human-readable summary and machine-readable index both enable quick access to all ADRs in a target area.
- **Scalability**: The flat structure avoids future complexity of managing nested folders, while metadata and indexes remain maintainable via automation.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision 
## Decision
We abandon the tested native Vue 3 extensibility approach and implement a new, type-safe plugin extension system that leverages standard Vue 3 patterns, avoids reliance on internal Vue runtime logic, and preserves all extensibility capabilities of the current Component Factory. This system will unlock full use of native Vue tooling while ensuring long-term update safety.

### Key Implementation Breakdown:
1. **Template Extensibility**:
Replace the proposed `sw-block` component approach with a named slot-based system paired with a declarative extension registry. Base components will expose all modifiable template sections as named slots with fallback default content. Plugin developers can register extensions against these slot keys to override, prepend, or append content. This enables full template precompilation, supports native Vue tooling (linting, static analysis), and eliminates custom block wrappers that conflict with Vue's compiler.

2. **Script & Logic Extensibility**:
Implement a granular, hook-based system using Vue 3's Composition API. Base components will expose dedicated, concern-specific extension hooks (e.g., `useComponentPropsExtender`, `useComponentStateExtender`, `useComponentMethodExtender`) at key logic points. Plugin developers inject modifications or additional logic into these targeted hooks instead of merging entire component state—avoiding conflicts with Vue's reactivity optimizations and ensuring compatibility with future Vue updates.

3. **Centralized Extension Registry**:
Build a type-safe, centralized registry that formalizes how plugins declare and inject extensions into components. The registry will handle runtime application of extensions, ensuring proper reactivity and avoiding race conditions. It will also provide static analysis tooling support to help plugin developers catch extension-related errors early in the development process.

### Rationale:
This strategy resolves critical flaws in both the existing Component Factory and the tested Vue 3 approach:
- Eliminates reliance on internal Vue logic, making the system update-safe for future Vue releases.
- Unlocks full use of native Vue ecosystem tools (precompilation, linting, TypeScript support) without custom modifications.
- Maintains the same level of flexibility for plugin developers to modify or extend any part of administration components.
- Avoids performance and compatibility issues tied to runtime component generation and full component state merging.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision 
## Decision

_The change that we're proposing or have agreed to implement._

We will adopt a **changelog fragments workflow** for technical changelog and upgrade information, tailored to our trunk-based development process:

1.  **Per-change fragment files**: Each code change (bugfix, feature, breaking change, etc.) adds a single, small fragment file in a dedicated `changelog-fragments/` directory in the repository root.
2.  **Standardized fragment format**: Each fragment follows a consistent structure (Markdown or YAML) containing:
    - Type of change (e.g., `bugfix`, `feature`, `breaking-change`, `documentation`)
    - Clear, user-facing description of the change
    - Reference to the corresponding Jira issue (if applicable)
    - Upgrade instructions (for breaking changes or impactful updates)
3.  **Automated compilation**: During the release preparation process, a dedicated script will:
    - Collect all unprocessed fragments from `changelog-fragments/`
    - Organize them by change type and merge them into the main `CHANGELOG.md` file, associating them with the target release version
    - Move processed fragments to an archived directory (e.g., `changelog-fragments/archived/<version>/`) for auditability, or delete them after confirmation
4.  **PR requirement**: All pull requests introducing user-visible changes must include a corresponding changelog fragment to be merged.

## Rationale

_The reasoning behind our decision, addressing the original context and pain points:_

-   **Eliminates merge conflicts**: Since each change is encapsulated in a unique file, concurrent edits to the changelog no longer result in conflicts—removing a major friction point for trunk-based development.
-   **Resolves version uncertainty**: Developers do not need to predict the target release version when writing changelog entries; the version is assigned during compilation, aligning fragments with the actual release they ship in.
-   **Syncs code and changelog state**: Fragments are submitted as part of the same PR as the code change, ensuring the changelog entry is directly linked to the code it describes and eliminating discrepancies between Jira issue state and code state.
-   **Simplifies maintenance**: Fragments are lightweight and focused, making it easy for developers to write consistent changelog entries without navigating a large, monolithic file.
-   **Improves release reliability**: Automated compilation ensures no fragments are missed during release, and standardized formatting keeps the main changelog readable and structured for end-users.

## Tooling & Enforcement

-   We will use a custom script (or adopt an existing tool like `towncrier` if suitable) to handle fragment compilation and archiving.
-   A CI check will enforce that PRs with user-visible changes include a valid changelog fragment in the correct directory and format.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision 
## Decision

Implement the `DelayAction` using a persistent delayed job system with minimal, stable payloads that prioritize entity identifiers over full state, paired with mandatory data rehydration and rule reloading at the end of the delay window. The solution addresses stale data and rule changes via the following structured approach:

### 1. Delay Job Payload Structure
- **Avoid embedding full entity data or rule definitions**: These are prone to staleness and bloat payload size.
- **Store only stable, reference-based metadata**:
  - Unique flow execution ID (to track the ongoing flow instance and its state)
  - IDs of core domain entities (e.g., order ID, customer ID) that the action and dependent rules rely on
  - Flow step configuration ID (to reference the specific `DelayAction` and associated rule set)
  - Initial trigger metadata (correlation ID, timestamp of delay initiation)
  - Idempotency key (to prevent duplicate execution of the action if retried)

### 2. Data Rehydration on Delay Completion
When the delayed job triggers:
- Fetch the latest state of all referenced entities from the primary data store using their IDs.
- Handle missing entities:
  - Log a warning with the flow execution ID and missing entity ID for audit purposes.
  - Mark the flow step as ""failed"" or ""skipped"" based on the flow’s configured failure policy.
  - Proceed to subsequent flow steps (if allowed by policy) and trigger alerts for relevant stakeholders if configured.

### 3. Rule Re-Evaluation & Validation
- Load the current active version of rules associated with the flow step from the rule repository (not the version active at delay initiation).
- Handle rule changes or deletion:
  - If the rule no longer exists or is disabled: Log an error, mark the step as ""failed"", and do not execute the action.
  - Evaluate the reloaded rules against the rehydrated latest entity data (not the original stale data from delay start).

### 4. Transient & Permanent Failure Handling
- **Transient failures** (e.g., database connection timeouts during data rehydration): Use exponential backoff retries to re-attempt the step a configurable number of times before marking it as failed.
- **Permanent failures** (e.g., missing entity, deleted rule): Do not retry—log the issue, update the flow state, and proceed according to the flow’s failure policy.

### 5. Integration with Flow State Management
- Update the flow execution state to reflect delay progress: Mark as ""waiting on delay"" when the delay starts, and ""delay resolved"" when the job triggers.
- Check the flow execution state (e.g., canceled, completed) before executing the action: If the flow was canceled during the delay, skip the action and mark the step as ""canceled"".

### 6. Underlying Delay Mechanism
Leverage existing persistent delayed delivery capabilities (e.g., RabbitMQ Delayed Message Exchange, Hangfire, or the platform’s built-in job scheduler) to manage the delay window, ensuring reliability and avoiding custom delay loop implementations.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision 
## Decision

1. Integrate the mature app system functionality directly into the Shopware Platform core as a native, enabled-by-default feature, starting with the next scheduled minor or major platform release.
2. Maintain full functional parity between the native app system and the standalone plugin, ensuring identical behavior, APIs, and extension capabilities across both cloud and on-premises environments.
3. Implement an automated migration path for existing users:
   - Detect installation of the standalone app system plugin during platform upgrade.
   - Automatically disable the standalone plugin to avoid conflicts.
   - Preserve all installed apps, configurations, and app data to ensure uninterrupted operation post-migration.
4. Deprecate the standalone app system plugin:
   - Mark the plugin as deprecated in the Shopware Store and official documentation.
   - Cease active feature development on the plugin, limiting future updates to critical security or compatibility fixes until a defined sunset date (e.g., 6 months post-native integration release).
5. Align app system development with the Shopware Platform's standard release workflow:
   - Include app system feature updates, bug fixes, and security patches in platform release cycles.
   - Subject all app system changes to the same testing, validation, and versioning processes as core platform functionality.
6. Update communication and documentation:
   - Revise official documentation to position the app system as a native platform feature, removing references to the standalone plugin for new installations.
   - Notify existing plugin users and developer partners of the migration via platform release notes, admin dashboard notifications, and developer portal announcements.
7. Ensure backward compatibility for app developers:
   - Maintain all existing app system APIs and extension points to prevent breaking changes for already published apps.
   - Route future app system enhancements through the platform's regular deprecation and versioning policies.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision 
## Decision

### 1. Introduce a Unified `FlowStorer` Interface
Replace all per-implementation storer interfaces with a single, generic interface that standardizes core operations and applicability checks:
```typescript
interface FlowStorer<T = unknown> {
  /** Determine if this storer should handle the given event data */
  supports(eventData: unknown): eventData is T;
  
  /** Store the event data */
  store(eventData: T): Promise<void> | void;
  
  /** Optional: Restore stored data (default no-op) */
  restore?(): Promise<void> | void;
}
```
This eliminates the need for unique interfaces per storer and centralizes the logic for event routing.

### 2. Create an Abstract Base Class for Scalar Storers
Since most storers handle scalar values without restore logic, implement an abstract base class to encapsulate common boilerplate:
```typescript
abstract class AbstractScalarFlowStorer<T> implements FlowStorer<T> {
  constructor(private readonly dataType: string) {}

  supports(eventData: unknown): eventData is T {
    return typeof eventData === this.dataType;
  }

  restore(): void {
    // Default no-op for scalar storers that don't need restore logic
  }

  abstract store(eventData: T): Promise<void> | void;
}
```
New scalar storers can extend this class and only implement the store method, reducing repetitive code by ~70% for common cases.

### 3. Implement a Centralized `FlowStorerRegistry`
Create a registry to manage all storer instances, handle event routing, and simplify plugin interactions:
```typescript
class FlowStorerRegistry {
  private storers: FlowStorer[] = [];

  register(storer: FlowStorer): void {
    this.storers.push(storer);
  }

  unregister(storerType: string): void {
    this.storers = this.storers.filter(s => s.constructor.name !== storerType);
  }

  async handleEvent(eventData: unknown): Promise<void> {
    for (const storer of this.storers) {
      if (storer.supports(eventData)) {
        await storer.store(eventData);
      }
    }
  }
}
```
- Plugins can now enable/disable storers via the registry instead of bypassing individual interfaces.
- Events are automatically routed to applicable storers via the `supports` method, eliminating manual interface checks.

### 4. Enforce Type Safety with Generics
Use generics to ensure type consistency between a storer’s supported data type and its storage logic. For example, a number storer would be implemented as:
```typescript
class NumberFlowStorer extends AbstractScalarFlowStorer<number> {
  constructor() {
    super('number');
  }

  store(eventData: number): void {
    // Scalar storage logic here
  }
}
```

### Key Benefits
- **Reduced Boilerplate**: New scalar storers require only 1-2 methods instead of full interface definitions and support logic.
- **Simplified Plugin Integration**: Plugins interact with a single registry instead of multiple storer-specific interfaces.
- **Consistent Logic**: Centralized event routing and default restore logic eliminate redundant code across implementations.
- **Extensible**: Custom storers (with restore logic or non-scalar data) still implement the unified interface, maintaining compatibility with the registry.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision 
## Decision

Replace the custom sw-text-editor with **CKEditor 5** as the default text editor for the administration interface:
- CKEditor 5 provides a stable, actively maintained core that eliminates the low-level WYSIWYG bugs present in the current implementation.
- Its modular, well-documented architecture simplifies ongoing maintenance, feature extension, and developer onboarding, resolving the current editor’s readability and maintainability issues.
- It includes a comprehensive set of out-of-the-box features (text formatting, media embedding, list management) that meet our core requirements, with support for custom plugins to add administration-specific functionality as needed.
- We will implement a backward-compatibility layer during a 3-month transition period to ensure existing editor integrations in the administration interface function without disruption while teams migrate to the new editor’s API.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision 
## Decision

We formally adopt ""not_specified"" as the default salutation value for all cases where a salutation is not explicitly provided by a customer or configured by an administrator in Shopware 6. This change is implemented across the entire platform with the following requirements to ensure inclusivity and data consistency:

1. **Default Assignment**: For new customer registrations, admin-created customer entities, or profile updates where the salutation field is left empty/unselected, the system will automatically set the salutation to ""not_specified"".
2. **Historical Data Migration**: Existing records with null, empty, or unrecognized salutation values will be migrated to ""not_specified"" to standardize data across all historical and new entities.
3. **Inclusive Display Handling**: Storefronts and admin interfaces must map ""not_specified"" to a localized, gender-neutral greeting (e.g., ""Dear"" in English, ""Sehr geehrte/r"" in German, or region-appropriate neutral alternatives) instead of falling back to gendered defaults like ""Mr."" or ""Ms."".
4. **API Consistency**: All platform APIs (REST, GraphQL, Admin API) will accept ""not_specified"" as a valid salutation value, and will return this value (instead of null or empty strings) when no salutation is explicitly set for an entity.
5. **Admin UI Integration**: The admin interface will include ""Not Specified"" as a visible, selectable option in all salutation fields, and will set this as the default choice for new entities unless an alternative salutation is explicitly selected.

This decision eliminates gender assumptions, aligns with inclusivity principles, and ensures uniform salutation handling across all parts of the Shopware 6 ecosystem.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision 
## Decision
1. **Remove the `padaliyajay/php-autoprefixer` package** from the Storefront theme compilation pipeline entirely:
   - This eliminates the significant performance bottleneck in the `theme:compile` process, which is critical for maintaining efficient pipeline execution especially in our SaaS environment.
   - Resolves the unsustainable issue of hardcoded prefix rules that do not respect our `.browserslist` configuration.

2. **Implement targeted manual vendor prefixing** only for CSS properties where prefixes are still required to support our defined browser matrix:
   - Before adding manual prefixes, validate necessity using authoritative sources like [caniuse.com](https://caniuse.com/) aligned with our `.browserslist` settings.
   - Example: For properties with partial support in supported browsers, add prefixes directly in the relevant SCSS files (e.g., `-webkit-backdrop-filter` alongside `backdrop-filter` if needed for older Chrome versions in our support list).

3. **Periodically review prefix requirements**:
   - On major updates to Bootstrap or adjustments to our browser support policy, conduct a audit of Storefront CSS to remove obsolete manual prefixes and add any new necessary ones.
   - Document this manual prefixing workflow for developers to ensure consistency when contributing new CSS code.

This approach balances performance improvements, alignment with our actual browser support needs, and long-term sustainability without relying on a rigid, unmaintained autoprefixer solution. The reduced number of required prefixes (post-Bootstrap v5 update) makes manual management feasible and low-overhead.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision 
## Decision

1. We will replace or supplement npm's default audit behavior with SemVer 2.0.0-compliant vulnerability scanning tooling (e.g., Dependabot, Snyk, or `npm audit` with strict SemVer flags) to correctly evaluate pre-release versions against vulnerable stable ranges. This ensures pre-releases like `1.9.0-alpha1` are recognized as newer than vulnerable stable versions like `1.8.7`, triggering appropriate security alerts.

2. For internal packages:
   - When a security fix is released in a stable version (e.g., 2.0.0 addressing 1.8.7's vulnerability), we will backport the fix to all active pre-release branches (e.g., 1.9.x) that are newer than the vulnerable stable version but lack the fix.
   - We will publish updated pre-release versions (e.g., 1.9.0-alpha2) with the fix, including explicit release notes linking the pre-release to the security resolution.

3. We will add an explicit CI pipeline step to validate all pre-release dependencies against the latest vulnerability databases using a tool that strictly adheres to SemVer precedence rules for pre-releases. This step will fail if a pre-release dependency is found to inherit unpatched vulnerabilities from its stable lineage.

4. We will enforce a policy to avoid using pre-release dependencies unless:
   - The pre-release explicitly includes fixes for all known vulnerabilities in its preceding stable versions (verified via release notes or code review), or
   - We contribute a backport of the required security fix to the pre-release branch and confirm the maintainers have published an updated pre-release.

This approach eliminates the blind spot in vulnerability reporting for pre-release versions, ensures our internal pre-releases are secure, and standardizes our handling of pre-release dependencies across the organization.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision 
## Decision

To resolve the performance bottleneck of outdated cart entry deletions, we implement the following targeted changes:

### 1. Add a Composite Index to Enable Efficient Lookups
We create a composite index on the `cart` table that covers both `updated_at` and `created_at` columns. This index directly supports both branches of the delete condition and allows the database to avoid full table scans.

Implement this via a database migration:
```sql
CREATE INDEX idx_cart_updated_created ON cart (updated_at, created_at);
```
This index optimizes both:
- Rows where `updated_at IS NOT NULL AND updated_at <= {cutoff_date}`: Leverages the ordered `updated_at` column to perform fast range scans.
- Rows where `updated_at IS NULL AND created_at <= {cutoff_date}`: Groups all `NULL` `updated_at` entries and uses the ordered `created_at` column to filter outdated records quickly.

### 2. Simplify the Delete Query for Clarity and Compatibility
We rewrite the WHERE clause to use `COALESCE(updated_at, created_at)` for logical conciseness while maintaining full compatibility with the composite index:
```sql
DELETE FROM cart
WHERE COALESCE(updated_at, created_at) <= '2023-02-01'
LIMIT 1000;
```
This clause is logically equivalent to the original but easier to maintain. For MySQL 8.0+ environments, we can optionally add a functional index for further optimization:
```sql
CREATE INDEX idx_cart_coalesced_updated_created ON cart ((COALESCE(updated_at, created_at)));
```
The composite index remains the primary solution to support MySQL 5.7 (still supported in Shopware) and newer versions.

### 3. Retain Batch Processing to Avoid Service Disruption
We keep the `LIMIT 1000` clause to split large deletions into small, manageable batches. This prevents long-running transactions and table locks, which are critical for high-traffic shops to maintain uninterrupted service.

These changes will reduce execution time from over 30 seconds to milliseconds per batch, eliminate PHP timeouts/exceptions, and ensure consistent performance even under heavy load.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision 
## Decision

We will implement an official experimental feature release framework to enable early-stage feature deployment for user feedback, while explicitly excluding these features from our standard backwards compatibility promise. The framework includes the following core components:

1. **Explicit Experimental Labeling**:
   - All experimental features must be clearly marked across all touchpoints:
     - API endpoints will include an `/experimental/` prefix (e.g., `/experimental/v1/b2b/employees`).
     - UI interfaces will display a prominent ""Experimental"" badge with tooltips explaining the non-stable status.
     - Release notes, documentation, and marketing materials will feature a disclaimer outlining that these features are not covered by our backwards compatibility guarantee.
2. **Backwards Compatibility Exception**:
   - Experimental features are exempt from our standard backwards compatibility policy. We reserve the right to modify, deprecate, or remove these features without prior notice, except for sunsetting entirely—for which we will provide at least 30 days of advance communication to active users.
3. **Structured Feedback Collection**:
   - For each experimental feature:
     - Tag support tickets with an `experimental` label to track user issues and input.
     - Deploy in-app feedback prompts targeting users who interact with the feature.
     - Maintain a dedicated forum thread to gather structured suggestions and prioritize next iterations.
4. **Stabilization Pathway**:
   - An experimental feature will be promoted to stable status only when:
     - It has received user feedback confirming core utility and resolving critical pain points.
     - We can stabilize its implementation with a clear migration plan (if needed) to align with our backwards compatibility promise.
     - It meets our performance, security, and accessibility standards for stable releases.

We will pilot this framework with two high-priority features cited in the context: the multi-account B2B employee management system and the first iteration of the advanced search feature, to refine the framework before expanding to other initiatives.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision 
## Decision

### 1. Simplify Stock Model & Data Storage
- Remove the redundant `availableStock` field from the `product` table. Instead, calculate available stock dynamically as `base stock - sum of active stock reservations`.
- Introduce a new `product_stock_reservation` database table to track temporary stock holds:
  - Fields: `id`, `product_id`, `order_id`, `quantity`, `state` (active/confirmed/released), `created_at`, `updated_at`.
  - Add indexes on `product_id`, `state`, and `order_id` to optimize reservation sum queries for large catalogs.
- Provide a migration script to convert existing data:
  - Set product `stock` field to its current value.
  - Create active reservations for all open orders based on the difference between original `stock` and `availableStock` values.

### 2. Build Extensible Core Interfaces
Define a set of core interfaces to decouple stock logic from the platform, enabling plugin-based customization (e.g., ERP integrations):
- `StockLoaderInterface`: Responsible for fetching base stock and calculating available stock (base stock minus active reservations).
- `StockReservatorInterface`: Handles reserving stock when orders are placed or updated.
- `StockReleaserInterface`: Manages releasing reservations (e.g., when an order is canceled).
- `StockConfirmerInterface`: Converts active reservations to permanent base stock deductions (e.g., when an order is completed).
- `StockUpdaterInterface`: Processes direct base stock adjustments (e.g., restocks, manual changes).
- Allow decoration or full replacement of these interfaces via the plugin system, with default implementations provided for core use cases.

### 3. Optimize Performance
- Implement caching for calculated available stock (using Redis or Doctrine's cache system). Invalidate cache entries whenever base stock is updated or reservations are added/removed.
- Use batch database operations for bulk stock changes (e.g., restocking multiple products, reserving stock for bulk orders) to minimize query overhead.
- Precompute available stock for frequently accessed products (e.g., featured items) via scheduled background jobs to reduce real-time calculation load.

### 4. Align Stock Operations with Order Lifecycle
- Integrate stock actions directly with the order state machine via configurable transitions:
  - **Order Placement**: Trigger `StockReservatorInterface::reserve()` when an order transitions to `open` (or a custom ""reserved"" state).
  - **Order Completion**: Trigger `StockConfirmerInterface::confirm()` to deduct reserved stock from base stock and mark reservations as confirmed.
  - **Order Cancellation**: Trigger `StockReleaserInterface::release()` to free up reserved stock and mark reservations as released.
  - Allow merchants to customize which state transitions trigger stock operations via admin settings.

### 5. Maintain Backward Compatibility
- Keep legacy stock endpoints and fields (e.g., `availableStock` in API responses) functional by mapping them to the new model (calculating on-the-fly).
- Deprecate legacy stock fields/endpoints in a minor release, with full removal planned for the next major version.
- Ensure existing plugins relying on stock operations continue to work via decorators or compatibility layers for the new interfaces.

### 6. Add Admin Tooling
- Update the product detail page in the admin to display:
  - Base stock (current physical inventory).
  - Total active reserved stock (from open orders).
  - Calculated available stock.
  - A history of stock changes (restocks, reservations, deductions) for audit purposes.
- Provide bulk actions for adjusting base stock or managing reservations (e.g., manually releasing a reservation for a canceled order).","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision 
## Decision

1. **Categorize Client-Caused Exceptions as Non-Critical Log Events**:
   - All instances of `ShopwareHttpException` with an HTTP status code in the 400–499 range (client errors, e.g., missing fields, invalid input) are classified as expected, non-critical events. These will be logged at the `notice` level instead of the default `error` level to reduce log noise.
   - `ShopwareHttpException` instances with 5xx status codes (server errors) remain logged at the `error` level, as these indicate unexpected server-side issues requiring investigation.

2. **Extend Centralized Configuration for Global Consistency**:
   - Adopt the existing cloud environment’s exception log-level mapping mechanism as the global standard for all deployment environments (self-hosted, cloud, staging). This configuration will be maintained in a centralized location (e.g., Symfony’s Monolog configuration or a dedicated exception listener service) to avoid scattered code adjustments.
   - The configuration will dynamically evaluate the HTTP status code of `ShopwareHttpException` instances to set the log level, rather than relying on a static list of exception classes. This ensures coverage for all future subclasses or dynamic status code assignments.

3. **Preserve Default Behavior for Unrelated Scenarios**:
   - Retain the default Symfony/Monolog behavior of logging uncaught non-`ShopwareHttpException` exceptions at the `error` level, as these represent unhandled errors needing analysis.
   - Keep the existing practice of treating mapper-level exceptions as permanent failures (ack and proceed) unchanged, as this is orthogonal to client error logging.

4. **Document Convention for Developers**:
   - Update internal documentation to specify that throwing a `ShopwareHttpException` with a 4xx status code is the canonical way to handle client errors. Developers do not need to manually adjust log levels for these cases, as the centralized configuration will handle it automatically.
   - Clarify that 5xx status codes in `ShopwareHttpException` should only be used for genuine server-side failures that require error-level logging and investigation.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision 
## Decision

We will replace UUIDv4 with **UUIDv7** as the primary key type for all new database tables using UUIDs. For existing tables currently using UUIDv4:
1.  Prioritize migration to UUIDv7 for high-throughput tables where index inefficiency has a measurable impact on query/update performance.
2.  Retain UUIDv4 for low-throughput or legacy tables unless performance degradation is observed or a system-wide update warrants a full migration.

UUIDv7 will be implemented in alignment with the IETF draft specification (draft-peabody-dispatch-new-uuid-format-04), which embeds a millisecond-precision Unix timestamp as the prefix of the UUID. This time-ordered prefix addresses the B-tree index inefficiencies of UUIDv4 while preserving the core benefits of UUIDs: universal uniqueness, and seamless integration across diverse data sources.

Key implementation guardrails:
- Use a production-grade UUIDv7 generation library that guarantees timestamp monotonicity in high-concurrency environments to eliminate duplicate key risks.
- Ensure UUIDv7 values remain fully compatible with existing UUID field types in databases and application code (since UUIDv7 is a valid standard UUID format).
- For migrating existing UUIDv4 tables: adopt backward-compatible strategies (e.g., first adding a UUIDv7 secondary index, then gradually transitioning it to the primary key) to minimize downtime and data integrity risks.

This change will deliver measurable performance improvements by reducing B-tree fragmentation, cutting down on new page allocations, and maintaining more compact, efficient indexes.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision 
## Decision

1. **Define a Standardized `FlowEventExtension` Interface**: Create a core-maintained interface for app-specific flow events, requiring implementations to expose:
   - A namespace-qualified unique event type identifier (e.g., `myapp:user:on-profile-updated`) to avoid collisions.
   - Human-readable metadata: display name, description, category, and optional icon for UI consistency.
   - A validation method to validate event configuration parameters at flow design time.
   - An execution method that accepts flow context and configuration, and executes the event’s logic at runtime.

2. **Implement a Dynamic `FlowEventRegistry`**: Add a thread-safe registry to the core flow builder system that:
   - Allows apps to register their `FlowEventExtension` instances during startup.
   - Indexes events by their unique identifier for fast lookup during flow execution.
   - Exposes a read-only API for the flow builder UI to fetch all available events (core + custom).

3. **Update Flow Builder UI Components**: Modify the event picker and configuration UI to:
   - Dynamically fetch events from the registry instead of relying on hardcoded lists.
   - Render custom events alongside core events using consistent grouping, labels, and icons from the extension’s metadata.
   - Generate configuration forms dynamically using optional schema hints provided by the `FlowEventExtension`.

4. **Enhance Runtime Execution Logic**: Update the flow engine to resolve events via the registry:
   - Look up the event type in the registry to retrieve the corresponding extension during flow execution.
   - Throw clear, actionable errors for unregistered event types (e.g., ""Unsupported event: myapp:user:on-profile-updated"").
   - Invoke the extension’s execution method with flow context and stored parameters, propagating errors to the flow engine for consistent error handling.

5. **Provide Tooling & Guidance**:
   - Add a CLI scaffolding command (`flow generate event-extension`) to generate boilerplate code for `FlowEventExtension` implementations.
   - Publish guidelines for app developers covering namespace best practices, validation logic, and error handling for custom events.
   - Maintain example custom events (e.g., third-party webhook triggers, app-specific business logic) to demonstrate common use cases.

This approach enables app-specific flow event extensions while preserving core system compatibility, UI consistency, and maintainable integration patterns.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision 
## Decision
To enable secure direct communication between app clients (storefront/headless apps) and the App Server while verifying request origin and context, we will implement a client-side request signing mechanism tied to Shopware's existing authentication and app configuration.

This mechanism enforces the following requirements for all direct app-to-App Server requests:
1. **Client-Side Request Signing**
   - App clients must include these mandatory headers in all private/context-sensitive requests:
     - `X-Shopware-App-Signature`: HMAC-SHA256 signature generated from the request method, path, payload, UTC timestamp, and app-specific client secret.
     - `X-Shopware-Request-Timestamp`: UTC timestamp (UNIX epoch) of request creation to prevent replay attacks.
     - `X-Shopware-Sales-Channel-Id`: The sales channel ID associated with the Shopware instance that installed the app.
     - `X-Shopware-Customer-Token` (optional, for authenticated customer requests): The valid Store API token of the logged-in customer.

2. **App Server Verification Workflow**
   - **Replay Attack Prevention**: Reject requests where the timestamp is outside a 5-minute validity window.
   - **Signature Validation**:
     1. Fetch the app's client secret from Shopware's backend using the `X-Shopware-Sales-Channel-Id` and app identifier.
     2. Recalculate the HMAC-SHA256 signature using the same inputs (method, path, payload, timestamp) and fetched secret.
     3. Reject the request if the recalculated signature does not match the `X-Shopware-App-Signature` header.
   - **Customer Context Validation** (if token is present):
     1. Call Shopware's Store API to validate the `X-Shopware-Customer-Token` and retrieve the associated `SalesChannelContext`.
     2. Reject the request if the token is invalid, expired, or does not belong to the specified sales channel.

3. **Secret Management**
   - On app installation in Shopware, generate a unique client-specific secret (separate from backend-to-App Server secrets) and store it encrypted in the database.
   - Share this secret securely with the app client during installation.
   - Allow merchants to rotate the client secret via the Shopware Backend to mitigate security risks.

4. **Error Responses**
   - Return standardized HTTP status codes for verification failures:
     - `401 Unauthorized`: Invalid signature, expired timestamp, or missing mandatory headers.
     - `403 Forbidden`: Invalid customer token, or customer has no access to the requested resource.

### Example Request Headers
```http
POST /app-server/customer/orders
Host: my-app-server.com
Content-Type: application/json
X-Shopware-App-Signature: 7f9c28d5b85e4f3a9d6b7c1e2d0f3a5b8c7d9e0f
X-Shopware-Request-Timestamp: 1718000000
X-Shopware-Sales-Channel-Id: 01234567-89ab-cdef-0123-456789abcdef
X-Shopware-Customer-Token: eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9...

{
  ""limit"": 10,
  ""page"": 1
}
```

### Example Verification Logic (Pseudocode)
```javascript
// App Server signature verification
function verifyRequest(request) {
  const { headers, method, path, body } = request;
  const secret = await fetchAppSecretFromShopware(headers['X-Shopware-Sales-Channel-Id'], APP_ID);
  
  // Validate timestamp
  const now = Date.now() / 1000;
  if (Math.abs(now - headers['X-Shopware-Request-Timestamp']) > 300) { // 5 minutes
    throw new Error('Request expired');
  }

  // Generate base string for signature
  const baseString = `${method}:${path}:${JSON.stringify(body)}:${headers['X-Shopware-Request-Timestamp']}`;
  const calculatedSignature = crypto.createHmac('sha256', secret)
    .update(baseString)
    .digest('hex');

  if (calculatedSignature !== headers['X-Shopware-App-Signature']) {
    throw new Error('Invalid signature');
  }

  // Validate customer token if present
  if (headers['X-Shopware-Customer-Token']) {
    const customerContext = await validateCustomerToken(headers['X-Shopware-Customer-Token'], headers['X-Shopware-Sales-Channel-Id']);
    if (!customerContext) throw new Error('Invalid customer token');
    request.customerContext = customerContext;
  }

  return true;
}
```","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision 
## Decision

We will implement a **Flow Builder Preview** feature to provide merchants with transparent, side-effect-free visibility into how their custom flows execute, directly addressing the pain point of debugging flow behavior without risking impact on live systems or data.

The core design and implementation rules for the feature are:

1. **Non-Executable Path Visualization**:
   - The preview will compute and display flow execution paths using logical rule evaluation, but will never run real business logic (e.g., sending emails, updating database entities). This ensures no side effects or disruption to live shop operations.
   - It leverages the Flow Builder’s existing node metadata and decision logic in a read-only mode to determine branching outcomes and action triggers, without invoking any active handler code.

2. **Sample Data-Driven Path Demonstration**:
   - Users will be able to either select predefined sample trigger data (tailored to common scenarios for each trigger type, e.g., ""New Customer Registered"" with a sample customer profile) or input custom hypothetical values to test edge cases.
   - For decision nodes, the preview will explicitly show how sample data maps to evaluation criteria (e.g., ""Sample order total: $150 → satisfies 'Order Total > $100' condition → proceed to Discount Action"") to eliminate ambiguity about branching logic.

3. **UI-Integrated Transparency**:
   - The preview will be accessible via a dedicated ""Preview"" button within the Flow Builder edit interface, keeping it tightly aligned with the flow creation/editing workflow.
   - It will offer two complementary view modes:
     - **Step-by-Step Walkthrough**: Guides users through each node in the execution sequence, highlighting active nodes and explaining their role in the flow.
     - **Full Path Overview**: Shows all possible execution paths in the flow, with the path matching the selected sample data highlighted for quick reference.

4. **Clear Decision Logic Context**:
   - Every decision node in the preview will display its exact evaluation criteria (e.g., ""Stock Level ≥ 5"") alongside the sample data’s value, so merchants can immediately understand why a specific path was chosen.

### Consequences
- **Positive**: Merchants reduce time spent debugging flow issues by validating logic before activating flows, shifting reliability checks earlier in the creation process. The feature maintains separation between flow design and execution, avoiding unintended system impacts.
- **Technical**: Requires extending the Flow Builder’s rule engine to support a dry-run mode, building a sample data generator/input interface for all trigger types, and integrating path visualization logic into the existing UI.
- **Maintenance**: Predefined sample data must be updated alongside new trigger or node types to ensure the preview remains relevant and accurate as the Flow Builder evolves.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃÂÃÂ¡ÃÂÃÂ°realÃÂÃÂ¡ÃÂÃÂ± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision 
## Decision Outcome

Chosen option: ""Delegate URL generation to lazy-loaded methods on `MediaEntity`/`MediaThumbnailEntity`, replacing the `media.loaded` event subscriber"", because:

### Advantages
1. **Performance Optimization**: URL generation only runs when explicitly requested (e.g., when accessing a `getAbsoluteUrl()` method), eliminating unnecessary processing for media entities loaded in contexts where URLs are irrelevant (like backend metadata-only operations).
2. **Looser Coupling**: Removes tight dependency on the `media.loaded` event lifecycle. This makes extending or overriding URL generation logic simpler, as developers don’t need to modify core event subscribers to add custom behavior.
3. **Clearer, Intentional API**: Exposes URL retrieval directly on entity classes, making the codebase more intuitive and self-documenting compared to implicit URL population via events.
4. **Preserves Existing Flexibility**: Maintains full compatibility with all implemented `UrlGeneratorInterface` and `PathnameStrategyInterface` implementations. Existing path configuration rules and URL generation logic remain unchanged.

### Implementation Details
- Add lazy-loaded methods to `MediaEntity`: `getAbsoluteUrl()`, `getRelativeUrl()` that delegate to the `UrlGeneratorInterface`.
- Add corresponding methods to `MediaThumbnailEntity` for thumbnail URL retrieval.
- Inject the `UrlGeneratorInterface` into entities using Symfony’s property injection or a factory pattern to avoid tight coupling between domain entities and services.
- Remove the existing `media.loaded` event subscriber that pre-generates URLs for all loaded media entities.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision 
## Decision

We will implement a dedicated, versioned one-time data migration system separate from indexers and schema migrations to safely execute one-time data transformation tasks during Shopware version upgrades. Key details of this system:
1. **Dedicated Migration Type**: Introduce a new class of ""Data Migration"" with a standardized base class and naming convention (e.g., `VersionXYZDataMigration` corresponding to the target Shopware version), distinct from database schema migrations and recurring indexer logic.
2. **Tracking & Idempotency**: Store the execution state of each data migration in the database (similar to schema migrations) to ensure each migration runs exactly once per environment, even if the update process is retried.
3. **Integration with Update Workflow**: Automatically trigger pending data migrations during the existing Shopware update pipeline (both CLI and web updater flows), after schema migrations complete but before re-enabling normal system operation.
4. **Separation of Concerns**: Enforce that indexers only handle recurring data sync/indexing tasks (not one-time transformations) and deprecate the practice of embedding one-time migration logic in indexers.
5. **Error Handling & Rollback**: Add safeguards to halt the update process if a data migration fails, with clear logging to facilitate debugging, and allow manual recovery or rerun of failed migrations via CLI commands.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision 
## Decision

### 1. Extend the `@experimental` Annotation with Mandatory Feature Grouping
Update the existing `@experimental` annotation to require a `featureId` parameter, which acts as a unique, human-readable identifier for the experimental feature (e.g., `featureId=""advanced-inventory-forecasting""`). This links every piece of experimental code to its parent feature, eliminating ambiguity about feature scope. The annotation structure will be:
```php
/**
 * @experimental stableVersion=""6.7.0"" featureId=""advanced-inventory-forecasting""
 * Description of the experimental functionality.
 */
```
- Backward Compatibility: For existing experimental code without a `featureId`, we will run a migration phase where all such code must be assigned a valid feature ID within the next minor release cycle. Static analysis tooling will enforce this requirement thereafter.

### 2. Introduce a Central Experimental Feature Registry
Create a version-controlled, machine-readable registry file (e.g., `experimental-features.yaml` in the repository root) to track metadata for each experimental feature. Each entry will include:
- `featureId`: Unique identifier (matches the annotation parameter)
- `name`: Human-readable feature name
- `state`: Current state (`active`, `prolonged`, `killed`)
- `targetStableVersion`: Planned major/minor version for stabilization (or `none` for killed features)
- `deprecationVersion`: Version where the feature will be marked as deprecated (for killed features)
- `owner`: Team/individual responsible for the feature
- `description`: Brief overview of the feature's purpose

Example entry:
```yaml
advanced-inventory-forecasting:
  name: Advanced Inventory Forecasting
  state: prolonged
  targetStableVersion: 6.7.0
  deprecationVersion: null
  owner: Inventory Team
  description: Adds AI-powered inventory demand forecasting capabilities
```

### 3. Implement Tooling for Feature Lifecycle Management
Build or integrate tooling to automate and enforce feature lifecycle tasks:
- **Code Scanning CLI Command**: A command (e.g., `bin/console experimental:list-feature-code <featureId>`) that uses static analysis to locate all classes, methods, properties, and config entries tagged with a specific `featureId`. This solves the problem of scattered code detection.
- **Static Analysis Enforcement**: Add PHPStan/ESLint rules to:
  - Reject `@experimental` annotations missing a valid `featureId` from the registry.
  - Flag discrepancies between an annotation's `stableVersion` and the registry's `targetStableVersion` for prolonged features.
- **Deprecation Automation**: For features marked as `killed` in the registry, a tool (e.g., `bin/console experimental:deprecate-feature <featureId>`) will:
  - Replace all `@experimental` annotations for the feature with `@deprecated` annotations, including the deprecation version from the registry.
  - Generate a report of all modified files for review, ensuring compliance with the ""Killing Feature"" rule (retain code until the next major version).
- **Registry Validation Command**: A command to validate that all feature IDs in the registry have corresponding experimental code, and vice versa, to catch orphaned entries or untagged code.

### 4. Standardize Lifecycle Workflows
- **Prolonging an Experiment**: When extending an experiment, update the `targetStableVersion` in the registry first, then use the code scanning tool to locate and update all relevant `@experimental` annotations (or use automation to propagate the change).
- **Killing a Feature**: Mark the feature as `killed` in the registry and set its `deprecationVersion` to the current minor version. Use the deprecation automation tool to tag all related code as deprecated. Track these features until the next major version, where they will be fully removed.

This approach addresses all core problems:
1. Eliminates scattered experimental code ambiguity via `featureId` grouping.
2. Simplifies updating `stableVersion` for prolonged experiments using centralized registry and scanning tooling.
3. Enables accurate detection and deprecation of killed features, ensuring compliance with existing experimental feature rules.
4. Provides clear visibility into all experimental features and their associated codebase components.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision 
## Decision
We will implement support for stable, instance-unique technical identifiers for payment and shipping methods across the Sales Channel API, Store API, and Admin API to eliminate the need for app servers to call the Admin API for identifier lookup. The specific changes are:

1. **Mandate Technical Identifiers**: Core payment/shipping methods will be assigned fixed, non-editable technical identifiers. Extension developers must specify a unique technical identifier when registering custom payment/shipping methods, with platform-level enforcement to ensure no duplicates exist per instance.
2. **Dual Identifier Support**: All API endpoints that accept payment/shipping method IDs will be updated to accept both numeric database IDs and technical identifiers as valid input values.
3. **Documentation and Guidance**: Official documentation will prioritize technical identifiers as the recommended reference method for extension developers and app servers, explicitly highlighting that pre-lookup calls to the Admin API are no longer necessary.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision 
## Decision

1. **Prefer Bootstrap Utility Classes for Basic Styling**: For simple styling needs (spacing, borders, typography alignment, color utilities, etc.), replace custom SCSS rules with corresponding Bootstrap utility classes applied directly in the template. This reduces redundant custom CSS and leverages maintained, consistent utility styles.
   - Example replacement:
     ```diff
     <!-- Before: Custom SCSS .register-login-collapse-toogle { margin-bottom: $spacer; } -->
     - <div class=""register-login-collapse-toogle""></div>
     <!-- After: Use Bootstrap spacing utility + retain semantic class -->
     + <div class=""register-login-collapse-toogle mb-3""></div>
     ```

2. **Mandate Semantic Class Retention**: Always keep meaningful, component-specific class names on HTML elements even when using utility classes. These classes provide stable, clear targets for theme developers to customize styles without relying on fragile combinations of utility classes.

3. **Reserve Custom SCSS for Complex Scenarios**: Only implement custom SCSS in the following cases:
   - Complex layouts that cannot be achieved cleanly with Bootstrap utilities or default components.
   - Component-specific state styles (e.g., conditional hover, active states) that require logic not covered by utilities.
   - Cases where using multiple utility classes would result in unreadable, hard-to-maintain template markup.

4. **New Component Best Practices**: When building new UI components:
   - Include semantic wrapper and element classes (e.g., `shipping-modal-actions`, `shipping-abort`) even if no default SCSS is applied initially. This ensures extensibility for future theme customization.
   - Reuse Bootstrap core variables (e.g., `$spacer`, `$border-radius`, `$primary`) in any custom SCSS to maintain alignment with the Storefront's design system.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision 
## Decision

1. **Adopt Behavior-Driven Development (BDD) with Cucumber Integration**
   - Integrate Cucumber.js with the existing Cypress framework to enable test authoring in Gherkin syntax. This aligns test cases directly with product requirements (using Given/When/Then scenarios derived from user stories) and creates human-readable tests that non-technical stakeholders can comprehend, fulfilling the need for behavior-validating, accessible tests.

2. **Implement Environment-Agnostic, Isolated State Management**
   - Replace environment-dependent test setup with a deterministic state strategy:
     - Use API-first test data provisioning (instead of UI interactions) to create and tear down isolated test states before/after each run, eliminating reliance on pre-existing environment state.
     - Configure all environment-specific parameters (base URLs, tokens, service endpoints) via environment variables, enabling seamless execution against local, staging, or cloud environments without code changes.
   - This ensures tests are portable and consistent across any target environment.

3. **Define a Layered Test Strategy for Speed and Reliability**
   - Restructure the test suite to follow a prioritized testing pyramid:
     - **Smoke Tests**: A small subset of critical end-to-end workflows (e.g., core transactions, user onboarding) for fast sanity checks on every commit.
     - **Behavioral E2E Tests**: Full BDD scenarios covering core product journeys, focused on validating user behavior rather than implementation details.
     - **Integration Tests**: Component-level tests (using Cypress Component Testing) to validate module interactions, reducing reliance on slow end-to-end tests for non-critical logic.
   - Schedule full regression tests for release validation or periodic cadence to catch regressions while keeping quick feedback loops for day-to-day changes.

4. **Mitigate Flakiness and Optimize Performance**
   - Enforce stable selector practices: Use semantic data attributes (e.g., `data-testid=""checkout-button""`) instead of fragile CSS/XPath selectors tied to UI implementation.
   - Replace hard-coded delays with Cypress’s built-in waits for element visibility, state changes, or API responses to ensure tests wait only as long as necessary.
   - Enable test parallelization in CI/CD pipelines and flaky test retries (with a strict limit) to reduce execution time and handle transient failures without false positives.

5. **Establish Governance and Team Enablement**
   - Create a standardized test style guide and documentation covering Gherkin writing, data management, and framework usage to ensure consistent test creation.
   - Conduct cross-functional training for technical and non-technical teams on BDD principles and test suite navigation to foster shared ownership of validation.
   - Implement peer reviews for all new test cases to maintain quality and alignment with product requirements.

This set of decisions addresses all stated goals: deterministic, environment-agnostic tests; a structured, reliable test strategy; behavior-focused tests tied to product needs; and an accessible, easy-to-learn framework with readable syntax.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision 
## Decision

1. **Centralize and Standardize Transaction Management for Flow Actions**
   - Flow actions are prohibited from manually initiating, committing, or rolling back database transactions. All transactional operations within flow actions must use the transaction context provided by the flow dispatcher.
   - The flow dispatcher will manage transaction scopes with nested execution awareness:
     - If the flow is triggered within an existing outer transaction, the dispatcher will enable savepoints for the database connection (overriding the default Shopware configuration temporarily) to isolate flow action operations. On failure, only the savepoint is rolled back, leaving the outer transaction connection state intact.
     - If no outer transaction exists, the dispatcher will start a dedicated transaction for the full flow execution. It will commit the transaction only if all flow actions succeed, and immediately rollback on any failure.

2. **Eliminate Error Suppression for Transaction Failures**
   - Remove the logic in the flow dispatcher that suppresses transaction commit/rollback errors. All transaction-related exceptions will be propagated up to the calling code instead of being ignored.
   - Enhance error logging to include granular context: the specific flow action that failed, the transaction type (inner/outer), connection state (e.g., rollback-only), and stack traces to enable precise debugging of failure root causes.

3. **Enforce Compliance with Transaction Rules**
   - Add runtime guards that detect flow actions attempting manual transaction management. During development, this will throw a clear, actionable exception (e.g., `ManualTransactionNotAllowedException`) with guidance to use the dispatcher-provided transaction context.
   - Include a linter rule in the development toolchain to flag non-compliant code before deployment, preventing manual transaction handling in new flow actions.

4. **Legacy Migration and Fallback**
   - Provide a migration guide for existing flow actions, including code examples to replace manual transaction calls with the dispatcher's transaction context.
   - Introduce an explicit opt-in flag for legacy flow actions that require manual transaction management. This flag will trigger a prominent warning log, acknowledging the risk of outer transaction failures, and recommend enabling savepoints for the database connection to mitigate conflicts.

This decision resolves the core issues by:
- Preventing unexpected outer transaction failures caused by unhandled nested transaction errors.
- Ensuring callers are aware of transaction issues in flow actions and can take corrective action.
- Standardizing transaction handling to eliminate inconsistent manual management across flow actions.
- Reducing the cognitive load on flow action developers by abstracting away transaction complexity.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision 
## Decision

We will retain Vue 3's compatibility mode enabled in the Administration for the current major release. This decision prioritizes reducing migration friction for plugin developers and splits the broader Vue 3 migration effort into two manageable major releases, rather than consolidating all breaking changes into a single update.

### Key Rationale
1. **Plugin Migration Ease**: Keeping compatibility mode active ensures existing Vue 2-based plugin code continues to function without immediate full rewrites. This gives plugin developers sufficient time to adapt their codebases to pure Vue 3 standards at their own pace, avoiding urgent, high-pressure migration deadlines alongside the core Administration update.
2. **Phased Migration Work**: Splitting the migration into two majors allows the core team to first stabilize the Administration under Vue 3 with compatibility mode, then focus fully on deprecating compatibility mode and refactoring remaining Vue 2-style code in the next major release. This reduces the risk of overwhelming both core and plugin teams with excessive breaking changes in one iteration.

### Tradeoffs
- **Temporary Technical Overhead**: Compatibility mode introduces minor performance costs and retains Vue 2-specific code paths in the core Administration. We will mitigate this by documenting deprecated Vue 2 patterns internally and planning incremental refactors where feasible during the current major.
- **Delayed Full Vue 3 Adoption**: We will not leverage all of Vue 3’s performance and feature optimizations (such as unconstrained Composition API use) until compatibility mode is disabled in the next major release.

### Future Plan
In the subsequent major release, we will:
1. Disable Vue 3's compatibility mode entirely.
2. Deprecate all remaining Vue 2-style APIs and patterns in the Administration.
3. Publish detailed migration guides for plugin developers to transition their code to pure Vue 3 standards before the compatibility mode disablement takes effect.
4. Refactor any core Administration code relying on compatibility mode to use native Vue 3 features exclusively.

This approach balances short-term stability and migration ease with long-term technical excellence and full adoption of Vue 3's capabilities.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision 
## Decision

We will use `patch-package` to apply a custom patch to Vuex 4.0.2, addressing the module failure bugs that initially prevented us from downgrading from 4.1.0. This allows us to avoid the getter reactivity bug present in Vuex 4.1.0 while ensuring module functionality remains stable.

### Rationale
- Vuex 4.1.0 breaks getter reactivity under specific conditions, which would lead to incorrect state handling and unpredictable behavior in the administration app.
- Downgrading to 4.0.2 was blocked by critical module failure bugs, but a targeted, minimal patch resolves these issues without compromising the stable getter reactivity of this version.
- Using `patch-package` lets us implement the fix locally without waiting for an official Vuex release, minimizing disruption to ongoing development and production stability.

### Next Steps
1. Create a targeted patch for Vuex 4.0.2 that fixes the module failure bugs, stored in the project’s `patches` directory.
2. Add `patch-package` as a dev dependency and configure a `postinstall` script in `package.json` to apply the patch automatically for all developers.
3. Add regression tests for both getter reactivity and module initialization flows to catch similar issues during future Vuex version upgrades.
4. Monitor the official Vuex repository for an official release (targeting 4.1.1 or higher) that resolves both the getter reactivity bug and module stability. Once validated, upgrade to this official version and remove the custom patch.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision
## Decision

Adopt an event-driven hook architecture as the primary mechanism for third-party extensibility, and restrict the use of decoration, Adapter, and Factory patterns to targeted, high-value use cases. This change resolves the compatibility, process extension, and code proliferation issues in our current architecture.

### Key Implementation Rules
1. **Centralized Typed Event System**: Build a core event system with strongly-typed Data Transfer Objects (DTOs) representing key process lifecycle stages (e.g., `BeforeResourcePersistEvent`, `AfterActionCompletedEvent`). Each event DTO will encapsulate relevant context (e.g., resource data, execution state) for the process stage.
2. **Event-Driven Process Extension**: Inject event emission at logical checkpoints within all core processes (pre-execution, post-execution, error handling) instead of extracting subprocesses into separate classes for extensibility. Third-party developers will subscribe to these events to inject custom logic.
3. **Targeted Pattern Retention**:
   - **Adapter Patterns**: Retain only for abstracting external service integrations (e.g., database drivers, external APIs) where they decouple core logic from vendor-specific implementations.
   - **Factory Patterns**: Retain only for complex object creation that varies by environment or use case (not as a default extension mechanism).
   - **Decoration Patterns**: Phase out use for extensibility; replace with event hooks where possible. Retain only for rare cases where runtime modification of core behavior (not just extension) is strictly required.
4. **Compatibility Guardrails**:
   - All event DTOs will use optional properties or default values to preserve backward compatibility when new context is added.
   - Implement a deprecation policy for events: mark outdated events with warnings, provide migration paths to replacement events, and remove deprecated events only after a minimum support window.

### Rationale
1. **Backward and Forward Compatibility**:
   - Event listeners are decoupled from core process implementations. Subscribers depend only on event types and their data, not on rigid interfaces or class hierarchies. Adding new context to events or introducing new hooks does not break existing extensions.
   - Gradual event deprecation allows third-party developers to adapt to changes without immediate breakage, reducing the labor of maintaining compatibility.

2. **Process Extension Flexibility**:
   - Event hooks eliminate the need to pre-extract every extensible subprocess into a separate class and interface. Developers can extend any process that emits events—even granular subprocesses—without requiring core code changes to extract functionality.
   - Standardized lifecycle hooks create predictable, documented extension points that third-party developers can rely on, resolving ambiguity about where to inject custom logic.

3. **Reduction in Code Proliferation**:
   - Removing unnecessary decoration, Adapter, and Factory implementations cuts down on redundant interfaces, abstract classes, and wrapper classes in the codebase.
   - The centralized event system consolidates extension points, making the architecture easier to navigate for both core and third-party developers, who no longer need to traverse a complex web of pattern implementations to find extensibility opportunities.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision 
## Decision
We will create a separate Jest configuration with Vue compat mode disabled, and set up a dedicated Jest runner targeting tests for components where we want to eliminate compat mode. 

To support incremental migration without disrupting existing test workflows:
1. We will adopt a clear naming convention (e.g., `<Component>.spec.no-compat.js`) for tests intended to run without compat mode.
2. Update our Jest setup to route tests matching this naming pattern to the new non-compat configuration, while existing tests continue to use the compat-mode-enabled runner.

This approach allows us to validate each component's functionality without compat mode individually, before fully deprecating the compat runner in the long term.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision 
## Decision

To proactively detect component and functional failures before or immediately after system rollout, and enable real-time monitoring of system health, we implement the following structured health check strategy:

1. **Tiered Health Check Framework**
   - **Component-Level Checks**: Validate the health of individual system components (e.g., database connectivity, cache availability, message broker uptime, service dependency reachability). Each component will expose a dedicated health check that returns its status (healthy/unhealthy/degraded) with context on failures (e.g., connection timeout, authentication error).
   - **Integration-Level Checks**: Validate cross-component workflows (e.g., API service successfully writing to the database, message broker successfully routing events between services). These checks ensure that even if individual components are healthy, their integration does not introduce failures.
   - **End-to-End (E2E) Functional Checks**: Validate critical user-facing and core business workflows (e.g., user login, transaction processing, data export). These checks mirror real user interactions and will catch functional gaps that component/integration checks might miss—addressing the post-rollout undetected failures highlighted in the context.

2. **Post-Rollout Validation Mandate**
   - Automatically execute a subset of high-priority E2E functional and integration health checks immediately after every system deployment. This suite will be configured to block deployment finalization or trigger an automatic rollback (for critical services) if any check fails. For non-critical services, it will flag a deployment warning and require manual engineering sign-off before proceeding.

3. **Standardized Health Check Exposure & Monitoring Integration**
   - Expose a centralized health check endpoint (aligned with RFC 9470 for HTTP-based services, or equivalent protocols for non-HTTP components) that aggregates results from all tiered checks. The endpoint will return:
     - An overall system status (healthy/unhealthy/degraded)
     - Detailed breakdown of failed/degraded components/workflows
     - Timestamp of last check execution
   - Integrate this endpoint with the system’s monitoring and alerting pipeline (e.g., Prometheus + Grafana, PagerDuty). Configure alerts for:
     - Unhealthy system status (critical alerts for immediate remediation)
     - Degraded component status (warning alerts for proactive investigation)
     - Failed post-rollout validation checks (deployment-related critical alerts)

4. **Degraded State Definition & Handling**
   - Define clear rules for classifying system status:
     - **Healthy**: All component, integration, and critical E2E checks pass.
     - **Degraded**: Non-critical components are unhealthy, but core business workflows remain functional. The system is usable but requires investigation.
     - **Unhealthy**: Any critical component fails, or core E2E functional checks fail. The system cannot deliver its primary service.
   - For degraded states, trigger non-urgent alerts to the engineering team. For unhealthy states, trigger on-call alerts and include automated remediation steps where possible (e.g., restarting a failed component).

5. **Continuous Health Check Improvement**
   - Maintain a log of past post-rollout failures and update the health check suite to include corresponding E2E or integration checks for each failure scenario. This ensures recurring issues are caught early in future deployments or during ongoing monitoring.
   - Conduct quarterly reviews of the health check suite to remove obsolete checks, add new ones for recently added features, and adjust alert thresholds based on system usage patterns.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision 
## Decision

We will retain the existing restriction in `TestPreparationStartedSubscriber` that limits the enabling of major feature flags during unit test execution exclusively to the `Shopware\Tests\Unit` namespace.

### Rationale
1. **Unit Test Isolation**: Unit tests are designed to validate isolated component behavior. Allowing major feature flag toggling here enables developers to test component functionality with and without the feature enabled, without interference from integration-level dependencies managed in other test suites.
2. **Avoid Flaky Higher-Level Tests**: Integration and functional tests rely on controlled environment configurations to replicate real-world scenarios. Permitting arbitrary feature flag enabling in these suites could lead to inconsistent test states, flaky results, and violations of test isolation principles.
3. **Enforce Clear Test Boundaries**: This restriction upholds a clear separation of concerns: unit tests handle component-level feature validation, while higher-level tests use suite-specific mechanisms (e.g., environment variables, test setup fixtures) to manage feature flags for integrated context testing.

Should valid use cases emerge in the future requiring major feature flag support in other test namespaces, a follow-up Architecture Decision Record will be drafted to evaluate the need for adjusting this restriction, including implementing additional safeguards to maintain test reliability.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision 
## Decision

We will adopt an automated-first migration workflow to convert existing Vue 2 Options API components to the Vue 3 Composition API, using the official `vue-codemod` tool suite as the foundation, supplemented with custom internal migration scripts tailored to our codebase’s specific patterns and utilities.

### Key Details of the Approach
1. **Base Automation with Official Tooling**:
   We will leverage pre-built `vue-codemod` migration scripts to handle standard, idiomatic Options API patterns out of the box. This includes converting:
   - `data`, `computed`, `methods`, and `watch` options to their Composition API equivalents using the `setup()` function (with a path to adopt `<script setup>` in subsequent phases).
   - Vue 2 lifecycle hooks (e.g., `created` → aligned Composition API hooks like `onBeforeMount` or `onMounted` where semantically appropriate) to Vue 3’s lifecycle functions.
   - Prop and event handling logic to comply with Vue 3’s Composition API syntax and type expectations.

2. **Custom Scripts for Shopware-Specific Patterns**:
   To address our codebase’s unique utilities (e.g., Mixin Factory, Component Factory, twig-integrated Vue templates), we will develop custom codemod scripts that:
   - Automatically adjust references to internal helper functions to work with Composition API reactivity patterns.
   - Convert shopware-specific mixin usages to composable functions where feasible, ensuring compatibility with our existing plugin ecosystem.
   - Mask twig syntax in templates during migration to avoid breaking the linter/transformer workflow (using our existing twig-to-comment conversion logic as a foundation).

3. **Phased Migration Workflow**:
   - **Bulk Automated Pass**: Run combined official and custom scripts to convert large batches of components, focusing on low-complexity, high-volume components first.
   - **Mandatory Manual Review**: Teams will validate migrated components for functionality, fixing edge cases automation cannot resolve (e.g., complex reactive logic with side effects, non-standard state management integrations).
   - **Prioritization**: Legacy components will be migrated based on business criticality and maintenance frequency (e.g., high-traffic admin sections, frequently updated plugins are prioritized to maximize early benefits).

4. **New Component Standard**:
   All new components developed after this decision will use the Vue 3 Composition API exclusively (prioritizing `<script setup>` for brevity and type safety).

### Rationale
- **Efficiency**: Automation handles 80-90% of routine conversion tasks, reducing manual effort and time-to-completion compared to full manual migration.
- **Consistency**: Automated tooling ensures uniform application of Composition API patterns across the codebase, minimizing inconsistencies from manual implementation.
- **Risk Mitigation**: Combining vetted official tooling with custom scripts tailored to our codebase reduces the likelihood of breaking existing functionality during migration.
- **Future-Proofing**: Aligning with modern Vue best practices simplifies future transitions (e.g., to Vue 3’s latest syntax, improved TypeScript support) and enhances long-term code maintainability.

### Exceptions
Components with highly non-standard reactive logic, deep legacy plugin integrations, or complex state management patterns may require full manual conversion if automation fails to produce a working result. These cases will be documented and handled on a case-by-case basis by technical leads.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃÂÃÂ¡ÃÂÃÂ¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃÂÃÂ¡ÃÂÃÂ¯s templating and reactivity model.","## Decision 
## Decision
We replace the existing TwigJs-based block inheritance system with a native Vue.js implementation using two new components: `sw-block-parent` and `sw-block`. This shift aligns the block system with Vue’s reactive, component-first architecture, while preserving and enhancing the extensibility plugin developers rely on.

### Core Functionality of the New Components
- **`sw-block-parent`**: Defines a named, extendable block within a Vue component’s template. It acts as a container that renders its default content unless overridden or extended by a `sw-block` component targeting its name.
- **`sw-block`**: Targets a specific `sw-block-parent` by name, enabling two key operations:
  - **Override Mode**: Replaces the default content of the targeted block entirely by omitting an `extend` prop.
  - **Extend Mode**: Prepends or appends custom content to the default block content when the `extend` prop is set to `prepend` or `append`.

### Key Benefits Over TwigJs
1. **Reactivity Integration**: Content within blocks leverages Vue’s reactivity system, ensuring dynamic updates when underlying data changes—something TwigJs (a template-only, non-reactive system) cannot natively support.
2. **Component Lifecycle Compatibility**: Blocks now participate in Vue’s component lifecycle, enabling hooks like `mounted` or `updated` for content within blocks, which was not possible with TwigJs.
3. **Simplified Developer Experience**: Plugin developers work entirely within Vue’s templating syntax instead of mixing TwigJs and Vue, reducing context switching and potential syntax conflicts.
4. **Dynamic Block Management**: Unlike static TwigJs blocks, the Vue implementation allows runtime adjustments to block content (e.g., conditional overrides) using Vue’s data and directive systems.

### Migration & Compatibility
- We will maintain backward compatibility for existing TwigJs-based blocks in the current major version, marking them as deprecated. A detailed migration guide will be provided to help developers transition to the new Vue-based blocks.
- In the next major version, TwigJs block support will be removed to fully align with the native Vue.js architecture and eliminate technical debt from mixed template systems.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃÂÃÂ¡ÃÂÃÂ¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃÂÃÂ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision 
## Decision

To address the downsides of the universal asterisk suffix on product prices, we will implement the following context-aware, user-centric refactor:

### Chosen Option
Remove the mandatory asterisk from all product price displays in the default Storefront and replace it with a centralized, region-compliant disclaimer system tied directly to price contexts.

### Key Actions
1. **Reusable, snippet-driven disclaimer component**:
   - Create a new template component `Resources/views/storefront/component/disclaimer/price-disclaimer.html.twig` that renders dynamic, configuration-aware text. This component will be placed in context-relevant locations:
     - Product Detail Page: Directly below the price block
     - Category/Search Pages: In a dedicated section above/below the product grid
     - Cart/Checkout Summary: Above the total price calculation
   - Use region-specific snippets (e.g., `disclaimer.prices.eu`, `disclaimer.prices.rest-of-world`) to align with local tax communication rules.

2. **Dynamic text based on tax configuration**:
   - Leverage existing Shopware tax display settings (inclusive/exclusive of VAT) to auto-adjust disclaimer content:
     - For tax-inclusive regions: `All prices incl. applicable VAT. Shipping costs and additional fees may apply.`
     - For tax-exclusive regions: `All prices excl. applicable VAT. Shipping costs and additional fees may apply.`

3. **Accessibility improvements**:
   - Link price elements to their corresponding disclaimer using ARIA attributes (`aria-describedby`), ensuring screen readers can explicitly associate prices with their context, eliminating the ambiguity of the asterisk.

4. **Backward compatibility toggle**:
   - Add a temporary admin setting under `Settings > Storefront > Display` to re-enable the legacy asterisk + footer disclaimer for merchants needing to maintain existing workflows. This setting will be marked as deprecated and removed in the next major Shopware release.

5. **Core template cleanup**:
   - Remove all hardcoded asterisk suffixes from price templates (e.g., `product-price.html.twig`, `line-item-price.html.twig`) and replace them with optional references to the new disclaimer component.

### Rationale
This approach resolves the core issues of the legacy system:
- Eliminates UI clutter and ambiguous semantic meaning for users and assistive technologies
- Ensures compliance with regional tax rules by adapting to configuration and customer location
- Reduces maintenance overhead by centralizing disclaimer logic in a single extensible component
- Provides clear customization paths for third-party developers without modifying core price templates","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision 
## Decision

Replace the current DomPDF library with WeasyPrint as Shopware 6's primary PDF generation library to achieve WCAG-compliant document accessibility, with a structured transition plan to maintain backward compatibility.

### Key Implementation Actions:
1. Adopt WeasyPrint for its native support of PDF/UA (Universal Accessibility) standards, which preserves semantic HTML structure (e.g., headings, lists, image alt text) critical for screen readers and assistive technologies.
2. Maintain a dual-library compatibility layer during transition: allow merchants to configure which library (DomPDF or WeasyPrint) to use for document generation, enabling gradual migration.
3. Develop template migration guides and automated tools to help merchants update custom document templates to follow WCAG-aligned semantic markup practices (e.g., proper heading hierarchy, ARIA labels, high-contrast text).
4. Integrate automated accessibility validation (via tools like the PDF Accessibility Checker or axe-core for source HTML) into the document generation pipeline to flag non-compliant content before PDFs are delivered to users.

This decision ensures Shopware 6's document solution meets legal accessibility requirements, expands usability for customers with disabilities, and aligns with WCAG standards.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision 
## Decision

We will refactor the core rule system to adopt a **declarative, data-driven condition model** that replaces the current class-based rule condition implementation, directly addressing complexity, maintainability, and evolution challenges while preserving flexibility for core and app extensions.

### Key Implementation Changes:
1. **Centralized Condition Registry**:
   - Replace tagged service-based rule condition registration with a `RuleConditionRegistry` service that stores all condition metadata (core, app, legacy) in a structured, queryable format.
   - Core rule conditions will be defined via declarative configuration files (YAML/XML) specifying:
     - Unique condition identifier
     - Human-readable labels/descriptions
     - Parameter schema (aligned with the existing `RuleConfig` API for UI consistency)
     - Supported operator sets
     - Reference to a reusable `RuleConditionEvaluator` service for evaluation logic

2. **Reusable Evaluation Logic**:
   - Extract common evaluation behavior into stateless, shared `RuleConditionEvaluator` services (e.g., `StringMatchEvaluator`, `EntityExistsEvaluator`, `DateComparisonEvaluator`). These services handle core logic for checking condition parameters against the `RuleScope` context.
   - This eliminates code duplication across class-based conditions and enables consistent updates to evaluation behavior across all conditions using the same evaluator.

3. **Backward Compatibility & Migration**:
   - Introduce a legacy adapter to automatically register existing class-based rule conditions in the `RuleConditionRegistry`, ensuring no immediate breakage.
   - Deprecate direct extension of rule condition base classes (e.g., `RuleCondition`), with clear migration guides for core and third-party extensions to adopt the declarative model.

4. **App Extension Alignment**:
   - Maintain parity between core and app condition capabilities:
     - App developers may choose between declarative evaluator-based conditions (for simple use cases) and script-based conditions (for custom logic)
     - Declarative app conditions follow the same schema as core conditions, ensuring consistency in the rule builder UI and evaluation flow

### Rationale:
This change eliminates the complexity of maintaining hundreds of individual rule condition classes, reduces the risk of unrestricted class extensions blocking core improvements, and streamlines future updates to evaluation behavior. It aligns with the direction outlined in RFC #5785, and will be rolled out incrementally to minimize disruption to existing functionality.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision 
## Decision

* Remove the `Iterator` helper class (`src/helper/iterator.helper.js`) completely from the Storefront JavaScript codebase, as it introduces unnecessary abstraction and complexity over standard native iteration methods.
* Replace all instances of `Iterator.iterate()` usage with native JavaScript iteration patterns tailored to the specific data type:
  - For arrays and array-like structures (e.g., NodeList, HTMLCollection): Use native `Array.prototype.forEach()` (utilize `Array.from()` or the spread operator `[...collection]` if targeting legacy environments where NodeList `forEach()` is not natively supported; modern browsers natively support this method).
  - For Map/Set collections: Leverage native `Map.prototype.forEach()` or `for...of` loops to iterate over entries, keys, or values.
  - For FormData objects: Use `for...of` loops to iterate over entries (e.g., `for (const [key, value] of formData.entries())`) or directly access values via native FormData methods.
  - For plain objects: Use `Object.entries()`, `Object.keys()`, or `Object.values()` paired with `forEach()`, or `for...in` loops (with appropriate prototype property checks if required).
* Standardize on native iteration methods across the codebase to eliminate inconsistent usage patterns, reduce developer confusion, and align with widely adopted web standards that require no special internal documentation or training.
* Remove all unnecessary imports of the `Iterator` helper from files where it was previously used, reducing dependency bloat and simplifying module structure.
* Enforce adherence to native iteration patterns in code reviews to maintain long-term consistency, readability, and alignment with standard JavaScript best practices for all contributors.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision 
## Decision

When the `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` feature flag is enabled, flow execution will be decoupled from the synchronous business process and moved to an asynchronous, queue-based execution model to address the identified drawbacks.

### Key Implementation Details:
1. **Asynchronous Dispatch**: Instead of executing flows inline during the business process, flow execution requests will be dispatched to a message queue (using Symfony Messenger) immediately after the triggering business process event completes successfully. This ensures core business actions (like checkout or product updates) finish without waiting for non-critical flow tasks (e.g., email sending, external API calls).
2. **Isolated Execution Environment**: Flows will run in dedicated worker processes, separate from the request/response cycle. This isolates flow execution errors, preventing fatal flow issues from canceling or disrupting the original user-triggered business process.
3. **Dedicated Flow Event Handling**: Replace the current EventDispatcher decorator with a dedicated event listener/subscriber responsible for queuing flow execution tasks. This simplifies debugging by reducing stack trace bloat and makes flow logic more modular and maintainable.
4. **Reliability Controls**: Implement retry mechanisms for transient flow failures and dead-letter queues for persistent errors, with detailed logging to track flow status, successes, and failures. For rare cases requiring synchronous flow execution (e.g., critical inventory adjustments), we will provide an opt-in mechanism to execute specific flows inline, though this remains an exception to the default async model.
5. **Backward Compatibility**: The feature flag preserves the existing synchronous flow execution behavior by default, enabling incremental testing and validation of the asynchronous model before full rollout.

### Advantages:
- **Business Process Resilience**: Flow execution errors no longer impact the completion of core user actions.
- **Performance Improvements**: Reduces latency in critical business processes by offloading resource-intensive flow tasks to asynchronous workers.
- **Simplified Debugging & Maintenance**: Isolated flow execution logic and dedicated event handling make tracing issues and extending flow capabilities easier.
- **Scalability**: Asynchronous queue processing allows independent scaling of flow execution to handle peak traffic loads effectively.
- **Future-Proofing**: The modular execution model simplifies adding new flow capabilities (e.g., conditional branching, complex scheduling) without affecting core business logic.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision 
## Decision Outcome

Chosen option: ""MADR 3.0.0"", because

* Implicit architectural assumptions need to be explicit to ensure long-term understandability of the project's design choices. This aligns with established practices, as referenced in [""A rational design process: How and why to fake it""](https://doi.org/10.1109/TSE.1986.6312940).
* MADR 3.0.0 provides a standardized, lean structure specifically tailored for capturing architectural decisions without unnecessary overhead, fitting iterative and collaborative development workflows.
* The format is widely adopted in the software industry, making it familiar to new team members and easy to maintain over the project's lifecycle.
* MADR supports clear documentation of decision drivers, evaluated alternatives, and direct consequences—critical for justifying architectural choices to stakeholders and future contributors.
* The MADR project remains actively maintained, ensuring access to ongoing improvements, community resources, and template updates.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision 
## Decision

We will implement a modular, user-centric repository utility layer for TUF, built on top of the modern Metadata API, with the following structure and guiding rules:

1. **Core Repository Library**: Develop a lightweight, composable core library that provides reusable primitives for essential TUF repository operations (e.g., signing metadata, tracking version increments, managing target file storage, validating repository state transitions). This library will avoid application-specific assumptions (such as eager loading of all metadata) and expose a minimal, explicit API with no global state dependencies.

2. **Use-Case-Specific Helpers**: Build optional, opinionated helpers on top of the core library to support common user scenarios:
   - A CLI-focused helper that simplifies small-scale repository management (including optional eager metadata loading for convenience).
   - A server-focused helper that optimizes for large deployments (e.g., lazy metadata loading, partial state access).
   
3. **Strict Dependency on Modern Metadata API**: All utility components will exclusively use the modern Metadata API for metadata creation, parsing, and manipulation, ensuring consistency with its design and reusing existing features rather than duplicating logic.

4. **Minimal Monolith Avoidance**: Each component (core library, helpers) will be maintained as a distinct, testable submodule with clear boundaries, preventing the accumulation of the legacy’s 7000+ line monolithic codebases.

## Rationale
This decision directly addresses the context’s pain points and requirements:
- **Fills Metadata API Gaps**: The core library provides the repository-specific tooling missing from the modern Metadata API, enabling users to implement TUF repositories without relying on legacy tools.
- **Eliminates Legacy Shortcomings**: By avoiding global state, reducing large method signatures, and splitting into modular components, we fix the legacy’s poor API design and maintainability issues.
- **Supports Diverse Use Cases**: The separation between unopinionated core primitives and opinionated helpers lets users choose tools tailored to their needs (small CLI repos vs large-scale servers) without being forced into one-size-fits-all decisions like eager metadata loading.
- **Ensures Long-Term Maintainability**: Modular components are easier to test, update, and debug, and strict reliance on the modern Metadata API ensures we benefit from its ongoing improvements.
- **Preserves Flexibility**: The core library’s primitives allow advanced users to build custom repository implementations, while helpers provide a quick, accessible path for common use cases.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision 
## Decision

* Migrate the Primer React component library to TypeScript, embedding type definitions directly in component implementation files to eliminate the disconnect between code and types.
* Implement an incremental migration approach to minimize disruption:
  - Convert components and utilities one at a time, prioritizing those with the most frequent TypeScript bug reports or highest user adoption.
  - Use TypeScript’s `allowJs` and `checkJs` options to enable type checking of existing JavaScript files during the transition, helping surface and resolve type inconsistencies early.
* Deprecate and eventually remove the standalone `index.d.ts` ambient declaration file once all core components and utilities are fully migrated to TypeScript.
* Configure the TypeScript compiler to emit auto-generated `.d.ts` files alongside compiled JavaScript outputs to maintain backward compatibility for both TypeScript and JavaScript consumers.
* Add TypeScript type validation to the CI/CD pipeline to enforce consistency between component implementations and their types, preventing regressions in type accuracy and reducing future manual maintenance overhead.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision 
## Decision

We will standardize on using the **Box component pattern as the primary approach for building new React components** in Primer React and consuming applications, while retaining the styled-components pattern as an exception for specific use cases where Box’s capabilities are insufficient. This balances consistency, maintainability, developer experience, and flexibility.

### Key Rationale
1. **Alignment with Core Primitives**:  
   Box is a foundational primitive in Primer React that encapsulates standardized theming, responsive styling, and the `sx` prop—all critical to maintaining design system consistency. Using Box ensures new components inherit these out-of-the-box capabilities without reimplementing common styling logic.

2. **Improved Readability & Approachability**:  
   The function-based Box pattern uses explicit prop handling and familiar React syntax, making it more accessible to developers who may not be deeply proficient in styled-components. This reduces the barrier to contributing and makes component logic easier to debug and maintain.

3. **Simplified Style Merging**:  
   Explicit merging of component styles with the `sx` prop (via `merge` as shown in the example) ensures predictable style composition, avoiding common pitfalls with styled-components where dynamic style logic can become tangled in template literals.

4. **Flexibility for Edge Cases**:  
   We retain styled-components for components requiring advanced styling that Box cannot accommodate, such as:
   - Complex keyframe animations
   - Fine-grained control over pseudo-elements/classes
   - Dynamic styles that are impractical to express via `sx` (e.g., highly conditional CSS rules)

### Guidelines for Implementation
- **Prefer Box for New Components**: Use the Box pattern for all standard components that can be built using Box’s props, theme integration, and `sx` styling. Define explicit TypeScript interfaces for component props, extending `BoxProps` where applicable to inherit core styling props.
- **Use Styled-Components Sparingly**: Only resort to styled-components when Box’s capabilities are insufficient. Ensure styled components are fully typed, leveraging TypeScript to enforce prop and style type safety.
- **Maintain Consistency in Existing Components**: Existing components built with either pattern will be retained unless refactoring provides clear maintainability or usability benefits, in which case refactor to the Box pattern where feasible.

### Example Standardized Box Component
```tsx
import { Box, BoxProps, merge, BetterSystemStyleObject } from '@primer/react'

interface AvatarProps extends BoxProps {
  size?: number
  alt?: string
  square?: boolean
}

const Avatar: React.FC<AvatarProps> = ({ 
  size = 20, 
  alt = '', 
  square = false, 
  sx = {}, 
  ...props 
}) => {
  const baseStyles: BetterSystemStyleObject = {
    display: 'inline-block',
    overflow: 'hidden',
    lineHeight: 'condensedUltra',
    borderRadius: square ? 0 : size / 2, // Example dynamic style
  }

  return (
    <Box
      as=""img""
      alt={alt}
      width={size}
      height={size}
      sx={merge(baseStyles, sx)} // Predictable style merging
      {...props}
    />
  )
}
```","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision 
## Decision Outcome

Chosen option: Implement formal lifecycle and usage guidelines for the `/drafts` export path in `@primer/react`, because:
- It eliminates ambiguity for maintainers around valid use cases for drafts and how to manage component transitions.
- It sets clear expectations for consumers about the stability, support, and roadmap of draft components, reducing unexpected workflow disruptions.
- It ensures consistent quality and alignment with product/design goals when promoting components from drafts to the production-ready root export path.

The guidelines will include:
1. **Eligibility for `/drafts`**:
   - Parallel major versions of existing root-exported components (to enable gradual, non-breaking migration).
   - Work-in-progress components with unstable APIs, incomplete documentation, or unvalidated production readiness.
2. **Promotion to root export**:
   - Parallel versions: Must pass full testing, receive design/product approval, and be scheduled to replace the root version in a major release.
   - WIP components: Must reach API stability, have complete documentation, pass accessibility/performance tests, and get maintainer sign-off for production use.
3. **Consumer guardrails**:
   - `/drafts` components are excluded from semantic versioning guarantees; breaking changes may occur in non-major releases.
   - Teams using draft components must coordinate with maintainers to provide feedback and align on roadmap timelines.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision 
## Decision

To address the maintainability challenges of scaling snapshot testing, we will adopt the following targeted practices:

1. **Scope snapshots to test specific, intentional behaviors**
   - Replace generic ""renders correctly"" test descriptions with explicit, intent-driven labels (e.g., ""renders disabled button state correctly"" or ""generates valid API response payload for product listing""). This clarifies exactly what the snapshot is verifying when it fails.
   - Use shallow rendering (instead of full deep rendering) for component snapshots to isolate the component under test. Mock child components to avoid capturing their implementation details, reducing snapshot size and limiting diffs to only the component's own output.
   - For complex components/objects, snapshot only critical sub-parts (e.g., a component's CSS class list for a given state, or a subset of an API response payload) instead of full trees, making diffs easier to debug.

2. **Improve snapshot readability and context**
   - Prefer inline snapshots (via `toMatchInlineSnapshot()`) for simple, low-complexity cases. This keeps the expected snapshot value directly in the test code, eliminating the need to cross-reference external `.snap` files and making test intent immediately clear.
   - Add comments to snapshot tests explaining why the snapshot is valuable (e.g., ""Snapshot validates that error state displays the correct error message text and styling"").

3. **Establish maintenance guardrails**
   - Require explicit sign-off for all snapshot updates in pull requests, ensuring changes are intentional and reviewed (not just auto-accepted).
   - Schedule quarterly audits of existing snapshots to prune outdated tests, merge redundant snapshots, and update test descriptions to align with current component behaviors.
   - Pair snapshot tests with targeted unit assertions for critical logic (e.g., explicitly testing that a form submits with the correct data, in addition to snapshotting the form's rendered state). This reduces over-reliance on snapshots as black-box tests and provides clear validation of core functionality.

These practices will minimize the pain of large, unmanageable snapshots, simplify debugging of failed tests, and ensure snapshot tests remain purposeful and maintainable as the codebase scales.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision 
## Decision

1. **Centralized Dev Warning Utility**: We will implement a reusable `warnDev` utility function that only triggers `console.warn()` when running in a development environment. In production builds, this utility will be compiled to a no-op, and tree-shakable bundlers will strip unused instances entirely. The utility will accept a structured message (and optional contextual details) to ensure consistent formatting for all developer-facing warnings.

2. **Mandatory Utility Adoption**: All developer-targeted warnings (including deprecations, unsupported feature usage, invalid prop configurations, and best practice reminders) must use the `warnDev` utility instead of direct `console.warn()` calls. Direct `console.warn()` invocations for these purposes will be prohibited via custom linting rules to enforce consistency across the codebase.

3. **Tree-Shakable Dev-Only Logic**: Warning-related conditional checks (e.g., verifying deprecated prop usage, detecting unsupported component patterns) must be wrapped in statically evaluable environment guards. For example:
   ```typescript
   if (process.env.NODE_ENV === 'development') {
     if (props.deprecatedProp !== undefined) {
       warnDev('`deprecatedProp` is deprecated; use `newProp` instead. See https://primer.style/react/deprecations/123 for details.');
     }
   }
   ```
   Bundlers like Webpack, Rollup, and Vite will eliminate these entire blocks from production builds when `process.env.NODE_ENV` is set to `""production""`.

4. **Build Tooling Configuration**: We will standardize build tooling to statically replace `process.env.NODE_ENV` values during compilation, ensuring reliable dead-code elimination of dev-only warning logic and no-ops in production bundles.

### Consequences
- **Production Bundle Efficiency**: Dev-only warning code and `console.warn()` calls are fully stripped from production builds, reducing bundle size and eliminating unintended runtime noise for end-users.
- **Improved Developer Experience**: Uniform, environment-scoped warnings provide clear, actionable guidance during development without disrupting end-user workflows.
- **Maintainability**: The centralized `warnDev` utility simplifies updates to warning behavior (e.g., adding integration with developer tools, adjusting formatting) without modifying every warning instance across the codebase.
- **Compliance**: Linting rules prevent accidental direct `console.warn()` usage, reducing technical debt and ensuring adherence to the pattern.
- **Clarity for External Developers**: Structured warning messages (including links to documentation) help external consumers of `@primer/react` quickly address deprecations or unsupported usage.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision 
## Decision

Adopt a centralized, accessible live region system for Primer React, consisting of a reusable root-mounted `<LiveRegion>` component, a utility hook (`useLiveRegion`), and enforced author guidelines. This system will standardize ARIA live region usage to ensure consistent, reliable announcements across all screen readers and browsers, while simplifying implementation for component contributors.

### Core Implementation Components

#### 1. Root-Mounted `<LiveRegion>` Component
A single, visually-hidden live region rendered once at the application root (or per isolated component tree if needed) to avoid dynamic injection pitfalls. It will:
- Support three live region modes: `polite`, `assertive`, and `alert`
- Default to `aria-atomic=""true""` for all modes to ensure full message announcements (not just partial changes)
- Use explicit `aria-live` attributes instead of role-based live regions (e.g., `role=""alert""`) for greater control, except for critical alerts where `role=""alert""` is reserved for time-sensitive, interruptive messages
- Maintain persistent presence in the DOM (never toggle visibility with `display: none` or `visibility: hidden`) to prevent screen reader detection failures
- Include visual hiding styles that comply with WCAG (e.g., `position: absolute; clip: rect(0 0 0 0); pointer-events: none;`)

#### 2. `useLiveRegion` Utility Hook
A custom React hook that abstracts interaction with the `<LiveRegion>` component, providing typed methods for different announcement priorities:
```ts
function useLiveRegion(): {
  announcePolite: (message: string) => void;
  announceAssertive: (message: string) => void;
  announceAlert: (message: string) => void;
}
```
- `announcePolite`: For non-urgent updates (e.g., search results count, form save confirmation) using `aria-live=""polite""`
- `announceAssertive`: For important updates requiring user attention (e.g., form submission errors, connection warnings) using `aria-live=""assertive""`
- `announceAlert`: For critical, time-sensitive messages (e.g., session timeout alerts) using `role=""alert""` (inherently `aria-live=""assertive""` and `aria-atomic=""true""`)
- The hook will handle debouncing duplicate messages and managing state updates to the root `<LiveRegion>` component.

#### 3. Author Guidelines
Enforce clear rules for component contributors to ensure consistent usage:
- **Mode Selection**:
  - Use `polite` for non-interruptive updates that don’t require immediate user action
  - Use `assertive` for updates that demand attention but aren’t emergencies
  - Use `alert` only for critical, time-sensitive messages that require urgent action
- **Avoid Dynamic Injection**: Never render live regions dynamically inside components (rely on the root `<LiveRegion>` instead)
- **Content Best Practices**: Keep messages concise, actionable, and free of jargon; avoid redundant phrases (e.g., ""Alert: "" is unnecessary for `announceAlert`)
- **Testing Mandate**: Require testing announcements across major screen readers (NVDA, VoiceOver, JAWS) and browsers to validate consistency

### Rationale
This approach directly addresses the core problems outlined in the context:
- **Consistent Announcements**: Root-mounted regions and standardized attribute usage eliminate issues from dynamic injection or visibility toggling, which are common causes of unannounced content
- **Simplified Authoring**: The hook abstracts low-level ARIA details, reducing errors from incorrect attribute combinations (e.g., missing `aria-atomic=""true""`)
- **Accessibility Compliance**: Aligns with WCAG 2.1 and WAI-ARIA 1.2 guidelines, ensuring compliance for all Primer React components
- **Scalability**: Centralized logic makes it easy to update live region behavior globally (e.g., adding new modes, adjusting debouncing rules) without modifying individual components

### Alternatives Considered
1. **Per-Component Live Regions**: Rejected due to high risk of inconsistent implementation, dynamic injection failures, and redundant DOM elements
2. **Role-Only Live Regions**: Rejected because role-based regions (like `role=""alert""`) are interruptive and lack the granular control of explicit `aria-live` attributes for non-critical messages
3. **No Centralized System**: Rejected as it would continue the current state of inconsistent, unreliable announcements across components","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"## Decision
## Decision

No architecture decision can be defined without additional context about the problem, constraints, existing system state, stakeholder requirements, or tradeoffs under consideration. Please provide details such as:
1. The specific inconsistency, gap, or pain point in the current system that requires resolution
2. Existing approaches, workarounds, or legacy behaviors related to the topic
3. Business or technical goals driving the need for a formal decision
4. Key constraints (e.g., backward compatibility, performance, maintainability, compliance)
5. Potential alternatives or preliminary options being evaluated

Once this contextual information is provided, a targeted, actionable architecture decision can be documented.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision 
## Decision

1. **Eliminate duplicate markup between Storybooks and Jest tests by adopting the Storybook Test Runner ecosystem**:
   - Integrate `@storybook/test-runner`, `@storybook/jest`, and `@storybook/testing-library` to run automated interactive tests directly against existing Storybook stories. This allows us to define component test scenarios once in stories (with args, controls, and interactive states) instead of reconstructing them in separate Jest files. All unit and interactive validation logic can be attached to stories, serving as a single source of truth for both manual UI exploration and automated testing.

2. **Add visual debugging capabilities to Jest workflows**:
   - Integrate `jest-preview` into the Jest setup to provide on-demand browser previews of rendered components during test runs. When a Jest test fails, developers can trigger a preview to inspect the component’s live rendered state, replacing CLI-only debugging with visual context.
   - For complex interactive tests, link failing Jest test cases directly to their corresponding Storybook stories via comments or scripted shortcuts. This lets developers debug components in the familiar Storybook UI, using its interactive controls and add-ons to isolate and fix issues more efficiently.

3. **Standardize a unified testing workflow**:
   - All core component scenarios are first authored as interactive Storybook stories with full coverage of edge cases and user interactions.
   - Automated tests (unit, interactive, and visual regression) are built on top of these stories:
     - Storybook Test Runner executes interactive tests against stories to validate user flows.
     - Chromatic runs visual regression checks on all story states as part of CI.
     - Existing isolated Jest tests are gradually migrated to leverage stories, or linked to stories for visual debugging if retention is necessary.
   - For quick sanity checks, developers can run a subset of story-based tests via the Test Runner, while using Storybook’s UI to manually validate complex interactions without switching contexts.

This approach resolves duplicate markup by reusing stories as test artifacts, improves Jest test debugging with visual tooling, and unifies manual exploration, automated testing, and visual regression checks into a cohesive workflow.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision 
## Decision

Adopt **npm Workspaces** as the official monorepo dependency and workspace management system for the `primer/react` repository, paired with a dedicated workspace for project scripts:

1. Define all existing workspaces (top-level `@primer/react`, `docs`, `examples/*`) in the root `package.json` using npm’s `""workspaces""` field. This will allow contributors to run a single root `npm install` to set up all workspace dependencies automatically, deprecating the custom `setup` script and eliminating manual per-directory installs. Formalize inter-workspace dependencies with standard workspace references (e.g., `""@primer/react"": ""workspace:*""`) instead of relative paths, file dependencies, or manual symlinks, leveraging npm’s built-in symlinking for reliable local package connections.

2. Refactor the existing `script/` directory into a dedicated `packages/scripts` workspace with its own `package.json`. Manage script-specific dependencies directly in this workspace to isolate script logic from other project components, while exposing common commands via the root `package.json` using npm’s cross-workspace script execution capabilities.

This approach aligns with the project’s existing npm tooling, reduces contributor overhead, standardizes workspace relationships, and resolves unstructured script dependency management without requiring adoption of unfamiliar tools.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision 
## Decision

1. Restructure the codebase to separate public and internal code:
   - Move all code intended for external consumer use into dedicated top-level subdirectories within `src` (e.g., `src/components`, `src/utilities` for public API surfaces).
   - Keep internal-only code (helpers, shared logic not meant for external import) in a distinct `src/internal` directory or other non-public subdirectories not exposed via package exports.

2. Update the package.json exports configuration:
   - Replace the broad `./lib-esm/*` export pattern with explicit patterns targeting only the restructured public directories (e.g., `./lib-esm/components/*`, `./lib-esm/utilities/*`). This restricts the public API to only the intended files and prevents accidental exposure of internal code.

3. Adjust the Rollup build setup:
   - Configure Rollup to only bundle and output files from the public directories. Ensure internal code is bundled only as part of public entry points (if required) but not exposed as standalone importable modules in the published package.

4. Add explicit documentation markers:
   - Annotate internal code files and functions with `@internal` JSDoc annotations to clarify their non-public status for contributors and tooling. Maintain clear public API documentation for all exposed components and utilities.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision 
## Decision

- Replace runtime dynamic style injection (including `sx` prop usage, on-the-fly style collection) with a static CSS-first architecture built on Primer Primitives.
  - This eliminates runtime style processing overhead, cutting initial page load times by leveraging parallel browser downloading of static CSS files instead of waiting for JS bundle processing.
  - For SSR, remove the requirement for double-rendering by shipping precompiled styles alongside markup, eliminating 20%+ of server-side rendering time as observed in the issues-app experiment.

- Refactor component libraries to emit static class names instead of generating inline/dynamic styles at runtime.
  - For dynamic state/props-based style changes, use CSS variables from Primer Primitives to theme or adjust styles natively via the browser, rather than generating new styles runtime. This reduces re-render times by up to 60% (per lab and application benchmarks) by offloading style updates to browser-native CSS processing.

- Standardize on Primer Primitives tokens in all static CSS to enforce guardrails:
  - Restrict style values to approved Primer token variables (color, spacing, typography, etc.) to ensure design system consistency and prevent arbitrary, unmaintainable style declarations.
  - Use CSS variables to inherit primitive values directly, enabling theme switching via variable value overrides without full style reprocessing.

- Migrate existing dynamic style usage (e.g., dynamic `sx` props) to either:
  - Predefined utility classes for common static style patterns, or
  - Component-specific CSS modules with conditional class application for state changes, using CSS variables to handle dynamic theme/context values.

- Optimize static CSS delivery via code splitting (per-route or per-component bundles) to ensure only necessary styles are loaded for each page, further reducing initial load overhead without sacrificing performance gains.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision 
## Decision

We supersede ADR 009's recommendation to use Storybook `play` functions for authoring user interaction tests, and instead require all new component interaction tests to be written using our existing Playwright framework. 

Existing interaction tests implemented via Storybook `play` functions will be migrated to Playwright over time (prioritizing critical or high-traffic components first). Going forward, Storybook `play` functions will be reserved solely for lightweight, interactive documentation demos within Storybook, not for formal testing purposes.

This unifies our interaction and visual regression testing workflows, leverages Playwright's robust browser automation and assertion capabilities, and maintains consistency with our established Jest/Playwright test tooling stack.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision 
## Decision
To eliminate layout shifts caused by responsive props during server-side rendering (SSR), we will implement a standardized, SSR-safe approach for handling responsive props across all components, combining enhanced hook logic, CSS-first layout resolution, and clear type contracts:

1. **Enhance the `useResponsiveValue` hook for SSR consistency**:
   - The hook will render a configurable server fallback value (default: `narrow` breakpoint) during initial server render and hydration. This ensures the client’s initial DOM matches the server output exactly, preventing layout shifts during hydration.
   - After hydration is complete, the hook will switch to using `matchMedia` to dynamically update the value based on the client’s actual viewport size.
   - Add an optional parameter to let component authors override the default fallback breakpoint for components where `narrow` is not the most appropriate initial state.

2. **Prioritize CSS-based resolution for layout/styling props**:
   - For props that control visual layout or styles (e.g., `gap`, `padding`), we will map responsive prop values to CSS custom properties or breakpoint-specific utility classes instead of JavaScript resolution. This leverages native CSS media queries to adjust styles without client-side JavaScript, eliminating SSR mismatches entirely.
   - Example: A `gap` prop with `{ narrow: 'condensed', regular: 'normal' }` will translate to inline styles using `@media` rules or pre-defined utility classes that apply the correct gap at each breakpoint.

3. **Standardize responsive prop interfaces and type safety**:
   - Define a reusable TypeScript type `ResponsiveValue<T>` that enforces valid responsive prop shapes (either a single value of type `T` or an object with keys matching our named breakpoints: `narrow`, `regular`, `wide`).
   - All components accepting responsive props will explicitly document this type and require use of either CSS-based resolution or the enhanced `useResponsiveValue` hook to ensure SSR safety.

4. **Preserve flexibility for non-visual responsive props**:
   - For props controlling component behavior (not just layout), authors will use the enhanced `useResponsiveValue` hook to handle dynamic value updates post-hydration, ensuring initial SSR consistency while supporting responsive functionality.

This approach eliminates SSR-related layout shifts, reduces code duplication by standardizing responsive prop handling, and maintains flexibility for both visual and behavioral responsive component features.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision
## Decision

To support a clear, accessible migration from the deprecated `title` prop on `ActionList.Group` to the new dedicated `ActionList.GroupHeading` component, we will implement the following structured approach:

1. **Deprecation Signaling & Runtime Guidance**:
   - Mark the `title` prop on `ActionList.Group` as deprecated in TypeScript types, component JSDoc, and library documentation.
   - Add a development-only runtime warning when the `title` prop is used, which includes:
     - A clear statement of deprecation.
     - A side-by-side code example showing the old prop usage vs the new `ActionList.GroupHeading` implementation.
     - A direct link to detailed migration steps in our component library guides.

2. **Backward Compatibility & Accessibility Safeguard**:
   - For the next two major versions, the `title` prop will remain functional: when provided, it will automatically render an accessible `ActionList.GroupHeading` (with appropriate ARIA attributes) to preserve screen reader support while keeping existing code working.
   - If both the deprecated `title` prop and a manual `ActionList.GroupHeading` child are provided, the runtime warning will explicitly note the prop will be ignored, urging authors to remove it entirely.

3. **Documentation Prioritization**:
   - Update the `ActionList` component documentation to feature the new API (using `ActionList.GroupHeading`) as the primary recommended pattern.
   - Add a dedicated ""Migration"" section that explains the accessibility benefits of the new approach (e.g., explicit heading hierarchy control via the `as` prop) and walks through upgrading from the old API.
   - Pin a prominent deprecation notice at the top of the `ActionList.Group` sub-component docs.

4. **Removal Timeline**:
   - The deprecated `title` prop will be fully removed in the third major version following this decision. This gives authors sufficient time to migrate their implementations while aligning with our standard deprecation cycle.

This approach balances backward compatibility for existing codebases with clear, actionable guidance to adopt the more flexible and accessible new API, ensuring a smooth transition for all users.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision 
## Decision

1. Adopt support for industry-standard proxy environment variables for self-hosted runners to align with widespread application conventions and fulfill enterprise proxy requirements:
   - Primary supported variables: `http_proxy` (for HTTP traffic), `https_proxy` (for HTTPS traffic), and `no_proxy` (for hosts excluded from proxying)
   - Compatibility fallback: Honor uppercase equivalents (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`) to accommodate systems where uppercase variables are configured. When both lowercase and uppercase variants are present, prioritize lowercase to match the behavior of most widely used tools (e.g., curl, docker, git).

2. Implement proxy behavior that adheres to common industry expectations:
   - The `no_proxy` variable will support standard wildcard patterns (e.g., `*.internal`, `192.168.0.0/16`) to allow exclusion of internal or trusted hosts from proxy routing.
   - All core self-hosted runner operations—including control plane communication, task artifact fetching, and job execution dependencies—will respect the configured proxy settings.

3. Prioritize convention over custom configuration to minimize user complexity:
   - Avoid introducing proprietary proxy configuration mechanisms (e.g., dedicated config files, CLI flags) initially, unless explicitly requested by enterprise users to address unmet edge cases.
   - Document the supported proxy variables, their behavior, and compatibility notes prominently in self-hosted runner setup and configuration guides to ensure seamless integration with existing enterprise proxy infrastructure.

4. Ensure transparent and debuggable proxy handling:
   - Log non-sensitive proxy configuration details during runner startup to help users verify settings are applied correctly and troubleshoot connectivity issues.
   - Maintain compatibility with system-wide proxy configurations by inheriting these environment variables from the runner's host environment by default.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision 
## Decision

1. **Default Command Echoing Behavior**: Command echoing will be disabled by default for all pipeline steps to reduce unnecessary log clutter for end users.
2. **Error Context Preservation**: If a command fails to process (returns a non-zero exit code), the command’s output will automatically be echoed to user logs. This ensures end users retain critical troubleshooting context for failed operations.
3. **Explicit Echo Control**: Two new special commands will be added to let users explicitly toggle echoing during pipeline execution:
   - `::echo::on`: Enables command echoing for all subsequent steps until explicitly disabled.
   - `::echo::off`: Disables command echoing for all subsequent steps until explicitly enabled.
4. **Debug Mode Override**: When the `ACTIONS_STEP_DEBUG` secret is enabled, command echoing will default to on instead of off. This maintains full output visibility for debugging workflows where granular command details are critical.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision 
## Decision

Implement an extensible metadata injection mechanism to include additional context (e.g., OS/software versions for hosted images) in the ""Set up job"" step log, with the following specifications:

1. **Metadata Sources**:
   - For hosted runners, build a native provider that captures and formats OS version, pre-installed software versions, and key environment details.
   - For self-hosted runners, support an optional, user-configurable script (e.g., `runner-metadata.sh`/`runner-metadata.ps1`) that outputs custom metadata in line-separated `KEY=VALUE` or JSON format.

2. **Log Integration**:
   - The ""Set up job"" step will automatically detect and process metadata from the configured source.
   - Append a dedicated, structured section (e.g., ""Job Context Metadata"") to the step log, presenting metadata in human-readable key-value pairs or bullet points.

3. **Configuration**:
   - Add a runner setting (`enable_job_setup_metadata`) that defaults to `true` for hosted runners (to deliver the requested OS/software info) and `false` for self-hosted runners.
   - Self-hosted users can enable the setting and point to their custom metadata script via an additional configuration field (`job_setup_metadata_script`).

4. **Backward Compatibility**:
   - If no metadata source is available or the feature is disabled, the ""Set up job"" log will behave as before.
   - Invalid or missing metadata scripts will not cause job failures—they will trigger a silent skip of the metadata section.

### Rationale
- Delivers the requested transparency into hosted image OS/software details, aiding developers and packagers in validating environment configurations.
- Extensible design accommodates both managed hosted environments and custom self-hosted setups.
- Configurable opt-in/opt-out ensures minimal disruption to existing workflows.
- Graceful failure handling prevents job execution issues from misconfigured metadata sources.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision 
## Decision Outcome

Chosen option: ""Implement `steps.<id>.outcome` and `steps.<id>.conclusion` properties in the steps context"", because:
1. It directly addresses the high-priority need to conditionally execute downstream steps based on the status of prior steps, enabling flexible workflow automation that aligns with user expectations.
2. The dual-property approach covers both simple and granular use cases: `outcome` provides a binary success/failure status for basic conditional logic, while `conclusion` includes granular results (e.g., ""skipped"", ""canceled"", ""timed out"") to handle edge cases.
3. The change maintains full backward compatibility, as it extends the existing `steps` context without modifying or deprecating the established `outputs` property.
4. It eliminates the need for brittle workarounds (like parsing log outputs or external state tracking) that users currently rely on to access step status information.

We will document the exact possible values for both properties (e.g., `outcome`: ""success""/""failure""; `conclusion`: ""success"", ""failure"", ""skipped"", ""canceled"", ""timed_out"") to ensure clear guidance for users writing conditional workflow logic.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision 
## Decision Outcome

Chosen option: ""Add a `--labels` command-line argument to the `runner register` command, accepting a comma-separated list of custom labels"", because

- It directly addresses the core automation requirement: allows custom labels to be configured in a single, scriptable registration step, eliminating the need for fragile post-registration config file modifications.
- Seamlessly integrates with existing behavior: the runner will merge the provided custom labels with auto-detected built-in labels (os, arch) during registration, maintaining backward compatibility while adding flexibility.
- Follows intuitive command-line patterns (comma-separated values) that are familiar to automation engineers and easy to integrate into CI/CD setup workflows.
- Resolves the key pain point in Issue #262 by providing an official, supported mechanism for label configuration, reducing user reliance on unofficial workarounds.

Rejected alternative options:
1. **Post-registration only label configuration**: Requires additional script steps and introduces potential race conditions between runner startup and label updates, complicating automation.
2. **Manual config file edits**: Error-prone in scripted environments and breaks the end-to-end automated setup flow that self-hosted runner users depend on.
3. **Separate label management command**: Adds unnecessary complexity by splitting the registration and label setup into disjoint steps, rather than unifying them in the logical registration phase.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision 
## Decision

We will implement a layered fault tolerance strategy to handle transient and extended faults with third-party authentication providers, consisting of the following core components:

1. **Configurable Retry Mechanism for Transient Errors**
   - Implement an exponential backoff retry strategy with jitter for transient provider failures, including network timeouts, 5xx-level HTTP status codes, and provider-specific error messages explicitly indicating temporary issues (e.g., ""rate limited"" or ""service temporarily unavailable"").
   - Exclude non-transient errors (e.g., 401 Unauthorized, 403 Forbidden) from retries to avoid unnecessary traffic and failed loop scenarios.
   - Make retry parameters (max attempts, initial backoff, max backoff, jitter percentage) configurable via environment variables or system config files.

2. **Circuit Breaker Pattern for Extended Outages**
   - Integrate a circuit breaker to prevent overwhelming the auth provider during prolonged failures. The circuit breaker will:
     - Track the rate of failed auth requests against a configurable threshold.
     - ""Open"" the circuit to block new requests when the threshold is breached, returning a predefined error or triggering fallback logic.
     - Transition to a ""half-open"" state after a configurable timeout to test if the provider has recovered, resuming normal operation only if test requests succeed.

3. **Fallback Authentication Measures**
   - Implement short-lived caching of valid authentication tokens (with a configurable TTL, e.g., 15-30 minutes) to allow existing authenticated users to continue accessing services without re-authenticating during transient provider outages.
   - Support optional alternative auth providers (if configured) to redirect unauthenticated users to a working provider when the primary is unavailable.
   - Add a temporary session extension fallback for existing users during extended outages: extend valid sessions by a limited, auditable window (e.g., 1 hour) and log all such extensions for compliance.

4. **Centralized Monitoring and Alerting**
   - Log detailed metadata for all auth provider faults, including error type, retry attempts, circuit breaker state, and fallback activation status.
   - Set up alerts for critical events: high transient error rates, prolonged circuit breaker open states, and frequent fallback mechanism usage to enable rapid incident response.

5. **Graceful Degradation**
   - Ensure the system provides clear, user-friendly error messages during outages, distinguishing between transient issues (with a retry prompt) and extended outages (with fallback options or status updates).
   - Maintain core system functionality for already authenticated users via cached sessions, while restricting new authentication attempts to working providers or fallback methods.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision 
## Decision Outcome

Chosen option: ""MADR 4.0.0 with open-collaboration enhancements"", because:
* **Cross-domain coverage**: MADR 4.0.0 is designed to capture decisions spanning architecture, code, tooling, process, and community practices—aligning with our goal to document all project decisions in a single, consistent format, regardless of their focus area.
* **Low barrier for external contributors**: The template’s lean, intuitive structure (with distinct sections for context, problem statement, options considered, decision outcome, and consequences) is easy for new team members and open-source contributors to understand, reducing friction when reviewing or providing input on decisions.
* **Transparent collaborative workflow**: We will extend the core MADR template to include two targeted subsections tailored to open project needs:
  - `Community Input`: To record feedback, suggestions, or alternative perspectives from external contributors (with attribution to individuals or public discussions where appropriate), ensuring their contributions are visible and acknowledged.
  - `Decision Timeline`: To track key milestones (e.g., draft publication date, feedback closure deadline, final decision date) to make the decision-making process transparent to the broader community.
* **Timeless, maintainable records**: The format requires capturing the decision’s timestamp, status, and explicit rationale as of the time it was made. Superseded decisions link to their successors, ensuring records remain accurate and relevant without becoming out of date.
* **Open-source ecosystem compatibility**: MADR is widely adopted in open-source projects, with existing tooling support (e.g., Log4brains static site generator, MkDocs plugins) that enables us to publish decision records publicly as part of our project’s documentation site, making our decision-making process accessible to all interested contributors.
* **Explicit assumption tracking**: The template’s structured sections force implicit assumptions to be documented, which helps both internal and external stakeholders understand the context that led to a decision—critical for building trust in open project governance.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂÃÂ¡ÃÂÃÂª with all relevant discussion summarized ÃÂÃÂ¡ÃÂÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision
## Decision

1. **Standardize Supported Decentralized Storage URI Schemas**
   Extensions must reference their EDN definition files using a curated set of widely adopted, secure decentralized storage URI schemas to ensure consistent resolution and reliability. Initially supported schemas are:
   - `ipfs://` (InterPlanetary File System): Leverages content-addressed CIDs to guarantee definition file integrity.
   - `ar://` (Arweave): Uses immutable, permanent storage for long-term persistence of extension definitions.
   - `ens://` (Ethereum Name Service): Mapped to IPFS content, enabling human-readable URI aliases for extension locations.
   Unsupported schemas will be rejected during extension registration to avoid resolution failures.

2. **Implement a Pluggable URI Resolution Pipeline**
   Deploy a dedicated, modular resolution service to handle fetching EDN extension definitions from decentralized storage. Key capabilities include:
   - **Schema Validation**: Preprocess all incoming URIs to validate structural correctness against supported schemas, rejecting malformed or maliciously constructed URIs upfront.
   - **Schema-Specific Resolution**: Use maintained, battle-tested libraries (e.g., `go-ipfs` for IPFS, `arweave-go` for Arweave) to resolve URIs to retrievable EDN content.
   - **Caching Layer**: Cache resolved EDN files with configurable TTLs (default 24 hours) to reduce redundant decentralized storage requests, improve extension loading latency, and mitigate network congestion.
   - **Resilience Controls**: Implement exponential backoff retries for transient resolution failures (e.g., network timeouts, node unavailability) to handle intermittent decentralized storage network issues.

3. **Enforce Content Integrity and Security**
   - **Hash Verification**: For content-addressed schemas (IPFS, Arweave), validate that retrieved EDN content matches the hash embedded in the URI (e.g., IPFS CID) to prevent tampering or substitution of extension definitions.
   - **EDN Sanitization**: Sanitize all retrieved EDN content to remove or escape potentially malicious values (e.g., executable code snippets, invalid data types) before parsing, reducing injection attack risks.

4. **Enable Extensible Schema Support**
   Design the resolution service with a plugin architecture to allow future addition of new decentralized storage URI schemas. New schemas will be added only after meeting criteria for:
   - Decentralization and censorship resistance.
   - Availability of reliable, secure resolution tooling.
   - Community adoption and long-term viability.

5. **Integrate Developer Tooling**
   Provide CLI and SDK tools for extension developers to:
   - Validate extension URI compliance against supported schemas during development.
   - Test resolution of their EDN definition files using the same pipeline as the production system.
   - Automatically generate content-addressed URIs (e.g., IPFS CIDs) for their EDN files during extension packaging.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision 
## Decision

Private group chat functionality is explicitly excluded from the Q2 beta release scope. To align with the tight timeline and critical path focus, the following actions are mandated:

1.  **Pause all active work on private group chat**: Immediately halt development, bugfixing, and troubleshooting efforts related to private group chat functionality. This includes addressing existing regressions labeled under the `group-chat` GitHub label.
2.  **Redirect resources to critical path items**: Reallocate engineering time previously dedicated to group chat to core beta requirements: stabilizing 1-1 chat, optimizing public chat, and addressing beta-specific readiness gaps.
3.  **Resolve scope ambiguity**: Finalize this decision as the binding direction for all teams to eliminate recurring scope discussions and prevent further wasted effort on out-of-scope features.
4.  **Post-beta prioritization**: Document private group chat as a high-priority post-beta feature. Reintroduction will be contingent on:
    - Stable launch and validation of core 1-1 and public chat functionality
    - Resolution of security/encryption complexity concerns unique to private group chat
    - Alignment with post-beta roadmap priorities approved by product leadership
5.  **Scope guardrails**: Implement a formal review process for any future requests to reintroduce group chat to an active release cycle, requiring sign-off from both engineering and product leads to confirm timeline feasibility and risk mitigation of security and complexity challenges.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision 
## Decision

We will replace the hardcoded peer configuration with the Discovery v5 (Discv5) protocol for dynamic peer discovery in the messaging app, to address scalability limitations and improve high availability, reliability, and security.

### Key Implementation & Operational Plan:
1. **Integrate Discv5 Protocol**: Adopt the standardized Discv5 implementation (aligned with the referenced EIP and Status research) to enable automatic peer discovery from the app. This will include implementing core protocol features: encrypted message exchange, authenticated node record verification, and distributed routing table management.
2. **Bootstrap Network Deployment**: Launch a set of dedicated Discv5 bootnodes to serve as the initial entry point for peer discovery. These bootnodes will be maintained to ensure reliable onboarding for new app instances and existing nodes.
3. **Smart Peer Management**: Build internal logic to:
   - Prioritize connections to our clustered server peers to maintain service capacity and performance as user demand grows.
   - Dynamically drop unresponsive or unhealthy peers and replace them with newly discovered nodes, ensuring continuous service availability.
   - Enforce per-node peer count limits to prevent overload, while enabling independent horizontal scaling of our server cluster without app-side configuration changes.
4. **Hardcoded Peer Phase-out**: Once Discv5 integration is fully validated in staging and production environments, gradually retire hardcoded peer configurations to complete the transition to dynamic discovery.

### Rationale:
This decision directly resolves the scalability bottleneck of hardcoded peers by enabling dynamic, on-demand discovery of server nodes. Discv5’s built-in security mechanisms (authenticated node identities, encrypted communication) enhance the app’s security posture. Additionally, dynamic peer discovery reduces dependency on static, potentially unresponsive nodes, significantly improving overall service reliability for growing user bases.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision 
## Decision

We will implement standardized key derivation from 12-word mnemonic phrases to enable cross-wallet compatibility, aligning with industry-wide specifications while preserving backward compatibility for existing Status users.

### Core Technical Specifications
1. **Adopt BIP-39 for Mnemonic & Seed Generation**
   - Use the official BIP-39 English wordlist as the default for generating new mnemonics.
   - Derive the master seed from mnemonics using BIP-39 standard parameters: PBKDF2 with HMAC-SHA512, 2048 iterations, and an optional user-provided passphrase (empty string by default) to support salted seed generation.
   
2. **Implement BIP-44 Hierarchical Key Derivation**
   - For Ethereum-compatible addresses, use the standard BIP-44 derivation path: `m/44'/60'/0'/0/0` for the first account. Support multiple accounts by incrementing the final index (e.g., `m/44'/60'/0'/0/1` for the second account).
   - Generate private keys from the master seed using HMAC-SHA512 as specified in BIP-32, then derive Ethereum addresses via the standard process: keccak-256 hash of the uncompressed public key, truncated to the last 20 bytes, and formatted with EIP-55 checksum.

### User Experience & Backward Compatibility
1. **New Wallet Creation**
   - Default to the standardized BIP-39/BIP-44 derivation path for all new wallets created in Status. This ensures new wallets can be imported into any other BIP-compliant wallet using the same 12-word mnemonic.
   
2. **Mnemonic Import Flow**
   - Add an optional field for users to input their BIP-39 passphrase during import, to support wallets generated with a passphrase.
   - For imported mnemonics, automatically derive addresses using the BIP-44 path first; if no valid funds/activity are detected, offer the option to attempt legacy Status derivation for backward compatibility.

3. **Legacy Wallet Support**
   - Preserve the existing Status key derivation method for all existing wallets. Store metadata with each wallet indicating its derivation path (standard vs legacy) to ensure correct key retrieval.
   - Add an explicit option in the wallet creation/import flow for users to create or access a legacy Status wallet, for those migrating from older versions or needing to access non-standard addresses.

### Validation & Testing
- Conduct cross-wallet compatibility tests with major Ethereum wallets (MetaMask, Trust Wallet, Ledger Live) to confirm address consistency when importing/exporting 12-word mnemonics.
- Verify that passphrase-protected mnemonics are correctly supported, generating the same seed and addresses as other BIP-39 compliant wallets.
- Ensure backward compatibility by validating that existing Status users can access their wallets without any changes to their mnemonics or passphrases.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision 
## Decision

We will implement two parallel paths to address performance, decentralization, and security gaps while advancing toward our long-term goals:

### Short-Term (Beta Readiness)
1. **Optimize LES performance with CHT integration**: Deploy the CHT (Checkpoint Hash Tree) BloomTrie implementation to mitigate key LES-related pain points (slow/hot sync, excessive bandwidth/disk usage) on Mainnet. This will improve user experience for beta users relying on native LES sync without extensive LES overhauls.
2. **Enable user-configurable upstream nodes**: Add a setting allowing users to connect to self-hosted or trusted third-party Ethereum nodes instead of relying exclusively on Infura. This reduces single-point-of-failure risk and gives users more control over their trust model while ULC is in development.

### Long-Term (Decentralized, Performant Sync)
3. **Prioritize ULC (Ultra Light Client) development and integration**: Treat ULC as the primary long-term solution to replace both LES and Infura reliance. We will allocate engineering resources to accelerate ULC readiness, leveraging its ability to connect to multiple peer nodes to enhance decentralization, security, and performance. Once ULC is production-ready, we will phase out default Infura reliance and transition native sync to ULC as the preferred method.

This approach balances immediate beta usability and security mitigations with our long-term commitment to a decentralized, trust-minimized client experience.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision 
## Decision

1. **Introduce a dedicated `Secret` type for sensitive data**
   We will define a strongly typed `Secret` struct (or equivalent in the target language) that wraps sensitive values (e.g., passwords, API keys, private tokens). This type will enforce safe handling by default, with explicit opt-in for accessing the raw sensitive data.

   ### Key Behaviors of the `Secret` Type:
   - **Redacted Default String Representation**: The type's built-in string conversion method (e.g., `String()` in Go, `toString()` in Java) will return a fixed redacted value such as `[REDACTED]` or `<sensitive data>` instead of the raw secret. This ensures accidental logging or printing of the `Secret` instance does not leak sensitive information.
   - **Explicit Raw Access**: A named, intentionally explicit method (e.g., `UnsafeRaw()`, `Reveal()` or `Value()`) will be provided to retrieve the raw sensitive value. This method serves as a clear signal to developers that handling the raw data carries risk and requires careful review.
   - **Constant-Time Comparison**: Implement a safe comparison method (e.g., `Equals(other Secret) bool`) that uses constant-time operations to prevent timing attacks. Direct equality checks on the wrapped value or type will be avoided to mitigate cryptographic vulnerabilities.
   - **Safe Serialization**: By default, serialize the `Secret` type to redacted values in all standard serialization formats (JSON, YAML, etc.). Explicit, opt-in methods (e.g., `MarshalRawJSON()`) can be added for rare cases where raw serialization is necessary (only after validating the use case justifies the risk).
   - **Non-Implicit Conversion**: Prevent implicit conversion from `Secret` to non-sensitive types (e.g., string) in the language's type system where possible. This forces developers to intentionally handle sensitive data rather than accidentally treating it as a regular value.

2. **Mandate use of `Secret` for all sensitive data in transit**
   All sensitive user inputs (e.g., passwords, MFA codes) and internal sensitive values (e.g., API keys, temporary tokens) must be wrapped in the `Secret` type as early as possible in their lifecycle (e.g., immediately after receiving from user input or reading from secure storage). This eliminates the risk of sensitive data being stored or passed as regular strings at any point in the codebase.

3. **Hash secrets at rest wherever possible**
   For secrets that need to be stored long-term (e.g., user passwords), use cryptographic hashing algorithms (e.g., bcrypt, Argon2) instead of storing the raw `Secret` value. The `Secret` type should be used only during transit (e.g., from input to hashing) and never persisted directly to storage.

4. **Developer Education and Tooling**
   - Update style guides to require the `Secret` type for all sensitive data.
   - Add linting rules to detect cases where sensitive data is handled as a regular string (e.g., assigning a password input to a string variable).
   - Document best practices for using the `Secret` type, including when to use raw access, safe comparison, and serialization guidelines.

### Example Implementation (Go):
```go
package util

import (
	""crypto/subtle""
	""fmt""
)

// Secret wraps a sensitive value to prevent accidental exposure.
type Secret struct {
	value string
}

// NewSecret creates a new Secret from a raw string.
func NewSecret(value string) Secret {
	return Secret{value: value}
}

// String returns a redacted representation of the secret.
func (s Secret) String() string {
	return ""[REDACTED]""
}

// UnsafeRaw returns the raw sensitive value. Use with extreme caution.
func (s Secret) UnsafeRaw() string {
	return s.value
}

// Equals compares two secrets using constant-time equality to prevent timing attacks.
func (s Secret) Equals(other Secret) bool {
	return subtle.ConstantTimeCompare([]byte(s.value), []byte(other.value)) == 1
}

// MarshalJSON redacts the secret in JSON output.
func (s Secret) MarshalJSON() ([]byte, error) {
	return []byte(`""[REDACTED]""`), nil
}
```

### Migration Steps:
1. Identify all instances of sensitive data handled as regular strings in the codebase (e.g., password inputs, API key fields).
2. Replace these with the `Secret` type, updating all functions and methods that process or transmit this data to accept and return `Secret` instances.
3. Refactor any existing logging or serialization code that previously handled sensitive strings to use the `Secret` type's redacted default representation.
4. Add linting rules and conduct code reviews to enforce use of the `Secret` type for new sensitive data.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision 
## Decision

1. **Retire the existing JS Jail-based status extension system entirely**: We will stop supporting and maintaining the current JavaScript jail execution environment, including removing all jail-related code from the Status app and deprecating any associated SDKs or documentation. This eliminates the performance overhead of repeated RN Bridge ""ping-pong"" calls, the complexity of asynchronous logic chains, and the technical debt from half-implemented, hard-to-test extension code.

2. **Consolidate all remaining command logic into native ClojureScript/Re-frame code**: Since no command is fully implemented in the jail system (all rely on hardcoded native logic), we will migrate any leftover jail-dependent command logic to the native app codebase. This will clean up the scattered ""magic"" hardcoded parts, improve testability (by enabling proper unit testing without jail-related mocks), and unify command execution under the app's native state management pattern.

3. **Shift all extensibility focus to the Project Pluto extension framework**: We will prioritize building out Pluto's capabilities to cover the use cases previously intended for the jail system—including command message hooks—while leveraging its declarative, comprehensive extension model. Pluto’s design allows for extending multiple Status features (wallet assets, chat contacts, command messages, etc.) in a unified way, addressing the limitations of the old jail system’s narrow, poorly designed API.

4. **Deprecate support for new jail-based extensions**: We will communicate to existing and potential developers that the JS jail system is no longer supported, and direct them to Project Pluto as the official path for building Status extensions. This aligns with the lack of real-world 3rd party adoption of the jail system and the superior capabilities of Pluto.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision 
## Decision

We will implement a **Tribute to Talk (TtT) subsystem** integrated with the Status network's SNT staking and wallet infrastructure, following these core rules aligned with the whitepaper’s proposal:

1. **Recipient Stake Configuration**:
   - Each Status user (recipient) may set a configurable minimum SNT stake requirement for incoming ""cold"" direct contact requests. This requirement must be at least a network-defined minimum threshold and can be adjusted or disabled at the recipient’s discretion.
   - Recipients can whitelist trusted contacts/groups to exempt them from TtT deposit requirements, bypassing the anti-spam filter for pre-approved parties.

2. **Sender Deposit Escrow**:
   - When initiating a cold direct contact request to a recipient with an active TtT stake, the sender must deposit SNT equal to the recipient’s current minimum requirement into a temporary, dispute-resistant escrow account managed by the TtT subsystem.
   - Requests without a valid, full deposit (or from blocked senders) are rejected immediately and not delivered to the recipient.

3. **Deposit Resolution Logic**:
   - **Forfeiture to Recipient**: If the recipient explicitly replies to the contact request within a configurable time window (default 7 days), the full escrowed deposit is transferred to the recipient’s SNT wallet.
   - **Refund to Sender**: 
     - The deposit is fully refunded to the sender if the recipient ignores the request beyond the configured time window, or if the sender cancels the request before the recipient opens it.
   - **Dispute Handling**: Escrowed funds are held in a timelocked smart contract (or decentralized state layer) to eliminate counterparty risk, with resolution rules enforced autonomously without intermediaries.

4. **Transparency & Events**:
   - The TtT subsystem will emit on-chain events for all critical actions: stake requirement adjustments, deposit escrows, refunds, forfeitures, and whitelist/blocklist changes. These events are accessible to all network participants for audit and tracking purposes.

5. **User Experience Safeguards**:
   - Recipients will be clearly notified of the deposit amount associated with each pending cold contact request before deciding to reply or ignore. Senders will receive real-time feedback on deposit status (escrowed, refunded, forfeited) via their client interface.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision 
## Decision

Establish a **Mobile App Section Lead** role for each distinct functional area of the mobile app (Communities, Syncing & Onboarding, Activity Center & Shell, Wallet, Messaging) to address gaps in oversight, drive UX/UI polish, and strengthen domain-specific ownership. This role will complement the existing Mobile Team Lead by delegating granular, area-specific accountability while maintaining centralized cross-section alignment.

### 1. Role Definition & Selection
- **Scope**: One Section Lead per app functional area, defined as:
  - Communities: Owns polish of community creation, management, and interaction flows
  - Syncing & Onboarding: Owns polish of user onboarding, account setup, and cross-device sync experiences
  - Activity Center & Shell: Owns polish of the app’s core navigation, notification system, and home shell
  - Wallet: Owns polish of crypto wallet operations, transaction history, and asset management
  - Messaging: Owns polish of direct/group messaging, media sharing, and chat thread experiences
- **Selection Process**:
  1. Internal team nominations (self or peer-nominated) based on domain expertise and attention to detail
  2. Final approval by the Mobile Team Lead, with input from the Design Team to validate alignment with polish priorities
  3. Initial 3-month term with renewal eligibility; role rotations allowed every 6 months to foster cross-domain expertise
- **Eligibility**: Mobile team members with demonstrated experience in their target section and a track record of collaborating effectively with design stakeholders.

### 2. Core Responsibilities
Section Leads will be accountable for the following domain-specific outcomes:
- **Polish Oversight**: Own end-to-end UX/UI and functional polish of their section, including resolving design-alignment bugs, optimizing interaction flows, and ensuring consistency with brand guidelines
- **Stakeholder Liaison**: Act as the primary point of contact between the mobile team and design team for their section, including conducting weekly syncs to review design specs, provide implementation feedback, and validate final deliverables
- **Quality Control**: Review all pull requests (PRs) related to their section to ensure adherence to design requirements, code quality standards, and polish benchmarks
- **Backlog Management**: Maintain and prioritize a section-specific backlog of polish tasks, bugs, and incremental improvements, aligned with the overall mobile roadmap
- **Cross-Section Collaboration**: Escalate cross-domain dependencies, conflicts, or resource gaps to the Mobile Team Lead, and contribute to cross-section initiatives (e.g., app-wide theme updates)
- **Progress Reporting**: Share section-specific polish progress metrics in bi-weekly mobile team syncs and monthly design-mobile alignment meetings

### 3. Governance & Accountability
- **Reporting Line**: Section Leads report directly to the Mobile Team Lead, who retains overall responsibility for cross-section roadmap alignment, resource allocation, and stakeholder communication
- **Check-In Cadence**:
  - Weekly 15-minute section-specific standups with team members working in the area to track polish task progress
  - Bi-weekly 30-minute sync between all Section Leads and the Mobile Team Lead to align on roadmap priorities and resolve cross-section blockers
  - Monthly 60-minute review with the Design Team to validate polish improvements and set next-quarter polish goals
- **Performance Evaluation**: Quarterly reviews based on design-alignment bug resolution rates, PR review efficiency, design team feedback, and section-specific polish metrics

### 4. Success Metrics
The effectiveness of the Section Lead role will be measured by:
- A 30% reduction in design-related bug tickets for their section within 6 months of role activation
- Achieving a 90%+ score in monthly design alignment audits (conducted by the Design Team) for their section’s UI/UX
- A 20% decrease in average PR review time for section-specific changes
- Positive feedback from design and mobile team members on improved collaboration and oversight

### 5. Transition Plan
- **Week 1**: Nominate and finalize Section Lead appointments for all 5 app areas
- **Week 2**: Conduct onboarding sessions for Section Leads, including deep dives with the Design Team to document current polish gaps in their sections
- **Week 3**: Launch section-specific backlogs, standups, and PR review workflows
- **Week 4**: Hold first cross-section sync with all Section Leads and Mobile Team Lead to align on initial polish priorities
- **Month 1**: Conduct first monthly design-mobile alignment review to set baseline metrics and validate early progress","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision 
## Decision Outcome

We will implement a hybrid approach for handling Product Option Values in the Sylius API, balancing ease of use for read operations and efficiency for write operations:

1. **Read Operations**: Embed the Product Option Values collection directly within the Product Option resource response for all GET requests.
   - Use API Platform serialization groups (e.g., `product_option:read`) to include the values by default, with configurable eager loading via Doctrine’s `fetch: EAGER` or query joins to eliminate N+1 performance issues.
   - Maintain a nested response structure that keeps related values accessible in a single request, simplifying frontend integration.

2. **Write Operations**: Expose Product Option Value as a standalone API resource with its own CRUD endpoints (`/api/product-option-values`), linked to the parent Product Option via a required relationship.
   - Allow granular creation, update, and deletion of individual values without needing to modify or re-submit the entire Product Option resource.
   - Enforce validation to ensure no Product Option Value exists without an associated parent Product Option, preserving data integrity.

---

### Options Considered & Rationale
- **Option 1: Full Embedding (Read/Write)**: Rejected. While reads are simple, updating a single value would require sending the entire Product Option payload, leading to inefficient payloads and increased risk of accidental overwrites.
- **Option 2: Fully Separate Resources**: Rejected. Frontends would need multiple requests to retrieve a Product Option and its values, adding unnecessary complexity for common read workflows.
- **Option 3: Hybrid (Embedded Reads, Standalone Writes)**: Accepted. This combines the best of both worlds: single-request access to full Product Option data for reads, and granular, efficient operations for modifying values. It aligns with Sylius’s user-centric API design principles while maintaining performance.

---

### Implications
- Developers working with the API will need to use separate endpoints for managing individual Product Option Values, but this is offset by clearer, more maintainable operation boundaries.
- We will need to configure API Platform serialization groups and resource relationships carefully to ensure proper embedding and access control.
- Eager loading strategies must be documented to help API consumers optimize read performance for large Product Option Value collections.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision 
## Decision

To establish a clear, consistent, and customer-centric approach to handling locale-aware resources in Sylius, we will implement the following standards across all API endpoints:

### 1. Priority-Based Locale Resolution
We will use a flexible, standard-compliant system to determine the customer’s desired locale, with the following priority order:
- **Explicit Query Parameter (Highest Priority)**: Clients may specify a locale via the `locale` query parameter (e.g., `GET /api/products?locale=es-ES`). This accepts valid BCP 47 language tags (e.g., `fr`, `fr-CA`).
- **Accept-Language Header**: If no `locale` parameter is provided, we parse the `Accept-Language` request header (per RFC 7231) to select the highest-priority supported locale.
- **Default Locale Fallback**: If the requested locale is unsupported or no locale hint is provided, we fall back to the store’s configured default locale. A `X-Locale-Fallback` response header will be included to notify the client that the fallback was used.

### 2. Frontend-Optimized Response Structure
- **Primary Payload**: For customer-facing endpoints, locale-aware fields (e.g., product name, shipping method label) will be returned directly in the resolved locale within the main resource payload (e.g., `name: ""Camiseta""` instead of a nested object of all translations). This aligns inconsistent existing endpoints to a uniform, easy-to-consume format.
- **Optional Full Translations**: For admin or integrator use cases requiring all available translations, we will support an `include=translations` query parameter. When included, the response will embed a nested `translations` association containing all locale-specific values for the resource (e.g., `translations: [{ ""locale"": ""en-US"", ""name"": ""T-shirt"" }, { ""locale"": ""es-ES"", ""name"": ""Camiseta"" }]`).

### 3. Consistency Enforcement
We will refactor all existing endpoints that deviate from this pattern to adhere to the above standards. This includes updating endpoints that currently return partial translatable embeds to use the unified locale resolution and response structure.

This approach eliminates manual localization work for frontend developers, maintains flexibility for advanced use cases, and ensures a consistent experience across all Sylius API resources.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision 
## Decision

We will adopt **Architecture Decision Records (ADRs)** as the lightweight, discoverable method to capture all non-proposal architectural and technical decisions for the Terraform Controller project, following these structured guidelines:

1. **Template**: Use a simplified, core template to keep the process lightweight, consisting of four mandatory sections:
   - **Status**: Tracks the decision’s lifecycle (Proposed → Accepted → Deprecated → Superseded)
   - **Context**: Describes the problem, constraints, or situation that necessitated the decision
   - **Decision**: States the agreed-upon resolution clearly
   - **Consequences**: Outlines positive, negative, and neutral impacts (to help future contributors understand tradeoffs)

2. **Storage & Discoverability**:
   - Host all ADRs in a dedicated `docs/decisions` directory within the project’s main repository, ensuring they are version-controlled alongside the codebase
   - Name files using a sequential numbering scheme (e.g., `0001-terraform-state-storage.md`) to maintain chronological order and simplify reference in code comments, PRs, or documentation

3. **Workflow Integration**:
   - New decisions are drafted as an ADR and submitted via a pull request (PR) for team review, allowing asynchronous discussion and feedback
   - Once approved, the ADR is merged into the repository with its status updated to ""Accepted""
   - For deprecated or superseded decisions, update the existing ADR to reflect its new status and include a link to any replacement decision or relevant context

4. **Scope**: This process applies to all decisions affecting Terraform Controller’s design, implementation, operations, or governance that are not already captured via formal project proposals.

## Consequences
## Consequences
- **Positive**:
  - Decisions are directly accessible alongside the codebase, making them easy for future contributors to find and reference
  - Version control via Git maintains a full history of decision changes, including why and when adjustments were made
  - The lightweight template and PR integration ensure the process does not slow down development
- **Negative**:
  - Requires consistent team adherence to ensure all relevant non-proposal decisions are captured
  - Sequential numbering may require manual coordination if multiple ADRs are drafted in parallel
- **Neutral**:
  - ADRs become a living document set, with status updates reflecting the project’s evolving architecture over time","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision 
## Decision

We will use GitHub webhooks for real-time pull request (PR) change detection, supplemented by a lightweight polling fallback using the GitHub API to mitigate against missed webhook events.

### Implementation Details:
1. **Webhook Setup**: Configure repository webhooks to send critical `pull_request` events (including `opened`, `synchronized`, `closed`, and `edited`) to our service endpoint. Enable HMAC signature verification to authenticate all incoming webhook requests.
2. **Webhook Resilience**: Process webhook events asynchronously via a queue system, with 3 automatic retries (using exponential backoff) for transient failures like network outages.
3. **Fallback Polling**: Implement an hourly polling job that fetches PR updates using the GitHub API's `since` parameter to target only changes since the last poll. This job will reconcile any gaps in change detection caused by undelivered or lost webhooks.
4. **Rate Limit Compliance**: Optimize polling requests to stay within GitHub API rate limits by batching queries and leveraging cached responses for non-critical PR metadata.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision 
## Decision

We will retain support for cross-namespace references in the three identified fields, but implement layered security controls to mitigate risks of broken namespace isolation and overproliferated permissions, while minimizing disruption to existing deployments:

1. **For `Terraform.spec.sourceRef`**:
   - Preserve cross-namespace references to Flux source objects (e.g., `GitRepository`, `OCIRepository`), but enforce that the tf-controller's service account holds explicit, narrow RBAC permissions to access the source's namespace.
   - Introduce a cluster-scoped `SharedTerraformSource` Custom Resource Definition (CRD) as a secure alternative for sharing sources across namespaces. This resource will be managed by cluster administrators to grant limited, scoped access to approved namespaces, avoiding broad cross-namespace RBAC grants.
   - Add validation webhooks to verify cross-namespace source references point to existing objects and that the controller has required permissions.

2. **For `Terraform.spec.dependsOn[]`**:
   - Restrict cross-namespace dependencies to only reference tf-controller-managed `Terraform` objects.
   - Introduce a namespace-scoped `CrossNamespaceDependencyPolicy` CRD that allows namespace administrators to explicitly approve incoming dependencies from specific trusted namespaces. Cross-namespace `dependsOn` references will be rejected unless the target namespace's policy includes the source namespace.

3. **For `.data.resources[]` in the branch planner**:
   - Disallow cross-namespace references entirely, as this field is intended for branch-specific, namespace-scoped configuration.
   - Provide migration guidance for users currently using cross-namespace references here, recommending they duplicate resources in the target namespace or use `SharedTerraformSource` for shared assets.

4. **Universal Safeguards**:
   - Implement validation webhooks to enforce all constraints at resource creation/modification time, returning clear error messages for invalid cross-namespace references.
   - Update official documentation to prioritize in-namespace references as the default best practice, with explicit guidance on when and how to use cross-namespace references safely via the new policy and shared resources.

This approach balances backward compatibility with improved security, limiting cross-namespace access to explicit, controlled scenarios while reducing risks of over-broad permissions and broken namespace isolation.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision 
## Decision

We will implement a persistent, self-managing caching mechanism for Workspace BLOBs within the TF-Controller to enable reliable single TF object deletion, with the following core components:

1. **Persistent Cache Storage**:
   - Reuse the TF-Controller's existing storage backend (e.g., Kubernetes persistent volume or configured object storage) to store cached Workspace BLOBs, ensuring persistence across controller restarts and pod failures.
   - Assign each cached BLOB a unique key derived from the TF object’s fully qualified name (namespace/name) and stable UID to avoid cross-object collisions.

2. **Cache Population & Invalidation**:
   - Automatically populate the cache whenever a Workspace BLOB is generated and stored after a successful Terraform plan/apply cycle.
   - Invalidate the cached BLOB immediately if the TF object’s associated resources (Source, Secrets, ConfigMaps) are modified, or if the TF object’s spec is updated to alter the Workspace configuration.
   - Retain the cached BLOB for any TF object marked for deletion until the deletion process completes successfully.

3. **Deletion Workflow Integration**:
   - Modify the TF object’s deletion finalizer to first retrieve the cached Workspace BLOB (if available) instead of relying on original dependent resources.
   - Use the cached BLOB’s content to reconstruct the exact Workspace environment (including backend config, variables, and module sources) that was used to create the Terraform resources, enabling a valid `terraform destroy` execution even if dependent resources are deleted or modified prematurely.
   - Add a fallback: if the cached BLOB is missing, attempt to retrieve any remaining dependent resources, log a warning, and prompt users to regenerate the Workspace BLOB if deletion is blocked.

4. **Cache Cleanup & Garbage Collection**:
   - Delete the cached BLOB immediately after successful completion of the TF object’s deletion finalizer.
   - Implement a periodic garbage collection job to remove stale cached BLOBs associated with non-existent TF objects (verified via Kubernetes API checks) or those exceeding a configurable TTL (default: 7 days) to optimize storage usage.

This approach eliminates single object deletion obstructions by ensuring the controller always has access to the complete, immutable Workspace environment needed to clean up Terraform resources reliably. Establishing this robust single object deletion flow is a critical prerequisite for addressing resource dependency-based deletion and namespace-wide deletion in subsequent phases, as required by the problem priority order.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision 
## Decision

We will automate E2E test execution for all PRs and main branch merges, replace the slash command dispatch workflow, and refactor the E2E test framework to support multi-distribution testing and local execution.

### Key Implementation Steps:
1. **Automate CI E2E Runs**:
   - Remove the `peter-evans/slash-command-dispatch` dependency and trigger a core set of fast E2E tests automatically on every PR submission, commit push to PR branches, and main branch merge.
   - Adopt a tiered testing strategy to balance speed and coverage:
     - A ""Quick"" E2E subset (focused on critical core workflows) runs on PRs to provide rapid feedback to developers.
     - A ""Full"" E2E subset (covering all supported distributions and edge cases) runs nightly to ensure broad compatibility without slowing down PR cycles.

2. **Support Multi-Distribution Testing**:
   - Use GitHub Actions matrix builds to execute the full E2E suite against:
     - Target Kubernetes distributions: Kind, K3s, and EKS Distro.
     - Target Linux distributions: Ubuntu LTS and Alpine Linux.
   - Maintain centralized configuration profiles for each distribution in the E2E framework to handle distro-specific setup and test adjustments consistently.

3. **Enable Local E2E Execution**:
   - Refactor the E2E test suite to rely on local, containerized Kubernetes clusters (e.g., Kind or Minikube) instead of EC2 instances, eliminating cloud infrastructure dependencies for local development testing.
   - Document a step-by-step local setup guide, including installation of required tools (Docker, kubectl, Kind, Zarf) and commands to launch and run the quick E2E subset locally.
   - Package all test dependencies (harnesses, mock services) as containers or portable scripts to ensure they run seamlessly in local environments without CI-specific credentials or resources.

This approach will boost developer velocity via faster local feedback, increase confidence in cross-distribution compatibility, and maintain secure, automated coverage now that registry secrets are no longer required for init package creation.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision
## Decision

Zarf will implement a temporary, cluster-internal OCI registry solution with automated cluster-wide trust configuration as the universal method for image injection across all Kubernetes environments. This approach eliminates dependencies on CRI-specific CLI tools or pre-existing registry infrastructure, ensuring compatibility with unknown or air-gapped environments.

### Core Implementation Details:
1. **Temporary Registry Deployment**:
   - During Zarf's initialization phase, deploy a lightweight, single-replica OCI registry pod via a Kubernetes Deployment. The registry will use a dynamically generated self-signed TLS certificate (stored in a Kubernetes Secret) to enable secure image transfers, and will be exposed internally via a ClusterIP Service to restrict access to the cluster.

2. **Cluster-Wide TLS Trust Establishment**:
   - Deploy a privileged DaemonSet to run on every cluster node, responsible for:
     - Retrieving the temporary registry's self-signed certificate from the Secret.
     - Adding the certificate to both the node's system-wide trusted root CA bundle (e.g., `/etc/ssl/certs` for Linux nodes) and the CRI-specific trusted certificate directory (e.g., `/etc/containerd/certs.d/` for containerd, `/etc/docker/certs.d/` for Docker Engine).
     - Restarting the CRI service (if required) to apply the updated trust configuration, ensuring nodes recognize the temporary registry as a valid image source.

3. **Image Injection & Manifest Patching**:
   - Zarf will push pre-cached or locally built images directly to the temporary registry using standard OCI push commands, leveraging the ClusterIP address for cluster-internal connectivity.
   - All target workload manifests will be automatically patched by Zarf to replace original remote image references with corresponding URIs pointing to the temporary registry (e.g., replacing `nginx:latest` with `<registry-cluster-ip>:<port>/nginx:latest`).

4. **Optional Post-Deployment Cleanup**:
   - Provide a user-configurable cleanup step to terminate the temporary registry Deployment, DaemonSet, and associated Secrets/Certificates once all workloads have successfully pulled their images and are running. For air-gapped environments requiring ongoing image updates, users may opt to retain the registry.

### Rationale:
- **Universal Compatibility**: Avoids reliance on CRI-specific image injection CLI tools (e.g., `kind load`, `k3s ctr image import`) that are not available across all Kubernetes distributions.
- **No Pre-Existing Infrastructure Assumptions**: Eliminates the need for a pre-deployed registry, making it suitable for unknown, isolated, or bare-bones cluster environments.
- **Consistent Trust Across Nodes**: The DaemonSet removes the need for manual per-node configuration or pod affinity/taint workarounds, ensuring any workload in the cluster can pull images from the temporary registry without trust errors.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision 
## Decision

To meet the requirements of Executive Order 14028 (Improving the Nation’s Cybersecurity) for software deployed on government hardware, we will implement the following SBOM (Software Bill of Materials) compliance framework:

1. **SBOM Format Standardization**:
   - Adopt **SPDX 2.3+** as the primary SBOM format for federal compliance reporting, due to its widespread support in government and industry compliance workflows.
   - Support **CycloneDX 1.4+** as a secondary format to accommodate customer-specific requirements.

2. **Automated Generation & Pipeline Integration**:
   - Integrate SBOM generation into all CI/CD pipelines for every build (development) and official release (production) of software intended for government hardware.
   - For production releases, generate full SBOMs that include direct and transitive dependencies, using ecosystem-specific tools (e.g., `syft` for container images, `cyclonedx-maven-plugin` for Java, `spdx-dotnet-tool` for .NET) to ensure completeness.

3. **Mandatory SBOM Content**:
   - Ensure all SBOMs include the fields required by EO14028 and NIST SP 800-218, including:
     - Unique artifact identifier (e.g., package URL, SHA-256 hash of the software binary)
     - Full list of direct/transitive components with names, versions, and unique identifiers (CPEs, PURLs)
     - Dependency relationships between components
     - Timestamp of SBOM generation
     - Maintainer contact information
     - Licensing details for each component
     - Vulnerability data references (where available)

4. **Storage & Distribution**:
   - Attach SBOMs (in both primary and secondary formats) to all official software releases published for government use.
   - Store immutable, versioned copies of SBOMs in a secure, access-controlled repository (e.g., encrypted S3 bucket, internal Git LFS repo) with audit trails for all access and modifications.
   - Provide SBOMs to government customers upon request, aligned with their access control and data sharing policies.

5. **Validation & Maintenance**:
   - Implement automated SBOM validation in CI/CD pipelines using tools like `spdx-sbom-validator` or `cyclonedx-cli` to enforce completeness and format compliance; fail builds/releases if validation fails.
   - Update SBOMs within 72 hours of modifying software dependencies or patching components, with change logs tracking all SBOM updates.

6. **Compliance Auditing**:
   - Conduct quarterly internal audits to verify that all software deployed to government hardware has a valid, up-to-date SBOM.
   - Maintain audit trails of SBOM generation, distribution, and updates to demonstrate compliance during government-led audits.
   - Integrate SBOM checks into pre-deployment gates for government hardware environments; block deployments if a valid SBOM is not present or is out of date.

7. **Vulnerability Integration**:
   - Connect our SBOM management workflow to a vulnerability scanning tool (e.g., Dependency-Track, Snyk) to automatically flag high/critical vulnerabilities in components listed in SBOMs, and remediate them before deployment to government hardware.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision 
## Decision

We will enhance Zarf’s core package handling to automate manifest mutations for image paths, secrets, and internal Git server references—eliminating the need for manual templating by package authors and enabling seamless compatibility with both Zarf’s native deployment workflow and GitOps tools like Flux and ArgoCD.

Key implementation details:
1. **Built-in Automatic Transformation**: Add a native manifest processing phase during package creation and `zarf prepare` execution that automatically:
   - Rewrites all image references to point to the Zarf Registry using the cluster’s configured registry URL.
   - Injects secrets for authenticated access to the Zarf Registry into manifests where required.
   - Updates CRD and resource references to the internal Gitea server with the cluster’s configured Git server address.
   
   These transformations will be driven by Zarf’s internal configuration (derived from cluster setup or package values), so package authors no longer need to hardcode or template these values.

2. **GitOps-Friendly `zarf prepare`**: Modify the `zarf prepare` command to run these automatic transformations by default when generating manifests for GitOps workflows. The output will be fully mutated, ready-to-use manifests that can be directly committed to Git repositories and consumed by Flux/ArgoCD without additional author work.

3. **Deprecate Manual Templating for Common Mutations**: Document the new automatic transformation as the recommended approach, and deprecate the requirement for package authors to implement custom templating for image, secret, or Git server references. Optional escape hatches will be provided for edge cases where authors need to override default behavior.

4. **Backward Compatibility**: Maintain support for existing packages using manual templating to avoid breaking changes, but prioritize and promote the new automatic approach in documentation and package creation guidelines.

This approach reduces the burden on package authors, ensures consistency across deployments, and closes the compatibility gap between Zarf’s native workflows and modern GitOps tooling.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision 
## Decision

1. **Eliminate the stage1 Rust binary**: Replace the configmap-based reassembly of the registry:2 tarball and stage2 binary with a pre-packaged injector pod image that embeds the registry:2 OCI image directly (as a compressed tarball or pre-extracted OCI layout). This removes the need to split and reassemble assets via configmaps, cutting init time and reducing cluster API load.
2. **Replace the stage2 Go registry binary with a minimal OCI registry server**: Implement a tiny, purpose-built Go utility (leveraging the `go-containerregistry` library) that serves only the subset of the OCI Distribution Specification required to host the single registry:2 image in read-only mode. This utility will:
   - Unpack the embedded registry:2 image into an in-memory or on-disk OCI layout.
   - Start a lightweight HTTP server optimized for this single-image use case.
   - Shutdown automatically once the `zarf-registry` pod successfully pulls the image (or after a short, configurable timeout to clean up).
3. **Streamline the injector pod workflow**: Combine the asset unpacking, serving, and cleanup logic into a single init container that runs alongside the zarf-registry pod, eliminating the separate injector pod lifecycle entirely.

This decision resolves all identified overheads:
- Reduces the total number of Zarf ecosystem binaries from 3 to 2 (removing stage1).
- Cuts init cluster configmap load by eliminating fragmented asset storage.
- Trims build pipeline complexity by removing cross-compilation of the stage1 Rust binary.
- Replaces the wasteful full-featured Docker registry with a minimal, single-purpose server that uses far fewer resources.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision 
## Decision

We will introduce unit testing in a defined, limited capacity within Zarf, focused on addressing gaps in our existing end-to-end (e2e) testing coverage and reinforcing the reliability of our public interfaces as Zarf transitions to a reusable library.

### Guiding Principles for Implementation:
1. **Targeted Scope**: Unit tests should only be written for components that fall into one of two high-priority categories:
   - **Hard-to-test via e2e**: Internal utility functions, complex data transformation logic, or edge cases that require isolated conditions impossible to replicate in end-to-end workflows.
   - **Critical library interfaces**: Public API contracts, type definitions, or core library entrypoints that need explicit validation to ensure stability for dependent products.
2. **Interface-First Testing**: All unit tests must validate behavior against defined interfaces or public API contracts, not implementation details. This ensures tests remain effective as the codebase evolves—avoiding brittle tests that break when internal logic is refactored.
3. **Maintenance Guardrails**:
   - Mandate peer review focused specifically on unit test quality to ensure adherence to interface-first principles and prevent over-testing of implementation details.
   - Limit test complexity: Avoid overly elaborate mocks or setup logic that increases long-term maintenance burden; prioritize simplicity and clarity.
4. **Complement, Not Replace**: Unit tests will supplement (never replace) our existing e2e testing strategy. E2e tests will continue to validate end-to-end user workflows and functional integration, while unit tests fill gaps in coverage for isolated or interface-critical code.

### Rationale:
This approach balances the core needs of increased test coverage and interface reliability (critical for Zarf’s new role as a library) with a sustainable maintenance burden. By focusing on high-value, low-overhead areas, we avoid the pitfalls of full-scope unit testing (brittle, obsolete tests) and the complexity of expanded integration testing (heavy orchestration, mock management).

Alternative approaches were rejected for the following reasons:
- **Full Zarf-wide unit testing**: Carries excessive long-term risk, as tests tied to implementation details would quickly become obsolete as Zarf evolves, undermining their value over time.
- **Expanded integration/partial e2e testing**: Fails to efficiently target isolated interface or utility gaps and would introduce unnecessary complexity in orchestration, increasing maintenance overhead without addressing core concerns.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision 
## Decision

We will implement SBOM generation for non-image artifacts by running Syft against explicitly included `files` and `dataInjections` components, plus targeted handling for artifacts explicitly pulled from git repositories (e.g., specific files/directories included via Zarf's git component configuration, not the full repo working directory).

### Rationale
- This approach prioritizes accuracy by only including artifacts that are definitively present in the final Zarf package, eliminating confusing noise from extraneous files (like git index metadata) that would undermine user trust in the SBOM (resolving the critical drawback of Option 1).
- It covers the most common non-image artifact patterns (files and dataInjections) already used in our own init package, delivering immediate value for core use cases.
- By adding targeted git artifact handling, we address the gap in Option 2 without overscanning, ensuring we capture artifacts intentionally included from repos while avoiding irrelevant repo content.
- It avoids the implementation complexity and inconsistent adoption risks of Option 3 (user-driven flexibility), instead providing a reliable, out-of-the-box solution that ensures complete SBOM coverage for standard Zarf package structures.
- This aligns with our core goal of providing users a clear, trustworthy picture of all vulnerabilities across every component in their Zarf packages.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision 
## Decision

We will introduce a **connected deployment mode** for Zarf that enables declarative deployment and upgrade of Zarf packages without requiring a pre-deployed Zarf init package, optimized for connected Kubernetes environments. Key implementation details include:

1. **Mode Activation**: Add a `--connected` flag to the `zarf package deploy` command (and corresponding `connected: true` configuration option in `zarf.yaml`) to enable this mode. When activated, Zarf will bypass mandatory checks for a Zarf init package and skip initialization of disconnected environment-specific components.

2. **Resource Resolution Overrides**:
   - **Image Handling**: In connected mode, Zarf will pull container images directly from their original public/private registries (as defined in the package’s `zarf.yaml`) instead of requiring images to be bundled with the package or hosted in a Zarf-managed registry.
   - **Dependency Management**: Allow packages to reference external git repositories, HTTP endpoints, or cloud-based storage for application code/configuration assets, eliminating the need to bundle these resources or rely on the Zarf git server.
   - **Core Component Skipping**: Omit deployment of Zarf’s disconnected-focused core services (agent, local registry, local git server, etc.) since they are unnecessary in connected environments.

3. **Validation Rule Adjustments**:
   - Relax validation rules that enforce dependencies on init package components when connected mode is enabled.
   - Add connected-mode-specific checks: confirm external registry URLs are reachable, verify access to external git/repo dependencies, and ensure package configurations do not reference disconnected-only resources.

4. **Backward Compatibility**:
   - Preserve full functionality of the existing disconnected workflow (init package required, local resource management) as the default behavior.
   - Allow packages to declare optional connected-mode configurations in their `zarf.yaml` (e.g., a `connected:` section) to override resource resolution logic for connected environments (e.g., specifying alternate external registry paths for images).

5. **Upgrade Support**: Extend connected mode to handle package upgrades by reusing existing connected environment resources, leveraging Kubernetes native upgrade mechanisms, and skipping reinitialization of Zarf core components during upgrade operations.

This approach retains Zarf’s core declarative deployment value proposition while expanding its utility to connected environments, giving users flexible workflow options aligned with their operational context.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision 
## Decision

We will evolve the `scripts` noun into a more flexible, explicit, and extensible system to address unmet team needs while preserving backward compatibility with existing configurations. Key changes include:

1. **Formalize and Expand Script Phases**:
   - Document strict, unambiguous purposes for existing phases (`before`, `after`, `prepare`) and add new standardized phases to cover common gaps:
     - `validate`: For pre-package creation or pre-deployment validation checks that must pass before proceeding.
     - `cleanup`: For post-deployment or post-package teardown tasks (e.g., temporary file removal, environment reset).
   - Each phase will have a defined execution trigger (e.g., `prepare` runs during `zarf package create`, `validate` runs during `zarf package deploy` pre-flight checks) to eliminate confusion about where logic should reside.

2. **Replace Flat Arrays with Structured Script Objects**:
   - Move from simple arrays of script paths/commands to a structured object format for each script. This format will support optional, self-documenting parameters:
     - `command`: The script path or shell command to execute (required).
     - `continueOnError`: A boolean flag to determine if the overall operation should proceed if the script fails (defaults to `false`).
     - `interactive`: A boolean flag to block non-interactive runs if the script requires user input (defaults to `false`).
     - `environment`: A map of custom environment variables to pass to the script execution context.
     - `workingDir`: A specific directory to execute the script from (defaults to the package root or Zarf's temp context).
   - Maintain backward compatibility by implicitly converting legacy array-based script definitions to the new structured format.

3. **Add Dependency and Ordering Controls**:
   - Allow scripts within a phase to declare an `order` integer to enforce explicit execution sequence (lower numbers run first).
   - Support a `dependsOn` field to reference other scripts (by name or phase) to ensure dependent logic runs only after prerequisite scripts complete successfully.

4. **Enhance Error Handling and Visibility**:
   - Capture and log detailed context for each script execution (phase, name, configuration, exit code) to simplify debugging.
   - Add support for script-specific error messages via an `errorMessage` field, enabling teams to provide user-friendly feedback when a script fails.

These changes will resolve the ambiguity and limitations of the original implementation while letting teams adopt new functionality incrementally without rewriting existing package configurations.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision 
## Decision

1. **Default to Memory-Safe Crane Image Loading for Local Docker Daemon**
   We will modify Zarf's local image import logic to use Crane's memory-optimized streaming option when pulling images from the local Docker daemon. This option avoids loading the entire tar stream into memory, eliminating the OOM crashes experienced with large images. While this incurs a minor performance penalty compared to the current unsafe Crane implementation, benchmarks confirm it still outperforms using the Docker CLI/Go client directly (even for large 20GB images, cached imports complete in 2m10s vs. Docker's 5m2s) and aligns with our long-term OCI package schema roadmap.

2. **Add an Optional Flag for Performance-Prioritized Loading**
   To support users with sufficient system memory who prioritize maximum import speed over memory safety, we will introduce a configuration flag (e.g., `--allow-unsafe-fast-import`) that re-enables the original in-memory Crane loading behavior. This flag will be disabled by default to prioritize stability for all users, especially those working with large images.

3. **Reject Legacy Docker Format Tools**
   We will not adopt the Docker CLI or Go client directly for local image imports. These tools produce legacy tar formats that conflict with our upcoming OCI package schema changes (Issue #1319) and deliver consistently slower import performance than the memory-safe Crane implementation, creating both technical debt and a suboptimal user experience.

4. **Rule Out Skopeo as an Alternative**
   Skopeo was evaluated but rejected due to equivalent or worse performance compared to the safe Crane option, with no clear functional advantages over Crane for our specific use case of local image imports.

This approach resolves the critical OOM crash issue, maintains strong performance relative to alternatives, aligns with our future OCI roadmap, and provides flexibility for users with specific resource needs.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision
## Decision

To address the complexities of deploying Big Bang with Zarf, we will implement a dynamic, value-driven Zarf Big Bang package pipeline that leverages Zarf’s native capabilities and Big Bang’s tooling to resolve all identified pain points. The solution includes four core components:

### 1. Auto-Managed Flux Prerequisite
Include Flux as a required, auto-deployed component in the Zarf Big Bang package. Zarf will handle installing the exact Flux version compatible with the target Big Bang release prior to deploying Big Bang itself. This eliminates manual Flux setup for end-users and ensures strict alignment with Big Bang’s dependency on Flux CRDs.

### 2. Dynamic Image/Repository Pruning
Automatically filter out unused images and git repositories to eliminate storage bloat:
- Provide users with a template `values.yaml` to define their desired Big Bang configuration (enabled components, custom settings).
- Use Big Bang’s `helm template` command with the user-provided values to generate complete Kubernetes manifests for the target deployment.
- Parse the generated manifests to extract only the required images and git repositories, then run `zarf prepare prune` to remove all non-essential assets from the package. This reduces the package size from tens of gigabytes to only what’s needed for the specific deployment.
- Dynamically filter Big Bang’s `package-images.yaml` to include only images referenced in the templated manifests.

### 3. Simplified Values Injection
Streamline custom configuration integration using Zarf’s native features:
- Allow users to pass their custom `values.yaml` as a Zarf config input during package creation or deployment.
- Use Zarf’s `set` functionality to inject these values directly into the Big Bang Helm release, avoiding manual edits to Big Bang’s base `kustomization.yaml`.
- Package Big Bang’s base Kustomization as a Zarf component, with overrides driven by the user’s values to enable/disable components and apply custom settings.

### 4. Reusable Package Generation Pipeline
Create a CI/CD pipeline (e.g., GitHub Actions, GitLab CI) for end-users to generate tailored Zarf Big Bang packages:
- The pipeline accepts a user-provided `values.yaml` as input.
- It executes the dynamic pruning and Flux setup steps automatically.
- It outputs a trimmed, ready-to-deploy Zarf package containing only assets relevant to the user’s specific deployment.

### Supporting Documentation
Publish step-by-step guides including:
- Example `values.yaml` configurations for common deployment scenarios.
- Instructions for running the pipeline and deploying the generated package.
- Troubleshooting tips for Flux integration and pruning edge cases.

### Rationale
- **Flux Management**: Abstracts manual dependency setup while ensuring compatibility with Big Bang’s requirements.
- **Dynamic Pruning**: Directly resolves storage bloat by eliminating unused assets, making Zarf packages practical for individual deployments.
- **Simplified Values Injection**: Eliminates complex edits to Big Bang’s base structure by leveraging Zarf’s native config system.
- **Automated Pipeline**: Enables consistent, repeatable package generation, reducing user error and aligning with Big Bang’s deployment best practices.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision 
## Decision

Zarf will implement native support for publishing, storing, and pulling Zarf packages to/from OCI-compliant container registries, and integrate standards-based cryptographic package signing and verification to enforce supply chain security.

### Key Implementation Details:
1. **OCI Registry Integration**:
   - Extend the Zarf CLI with `zarf publish` and `zarf pull` commands that natively interact with popular OCI-compliant registries (including Docker Hub, GitHub Container Registry, AWS ECR, and GitLab Container Registry).
   - Map Zarf package components (bundled container images, binary artifacts, configuration metadata, and Zarf-specific manifests) to OCI specification objects (manifests, layers, config blobs) while preserving the full Zarf package structure and functionality. This leverages existing registry infrastructure for scalable distribution without requiring custom storage solutions.
   - Maintain full backward compatibility for local package builds and manual file transfer workflows, ensuring Zarf’s offline-first capabilities remain intact.

2. **Supply Chain Security via Signing/Verification**:
   - Integrate [Sigstore Cosign](https://sigstore.dev/) to enable package creators to sign Zarf packages during publication, with support for both ephemeral (keyless) signing and traditional asymmetric key pairs to accommodate diverse security needs.
   - Store package signatures alongside the Zarf package in the OCI registry (using OCI manifest annotations or signed artifact manifests aligned with Sigstore best practices) to keep verification artifacts linked directly to the package.
   - Add optional/mandatory signature verification steps to `zarf pull` and `zarf deploy` commands, allowing deployers to validate the origin and integrity of packages before use. This includes support for verifying signatures against trusted identities (e.g., GitHub Actions workload identities, organizational key pairs, or Sigstore’s Fulcio root certificates).

This approach resolves the scalability gap in Zarf package distribution while prioritizing security and trust, fully meeting both the primary and secondary success goals outlined.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision 
## Decision

1. **Add Multi-Purpose Package Registry Proxy/Cache**:
   - Introduce an optional Zarf component (`dev-package-registry`) that acts as a unified proxy and cache for common language package managers (npm, PyPI, Maven, Go Modules, RubyGems, etc.).
   - The registry will mirror upstream packages on first access (when internet-connected) and cache them in Zarf's persistent storage layer for airgap use. It will also support pre-seeding specific packages/versions during Zarf package creation via `zarf package create --dev-packages` pointing to a manifest file.

2. **Extend Git Support for Native Client Access**:
   - Enhance Zarf's existing git server capability to function as a transparent proxy/mirror for native git clients (supporting both SSH and HTTPS protocols).
   - Maintain full backward compatibility with Flux `GitRepository` objects while adding support for generic git clone, pull, and push operations to mirrored repositories.
   - Allow users to define git repository mirrors in the Zarf config (`zarf.yaml`) via a new `gitMirrors` section, which can be pre-populated with content during package creation or set to sync on-demand from upstream when internet-connected.

3. **Simplify Developer Configuration**:
   - Generate auto-configured files (e.g., `.npmrc`, `pip.conf`, `.gitconfig`) that redirect package manager and git client traffic to Zarf's airgap services. Provide a CLI command (`zarf dev configure-client`) to export these files or inject them into development environments (e.g., dev containers, VMs).
   - Implement a single declarative section in the Zarf config (`devAirgapResources`) to define all development-focused airgap resources (package registries, git mirrors) for streamlined setup and management.

4. **Dynamic Resource Management**:
   - Add runtime CLI commands to manage airgap development resources: `zarf dev sync-git-mirror` to manually update a git mirror from upstream, and `zarf dev add-package-registry` to register a new package manager proxy without rebuilding the Zarf package.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision 
## Decision

We will execute a phased, user-centric approach to bring Zarf to a reliable v1.x.x General Availability (GA) release suitable for mission-critical operations, with the following key commitments:

1. **Adopt Strict Semantic Versioning (SemVer) & Freeze Core Interfaces**
   - Immediately enforce SemVer for all pre-v1 releases: breaking changes will only be allowed in minor version increments (e.g., 0.29.x → 0.30.x) with explicit deprecation warnings, migration guides, and at least one minor version of overlap for users to update.
   - Freeze the core public interfaces (CLI commands/flags, `zarf.yaml` schema, Zarf package format, and any exposed SDKs) no later than the 0.30.x release. No further breaking changes to these interfaces will be made before v1.0.0.

2. **Implement a Mission-Critical Focused Test Strategy**
   - Expand automated testing to cover:
     - End-to-end (E2E) tests for high-priority mission-critical deployment patterns (air-gapped Kubernetes clusters, edge infrastructure, stateful workloads like databases, and multi-package dependency chains).
     - Compatibility tests against supported infrastructure versions (e.g., Kubernetes 1.25–1.29, major Linux distributions, cloud provider edge services).
     - Chaos engineering tests to simulate real-world failures (network partitions, node outages, storage disruptions) and validate Zarf’s resilience.
     - Regression tests for all critical bug fixes to prevent recurrence.
   - Establish a ""mission-critical test suite"" that must pass for every release, with failures blocking deployment until resolved.

3. **Establish a GA Readiness Sign-Off Process**
   - Create a v1.0.0 GA readiness checklist that requires sign-off from engineering, product, and user success teams before release. The checklist will include:
     - 100% automated test coverage of core public interfaces.
     - Validation of Zarf in at least two production-grade mission-customer beta environments, with no critical unresolved issues.
     - Complete, up-to-date documentation for all core features, migration guides, and mission-critical troubleshooting workflows.
     - Resolution of all high-severity (P0/P1) bugs reported by users and beta testers.

4. **Post-GA Change Control for Stability**
   - After v1.0.0 GA, implement a mandatory design review process for any proposed changes to core public interfaces. Breaking changes will only be permitted in major version releases (e.g., v2.x.x) with 6+ months of advance notice, detailed migration guides, and a deprecation period in the last minor version of the preceding major release.

This approach will reduce reliance on Zarf lifecycle experts for mission-critical deployments, eliminate unexpected breaking changes, and build user trust in Zarf’s stability for production use cases.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision 
## Decision

Zarf will integrate native, transparent support for ECR as an external registry, addressing both token expiration and push-to-create limitations without requiring package-specific actions or pre-deployment user effort. This support will preserve system-agnostic package design and align with core Zarf functionality.

### 1. Automatic ECR Auth Token Rotation
- Extend the `zarf init` command to deploy a Kubernetes CronJob **only when ECR is selected as the external registry**. This CronJob will run every 11 hours (before the 12-hour token expiration window) to refresh the ECR authentication token.
- The CronJob will use AWS credentials configured during `zarf init` (environment variables, AWS config files, or IAM Roles for Service Accounts (IRSA) for EKS clusters) to fetch a new token via the AWS ECR API.
- The CronJob will update the cluster’s image pull secret linked to ECR, ensuring pods always have valid credentials to pull images without user intervention.

### 2. Automatic ECR Repository Creation
- Modify Zarf’s package deployment pipeline to automatically create required ECR repositories before pushing package images.
- During deployment, Zarf will parse all component manifests to extract image URIs, identify corresponding ECR repository names, and check their existence using the AWS ECR API.
- For non-existent repositories, Zarf will create them with sensible default configurations (private visibility, standard image scanning) that can be customized via optional `zarf init` flags.
- This logic will only execute if ECR is the active external registry, leaving workflows for other registries unchanged.

### 3. Seamless User and Package Creator Experience
- Package creators do not need to modify manifests or add custom actions to support ECR; Zarf’s core logic handles all ECR-specific steps automatically, preserving system-agnostic package design.
- Users only need to select ECR as the external registry during `zarf init` (with valid AWS permissions) and deploy packages as normal—no pre-deployment repository creation or token management is required.

### 4. Permission and Security Best Practices
- Zarf will document the minimum AWS permissions required for `zarf init` (e.g., `ecr:GetAuthorizationToken`, `ecr:CreateRepository`, `ecr:DescribeRepositories`) and cluster-level token rotation (e.g., `ecr:GetAuthorizationToken`).
- For EKS clusters, Zarf will support configuring IRSA for the token rotation CronJob to eliminate long-lived credentials in the cluster, aligning with AWS security best practices.

This approach eliminates workarounds with custom actions, reduces operational overhead for cluster maintainers, and ensures packages remain portable across different registry providers.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision 
## Decision

We will compile the battle-tested Zarf Go transformation library into WebAssembly (WASM) modules and integrate these modules directly into Pepr's TypeScript codebase to handle admission transformations. This approach replaces the need to maintain two separate transformation libraries and decouples the logic from Zarf while preserving its reliability.

### Rationale for Choosing WASM Over Alternatives
1. **Preserves Battle-Tested Logic**: Avoids the risk and effort of rewriting the hardened Go transformation library in TypeScript, ensuring we retain existing reliability and security guarantees.
2. **Low Overhead Execution**: WASM modules execute in-process within Pepr's TypeScript runtime (via tools like Wasmer-JS or Deno's WASM support), eliminating the latency and complexity of inter-process communication (IPC) inherent to gRPC or REST.
3. **Simplified Synchronization**: When the Zarf Go library is updated, we recompile it to a WASM module and distribute the updated module to Pepr. This avoids manual sync of TypeScript and Go codebases and ensures alignment of transformation behavior.
4. **Clean Decoupling**: The WASM module acts as a self-contained, versioned artifact. Pepr can consume it independently without direct dependencies on Zarf's codebase, fulfilling the requirement to decouple transformations from Zarf.

### Implementation Details
- **Go to WASM Compilation**: Use the official Go WASM toolchain to compile the Zarf transformation library into a WASM module, exposing a minimal, typed interface for TypeScript to interact with transformation functions.
- **TypeScript Integration**: Wrap the WASM module in a thin TypeScript layer that handles input/output serialization (converting Kubernetes admission request objects between TypeScript and Go/WASM formats) and error propagation.
- **Versioning**: Align WASM module versions with Zarf library releases to ensure traceability and simplify updates. Include a validation step in Pepr's CI to confirm the module version matches the expected Zarf transformation logic version.
- **Fallback Mechanism**: Add a lightweight TypeScript-based fallback for critical transformation paths (e.g., simple label mutations) to ensure availability in the rare event of WASM runtime issues.

This approach balances performance, maintainability, and reliability while meeting the goal of decoupling transformations from Zarf and aligning Pepr's admission capabilities with the trusted Zarf transformation library.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision 
## Decision

We will standardize package source handling across all core Zarf actions to ensure consistent, predictable functionality for every supported package source type. This resolves current inconsistencies in inspect, remove, list, and deploy workflows while establishing a unified foundation for future package source additions.

### Specific Implementations & Parity Requirements
To achieve this, we will:
1. **Build a unified package source resolver** that all core Zarf actions (deploy, inspect, remove, list) rely on. This abstraction will handle parsing, validation, and retrieval of package metadata/content for all supported source types, eliminating code duplication and ensuring consistent behavior across workflows.
2. **Ensure full parity for applicable actions per source type**:
   - **Local Tarballs (single/split)**:
     - Supported actions: Deploy, inspect, remove (once deployed to cluster), list (once deployed)
     - Resolve gaps: Automatically reassemble split tarballs during inspection; support removing split tarball-deployed packages via either the local source or in-cluster name.
   - **OCI Packages (remote `oci://` or local cached)**:
     - Supported actions: Publish (create), deploy, inspect (remote or local), remove (once deployed), list (once deployed)
     - Resolve gaps: Enable inspecting remote OCI packages by fetching metadata (or full content if needed) without local caching/deployment; ensure authenticated OCI repositories work consistently across all actions.
   - **In-Cluster Deployed Packages**:
     - Supported actions: List, inspect, remove
     - Resolve gaps: Implement inspect functionality to retrieve package metadata (version, components, configuration) directly from the cluster’s Zarf state; ensure remove operations align with other source type workflows.
   - **HTTP(S) URLs (remote tarballs or OCI redirects)**:
     - Supported actions: Deploy, inspect
     - Resolve gaps: Enable inspecting HTTP(S) sources by fetching the remote package (or its metadata) to display details without deployment; validate remote package integrity (via built-in hashes/signatures) during both inspection and deployment.
3. **Define clear action applicability rules** with consistent error messaging for unsupported combinations (e.g., ""Cannot create an HTTP(S) package directly; use `zarf package create` to generate a tarball or OCI package, then host it remotely"").
4. **Maintain backward compatibility**: Existing valid workflows for all source types will continue to function without breaking changes to core command syntax.

### Rationale
- **Eliminates user friction**: Resolves confusion from inconsistent feature support, allowing users to interact with packages uniformly regardless of their source.
- **Enhances security**: Enables pre-deployment inspection of remote packages (OCI, HTTP(S)), letting users verify content integrity and components before installation.
- **Reduces technical debt**: Centralized source resolution logic simplifies future integration of new package sources (e.g., S3 buckets) by ensuring consistent behavior across all actions.
- **Improves transparency**: Clear rules and error messaging help users understand valid operations for each source type, reducing troubleshooting time.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision
## Decision

We will refactor Zarf's component composability pipeline into a unified, centralized system to resolve the three core issues outlined, with the following specific changes:

1. **Scalable Import Chain Processing**
   - Replace shallow import handling with a recursive, depth-first resolution pipeline that fully processes all nested import layers (including imports of imports).
   - Implement a component tracking mechanism to deduplicate shared components across import chains and enforce consistent configuration/versioning for reused components.
   - Expand test coverage to include multi-layer import scenarios (3+ layers deep) to validate correctness at scale.

2. **Unified OCI Skeleton Integration**
   - Abstract component source resolution into a common `ComponentSource` interface that supports both local filesystem components and OCI skeleton components.
   - Modify the core composability pipeline to consume components via this interface, eliminating bolt-on OCI-specific logic and ensuring identical processing for all component sources during composition.
   - Update OCI skeleton creation/retrieval logic to emit components in a structure compatible with the core pipeline, ensuring seamless integration with import workflows.

3. **Centralized Component Filtering**
   - Build a single, stateless `ComponentFilter` service that handles all component filtering (including the `only` flag) using a standardized set of rules.
   - Invoke this centralized filter after collecting all components (base package + all imports) but before final package assembly, ensuring all component sources are subject to identical filtering rules.
   - Remove duplicate filtering logic from individual import/OCI handling paths to eliminate inconsistencies and bugs.

We will also audit the refactored pipeline for edge cases (e.g., conflicting component configurations across imports) and update official documentation to reflect the unified composition workflow for local packages, imports, and OCI skeletons.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision 
## Decision

To streamline the iterative package development lifecycle and reduce the friction of debugging deployment errors, we will formalize and expand the existing YOLO mode into a dedicated, CLI-exposed development workflow:

1. Introduce a new `zarf package dev` command designed explicitly for iterative, connected environment package development. This command will:
   a. Operate in an enhanced YOLO mode that skips full tarball package creation, instead using local resources and cached assets directly to avoid redundant work.
   b. Maintain a persistent local cache of fetched dependencies (Docker images, binaries, Helm charts, etc.) with hash-based validation—only re-fetching assets if their source (e.g., image tag, file content hash, chart version) has changed since the last run.
   c. Support incremental deployments: automatically detect changes to `zarf.yaml`, component manifests, or linked local files, and redeploy only the modified components (or updated parts of components) without requiring a full cluster reset.
   d. Bypass the need for `zarf init` by leveraging the developer’s existing Kubernetes cluster credentials (if available) or auto-provisioning a lightweight ephemeral cluster (e.g., k3d) with minimal overhead for isolated testing.
   e. Include a `--cleanup` flag to remove only the package’s resources from the cluster (instead of deleting the entire cluster) when resetting between test runs, and a `--reset` flag to fully wipe cached assets and cluster state for a fresh start.

2. Enhance the underlying YOLO mode infrastructure to:
   a. Allow selective component targeting via a `--components` flag in `zarf package dev`, enabling developers to test specific parts of a package without deploying all components.
   b. Add real-time validation of `zarf.yaml` changes during development, providing immediate feedback on syntax or configuration errors before attempting deployment.
   c. Maintain compatibility with existing YOLO mode use cases (e.g., testing without heavy dependencies) while extending it to support full package iteration workflows.

3. Ensure backward compatibility:
   a. Keep the existing `zarf package create` and `zarf package deploy` commands intact for air-gapped production workflows, so we do not disrupt established production package creation and deployment processes.
   b. Clearly document the `zarf package dev` command in official documentation, highlighting its intended use for development only and its limitations in air-gapped environments.

This decision directly addresses the pain points of repeated cluster resets, redundant dependency fetching, and full package recreation during iterative development while preserving Zarf’s core value proposition for air-gapped deployments.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision 
## Decision

Migrate the documentation site from Docusaurus 2 to **VitePress** as the primary static site generator and documentation framework, with a phased transition to address performance and dependency management limitations.

### Key Implementation Steps:
1. **Project Initialization**:
   - Set up a new VitePress project in the existing docs repository, preserving the current markdown content directory structure to minimize content migration overhead.
   - Use `pnpm` as the package manager to enforce strict dependency versioning and reduce redundant package installs, improving dependency consistency.

2. **Performance Optimization**:
   - Leverage Vite's native ES module bundling and tree-shaking to cut build times by 40-60% and reduce client-side bundle sizes.
   - Enable VitePress's built-in features: lazy loading for images/code blocks, automatic code splitting, and optimized static asset delivery to improve initial page load speed.

3. **Dependency Streamlining**:
   - Adopt VitePress's minimal core dependency set, only adding lightweight, maintained plugins for critical features (e.g., Algolia search, syntax highlighting) instead of Docusaurus's heavy, monolithic plugin ecosystem.
   - Pin all dependencies in a `pnpm-lock.yaml` file to eliminate version conflicts and simplify future updates.

4. **Feature Parity & Enhancement**:
   - Replicate existing Docusaurus functionality (sidebar navigation, dark mode, versioned docs) using VitePress's native tools or community plugins.
   - Add support for dynamic content (e.g., auto-generated file paths, current version numbers) via VitePress's frontmatter template variables, addressing gaps in static content generation.

5. **Phased Transition**:
   - Run the VitePress site in parallel with the Docusaurus site on a staging URL for 2 weeks to validate functionality, performance, and content accuracy.
   - Redirect all production traffic to the new VitePress site once validation is complete, then decommission the Docusaurus setup.

### Rationale:
VitePress directly resolves the core issues with Docusaurus 2:
- **Performance**: Vite's dev server provides instant hot reloads, and production builds are significantly faster than Webpack-based Docusaurus. Client-side pages load quicker due to optimized, modern asset delivery.
- **Dependency Management**: VitePress has a smaller, more focused dependency tree, reducing the risk of conflicts and making updates more predictable. `pnpm` further enforces clean dependency management.
- **Long-Term Support**: VitePress is built on Vite, the de facto standard for modern frontend tooling, ensuring better compatibility with future ecosystem advancements compared to Docusaurus's older Webpack 5 foundation.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision 
## Decision

1. **Decouple Core Logging from CLI UI Concerns**
   - Define a core logging interface (e.g., `Logger`) in a shared package that abstracts all UI-specific behavior. This interface will expose methods for logging at different levels (debug, info, warn, error) without assuming pterm as the backend.
   - Move all CLI-specific logging (progress bars, spinners, formatted user messages) to a dedicated CLI logging implementation that implements the core `Logger` interface. Core packages will depend only on the core interface, not the pterm-backed implementation.
   - Justification: Eliminates tight coupling between core packages and CLI UI components, making it easier to disable or mock logging in tests and avoid thread-safety issues with global state.

2. **Replace Immediate-Exit Fatal Logging with Structured Error Handling**
   - Remove the program-exit behavior from `message.Fatal` and similar fatal logging methods. Instead, these methods will log the error to the appropriate destination (STDERR + debug log) but return a fatal error object instead of exiting.
   - Implement a top-level CLI error handler that receives all fatal errors from command execution. This handler will trigger any registered cleanup routines, propagate context cancellation (via `context.Context`), and exit the program with the correct exit code after all necessary operations are completed.
   - Justification: Fixes unit testing challenges by allowing tests to catch and assert on fatal errors without the program exiting. Ensures proper cleanup and context handling before termination, preventing resource leaks and incomplete operations.

3. **Eliminate Global State for Progress Indicators**
   - Wrap progress bars, spinners, and other interactive UI elements in injectable service interfaces (e.g., `ProgressTracker`). Functions requiring progress updates will receive an instance of this service via method parameters or `context.Context`, rather than accessing a global singleton.
   - Add configuration options to the `ProgressTracker` interface (e.g., `WithDisabled(bool)`) to easily disable UI output in non-interactive environments, multi-threaded scenarios, or test runs.
   - Justification: Resolves thread-safety issues by removing shared global state. Simplifies disabling progress indicators without complex access patterns, as each service instance can be configured independently based on execution context.

4. **Standardize Log Destinations and Structured Logging**
   - Configure core package logging to output structured JSON logs by default, optimized for machine readability and integration with monitoring tools.
   - Maintain separate log sinks: debug logs (including structured core logs) are written to a Zarf-specific log file, while user-facing CLI output (formatted via pterm) is written to STDOUT/STDERR.
   - Add a flag (e.g., `--log-format`) to override log format for CLI runs, allowing users to switch between human-readable and structured outputs as needed.
   - Justification: Separates concerns between machine-readable audit/debug logs and user-friendly CLI feedback, while adhering to best practices for logging in testable or distributed applications.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision 
## Decision

To address schema consistency, backward compatibility, and establish predictable long-term support for users, we will implement the following changes for Zarf v1:

### 1. Formalize Explicit Stable Schema Versioning
- Introduce `schemaVersion: ""1.0""` as a required top-level field in all v1-compatible `zarf.yaml` files, marking the first stable, versioned schema contract with the user base.
- Remove all pre-v1 deprecated keys scheduled for deletion in v1 from the v1 schema, including:
  - `setVariable` (replaced by `setVariables`)
  - `scripts` (replaced by `actions`)
  - `group` (replaced by functionally distinct `flavor`)
  - `cosignKeyPath` (replaced by create-time key path specification)
- The v1 Zarf binary will reject new package creation attempts using these removed deprecated keys, enforcing strict adherence to the v1 schema for new packages.

### 2. Provide Guided Migration Tools for Pre-v1 Users
- Build a `zarf migrate` command to streamline transitions from pre-v1 `zarf.yaml` files to the v1 schema:
  - For auto-migratable deprecated keys (e.g., `setVariable` → single-item `setVariables`, `scripts` → `actions`), the command will automatically transform fields to their v1 equivalents.
  - For non-auto-migratable deprecated keys (e.g., `group` → `flavor`, with functional differences), the command will emit actionable warnings explaining the behavioral changes and step-by-step guidance for manual adjustment to the v1 schema.
- The migrate command will automatically add the required `schemaVersion: ""1.0""` tag to converted files.

### 3. Maintain Limited Backward Compatibility for Existing Deployments
- The v1 Zarf binary will continue to support deployment of pre-v1 packages (with deprecated keys) to avoid breaking existing airgap and delivery workflows, but will emit prominent, repeated warnings during deployment urging users to migrate to the v1 schema.
- Pre-v1 packages will not receive v1-specific feature enhancements or schema-related security patches.

### 4. Establish a v1 Schema Deprecation Policy
- For any future features deprecated within the v1 schema lifecycle:
  - Deprecated features will remain fully functional for the entire supported duration of v1.
  - Clear deprecation warnings will be emitted at package creation and deployment time, with fully documented migration paths to replacement features.
  - Deprecated v1 features will only be removed in the next major schema version (e.g., v2), with advance notice provided in minor releases leading up to the major version change.

This decision balances the need for a stable, predictable v1 schema with backward compatibility for existing users, while providing clear tools and policies to guide transitions and maintain long-term trust with the user base.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
