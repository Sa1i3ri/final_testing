context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision

We will implement native lazy loading for images in the Storefront using the `loading` attribute provided by modern browsers. This approach will allow us to leverage the browser's built-in support for lazy loading without relying on third-party extensions like ""lazysizes"". 

**Reasons for the decision:**
1. **Browser Support:** Native lazy loading is now widely supported across most modern browsers, as confirmed by resources like caniuse.com.
2. **Performance Improvement:** Lazy loading will improve the user experience by reducing page load time and decreasing unnecessary network requests for images that are not immediately visible.
3. **Simplicity:** Using the native `loading=""lazy""` attribute simplifies the implementation process by removing dependencies on external libraries, which also minimizes maintenance overhead.
4. **Future-proof:** Native lazy loading aligns with web standards and is likely to receive continued browser support and enhancements.

Implementation tasks will include:
- Updating the relevant image tags in the Storefront templates to include the `loading=""lazy""` attribute.
- Testing across supported browsers to ensure functionality and fallback behavior.
- Measuring performance improvements and validating against defined performance goals. 

If any compatibility issues arise in specific use cases or browsers that do not support the feature, fallback mechanisms can be evaluated and implemented as needed.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision

We will set the default value of the `autoload` parameter for `OneToOneAssociationField` and `ManyToOneAssociationField` associations to `false`. This ensures that associations are not automatically loaded unless explicitly required by the application logic.

### Rationale:

1. **Performance Optimization**:
   - Prevent unnecessary SQL joins, reducing query complexity and execution time.
   - Avoid hydrating and processing unused data within the application, improving overall performance.

2. **API Efficiency**:
   - Reduces the size of API payloads by excluding unused associated data.

3. **Explicit Data Loading**:
   - Developers will have finer control over which associations are loaded by specifying them explicitly when necessary. This makes the data retrieval process more intentional and aligned with the application's requirements.

4. **Backward Compatibility**:
   - While the change in default behavior might require adjustments for some legacy implementations, the improvement in performance and efficiency justifies the decision.
   - Clear documentation and migration guidelines will be provided to ensure developers can easily update their codebases if needed.

By adopting this approach, we aim to strike a balance between flexibility for developers and maximizing performance for the application.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision

Implement a tagging and indexing system for ADRs along with a chronological ordering mechanism.

1. **Tagging System**:
   - Assign tags/labels to each ADR (e.g., ""Database"", ""Frontend"", ""Security"", etc.).
   - Tags will allow users to easily filter and locate ADRs related to specific topics or areas.

2. **Index Page**:
   - Create a central ADR index file that lists all ADRs.
   - The index will include the ADR number, title, tags, and date of creation.
   - The list will be presented in reverse chronological order by default so users can easily find and read the latest ADRs.

3. **Search Optimization**:
   - Provide a search functionality that allows users to search ADRs based on keywords or tags.
   - Resolve duplicate matches by including context (like folder/area) alongside titles in the search results.

4. **Git Blame Alternative**:
   - For users looking for the latest ADRs, the index file solves the primary challenge as the reverse chronological list provides the newest ADRs without requiring shell/IDE usage.

5. **Alternative Folder Structure**:
   - Keep the existing area-based folder structure for easy case 3 usage but augment it with the central index and tagging to support cases 1 and 2 effectively.

This multi-pronged approach balances the current folder structure benefits while ensuring an easier and more user-friendly system for locating and reading ADRs based on different needs.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision

Given the challenges and limitations identified with the tested approach utilizing native Vue tools and the Composition API, we will **continue using the current plugin extension system based on the Component Factory** while integrating selective improvements to address some of its drawbacks without fully transitioning to the proposed architecture. Specifically, this decision includes:

1. **Incremental Improvements to the Current System**:
   - Explore ways to compile Vue components at build time instead of runtime where feasible, to improve performance and simplify the workflow.
   - Investigate mechanisms to enhance the developer experience, such as providing better tooling for static analysis, template linting, and debugging within the constraints of the existing system.

2. **Avoiding Non-Update-Safe Use of Vue Internals**:
   - Reevaluate the reliance on internal Vue logic for extensibility as proposed in the new approach since it risks breaking with every Vue update. Instead, stick to public APIs and documented methods to ensure compatibility and maintainability.

3. **Hybrid Approach for Component Extensibility**:
   - Retain the flexibility of the current Component Factory system for plugin developers to extend and customize components but experiment with integrating smaller, native Vue 3 components where the performance and static analysis gains outweigh the development overhead.

4. **Revisit the Architecture in the Future**:
   - Postpone a complete overhaul of the plugin extension system until future Vue releases or significant enhancements in tooling allow for a more seamless transition to native Vue-based extensions without introducing excessive complexity or risks.

This hybrid, incremental approach aims to address known issues with the current system while avoiding the critical feasibility challenges revealed during testing with the new Vue 3-based approach.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision

Adopt a workflow where changelogs and upgrade information are written as individual files associated with each feature, fix, or improvement. This approach decouples the changelog from version numbering and avoids centralized editing of a single changelog file, thus minimizing merge conflicts.

---

### Rationale:
1. **Avoid Merge Conflicts**: By creating individual changelog files for each change, developers work in isolation without causing changes to a centralized file. Merge conflicts in a single file are eliminated.
   
2. **Decoupling from Fixed Version Number**: Each changelog file can be created at the time of development or pull request creation and later aggregated into a release changelog, reducing uncertainty around version numbers.

3. **Synchronize with Ticket States**: Changelog files can include metadata (e.g., Jira ticket number), ensuring clear linkage between changes in the codebase and the state of issues in the task management system.

4. **Trunk-Based Development Friendly**: Fits well with trunk-based development practice, as developers can commit individual changelog files immediately after implementing a change on a feature branch or directly to the trunk.

---

### Implementation:

1. **Changelog Directory Structure**: Create a dedicated folder (e.g., `changelog/unreleased/`) in the repository where developers add individual markdown files for each changelog entry.

2. **File Naming Convention**: Use a consistent naming scheme such as `<ticket-id>_<brief-description>.md` to easily identify files.

3. **Release Aggregator Tool**: Develop or adopt a script/tool (e.g., Python, Node.js) to automatically compile individual changelog files into a single release-level changelog at the point of creating a new release.

4. **Metadata Templates**: Provide a simple markdown template for developers to follow, containing fields like:
   - Description of the change
   - Issue/Ticket ID
   - Contributor name(optional)
   - Any breaking changes or upgrade notes
   
5. **Process Updates**: Update development guidelines to include the creation of changelog files as part of the pull request workflow.

---

### Consequences:
- Developers will need to adhere to the new workflow, which might require some initial training.
- The tool to aggregate changelog files should be maintained to ensure smooth integration with the release process.
- Changelogs will be clearer, easier to manage, and more aligned with trunk-based development practices.

This decision establishes a scalable and developer-friendly solution to the challenges posed by the current changelog process.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision

To address the issues related to outdated data and rules after a delay in `DelayAction`, we will implement a centralized mechanism that ensures data consistency and rule re-evaluation at the time the delayed action is executed. The solution comprises the following components:

### Key Decisions:
#### 1. Snapshot of Data at Delay Initiation
When an action is delayed, we will take a snapshot of the relevant data at the time of delay initiation. This snapshot will be stored in a secure and accessible location (e.g., a database or cache). 

- The snapshot allows the delayed action to always operate on the original data state even if updates occur during the delay.
- This strategy mitigates the risk of data inconsistencies due to changes during the delay period.

#### 2. Rule Re-Evaluation at Execution Time
Before executing a delayed action, the system will automatically re-evaluate the rules based on the most recent state of the system.

- At the time of execution, the rules associated with the delayed action will be fetched dynamically from the rule repository or rule engine.
- If the rule no longer exists (e.g., rules have been deleted or updated), the system will log this and gracefully skip the execution of the delayed action.

#### 3. Dependency Injection for Rules and Data
We will use dependency injection to load the latest rules and data dynamically into the delayed action execution workflow. By decoupling the rules and data from the original action, we enable flexibility and maintain robustness.

#### 4. Expiration Mechanism for Rules/Data
To ensure actions don’t operate on invalid or irrelevant rules/data:
- Each delay will be capped with a maximum expiration time.
- If the delay exceeds the expiration threshold, the system will invalidate the action and log an appropriate warning or error message.

#### 5. Optimized Event Handling
Events with outdated or updated data will include metadata to indicate the freshness of the event details. This metadata will aid in determining whether additional updates or reloading of the snapshot is needed prior to execution.

#### Summary:
- **Improve reliability:** Storing snapshots prevents old or inconsistent data issues.
- **Ensure adaptability:** Rules are dynamically retrieved/re-evaluated, allowing the system to adapt to changes over time.
- **Built-in safeguards:** Introduce expiration limits for delays to avoid processing invalid rules/data.

#### Timeline:
This decision will be implemented iteratively:
1. **Phase 1:** Snapshot implementation and dynamic rule fetching (2 sprints).
2. **Phase 2:** Metadata enhancements to events and expiration mechanism (1 sprint).","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision

The app system will be integrated directly into the Shopware platform as a core feature instead of remaining as a standalone plugin.

### Rationale:
1. **Stability and Reliability**: Integrating the app system into the core of Shopware sends a clear signal to partners, app manufacturers, and users that the system is stable and a critical part of the platform's architecture.
2. **Streamlined Workflow**: Aligning the app system with the Shopware 6 release cycle and upgrade process ensures that partners and app manufacturers can build and maintain apps with confidence in platform compatibility.
3. **Improved User Experience**: Users will no longer need to install an additional plugin to access the app system, simplifying onboarding and reducing the likelihood of errors or incompatibilities.
4. **Consistency Across Environments**: A core integration ensures a consistent extension mechanism for both cloud and on-prem environments without added complexity.

This decision reinforces the maturity and importance of the app system in the Shopware ecosystem while aligning its lifecycle with the rest of the platform for greater adoption and usability.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision

### Adopt a Unified FlowStorer Interface with Configurable Rules

### Rationale:
To simplify the interaction with `FlowStorer` implementations and reduce boilerplate code when adding new storers or bypassing certain events, we will consolidate the various existing interfaces into a single unified `FlowStorer` interface. This will allow for standardized behavior and enable configurable rules to determine whether event data should be stored or skipped.

### Key Points:

1. **Unified Interface:** Replace individual storer interfaces with one generic base interface, e.g., `FlowStorer<T>`. This interface will define core methods for storing scalar values and optionally restoring them.

2. **Configurable Rules:** Introduce a mechanism, such as predicates or configuration-based policies, to manage whether specific events should be stored. Plugins can override these rules without directly modifying the storer implementation.

3. **Reduced Boilerplate:** By consolidating logic into the unified interface and centralizing configuration options, there will be less repetition across different storer implementations, making it easier to add new storers.

4. **Future Scalability:** The unified interface and rule-based approach will provide a consistent foundation for future extensions, supporting both scalar and complex data storage requirements.

---

### Implementation Plan:

1. **Interface Design:** Develop a `FlowStorer` interface with core methods like:
    - `store(EventData data)`
    - `restore(String key)` (optional, for storers with restore logic)
    - `shouldStore(EventData data)` 

2. **Policy Framework:** Introduce a pluggable policy framework where rules for storage can be defined and configured (e.g., via dependency injection or config files).

3. **Migration:** Refactor existing `FlowStorer` implementations to use the unified interface. Verify compatibility by writing tests that ensure storers maintain their existing functionality.

4. **Plugin Integration:** Provide a clear API or extension point for plugins to customize `shouldStore` logic or storage behavior.

This approach eliminates redundancy, improves maintainability, and enhances the flexibility of `FlowStorer` implementations both for developers and plugin users.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision

We will replace the current `sw-text-editor` in the administration with a new text editor by integrating a widely-used, stable, and well-supported library, such as [TinyMCE](https://www.tiny.cloud/) or [Quill](https://quilljs.com/). Both libraries are open-source and offer excellent maintainability, extensibility, and a rich set of features for WYSIWYG text editing.

---

### Rationale:
1. **Ease of Maintenance**: Transitioning to a widely used and actively supported library reduces the technical debt and effort required for maintenance.
2. **Feature Set**: Libraries such as TinyMCE and Quill come with existing, robust WYSIWYG features that address common needs without requiring custom development.
3. **Extensibility**: Both libraries provide an API for customization and extension, allowing developers to integrate additional features as necessary.
4. **Stability**: Using a proven, community-supported library minimizes the risk of bugs or unexpected behavior, improving reliability for end-users.
5. **Documentation and Community**: These libraries are backed by strong communities and comprehensive documentation, ensuring that future developers can easily understand and work with the system.

---

### Implementation Steps:
1. Evaluate and select the best library based on:
   - Fit for the feature requirements.
   - Active development and community support.
   - Performance in the current administration system's environment.
2. Replace the existing `sw-text-editor` with the chosen library.
3. Perform integration testing to ensure stability and compatibility with existing workflows.
4. Provide documentation for future developers on using and extending the new text editor. 

---

### Consequences:
- **Positive**:
  - Reduced maintenance overhead.
  - Improved user experience with a stable and feature-rich editor.
  - Easier to introduce new functionality in the future.
- **Negative**:
  - Initial learning curve for developers unfamiliar with the chosen library.
  - Migration effort to replace the old editor with the new solution.

This approach addresses the immediate issues with the current `sw-text-editor` while providing a scalable solution for future needs.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision
We have decided to implement ""not_specified"" as the default salutation in Shopware 6 for cases where no salutation is provided by the customer or administrator. This change ensures inclusivity and consistency in the handling of unspecified salutations throughout the platform. The system will now default to ""not_specified"" when a salutation is missing, avoiding assumptions about gender or titles, and aligning with modern inclusive practices.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision

Remove the `padaliyajay/php-autoprefixer` from the `theme:compile` process and stop auto-prefixing using this tool. Instead, rely on the updated browser compatibility provided by the `.browserslist` configuration and Bootstrap v5's native support.

### Key Considerations:
1. **Modern Browser Support**: Bootstrap v5 has already dropped support for older browsers, and `.browserslist` defines the supported browsers explicitly. As a result, most vendor-prefixed styles from `padaliyajay/php-autoprefixer` are redundant.

2. **Performance Improvement**: By eliminating the auto-prefixing step using `php-autoprefixer`, the `theme:compile` process will be faster and more efficient, improving performance, particularly in the SaaS offering.

3. **Sustainability**: The static prefixing logic in `padaliyajay/php-autoprefixer` is inflexible and will fail to accommodate future changes in browser support, making it unsuitable for long-term maintainability.

4. **Dynamic Prefixing, if Needed**: If prefixing becomes necessary in specific situations, the JavaScript-based `Autoprefixer` tool (which works dynamically with `.browserslist`) could be integrated into the build process instead of relying on PHP-based libraries. However, this is unlikely to be needed given the current browser support requirements.

Moving forward, the generated CSS will avoid redundant vendor prefixes and will align with supported browser specifications, ensuring a clean and performant build pipeline.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision

To address the issue with pre-release package versions being treated as `<0.0.0` by npm, we will adopt the following strategy:

1. **Avoid Pre-release Versions in Production Dependencies**: Pre-release package versions (e.g., `1.9.0-alpha1`) will be avoided in production environments and critical dependencies. Instead, we will utilize only stable releases (e.g., `1.x.x`, `2.x.x`).

2. **Modify Dependency Management and Tooling Policies**: Update dependency management policies to enforce the resolution of stable versions for all production dependencies. Any pre-release versions will only be used in explicitly marked development or experimental branches, separate from the main production workflows.

3. **Automated Alerts for Insecure Versions**: We will integrate tools or scripts to automatically cross-check package versions against vulnerability databases (e.g., npm audit, Snyk) to ensure secure package usage. This will involve proper tooling configuration to include pre-release versions in vulnerability checks, where applicable.

4. **Testing and Rollout of Pre-release Packages**: Pre-release versions will only be used in testing or experimental environments, with clear documentation and understanding of their security status. They will only transition to production once they are released as stable and any associated issues have been resolved.

5. **Engage with npm Issue Tracking**: Collaborate with the npm community to highlight this issue and recommend changes to npm's versioning behavior, particularly in how pre-release versions influence version comparison and dependency management. While waiting for npm to address this, our internal policies will ensure secure package management as described above.

By adopting these steps, we mitigate the risk posed by overlooking vulnerabilities in insecure packages when using pre-release versions.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision

Introduce proper database indexing to optimize the performance of the SQL query responsible for deleting outdated cart entries.

### Details:
1. **Indexes Creation**: Add an index on the `updated_at` and `created_at` columns to support efficient lookups and deletions.
   - Single index: `CREATE INDEX idx_updated_at ON cart (updated_at);`
   - Combined index: Alternatively, a composite index on both `updated_at` and `created_at` might further improve query efficiency: `CREATE INDEX idx_cart_dates ON cart (updated_at, created_at);`

2. **Update Query**: Modify the query to leverage the new indexes:
   ```
   DELETE FROM cart
   WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
      OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01')
   LIMIT 1000;
   ```
   Modern databases’ query planner will automatically use the appropriate index to make the operation faster.

3. **Test**:
   - Run `EXPLAIN` on the modified query to ensure `possible_keys` reflects the use of the new indexes.
   - Measure the execution time before and after the change to validate the improvement.

4. **Implementation Strategy**:
   - Perform the index creation during off-peak hours to avoid impacting the production environment.
   - If dealing with large datasets, consider creating the index concurrently (if supported by the database, e.g., PostgreSQL's `CONCURRENTLY` or MySQL's `ONLINE`).

By introducing a proper index, the query optimizer can quickly locate the rows that need to be deleted, thereby significantly reducing the query execution time from over 30 seconds to a more manageable duration even in high-traffic environments.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision

Introduce an ""Experimental Feature Flag"" mechanism to allow releasing features in their early, non-final state for users to test and provide feedback while explicitly marking them as unstable and exempt from our backwards compatibility promise.

### Rationale:

1. **Feedback Loop:** By allowing early access to features, customers and users can provide real-world feedback, which helps the team prioritize and extend features more effectively based on actual use cases and real-world needs.
2. **Faster Delivery of Business Value:** Enables incremental delivery of functional components without waiting for the completion of the entire feature, allowing customers to start leveraging the benefits sooner while we continue iterating.
3. **Reduced Risk of Wrong Implementation:** Early user insights can minimize the risk of over-engineering or misinterpreting user needs before broad implementation.
4. **Backwards Compatibility Flexibility:** Marking experimental features ensures flexibility to make changes without violating the backwards compatibility promise, since customers will be aware that experimental flags imply non-final, mutable behavior.

### Key Considerations:

- **Clear Communication:** Experimental features must be explicitly marked and documented so that users understand they are not finalized and may change without notice.
- **Scope Limitation:** The ""Experimental"" status should only be applied to features that truly require feedback for evolving their functionality and design. Mature and stable features should remain outside the scope of this flag.
- **Opt-In Mechanism:** Experimental features should be disabled by default and released only to customers who explicitly opt-in, reducing the chances of confusing or affecting users who do not wish to engage with incomplete features.
- **Usage Metrics:** Add tracking to experimental features to collect metrics and insights on usage patterns, which will complement user feedback and help prioritize further development.

### Implementation:

1. Introduce an infrastructure-level toggle system for experimental features.
2. Establish guidelines for labeling and documenting experimental features, ensuring they are explicitly marked in the UI, APIs, and documentation.
3. Update contracts and terms to ensure that any experimental features released are clearly exempted from backward compatibility guarantees.
4. Develop and communicate internal workflows that specify how to graduate a feature from experimental to stable once it is ready.

This approach balances the need for user feedback and incremental feature delivery with maintaining the integrity of our backwards compatibility promise for stable features.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision 

To address the inflexibility and inefficiency of stock handling in Shopware 6, the following decisions will be implemented:

1. **Introduce an Extensible Stock Handling System:**
   - Implement an abstraction layer for stock handling that allows customizations and extensions. This will enable third-party integrations, such as communication with external ERPs, to replace or augment the stock management process.

2. **Unified Stock Value:**
   - Replace the distinct ""stock"" and ""available stock"" values with a singular, unified stock representation. This reduces complexity and makes the stock management system easier to understand and maintain.

3. **Event-Driven Stock Updates:**
   - Use event-driven architecture to decouple stock updates from the order lifecycle and allow customization of stock increase/decrease logic. Plugins and custom modules can hook into these stock-related events to implement specific business rules.

4. **Optimize Stock Calculations for Large Catalogs:**
   - Optimize the database queries and algorithms used for stock calculation to improve performance for large product catalogs. Consider introducing more efficient caching mechanisms or leveraging asynchronous processes where feasible.

5. **Lazy Evaluation for Order Stocks:**
   - Defer the calculation and deduction of order-related stock changes until they are necessary (e.g., at order completion). This reduces overhead and improves performance for scenarios with many open or pending orders.

This approach ensures that stock handling in Shopware 6 becomes more flexible, customizable, and performant while simplifying its architecture to better support high-volume use cases and integrations.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision
Implement a custom logging configuration in your Symfony project that adjusts the log level for specific exception classes to reduce unnecessary noise in your logs.

1. **Create a Custom Log Handler or Processor:** Extend the functionality of `symfony/monolog-bridge` to check for exceptions of type `ShopwareHttpException` (or other similar exceptions) and adjust their log levels to `notice` instead of `error`.

2. **Configuration-Based Rule List:** Maintain a configuration list (e.g., in `config/packages/monolog.yaml`) that maps specific exception classes, like `ShopwareHttpException`, to desired log levels. For example:
   ```yaml
   monolog:
       handlers:
           main:
               type: stream
               path: ""%kernel.logs_dir%/%kernel.environment%.log""
               level: error
           exception_handler:
               type: fingers_crossed
               action_level: error
               handler: main
               excluded_404s: 
                   # Map specific exceptions to lower log levels
                   - ShopwareHttpException
   ```

3. **Set Log Levels in Middleware or Exception Listener:** If you wish to add more granularity, use middleware or an exception event listener. You can dynamically adjust log levels for exceptions or exclude specific ones from being logged as errors altogether.

   Example of an event subscriber:
   ```php
   namespace App\EventListener;

   use Psr\Log\LoggerInterface;
   use Symfony\Component\HttpKernel\Event\ExceptionEvent;
   use ShopwareHttpException;

   class CustomExceptionListener
   {
       private LoggerInterface $logger;

       public function __construct(LoggerInterface $logger)
       {
           $this->logger = $logger;
       }

       public function onKernelException(ExceptionEvent $event): void
       {
           $exception = $event->getThrowable();

           if ($exception instanceof ShopwareHttpException) {
               $this->logger->notice('Handled ShopwareHttpException: ' . $exception->getMessage(), [
                   'exception' => $exception,
               ]);

               // Optionally, you can stop the exception logging by other handlers here
           }
       }
   }
   ```

4. **Testing and Iteration:** After implementing the changes, monitor your logs to ensure that only the desired exceptions are being logged with adjusted log levels and that the overall log quality improves. Adjust the configuration as needed for additional exception classes over time.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision

Adopt UUIDv7 as the primary key format for the database instead of UUIDv4.

UUIDv7 incorporates a time-based prefix, which improves the efficiency of B-tree indexing by clustering related data closer together. This reduces the fragmentation of database indexes, resulting in more compact indexes, improved lookup speeds, and reduced storage requirements for indexing. Moreover, it retains the uniqueness and scalability benefits needed for integrating multiple data sources. The switch to UUIDv7 also supports better performance in high-insert environments while maintaining compatibility with existing distributed systems.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision
Enable support for custom events in the flow builder, allowing apps to define and extend the list of available events. This will provide better flexibility and customization for app developers, improving their ability to create tailored user experiences within the platform.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision

To enable secure communication between an app and the App Server, we will introduce OAuth 2.0 as an authentication and authorization mechanism. The App Server will act as the resource server, and the client (app) will authenticate itself via an OAuth access token issued by the Shopware backend (acting as the OAuth authorization server). 

This will allow the App Server to securely verify the request's origin and context while ensuring that sensitive operations are performed on behalf of the authenticated customer. 

Here are the key aspects of the decision:

1. **OAuth Authentication Flow:** The app will follow the ""Authorization Code Flow"" to receive an access token. This ensures that only authorized apps with valid credentials can obtain a token.
   
2. **Access Token Verification by App Server:** The App Server will validate the access token received in requests from the app. This validation can either be done via introspection with the Shopware backend or locally (if tokens include details like expiration, user, and signature). 

3. **Scope and Permission Management:** The access token will include scopes and other claims to determine the level of access permitted, such as customer-specific actions or public read-only data access.

4. **Logged-in Customer Identification:** By including customer-related claims in the access token, the App Server can ensure that authenticated customer data is reliably used and protected.

This approach enables secure, standards-based communication directly between the app and the App Server while maintaining flexibility and scalability for future enhancements.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision

We will implement a **Flow Builder Preview** feature that enables users to visualize the sequence of steps and decisions within a flow without executing or simulating the actual actions of the flow. This feature will provide a graphical or detailed walkthrough of how the flow is expected to behave based on its current configuration.

By using the Flow Builder Preview, merchants will be able to:

1. **Understand their flows in greater depth** - The preview will expose the logical structure, steps, and pathways of the flow to help users verify correctness and intent.
2. **Diagnose potential logical issues proactively** - Users will be able to identify misconfigurations, missing conditions, or deviations in the logical structure of a flow before it’s executed.
3. **Reduce troubleshooting time** - By preventing errors from being introduced in the first place, this tool minimizes the overhead of debugging broken or unintended flows in live systems.
   
The preview functionality will focus solely on providing a clear representation of the logic (steps, conditions, outcomes) and will not simulate external dependencies or real-world events (e.g., sending emails, creating database entries). This scoped preview ensures a lightweight and high-performing tool while addressing a key issue in flow building.

By prioritizing this feature, we aim to empower merchants to build reliable and easily understandable flows, fostering confidence in the Flow Builder's usability and reliability.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃÂÃÂ¡ÃÂÃÂ°realÃÂÃÂ¡ÃÂÃÂ± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision
Refactor the URL generation process to decouple it from the `media.loaded` event and centralize it within the `UrlGeneratorInterface` service, leveraging dependency injection.

### Rationale
1. **Separation of Concerns**: The current implementation couples the URL generation logic to the `media.loaded` event, making it harder to test, maintain, and reuse outside of the event context. Centralizing the logic within the `UrlGeneratorInterface` ensures a single responsibility for URL generation.
   
2. **Direct Access**: Moving URL generation to `UrlGeneratorInterface` allows any service or part of the application to rely directly on the `UrlGeneratorInterface` for absolute or relative URLs, instead of requiring the `media.loaded` event to be triggered.

3. **Performance**: Decoupling the URL generation from the event mechanism avoids unnecessary event handling overhead, especially for non-event-driven parts of the application that may require URL generation.

4. **Flexibility**: This change will make future modifications or strategies for generating media paths more modular and easier to implement. New pathname strategies will only need to follow the `PathnameStrategyInterface` without dealing with event subscribers.

5. **Consistency**: Centralizing URL generation in the `UrlGeneratorInterface` ensures uniformity across all parts of the system in how URLs for media and thumbnails are generated.

### Implementation Steps
1. **Refactor the `UrlGeneratorInterface` Implementation**:
   - Move the logic for determining URLs from the event subscriber to the relevant methods in the `UrlGeneratorInterface` implementation.
   - Ensure the methods in `UrlGeneratorInterface` appropriately utilize the `PathnameStrategyInterface` to generate paths, hashes, and cache busters as needed.

2. **Update Pathname Strategies**:
   - Ensure that all implementations of `PathnameStrategyInterface` are aware that they will be called directly by the `UrlGeneratorInterface`.

3. **Remove Event Subscriber Logic**:
   - Remove the existing event subscriber logic that binds URL generation to the `media.loaded` event. Ensure that any dependent code retrieves URLs directly via the `UrlGeneratorInterface`.

4. **Ensure Backward Compatibility**:
   - Provide proper documentation and update any existing services or custom implementations that previously relied on the `media.loaded` event.

5. **Test Thoroughly**:
   - Verify that URLs are generated consistently and correctly across all areas of the system, including frontend, API, and backend usage.

6. **Documentation**:
   - Update the documentation to guide developers on using the `UrlGeneratorInterface` directly and implementing new or custom pathname strategies.

By implementing this decision, the media URL handling process becomes more modular, efficient, and maintainable.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision
Introduce a dedicated mechanism for one-time data migrations that is independent of the indexer. This mechanism will allow developers to define and execute single-use migration scripts explicitly. These scripts will ensure that data updates are performed only once, avoiding unintended re-execution and potential system errors.

The dedicated mechanism could make use of a migration registry where each migration is logged as executed upon successful completion. Future calls to this migration will check against the registry to confirm its prior execution and skip it if it has already been run. This will prevent the duplication of processes like the migration of critical data, e.g., media path information.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision
Introduce a Centralized Experimental Feature Registry to track and manage experimental features across the platform.

The registry will serve as a single source of truth where every experimental feature is explicitly registered with its associated metadata. This metadata can include details such as the feature name, `stableVersion`, current status, related components/modules, and deprecation plans. The registry will ensure easy and reliable identification, tracking, and maintenance of experimental features.

Key points of the decision:

1. **Centralized Registry**:
   - Create a dedicated file or data structure (e.g., JSON, YAML, or database) to maintain a clear mapping of experimental features and their related code components. This registry will be committed to the repository and updated regularly.
   - Metadata for each feature can include:
     - Feature name or identifier
     - `stableVersion`
     - Status (e.g., ""active"", ""extended,"" ""deprecated,"" ""killed"")
     - Related components/files/modules/packages
     - Date of introduction and expected deprecation timeline
     - Associated feature owners or teams

2. **Enhanced Annotation for Experimental Code**:
   - Extend the `@experimental` annotation in the codebase to include a reference to the experimental feature's unique identifier from the centralized registry.
   - For example: `@experimental(featureId = ""feature-name"")`.
   - This will allow developers to programmatically link code sections to the corresponding entry in the registry.

3. **Automated Tools for Maintenance**:
   - Develop tools or scripts that leverage the centralized registry to automate:
     - Detecting and updating `@experimental` annotations when extending an experiment (e.g., updating the `stableVersion`).
     - Identifying code components that are eligible for deprecation when a feature is marked as ""killed.""
     - Generating warnings or reports for outdated or inconsistent annotations during code reviews or CI pipelines.
   - Create IDE plugins or integrations where feasible to improve discoverability and correctness of experimental feature usage.

4. **Documentation and Communication Standards**:
   - Maintain clear developer documentation on how to register features, update the registry, and use the extended `@experimental` annotation consistently.
   - Ensure documentation reflects the process for extending experiments, deprecating killed features, and maintaining compliance with the experimental feature rules.

5. **Backlog Refactoring**:
   - Conduct an initial audit of the current experimental features in the platform and populate the registry with existing data. This one-time effort will create a baseline for managing existing experimental features under the new framework.

### Rationale
- A centralized registry ensures a complete and transparent overview of all experimental features, reducing fragmentation and improving traceability.
- By linking experimental code to a central registry, developers can efficiently locate and update experimental code sections when needed, reducing the risk of errors or missed updates.
- Automation of repetitive tasks like metadata updates, deprecation tracking, and compliance checks can save significant developer effort and improve process reliability.
- Clear documentation and communication result in a shared understanding of experimental feature management across teams, fostering better collaboration.

### Trade-offs
- This approach introduces overhead in maintaining an additional registry and tooling, which could initially slow down the adoption among teams.
- During the migration phase, developers will need to invest time auditing and refactoring legacy experimental features to align with the new structure.

### Conclusion
Implementing a Centralized Experimental Feature Registry along with enhanced annotations and developer tools will address the challenges associated with tracking, updating, and maintaining code related to experimental features. By streamlining this process, we can improve development speed, accuracy, and alignment with experimental feature management guidelines.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision
Introduce unique human-readable identifiers for payment and shipping methods in the system. These identifiers will be globally consistent and immutable once assigned, allowing extension developers and app servers to directly reference these methods without requiring additional calls to the Shopware Admin API. This will simplify the process of identification, reduce API overhead, and improve overall system efficiency.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision

Remove unnecessary custom SCSS and leverage Bootstrap utility classes for simple styles like spacing, borders, and padding.

### Rationale
1. **Maintainability**: Using Bootstrap utilities reduces the need for custom SCSS, which simplifies the codebase and makes maintaining the Storefront easier.
2. **Consistency**: Aligns the UI with Bootstrap's design system, ensuring a consistent appearance across the application.
3. **Flexibility**: Developers can still add semantic class names for better extensibility and theming, even when leveraging Bootstrap utilities.
4. **Efficiency**: Reduces redundant styling by favoring existing solutions in Bootstrap, making the application more lightweight and performant.

### Implementation Guidelines
1. **Replace Simple Styles**: 
   - Replace all custom SCSS styles that can be achieved through Bootstrap utility classes.
   - Example:
     ```diff
     - <div class=""register-login-collapse-toogle""><div>
     + <div class=""register-login-collapse-toogle mb-3""><div>
     ```

2. **Use Semantic Classes for Extensibility**:
   - Add additional semantic CSS classes for elements to allow future theming or customization, even if no associated SCSS exists initially.
   - Example:
     ```html
     <div class=""border p-3 mb-3 shipping-modal-actions"">
         <button class=""btn btn-light shipping-abort"">Abort</button>
         <button class=""btn btn-primary shipping-submit"">Submit</button>
     </div>
     ```

3. **Evaluate Case-by-Case**:
   - For complex layouts where Bootstrap utilities would lead to overly verbose or unclear HTML, continue using custom SCSS selectively.

4. **Document Changes**:
   - Update development guidelines to encourage using Bootstrap utilities for simple custom styling tasks, along with adding semantic classes for extensions.

5. **Review Process**:
   - As part of code reviews, ensure newly introduced styles are first evaluated for Bootstrap utility compatibility before adding custom SCSS.

### Consequences
- **Positive**:
  - Reduced technical debt and easier debugging.
  - Faster development due to reusable Bootstrap utility classes.
  - Improved adherence to a consistent design system.
- **Negative**:
  - Initial time investment to refactor existing SCSS where applicable.
  - Developers may need to familiarize themselves with Bootstrap utility classes to fully benefit from this approach.

This decision strikes a balance by simplifying unnecessary customization while allowing flexibility for future themes and complex layouts.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision

Adopt Playwright as the framework for the new E2E test suite.

### Rationale 

1. **Deterministic Tests Across Any Environment**: 
   Playwright supports creating isolated test environments with its built-in ability to run tests in parallel, control browser contexts, and manage test states. This makes it easier to avoid dependencies on the shared test environment, ensuring deterministic test results. Additionally, it supports testing across multiple environments, including modern browsers, mobile emulators, and cloud-based environments.

2. **Fast and Reliable Tests**:
   - Playwright provides enhanced speed through features like headless browser operation and efficient selector engines. Tests created with Playwright are typically faster than Cypress since it utilizes lower-level browser protocols.
   - Features like automatic retries and trace viewer make debugging straightforward and help address test flakiness.

3. **Structured and Strategic Testing**:
   - Playwright encourages the use of Page Object Model (POM) or similar design patterns, which promotes consistency in test creation. 
   - Support for test annotations (e.g., 'skipped' or 'focused' tests) allows teams to enforce a structured testing strategy.

4. **Alignment with Product Requirements**:
   - Test scripts can directly validate user behavior based on product requirements. Playwright's detailed API allows thorough interaction simulation with the application and validation of state changes.
   - It is possible to integrate Playwright with behavior-driven development (BDD) tools like Cucumber for writing tests in a behavior-based syntax tied to requirements.

5. **Ease of Use with Readable Syntax**:
   - Playwright has a modern API with a clean and intuitive syntax, making it easier for both developers and non-technical actors to read and understand test scripts.
   - The auto-waiting mechanism reduces the complexity of manual wait handling, making tests easier to write and maintain.
   - For non-technical stakeholders, the readable syntax combined with additional reporting tools (e.g., Allure Reports) can provide understandable test insights.

6. **Cloud Testing Capabilities**:
   Playwright integrates seamlessly with cloud providers (e.g., BrowserStack, Saucelabs) for cross-browser and cross-environment testing.

### Tradeoffs 

- **Learning Curve**: Current team members familiar with Cypress may undergo a transition period to learn Playwright. However, this is mitigated by Playwright's comprehensive documentation and intuitive API.
- **Dependency on Node.js**: As a Node.js-based tool, teams may need to ensure proper Node.js runtime environments are set up and maintained.

### Next Steps

1. Set up a Playwright prototype to validate its fit for our use case.
2. Establish a testing strategy, potentially including a test pyramid approach, to maximize efficiency and coverage.
3. Train the team in using Playwright and standardize test patterns, such as Page Object Models or Cucumber BDD.
4. Integrate Playwright into the existing CI pipeline and configure it to work across test and production environments.
5. Retire the existing Cypress test suite using a phased migration plan.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision
Implement a transactional wrapper mechanism for flow actions to ensure that database interactions are handled consistently and safely.

1. A transactional wrapper will automatically manage the lifecycle of transactions (start, commit, and rollback) for flow actions that need to interact with the database. This removes the responsibility from individual flow actions.

2. Modify the flow dispatcher to detect and properly handle transaction failures. If an error occurs when committing a transaction, the flow dispatcher will log a detailed error message and ensure that the connection is safely rolled back.

3. Introduce the use of save points in the connection configuration to support nested transactions reliably. This would allow individual flow actions within a larger transaction context to fail safely without impacting the overall transaction.

4. Ensure that the calling code will validate transaction success or failure explicitly by checking transaction states, preventing silent failures or unintended commits.

This approach will improve the robustness of the flow execution in terms of transaction management and provide better error visibility for debugging and resolution.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision

**Keep the Compatibility Mode Enabled Temporarily During the Transition Period**  

Given the described context, we will retain the Vue 3 compatibility mode in the administration during the transition period to ensure a smoother migration process. This allows for:  

1. **Easier Plugin Migration**: Plugins dependent on Vue 2 features can continue functioning until they are updated, reducing disruption for plugin developers and maintainers.
2. **Incremental Migration**: Separating the migration of administration and plugins into two phased efforts makes the transition process more manageable and reduces the overall risk of introducing breaking changes at once.
3. **Reduced Development Overhead for the Major Release**: Keeping compatibility mode enabled ensures fewer immediate adjustments to existing codebases, focusing efforts on core application updates first.

However, we will ensure this is a **temporary measure** by setting a clear timeline or plan to fully disable compatibility mode. This includes:  

- **Revising Plugin Development Guidelines**: Clearly communicating to plugin developers that maintaining compatibility with Vue 3 is necessary going forward.  
- **Phased Deprecation Plan**: Setting defined milestones for removing compatibility mode, allowing adequate time for both administration and plugins to complete migration.  
- **Establishing Monitoring and Testing**: Providing sufficient testing coverage and monitoring to ensure the eventual removal of compatibility mode does not result in degradation of functionality.  

In conclusion, the compatibility mode will remain enabled during the current transition period, but with proactive tracking and communication to facilitate its eventual, timely removal.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision 
We will implement a temporary patch to address the getter reactivity issue in Vuex `4.1.0` while waiting for the Vuex maintainers to provide an official fix. This patch will target the specific circumstances causing the bug without affecting broader functionality. Additionally, we will closely monitor the progress of Vuex maintainers and upgrade to the fixed version when available.

If the severity or frequency of the issue increases, we will evaluate alternative state management solutions, such as Pinia or Redux, to mitigate long-term risks. Documentation and tests surrounding the patched area will be updated to ensure visibility and to assist in seamless removal of the patch in the future.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision

Switch to an **Event-Driven Architecture** for extending and customizing functionality.

### Reasoning:
To address the challenges highlighted in the context, moving to an Event-Driven Architecture (EDA) offers the following benefits:

1. **Backward and Forward Compatibility:**
   - By introducing an event-driven system, customization and extension can be achieved by listening to specific events and implementing handlers. This reduces the need to modify core logic and ensures better backward and forward compatibility because core processes remain isolated from third-party extensions.
   - Changes to the core can occur independently without directly impacting third-party handlers, as long as the existing event contracts remain stable.

2. **Process Extension:**
   - Events inherently allow subprocesses to be extended by triggering custom logic based on emitted events. Third-party developers can hook into specific points in the process flow without needing the core system to expose additional interfaces or abstract classes.
   - For example, instead of creating new adapter layers or abstract factories for subprocesses, developers can write a custom event listener or subscriber.

3. **Reduction in Code Proliferation:**
   - By leveraging event dispatchers and handlers, the need for excessive interfaces and abstract design is reduced. Developers focus on implementing event-specific logic instead of dealing with class hierarchies.
   - Declarative event mappings make the system easier to understand and maintain, reducing the cognitive load for developers.

### Key Elements of the Decision:
- Implement a centralized **Event Dispatcher** in the core system.
- Define a clear contract for events (e.g., event names and payload structures). Ensure these are well-documented for third-party developers.
- Scenarios requiring extensions are transitioned to emit specific events that allow listening or overriding.
- Adhere to SOLID principles (particularly Open/Closed Principle) to ensure the architecture remains scalable.

### Trade-offs:
- **Initial learning curve:** Developers familiar with decoration and adapter patterns may need time to understand event-driven principles.
- **Performance:** Asynchronous event-driven systems can introduce performance bottlenecks if not carefully designed, especially if handlers are long-running or unoptimized.
- **Debugging complexity:** Debugging an event-driven flow may be slightly more complex due to non-linear execution paths.

### Next Steps:
1. Introduce an Event Dispatcher library or service into the application (e.g., Symfony Event Dispatcher or a custom lightweight implementation).
2. Identify key areas in the system where event hooks can replace existing decoration or adaptation patterns.
3. Provide third-party documentation and example listeners or subscribers to facilitate a smooth transition.
4. Gradually refactor older components to reduce dependency on decoration, adapter, and factory patterns.

The adoption of Event-Driven Architecture will mitigate current challenges while ensuring better scalability and maintainability of the system.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision

We will introduce a new Jest runner configuration specifically designed to run component tests without compat mode enabled. This allows us to systematically verify the behavior of our components under the desired settings and transition away from relying on compat mode.

Key steps:

1. **Create a custom Jest runner**:
   - Configure a new Jest runner with compat mode explicitly disabled.
   - Ensure the environment setup aligns with the requirements of our components when compat mode is turned off.

2. **Integrate the new Jest runner**:
   - Gradually migrate existing tests to utilize the new runner.
   - Mark tests that are being run under the new configuration for clarity (e.g., by grouping them or annotating appropriately).

3. **Parallel Testing**:
   - For a transition period, run the tests in both modes (enabled and disabled compat mode) to confirm compatibility and detect any regressions.

4. **Phase-out old compatibility mode usage**:
   - Once tests pass consistently under the new Jest runner, deprecate and eventually remove the usage of compat mode entirely across the relevant test files.

5. **Document the transition**:
   - Update documentation to capture the reasoning behind disabling compat mode and outline how to configure new tests using the updated Jest runner.

This decision ensures a smooth migration while maintaining confidence in the stability and correctness of the components across environments.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision

Implement a more robust and granular health check mechanism for the software system.

Each component of the system should have its own dedicated health check to monitor its specific functionality and dependencies. In addition to these individual checks, an aggregated system-level health check should be designed to provide a summary view of the overall system health. The health checks should differentiate between critical and non-critical issues to avoid unnecessary rollbacks or delays when non-essential components encounter problems.

Furthermore, automate health check validation within the CI/CD pipeline to detect and resolve errors before the system goes live. Periodically conduct real-time health check drills in staging environments to simulate failures and ensure health check mechanisms are reliable.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision

We will extend the scope of `TestPreparationStartedSubscriber` to allow additional namespaces beyond `Shopware\Tests\Unit` to be considered for enabling the major feature in the unit test suite. This will involve introducing a configuration mechanism or a more flexible namespace-checking logic to accommodate various test scenarios while maintaining strict control over namespace validation for stability and consistency.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision

We will use the [vue-codemod](https://github.com/vuejs/vue-codemod) tool to automate the migration of our Vue.js application from the Options API to the Composition API. This tool provides scripts specifically designed to refactor Vue 2 Options API components to the Vue 3 Composition API, ensuring consistency, reducing manual effort, and minimizing the risk of errors during the migration process.

This decision ensures that we can modernize our codebase efficiently while leveraging community-tested tools to facilitate the transition. After the automated migration, a thorough review and testing process will be conducted to address any edge cases or issues that the codemod tool may not handle.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃÂÃÂ¡ÃÂÃÂ¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃÂÃÂ¡ÃÂÃÂ¯s templating and reactivity model.","## Decision

**Transition from TwigJs to Vue.js: Replacing the Existing Block System**

To modernize and enhance the frontend architecture, the decision is to replace the current TwigJs-based block system with a native Vue.js-based approach by introducing two new components: `sw-block` and `sw-block-parent`. This decision is motivated by Vue.js' superior reactive, component-based system which aligns with the needs of modern frontend development.

1. **Introduction of `sw-block` and `sw-block-parent`:**
   - `sw-block`: Acts as a template for reusable content, ensuring modularity and separation of concerns. 
   - `sw-block-parent`: Provides the structure to allow overriding or extension of content dynamically within the Vue.js reactivity model.

2. **Enhanced Flexibility and Extensibility:**
   - Vue.js’ templating capabilities allow for a more dynamic and reactive approach to content rendering.
   - Plugin developers can now override or extend components easily by leveraging Vue.js' slot system, props, and reactive data handling, instead of relying on TwigJs inheritance mechanisms.

3. **Replicating Legacy Features While Introducing Improvements:**
   - Preserve the main benefits of TwigJs inheritance mechanisms to ensure backward compatibility in functionality.
   - Enhance these features by adding support for dynamic updates and real-time reactivity that Vue.js provides natively.

4. **Phased Migration Plan:**
   - Incrementally refactor legacy TwigJs-based implementations to the new Vue.js approach.
   - Provide a temporary adapter or compatibility layer to ensure a smooth transition for existing plugins.

5. **Anticipated Benefits:**
   - Simplified development process with Vue.js’ modern tooling and ecosystem.
   - Improved maintainability and scalability of the frontend architecture.
   - A more intuitive developer experience with Vue.js component-based architecture and its supporting documentation.

6. **Trade-offs and Limitations:**
   - The transition will require an initial development effort to migrate existing blocks and ensure compatibility.
   - Developers familiar with TwigJs may need training or guidance to adapt to Vue.js practices.

This strategic shift aims to future-proof the frontend architecture by leveraging Vue.js' versatility, efficiency, and popularity in the developer community.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃÂÃÂ¡ÃÂÃÂ¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃÂÃÂ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision

### Decision

Remove the asterisk `*` next to the product prices in the Storefront and provide the tax and shipping information in a more user-friendly and clear manner within the user interface.

---

### Reasoning

This change will simplify the price display, make the storefront visually cleaner, and reduce potential confusion for users caused by the presence of the asterisk. Instead of relying on a footer note, we will integrate clear and contextually appropriate explanations for tax and shipping costs into the buying process or near the price display as needed. 

Benefits:

1. Improved user experience by presenting prices in an uncluttered and straightforward way.
2. Easier comprehension, particularly for users unfamiliar with the asterisk and its associated details in the footer.
3. Increased trust and transparency by providing relevant pricing information where it is most needed (e.g., on product pages or during checkout). 

Implementation details can be iterated with stakeholders to ensure compliance with legal and user expectations.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision

We will replace the current DomPDF library with an alternative PDF generation library that supports accessibility (A11y) features and complies with PDF/UA (Universal Accessibility) standards. 

### Rationale
1. **Compliance with WCAG and Legal Requirements**: By switching to an A11y-compliant PDF generation library, we ensure that our solution adheres to WCAG and other accessibility regulations.
2. **Enhanced Usability**: Creating accessible documents will improve usability for all users, particularly those with disabilities.
3. **Wider Access to Tools**: Using a library with built-in accessibility features reduces the complexity of manually retrofitting PDFs for compliance.
4. **Future-Proofing**: Aligning with accessibility standards now will prevent technical debt and position us for smoother compliance with evolving regulations.

### Potential Alternatives
1. **wkhtmltopdf**: Supports proper tags and semantics in PDFs, allowing for better accessibility.
2. **PDF/UA-compliant libraries like PDFtron or Apache PDFBox**: These alternatives are known for better A11y support and can create compliant documents.
3. **Custom Accessibility Layer on DomPDF**: While possible, extensive development would be required to update DomPDF to be A11y-compliant, making this less practical.

### Actions
1. Research and evaluate suitable libraries that are PDF/UA-compliant.
2. Conduct a proof of concept with one or two selected libraries to validate performance and features.
3. Plan and implement a transition to the selected library.
4. Update existing tests and documentation to reflect the change.
5. Train the development team on the use of the new library and accessibility best practices in PDF generation.

By proactively addressing this issue, Shopware 6 will provide users with accessible, legally compliant document solutions.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision  
Adopt a controlled rule extension mechanism by introducing a unified rule interface and leveraging a configuration-driven approach to manage rule definitions.  

1. **Restrict Unchecked Extensions:** Transitions for creating and extending rule classes will go through a centralized and well-documented interface to prevent unrestricted modifications and enforce consistent behavior. This ensures the architecture is not unnecessarily convoluted by overly specialized or redundant rule implementations.  

2. **Configuration-Driven Styling:** Define rules through configuration files (preferably using JSON or YAML), which allow developers to control and adjust rules within boundaries while still maintaining flexibility. This would abstract the implementation details from developers unfamiliar with coding the full class structure.  

3. **Encapsulation and Simplification:** Create a wrapper for managing rule-related operations while hiding internal complexities. An abstraction layer should cater to common use cases and new constraints without tampering with the core logic every time.  

4. **Version Control Over Rules Extensions:** Introduce versioning to the rule configuration, ensuring seamless transitions for future updates and backward compatibility in required cases.  

5. **Dynamic Loading for Testing:** Integrate a testing system to simulate rule behaviors dynamically without actually updating the live rule set—this supports safer experiments without affecting production setups.  

By implementing these constraints and mechanisms, the rule system will become more maintainable while reducing unnecessary extensibility that leads to technical debt. This controlled strategy balances the need for flexibility with long-term sustainability.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision

Deprecate and remove the `iterator.helper.js` helper class and switch to using native JavaScript iteration methods directly in the codebase.

### Rationale:
1. **Reduced Complexity**: By removing this abstraction layer, the complexity in the codebase is reduced, and developers can use native JavaScript iteration methods (e.g., `for...of`, `.forEach`, `for`, etc.) depending on the type of data structure being used, leading to clearer and more direct code.

2. **Standardization**: Native iteration methods are part of the JavaScript language standard, well-documented, and widely understood by developers. This ensures uniformity across the codebase and eliminates the need to learn, document, and maintain a custom abstraction (`Iterator.iterate`).

3. **Improved Maintainability**: Using native constructs instead of a custom helper makes the code easier to maintain, debug, and refactor as it reduces dependencies and simplifies understanding for all contributors.

4. **Avoid Confusion**: Removing the iterator helper ensures developers do not have to decide between using the custom iterator or native methods, eliminating confusion caused by inconsistency in usage.

5. **Performance and Efficiency**: By directly using the appropriate iteration mechanism for each data type, you avoid the additional overhead of a helper and ensure that the most efficient native iteration method is applied directly.

6. **Deprecation Plan Favors Gradual Transition**: To ensure a smooth transition, the codebase can be refactored gradually to replace usages of `Iterator.iterate()` with equivalent native methods. During this period, the iterator helper can be deprecated and marked for removal in a specific future release.

### Action Plan:
1. **Update Documentation**: Communicate the decision to deprecate and remove `iterator.helper.js` to the development team. Update any internal documentation to advise developers to use native iteration methods.
   
2. **Mark as Deprecated**: Add a deprecation warning in `iterator.helper.js` to notify developers when it is used in the codebase.

3. **Gradual Codebase Refactor**:
   - Replace usages of `Iterator.iterate()` with equivalent native loops in existing files.
   - Ensure the refactored code uses the most appropriate iteration method depending on the data structure (e.g., `for...of` for `Maps`, `.forEach` for `NodeList`).

4. **Remove in a Future Release**: Completely remove `iterator.helper.js` in an agreed-upon future version of the software, ideally after ensuring all usages have been removed in the codebase.

5. **Testing**: Conduct regression tests after replacing the iterator usage to ensure functionality is preserved.

By adopting this decision, the codebase will be leaner, aligned with JavaScript standards, and easier to maintain in the long term.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision

**Decouple Flow Execution from Business Process**

Instead of executing flows within the business process, flows shall be executed asynchronously after the completion of the business process. The event-driven architecture currently used will remain, but the execution of flows will be delegated to dedicated event listeners and managed outside the immediate lifecycle of the business process.

### Justification:

1. **Error Isolation**: Decoupling flow execution ensures that errors in the flow logic do not interfere with the core business process. Fatal errors during flow execution will no longer have the potential to cancel or disrupt the primary user-triggered business process.

2. **Performance Improvement**: Offloading flow execution from the business process will significantly improve user experience by reducing the latency of critical paths like checkouts or product updates. Expensive operations like sending mails or performing external API calls will no longer delay business process completion.

3. **Enhanced Debugging**: By detaching flow execution from the `EventDispatcher` decorator and assigning a dedicated listener for flow-related events, debugging and tracing issues become more straightforward. This will reduce stack trace complexity and improve developer productivity in resolving flow-related issues.

4. **Scalability and Extensibility**: Moving flows into their own execution environment opens pathways for future improvements, such as running flows in parallel, adding retries, implementing rate limiting, or even executing flows in distributed environments. This aligns with any potential need to scale the solution as the number or complexity of flows increases over time.

5. **Experimental Flexibility via Feature Flags**: The use of the feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` ensures the decision can be tested and validated experimentally without disrupting the existing production workflow. This allows for phased rollouts and iterative feedback before full implementation.

By implementing this approach, the system will achieve improved robustness, performance, and maintainability.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision

We will use the **Architectural Decision Record (ADR)** format as proposed by Michael Nygard. The structure of the ADR will follow a standardized template to ensure clarity, consistency, and ease of maintenance.

### Key Reasons for This Decision:
1. **Clarity and Consistency**: The ADR format provides a well-defined structure that makes it easy to document and understand decisions across the team.
2. **Widely Adopted**: ADRs are a widely recognized format in the software engineering community, making it easy to share knowledge with new team members, collaborators, or external stakeholders.
3. **Lightweight and Flexible**: The ADR format is simple, requiring minimal overhead while remaining flexible to project needs.
4. **Versioning and Traceability**: Using ADRs allows embedding decision documentation in the project repository, aiding version control and facilitating easier history tracking.

### Structure of the ADR:
Each ADR will include:
1. **Title**: A clear and concise title summarizing the decision.
2. **Context**: A description of the problem or issue being solved, including relevant background information.
3. **Decision**: The decision made regarding the problem, clearly stated.
4. **Consequences**: The outcomes, trade-offs, and implications (both positive and negative) of the decision.
5. **Status**: The state of the decision (e.g., ""Proposed"", ""Accepted"", ""Superseded"", etc.).
6. **Date**: The date the decision was made or updated.

### Example ADR Template:
```
# [Short Title of Decision]

## Context
[A detailed description of the issue or problem.]

## Decision
[The specific architectural decision chosen.]

## Consequences
[The implications, trade-offs, benefits, and risks of the decision.]
```

By adhering to this format, we ensure that architectural decisions are well-documented and accessible throughout the development lifecycle.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision

Establish a new ""Repository Toolkit"" library that provides reusable, modular, and well-structured utilities for building and managing TUF repositories. This toolkit will complement the Metadata API and address the issues found in the legacy python-tuf implementation.

### Key Decisions:
1. **Modular and Cohesive Design**  
   - Build the toolkit as a set of modular components, each focusing on distinct repository management tasks (e.g., creating, updating metadata, signing, etc.).
   - Ensure each component has a clear and minimalistic API that avoids excessive arguments and dependencies.
   - Avoid reliance on global variables; use well-encapsulated state management techniques.

2. **Separation Between Library and Application Code**  
   - Ensure the toolkit strictly operates as a library with no application-specific code embedded in its functionality.
   - Provide support for applications to execute repository changes but avoid making ""high-level application choices"" (e.g., loading all metadata or enforcing specific usage patterns).

3. **Optimize for Scalability and Customization**  
   - Design the infrastructure to work efficiently with large repositories (e.g., avoid mandatory loading of all metadata files).
   - Allow developers to retain control over architectural choices (e.g., how metadata is loaded or cached).

4. **Minimize Maintenance Complexity**  
   - Reduce the size and fragmented nature of the codebase by reorganizing and streamlining functionality into smaller, reusable modules.
   - Provide clear documentation and examples for developers to encourage adoption and collaboration.

5. **Gradual Deprecation of Legacy Code**  
   - Clearly mark legacy modules (`repo.py`, `repository_tool.py`, `repository_lib.py`) as deprecated.
   - Provide migration guides and tools that simplify transitioning to the new repository toolkit while maintaining compatibility where necessary.

By developing this new repository toolkit, the implementation will achieve better maintainability, scalability, and developer experience while reducing the limitations associated with legacy codebases.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision

We will incrementally migrate the Primer React components codebase from JavaScript to TypeScript.

Rather than maintaining a separate ambient declaration file (`index.d.ts`), we will rewrite the implementation code of Primer React components in TypeScript, embedding type definitions directly into the source code. This approach ensures that the type definitions remain accurate and up-to-date as they are inherently tied to the component implementation. 

The migration will be conducted incrementally to minimize disruption to dependent teams and ensure we can address any unforeseen issues that arise during the transition. Components with higher usage priority or frequent type-related bug reports will be targeted first.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision

Adopt the pattern of creating components with `Box` going forward.

---

### Rationale

1. **Consistency across the codebase**: Using `Box` establishes a single, uniform approach for component creation, reducing fragmentation in design patterns and making the codebase easier to understand and maintain.
   
2. **Better alignment with Primer ecosystem**: `Box` is a core abstraction in Primer React that encapsulates styling capabilities. This aligns the components with Primer’s system properties (`sx`) and supports consistent theming.

3. **Flexibility**: `Box` provides more composability by allowing styles to be easily merged using the `sx` prop. This flexibility ensures the components can adapt to different use cases without duplicating code.

4. **Improved maintenance**: By leveraging `Box` and its inbuilt styling capabilities, the need for additional libraries like styled-components is reduced, minimizing external dependencies and risk of compatibility issues in the long term.

5. **Performance considerations**: The `Box` pattern avoids the runtime operations of libraries like `styled-components` (e.g., dynamic styles generation), which can improve performance in scenarios where styling is dynamic and frequently updated.

---

### Migration Plan

1. **Audit existing components**: Identify all instances where styled-components are used for component creation.
2. **Incremental updates**: Gradually refactor components to use the `Box` pattern, ensuring behavior and style remain consistent.
3. **Documentation updates**: Update coding guidelines and examples to reflect the preference for the `Box` pattern.
4. **Deprecation of styled-components usage**: Introduce warnings for new contributions using styled-components for component creation after sufficient migration progress.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision

Continue utilizing `Drafts` as a mechanism for creating, testing, and maintaining multiple versions of components in parallel, as well as housing work-in-progress components that are not yet ready for production or require collaboration before widespread usage.

### Rationale:
1. **Versioning Flexibility**: Drafts allow multiple versions of a component to coexist without disrupting the stable, production-ready root bundle of `@primer/react`.
2. **Collaboration Enablement**: Drafts provide a clear signal to developers that components are in development and require collaboration or feedback before production use.
3. **Controlled introduction**: Drafts help avoid prematurely releasing experimental or incomplete components into production environments, reducing the potential for breaking changes or misuse.
4. **Testing Ground**: They act as a sandbox for new features, enabling testing of functionality, API design, and behavior without impacting production consumers.

### Action Items:
1. Maintain the current draft structure where components under experimentation or revision are accessed via the `/drafts` path.
2. Define clear documentation and guidance for when and how to migrate a component from drafts to the stable release.
3. Expand communication to developers about the purpose and appropriate usage of draft components to ensure proper adoption and collaboration.
4. Regularly review draft components to decide on their promotion to production or removal if no longer relevant.

By aligning draft usage with our development strategy, we ensure a fast, iterative component creation process while safeguarding the stability of our core library.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision

Adopt snapshot testing with clear and concise guidelines to mitigate challenges associated with maintaining large numbers of snapshots and to improve test clarity and maintainability. The following practices will be adhered to for incorporating snapshot tests effectively:

### 1. **Use Snapshot Testing Sparingly**
   - Avoid snapshot testing components with overly large or deep trees. Instead, prefer testing them using more specific assertions or checking critical parts of their behavior.
   - Focus snapshots on simple and stable components, or components with straightforward output that are unlikely to change frequently.

### 2. **Combine Snapshot Testing with Unit Tests**
   - For complex components, use a combination of snapshot testing for the overall structure and granular unit tests to validate specific behavior or functionality. This ensures changes are better understood and isolated during debugging.

### 3. **Provide Clear Descriptions Within Tests**
   - Ensure that test descriptions clearly indicate the intent behind the snapshot. For example, instead of generic names like ""renders correctly"", use a description like ""renders a primary button with the correct label"".
   - Leverage test suite organization (grouping by functionality or component type) to clarify intent.

### 4. **Review and Minimize Large Snapshots**
   - Be cautious about snapshot size. Large, deep snapshots are harder to debug and review, and they can produce test failures that are difficult to address.
   - Manually review generated snapshots before committing them, ensuring that they capture only the necessary output and removing unnecessary elements if possible.

### 5. **Use Inline Snapshots for Simple Components**
   - For components with minimal markup, prefer inline snapshots directly inside the test file. This improves readability and makes it easier to track changes in diffs.

### 6. **Establish Practices for Handling Failing Snapshots**
   - Define guidelines for when to accept updated snapshots. This process should involve determining whether the snapshot failure was intentional (aligned with a deliberate change in requirements) or unintentional (possibly signaling a regression).
   - Encourage developers to include reasoning in commit messages when updating snapshots, explaining why the change was intentional.

### 7. **Enable Tools for Visual Review**
   - For UI-heavy components, consider using visual regression testing alongside or as an alternative to snapshot testing. Tools like Chromatic or Percy can help give reviewers a clear visual diff when UI changes occur.

### 8. **Regular Maintenance of Snapshots**
   - Periodically audit and prune unused or obsolete snapshots from the project.
   - Ensure snapshots are concise and reflective of the relevant component state to prevent the accumulation of unnecessary data.

By following these principles, snapshot testing can provide value by ensuring consistent component output without introducing undue overhead or maintenance challenges.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision

To address the need for providing developer warnings while ensuring they are excluded from production code, we will introduce a utility module specifically designed for logging development warnings. The utility will handle the conditional inclusion of warnings based on the environment (e.g., development or production).

### Key Points of the Decision:
1. **Create a Dedicated Utility Function**:
   We will implement a helper function, e.g., `warnDev()`, specifically for emitting development warnings. This function will wrap `console.warn()` and conditionally execute only in development environments.

2. **Use Environment Guards**:
   The utility function will use a check like `process.env.NODE_ENV === 'development'` to ensure that the warning messages are logged only in non-production environments.

3. **Dead Code Elimination**:
   During the build process, we will rely on tools like Webpack or Babel to remove any code branches related to development-only checks. For instance, if the build process sees a condition like `if (process.env.NODE_ENV !== 'development')`, the code block inside can be stripped out entirely in production bundles.

4. **Minimal API Surface for Development Warnings**:
   Keeping the utility simple (e.g., `warnDev(message: string)`), developers will be encouraged to use this approach consistently rather than directly invoking `console.warn()`.

5. **Documentation and Standards**:
   This pattern will be documented explicitly in `@primer/react`'s contribution guidelines to ensure consistent usage across the codebase and avoid accidental inclusion of direct `console.warn()` calls.

### Example Implementation:
Here’s how the utility function might look:

```ts
// warnDev.ts
export function warnDev(message: string): void {
  if (process.env.NODE_ENV === 'development') {
    console.warn(message);
  }
}
```

#### Usage:
```ts
import { warnDev } from './warnDev';

function ExampleComponent() {
  warnDev('ExampleComponent is deprecated and will be removed in the next major release.');

  return <div>Example</div>;
}
```

By adopting this approach, we maintain development warnings for local testing and debugging, while ensuring that production builds remain clean and free of unnecessary logs or code.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision

We will implement a centralized utility for managing accessible live region announcements across Primer React, ensuring a consistent and reliable experience for live region announcements.

This centralized utility, tentatively named `LiveRegionManager`, will provide a unified API for handling live region use cases and abstract away inconsistencies and complexities in ARIA live region implementation. It will be adopted by all Primer React components that require live region announcements.

### Key Decisions and Features:
1. **Centralized Live Region Management**:
   - All live region announcements will be routed through the `LiveRegionManager` to avoid ad-hoc and inconsistent implementations.

2. **Support for Common Scenarios**:
   - The utility will handle the three common use cases described:
     - Announcing content when an element is rendered or on page load (e.g., form submission feedback).
     - Announcing changes to the content of an element dynamically (e.g., count updates).
     - Announcing specific messages programmatically (e.g., results for a query).

3. **ARIA Roles and Attributes**:
   - The utility will provide support for `aria-live` attribute configurations (`polite` or `assertive`) and roles such as `alert`, `status`, or `log`, allowing authors to define the proper behavior for their scenarios.

4. **Programmatic Announcements**:
   - Expose an API for programmatic announcements so that developers can trigger announcements easily without managing `aria-live` behavior or DOM management manually.

5. **Dynamic Live Region Creation**:
   - Handle live regions dynamically by injecting the necessary DOM elements into the document when announcements are made and removing them when no longer needed. This ensures better screen reader compatibility, addressing cases where dynamically injected content in React does not get announced reliably.

6. **Screen Reader Compatibility**:
   - The utility will aim to account for known quirks and differences in how various screen readers handle live regions, ensuring the broadest possible compatibility.

7. **Accessibility Best Practices**:
   - Follow best practices, including the use of `aria-atomic` where necessary, to ensure complete and coherent announcements.
   - Avoid overloading users with redundant or unnecessary announcements by implementing throttling or deduplication where relevant.

### Implementation Details:
- The utility will be implemented as a singleton or context-based service that can be accessed globally across Primer React components.
- It will include thorough unit and integration testing with mock screen reader tools or APIs to validate expected behaviors.
- Documentation and examples will be provided to ensure easy adoption by developers.

By standardizing the approach to live region announcements with a centralized utility, we aim to enhance accessibility, reduce developer effort and confusion, and ensure consistent behavior across Primer React and GitHub applications.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"It seems you haven’t provided specific context or details for the architectural decision you're looking for. Could you please elaborate on the situation, requirements, or challenge? For example, what problem are you trying to solve, or what technology and constraints are involved?","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision

Adopt Storybook Interaction Testing to unify testing workflows and enhance debugging capabilities.

### Rationale:
1. **Reduce Duplication**: Utilize the same Storybook stories we've already created for Chromatic visual tests and manual testing to automate interaction tests using the Storybook Interaction Testing framework. This eliminates the need to recreate complex scenarios in both Jest and Storybooks, streamlining test writing and maintenance.

2. **Enhanced Debugging**: Storybook Interaction Testing offers a visual interface for running and debugging interactions, making it easier to identify issues during test failures. Developers can visually inspect and replay test steps, dramatically improving debugging efficiency and test visibility.

3. **Improved Collaboration**: Team members across different roles (e.g., QA, designers, developers) can easily understand and contribute to interaction tests using Storybook's visual representation, as opposed to trying to interpret Jest's CLI-based outputs.

4. **Extends Current Workflow**: The adoption of Storybook Interaction Testing extends Storybook’s utility without disrupting the current Chromatic visual testing setup. It complements, rather than replaces, Jest, which can still be used for unit tests and other accompanying cases. However, interaction-heavy UI scenarios will be better managed in Storybook.

5. **Seamless Maintenance**: Interaction tests sit alongside the stories, and any updates to the components would naturally update corresponding test cases, reducing the drift between tests and implementation.

### Implementation:
1. Add the `@storybook/testing-library` package to integrate Storybook's Interaction Testing capabilities.
2. Migrate existing Jest interaction-heavy tests to Storybook Interaction tests for applicable components.
3. Use Storybook’s Test Runner to automate running interaction tests as part of the CI pipeline to ensure continuous feedback.
4. Document this new testing workflow for the team and provide training sessions, if necessary, to enable team adoption.

### Alternatives Considered:
- **Keep current infrastructure**: Maintaining separate Jest tests and manual Storybook interaction testing would not address the duplication issue and debugging visibility gaps.
- **Adopt Cypress or Playwright**: While these tools are great for end-to-end tests, they add another layer to the tech stack and do not integrate as seamlessly with Storybook stories, making them less efficient for this purpose.

This decision optimizes the team’s existing toolset, reduces redundancy, and addresses both debugging and collaboration pain points effectively.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision

Adopt a monorepo tooling system like **TurboRepo** or **Nx** to manage dependencies, workspaces, and scripts across the `primer/react` project.

---

### Rationale

1. **Centralized Dependency Management**: By switching to a monorepo tooling solution, workspaces and their dependencies can be automatically managed from a single top-level configuration. Contributors no longer need to manually visit each directory or rely on custom scripts like `npm run setup` to install dependencies.

2. **Efficient Inter-workspace Dependency Sharing**: Using a monorepo solution allows workspaces to depend on each other with mechanisms such as automatic linking of packages and efficient caching. This eliminates the need for relative paths, file dependencies, or `npm link`, ensuring better maintainability and version control for internal dependencies.

3. **Consistent and Scalable Script Management**: Complex scripts under the `script` directory can be properly organized and run seamlessly within the monorepo tooling. These systems come with built-in support for task orchestration, caching, and parallel execution, which reduces redundancy and improves developer productivity as the codebase scales.

4. **Improve Contributor Workflow**: Contributors can use a single command (`install`, `build`, etc.) that automatically resolves dependencies and orchestrates operations across all workspaces. This reduces cognitive load and onboarding time for contributors.

5. **Future Scalability and Optimization**: Tools like TurboRepo and Nx provide robust caching mechanisms to speed up installations, builds, and tests across workspaces. As the repository grows in complexity, these optimizations ensure the workflow remains fast and efficient.

---

### Consequences

1. **Learning Curve**: Contributors will need to familiarize themselves with the chosen monorepo tooling framework (e.g., TurboRepo or Nx). Documentation and onboarding materials should be provided to ease this transition.

2. **Initial Setup Overhead**: Migration to the chosen tooling will involve refactoring workspace dependencies, scripts, and configurations. Some effort will be needed upfront to integrate the new tooling system.

3. **Tool Dependency**: Introducing monorepo tooling means the project now depends on another external library or framework. Efforts will be needed to keep the tooling version up-to-date and resolve any potential bugs or compatibility issues.

---

### Next Steps

1. **Evaluate Options**: Compare TurboRepo, Nx, and other monorepo tool solutions based on the team's needs (e.g., ease of use, task orchestration, community support, etc.).

2. **Trial Implementation**: Test the tooling on a small subset of workspaces (e.g., `docs` and `examples/*`) to verify compatibility and address potential edge cases.

3. **Migrate Repo**: Gradually migrate all workspaces and scripts into the new monorepo setup, updating documentation and workflows accordingly.

4. **Provide Contributor Documentation**: Create comprehensive guides and `README` updates to ensure contributors can work efficiently in the new system.

By adopting a monorepo tooling solution like TurboRepo or Nx, the `primer/react` project can grow more efficiently, with improved dependency management, scripting, and contributor workflows.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","### Decision

To ensure that only the intended public API of `@primer/react` is exposed to consumers, we will explicitly define which files and folders should be part of the public API in the `exports` field of the `package.json` file. Additionally, we will restructure the `src` directory to distinguish between public and internal modules.

#### Key actions:
1. **Public API Directory**: Move all files and folders that are part of the public API into a dedicated `src/public` directory. This will make it clearer which files are safe to expose.
2. **Internal Code Directory**: Move all internal files and folders that should not be exposed into a `src/internal` directory. Mark this directory as internal in documentation and development guidelines.
3. **Update Exports in `package.json`**: Modify the `package.json` `exports` configuration to explicitly include only the files within the `src/public` directory. For example:
    ```json5
    {
      ""exports"": {
        ""./public/*"": {
          ""import"": ""./lib-esm/public/*.js"",
          ""require"": ""./lib-public/*.cjs""
        },
        ""./index"": {
          ""import"": ""./lib-esm/public/index.js"",
          ""require"": ""./lib-public/index.cjs""
        }
      }
    }
    ```
4. **Review Rollup Config**: Update the Rollup configuration to separate the bundling process for the `public` and `internal` directories, ensuring only the public directory is included in the distributed package.
5. **Document Guidelines**: Add documentation for contributors to clearly define the distinction between public and internal code, emphasizing that only `src/public` should be relied upon by package consumers.

This approach minimizes the risk of accidentally exposing internal APIs while maintaining a clear structure within the codebase.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","### Decision

Adopt a styling architecture that makes use of **static `.css` files** with **CSS variables** for theming, instead of relying heavily on runtime-injected CSS. This decision is informed by the benchmarks and issues outlined above and aims to improve performance, scalability, and maintainability across applications.

Key technical decisions:

1. **Static CSS**: 
   - Transition from runtime-injected styles to pre-compiled static `.css` files.
   - Serve these `.css` files separately and ensure they are loaded in parallel with page resources, reducing initial page load times significantly.

2. **CSS Variables for Theming**: 
   - Introduce CSS variables (`var(--variable-name)`) to handle theming and customizability more efficiently.
   - This ensures lightweight dynamic style updates without the need for runtime computation and reduces the performance cost associated with style changes.

3. **Guardrails for Primer Consumption**:
   - Refine and simplify the API and design primitives offered by `@primer/primitives` to align with static CSS and avoid over-reliance on dynamic `sx` props.
   - Provide clear theming and component styling guidelines so developers can leverage CSS variables effectively.

4. **Enhanced Server-Side Rendering (SSR)**:
   - Eliminate the need for SSR to collect styles by using static CSS files.
   - This change will improve server-side rendering times while also reducing infrastructure overhead.

5. **Gradual Migration**:
   - Implement a phased migration plan to transition from runtime CSS injection to static CSS, ensuring minimal disruption to existing applications.
   - Provide backward compatibility wherever required and clear documentation/examples for teams to adopt the new approach.

---

### Justification

1. **Performance Gains**:
   - Reduced initial page load times due to parallel loading of static `.css` files.
   - Improved server-side rendering speeds by eliminating the need for additional passes to collect styles.
   - Significantly faster updates to styles during dynamic interactions, providing a smoother user experience.

2. **Scalability**:
   - Static CSS files scale better for large applications and datasets (e.g., files in repositories or PR diffs).
   - CSS variables provide a robust and reusable way to manage themes and styles across all applications.

3. **Developer Efficiency**:
   - Simplifies runtime styling by leveraging pre-defined CSS rules and variables instead of handling dynamic inline styles.
   - Reduced complexity for developers when consuming `@primer/primitives`, leading to fewer errors and more consistency across the codebase.

4. **Future-Proofing**:
   - CSS variables offer a foundation for modern theming solutions and can easily integrate with upcoming specifications or technologies in frontend development.

---

### Risks and Mitigation

1. **Risk**: Potential disruption to existing workflows during migration.
   - Mitigation: Roll out changes gradually, provide backward compatibility, and offer comprehensive migration guides and toolkits.

2. **Risk**: Initial overhead in transitioning styles to adhere to static CSS and CSS variables.
   - Mitigation: Prioritize migration for critical paths to realize performance benefits early, and introduce tools/scripts to automate parts of the transition.

3. **Risk**: Developers needing to adopt new patterns for theming with CSS variables.
   - Mitigation: Offer thorough training, examples, and tooling to ease adoption.

By adopting this approach, we aim to address the performance bottlenecks identified in the benchmarks while leveraging modern browser capabilities like CSS variables for improved theming and long-term maintainability.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision

We will move away from using Storybook's `play` functions for authoring tests based on user interactions and instead adopt Playwright for this purpose. 

### Justification:
1. **Increased Flexibility with Playwright**: Playwright provides a more comprehensive and robust API for simulating user interactions, which allows for more complex and nuanced testing scenarios compared to the `play` function.
2. **Separation of Concerns**: Storybook is primarily designed for component visualization, documentation, and static use case testing. Using Playwright for interaction-based tests keeps our testing tooling focused and specialized rather than coupling it with Storybook-specific utilities.
3. **Improved Tooling and Debugging**: Playwright offers enhanced debugging tools like trace viewer, video recordings, and detailed logging, which are critical for diagnosing issues in interaction tests. The `play` function in Storybook lacks comparable capabilities.
4. **Consistency Across Testing Layers**: Using Playwright for both Visual Regression Tests and interaction-based tests ensures a consistent approach to end-to-end testing and reduces the cognitive load for developers having to switch context between tools.
5. **Enhanced Community Support**: Playwright has an extensive and active community focused on improving user interaction testing, offering a future-proof choice with long-term benefits.

This decision establishes Playwright as the go-to tool for all interaction testing in our project. Jest will remain the primary tool for unit and integration testing, while Storybook will exclusively handle component previews and visual UI development.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision

Introduce a server-rendering-friendly API for responsive props.

To address the issue of layout shift caused by `useResponsiveValue` during server-side rendering, we will design a solution that provides a way to deliver consistent styles based on viewport-based responsive values. Here's the decision:

1. **Static Fallback for SSR**: Responsive props should allow authors to provide a static fallback value to be used during server-side rendering. This ensures layouts depend on a predictable style until hydration completes. For example:
   ```tsx
   <Stack gap={{ initial: 'normal', narrow: 'condensed', regular: 'normal', wide: 'spacious' }} />
   ```
   During SSR, the `initial` value (`'normal'` in this case) will be applied. Once the component is hydrated, viewport-specific values will be resolved.

2. **Responsive Styles using CSS**: Where possible, we will recommend authors rely on CSS-based media queries for responsive styles instead of runtime resolution via JavaScript. This approach avoids layout shifts during hydration as styles are determined by the browser directly and are in sync with the `@media` queries used in CSS.

3. **Viewport Detection during SSR**: For cases where responsive props are critical to server-rendered styles or layout, we will provide an advanced API to optionally detect the viewport size on the server. This would involve leveraging server frameworks (like Next.js or Remix) that can derive the user's viewport or device type based on the request headers (e.g., `User-Agent`). While this requires additional configuration, it can help deliver tailored responsive values during SSR.

4. **Enhanced Documentation and Guidance**: Clear documentation will be provided to ensure authors understand the trade-offs when using `useResponsiveValue` and how best to mitigate layout shifts during SSR.

### Rationale
- The fallback (`initial`) mechanism ensures predictable rendering during SSR.
- Leveraging CSS for responsiveness keeps the approach aligned with web standards and avoids excessive JavaScript complexity.
- Providing advanced viewport detection acknowledges specific use cases where server-rendered responsiveness is necessary but keeps this an opt-in for advanced users.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision

To ensure a smooth migration process for users to adapt to the new API while deprecating the old prop, the following decisions will be implemented:

1. **Deprecation Warnings:**
   - Introduce a runtime warning in development mode for the deprecated prop. This warning should clearly inform users about the deprecation, indicate the preferred usage of the new API, and provide a link to the migration guide.

2. **Migration Guide:**
   - Publish a detailed and concise migration guide in the documentation or release notes, outlining the changes from the deprecated API to the new API. Include code examples, benefits of the new approach, and an explanation of the reasoning behind the change.

3. **Backward Compatibility:**
   - Maintain support for the deprecated prop alongside the new API for a deprecation period. This will allow users sufficient time to migrate their code without breaking existing implementations.

4. **Linter Rule Enforcement:**
   - Provide an optional ESLint rule or static analysis tool to help developers automatically flag or refactor instances of the deprecated prop in their codebase.

5. **Testing and Validations:**
   - Ensure thorough testing is in place for both the deprecated and new APIs during the deprecation period to ensure a seamless transition.
   - Validate that the new API meets accessibility and flexibility requirements.

6. **Communication:**
   - Inform users well in advance about the deprecation and removal timeline. This should be clearly communicated in changelogs, deprecation warnings, and documentation updates.

7. **Code Mod (Optional):**
   - For highly adopted libraries or frameworks, provide a ""codemod"" script to automate the prop migration in user codebases, minimizing the effort for developers to adapt.

This approach prioritizes developer experience while maintaining a strong focus on accessibility and flexibility in the new API.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision

Support configuration of proxies via the `http_proxy`, `https_proxy`, and `no_proxy` environment variables in the self-hosted runner application. 

- The application will primarily support the lowercase variants of these environment variables (`http_proxy`, `https_proxy`, `no_proxy`), as they are widely accepted and understood by most tools and frameworks.
  
- Additionally, the application will include support for uppercase variants (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`) for compatibility purposes, enabling seamless integration within various environments.

- When both uppercase and lowercase variants are provided, the lowercase versions will take precedence to maintain consistency with common practices across tools.

This decision ensures broad compatibility with enterprise environments while aligning with widely-used conventions.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision

We will change the default behavior of command echoing to be opt-in rather than opt-out. This will minimize unnecessary log clutter for users while retaining the ability to explicitly enable echoing when needed.

### Key Points of the Decision:
1. **Default Behavior**: 
   - Command echoing will be disabled by default unless `ACTIONS_STEP_DEBUG` environment variable is enabled.
   - This ensures that logs are clean and focused for the end user unless debugging is actively enabled.

2. **Error Cases**: 
   - Echoing will remain enabled for commands that fail. This provides valuable context for debugging and troubleshooting command errors without requiring echoing to be globally enabled.

3. **Explicit User Control**: 
   - Users will have explicit control over command echoing through the introduction of two new commands:
     - `::echo::on`: Enables command echoing.
     - `::echo::off`: Disables command echoing.
   - This allows developers to toggle echoing easily and only when necessary.

4. **Backward Compatibility**:
   - Current workflows relying on echoing by default will need to adjust if the behavior is required outside of debugging scenarios. This is mitigated by introducing the new commands and supporting `ACTIONS_STEP_DEBUG` for more granular control. 

By implementing this change, we strike a balance between reducing log clutter for the average user while retaining flexibility for those who require detailed outputs for debugging or monitoring purposes.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision

We will enhance the runner to include additional context-specific information in the `Set up job` step's log to provide more visibility into the environment in which the job is being executed. This will be achieved by:

1. Capturing details about the OS and installed software from hosted runners and self-hosted runners.
2. Formatting and displaying the OS/Software information in the logs during the `Set up job` step.

For example:
- OS version (e.g., Windows Server 2022, Ubuntu 22.04)
- Runtime environment details (e.g., installed language runtimes, pre-installed tools)

This enhancement will make debugging and auditing job execution more efficient by ensuring this information is immediately visible in logs without additional steps. 

To support flexibility:
- Default: Include a pre-defined set of environment information in logs.
- Optionally: Provide a configuration option to disable or customize what details are included for specific jobs based on project needs or data sensitivity.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision

The addition of `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context is approved. This will enable downstream steps to determine the success or failure of preceding steps and adjust their behavior accordingly.

This decision makes the workflow more dynamic and allows greater flexibility by providing explicit information about the outcome and conclusion of each step. Maintaining compatibility with the existing `steps.<id>.outputs` ensures this change enhances functionality without introducing breaking changes.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision

Enhance the self-hosted runner registration process to accept and register custom labels via command-line arguments.

This decision allows greater flexibility and automation for users configuring self-hosted runners. By providing a mechanism to specify custom labels during the runner's registration process, we make it easier for users to streamline their CI/CD workflows and include metadata relevant to their specific use cases without requiring manual post-registration steps. 

This enhancement directly addresses the issue of enabling fully automated configuration scripts for runners and aligns with the request outlined in Issue #262.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision 
Implement a Retry with Exponential Backoff strategy to handle transient faults when communicating with the third-party authentication provider. 

### Details:
1. **Retry Mechanism**: When a fault or failure (e.g., timeout, rate limiting) occurs while communicating with the authentication provider, the system should automatically retry the request rather than failing immediately.
  
2. **Exponential Backoff**: Each retry should be delayed by an exponentially increasing time interval (e.g., 1s, 2s, 4s, etc.), up to a configurable maximum retry interval limit. This approach avoids hammering the third-party provider with repeated retries, which could worsen the situation.

3. **Max Retry Limit**: To prevent indefinite retries and possible resource overuse, there should be an upper limit on the number of retries.

4. **Jitter**: Introduce a small random jitter to the retry intervals to distribute retry attempts more evenly when dealing with concurrent requests.

5. **Fallback Mechanism**: After reaching the maximum retry limit, the system should fail gracefully, provide a meaningful error message to the caller, and log the failure for further investigation.

6. **Monitoring and Metrics**: Build metrics (e.g., retry counts, error rates, failures, latency) to monitor the health of the authentication process and identify any persistent issues with the third-party provider.

7. **Rate Limits Handling**: Include special handling for rate-limited responses (e.g., 429 Too Many Requests). Use the `Retry-After` header from the provider (if available) to determine when to attempt the next request.

By implementing this strategy, the system becomes robust in handling transient faults, ensures a better user experience, and avoids excessive stress on the authentication provider.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision

We will adopt the practice of documenting Architectural Decision Records (ADRs) for all significant decisions made during the project's lifecycle.

ADRs will serve as a single source of truth for decisions, providing transparency to both internal team members and external contributors. Each ADR will capture the context, the decision made, and the reasoning behind it, ensuring that:

1. New team members can quickly get up to speed with the project's history and decision-making rationale.
2. External contributors can engage with the project more effectively, understand its direction, and provide constructive feedback to enhance decision making.
3. Decisions remain visible and relevant as they are tied to the point in time they were made, avoiding issues of outdated documentation.

We will encourage all team members to create an ADR whenever a significant architectural, technical, or process-oriented decision is made. The ADRs will be stored in a version-controlled repository so that they are easily accessible, modifiable, and openly auditable.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂÃÂ¡ÃÂÃÂª with all relevant discussion summarized ÃÂÃÂ¡ÃÂÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision

We will implement a URI schema specifically tailored for identifying and accessing EDN files stored in decentralized storage. This schema will provide a standardized and consistent way to locate, retrieve, and manage extensions, ensuring compatibility and ease of integration across different components of the system.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision

Group chats will be deprioritized and will not be included in the beta release to ensure we stay on the critical path and meet the Q2 timeline. The team will focus on 1-1 chat and public chat as the core components of the MVP. Resources and efforts currently being spent on group chat regressions and implementation will be redirected toward enhancing the functionality and reliability of 1-1 and public chat features. Group chat can be revisited and considered for future releases post-beta.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision

We will implement the Discovery v5 protocol for peer discovery in the messaging application.

This decision will enable us to dynamically discover new peers instead of relying on hardcoded ones. By leveraging Discovery v5, our system will be capable of scaling more effectively, ensuring that as the number of active users grows, the load can be balanced across available server peers. Additionally, this protocol will contribute to achieving higher availability, reliability, and security by decentralizing and diversifying peer connectivity. This approach aligns with our objective of building a scalable, robust, and secure messaging ecosystem.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision

Adopt the new key generation approach based on the 12 mnemonic words as outlined in Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) to ensure compatibility with other wallet applications. This decision will enable users to seamlessly create and import wallets across different apps, enhancing interoperability and user experience within the Status ecosystem.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision

Transition from LES to ULC as the primary protocol for Ethereum node communication, focusing development efforts on completing ULC. In the interim, utilize Infura as a fallback while providing an option for users to connect to their own upstream node to mitigate decentralization and security concerns.

1. **Short-term Solution**:
   - Continue using Infura for Ethereum network communication until ULC is ready.
   - Allow users to configure their client to connect to their own upstream nodes, offering an optional decentralized alternative for users who value independence.

2. **Long-term Solution**:
   - Prioritize development of ULC (Ultra Light Client) to phase out dependency on LES.
   - Leverage ULC's ability to connect to multiple nodes to improve decentralization and security.
   - Implement mechanisms to vary load based on network conditions (e.g., Wi-Fi vs. Mobile).

3. **Performance Optimization**:
   - Deprioritize additional performance fixes related to LES unless a critical issue arises.
   - Explore ways to manage issues like hot sync, disk usage, and bandwidth with existing solutions (e.g., use of CHTs) only if necessary for stable beta functionality.

4. **Decentralization Goal**:
   - Strive to remove third-party reliance on infrastructure like Infura as soon as ULC becomes production-ready.
   - Promote node diversity by encouraging users to run their own Ethereum nodes and connect directly.

By focusing on completing ULC, the project can aim for a robust and decentralized architecture in the long run without committing excessive resources to short-term LES optimizations.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision

Introduce a ""Sensitive Data"" wrapper or type to encapsulate sensitive information (e.g., user passwords, API keys). This wrapper would provide controlled access to the data and prevent accidental logging by overriding the `toString()` or logging mechanisms to output a placeholder (e.g., `***` or ""Sensitive Data"").

- All sensitive information will be stored using this wrapper type instead of plain strings.
- Logging mechanisms will be updated to recognize this wrapper type and prevent any sensitive data from being printed unintentionally.
- Developers will be educated to always use the new `Sensitive Data` type for managing confidential data.

This approach ensures that sensitive data is safeguarded against accidental exposure, even in edge cases or developer errors.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision

Discontinue support for the current 3rd party status extensions based on the JavaScript jailed execution environment and replace it with a more robust and declarative extension system, leveraging the capabilities of Project Pluto.

### Key Points:
1. **Deprecation of Jail-Based Extensions:**
   - Phasing out the existing jailed execution environment API due to its high maintenance cost, performance inefficiencies, poor API design, and lack of usage or real interest from 3rd party developers.

2. **Migration to Project Pluto Extensions:**
   - Adopt the Project Pluto extension model, which promises a well-structured, declarative design.
   - Enable extensions to integrate seamlessly into various parts of the app (e.g., wallet, chat, assets/collectibles) while improving app maintainability and performance.

3. **Unified Programming Model:**
   - Prioritize leveraging the strength of the host application by supporting a stateless, declarative approach over mutable/state-heavy semantics.
   - Simplify the extension development process to make it more developer-friendly and better aligned with modern practices.

4. **Performance and Maintainability:**
   - Eliminate asynchronous ""ping-ponging"" with RN Bridge, which introduces unnecessary complexity and resource overhead.
   - Reduce hardcoded custom logic in the app by consolidating the extension implementation under a coherent framework.

5. **Focus on Real Needs:**
   - Shift efforts towards improving the experience for DAPP developers using existing and widely adopted web technologies (e.g., `web3.js`) instead of a custom API that has shown limited interest from developers.

6. **Improved Documentation and Support:**
   - Ensure the new extension model includes clear, up-to-date documentation and testable APIs to encourage widespread adoption and ease of development.

This decision avoids the pitfalls of the current system while laying the groundwork for a more modular, efficient, and developer-friendly extension architecture that aligns with the vision of Project Pluto.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision

Adopt the Tribute to Talk (TtT) mechanism as an anti-spam filter for direct messaging, requiring stakeholders to stake a minimum amount of SNT that a potential messenger must deposit to initiate contact. This deposit will only be forfeited to the recipient if they choose to reply, ensuring it remains an economics-based, opt-in system incentivizing thoughtful communication.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision:

To improve the oversight and polishness of the mobile application, we will create the role of **Section Owner** within the mobile development team. A Section Owner will be assigned to each of the key app areas: Communities, Syncing & Onboarding, Activity Center & Shell, Wallet, and Messaging.

### Responsibilities of the Section Owner:
1. **Oversight and Accountability**:
   - Be the primary point of responsibility for the assigned section, ensuring quality, consistency, and attention to detail.
   
2. **Collaboration with Design**:
   - Work closely with the design team to align development outcomes with design expectations and ensure a polished user experience for the assigned section.

3. **Code and Design Reviews**:
   - Actively participate in and lead code/design reviews for features related to the assigned section to ensure high standards.

4. **Knowledge Sharing**:
   - Act as the internal expert on the assigned section, sharing knowledge with team members and promoting best practices.

5. **Coordination**:
   - Coordinate tasks, priorities, and updates related to the assigned section with the team lead and other Section Owners to maintain alignment and integration across the application.

6. **Ownership of Backlog**:
   - Monitor and manage the backlog of tasks/bugs for the section, collaborating with the project manager to prioritize work effectively.

By assigning **Section Owners**, we are distributing the responsibilities more evenly across the team, enhancing focus and detail on each app segment, and improving the overall polish and delivery quality of the mobile application.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision

We will manage the Product Option's values collection by creating a dedicated sub-resource for Product Option Values in the Sylius API. This approach will leverage API Platform's built-in sub-resource functionality, allowing easy and efficient management of Product Option Values as a separate, structured, and well-defined resource. 

Additionally, we will ensure that:

1. **Relationships**: Product Option Values are linked as a sub-resource to their corresponding Product Options, ensuring a proper association while maintaining API structure clarity.
2. **CRUD Support**: Full CRUD (Create, Read, Update, Delete) operations will be enabled for Product Option Values, enabling flexibility in managing these values independently.
3. **Data Validation**: Validation at the API level will ensure data integrity when adding or modifying Product Option Values.
4. **Pagination & Filtering**: For efficiency, we will implement pagination and filtering options in the Product Option Values API to handle larger collections of values effectively. 

This approach enhances usability, simplifies querying, and aligns with API Platform's best practices, while ensuring performance and scalability for the system.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision

Implement a locale-aware API that serves translated content based on the customer's locale.

### Rationale:
1. **Frontend Simplification**: By delivering only the translated content specific to the customer's locale, frontend developers no longer need to filter or process unnecessary translations on the client-side, reducing complexity and potential for bugs.
2. **Consistency**: Providing a cohesive and consistent approach to localization across all endpoints ensures a predictable developer experience and avoids confusion around inconsistent behaviors.
3. **Improved User Experience**: Delivering localized content directly improves response times and ensures the end users always see content in their preferred language without additional configuration.
4. **Scalable Solution**: A centralized locale-aware handling reduces the risk of future technical debt when scaling the application or adding new translatable resources.

### Implementation Steps:
1. Add a `locale` parameter to the API requests. Customers can specify their locale (e.g., `en_US`, `fr_FR`) when making API calls.
2. Configure the backend to detect the `locale` parameter, fallback to user's default locale, or use the store's default locale if none is provided.
3. Modify all affected resources to return only the translation associated with the requested locale.
4. Maintain support for backend administrators or developers needing access to all translations for management or configuration purposes (via a special API endpoint or flag).

### Alternatives Considered:
1. Leaving localization to the frontend application (rejected due to inconsistent user experience and increased frontend complexity).
2. Serving all available translations by default (rejected due to inefficiency, increased response size, and unnecessary data processing).

This decision enables Sylius to maintain a scalable, user-friendly, and developer-friendly approach to managing translatable resources.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision  
Establish a lightweight Architectural Decision Record (ADR) system for Terraform Controller to document decisions.  

### Summary  
To ensure decisions impacting Terraform Controller are effectively captured and easily understood by contributors, we will adopt an ADR system. Each decision will be documented using a simple, template-based approach that is version-controlled alongside the project's codebase (e.g., stored in the `/docs/adr` directory).  

### Rationale  
1. **Lightweight:** A well-structured ADR system is easy to implement, avoiding unnecessary complexity and overhead for contributors.  
2. **Discoverable:** Storing ADRs directly in the project repository ensures transparency and maintainability. Contributors can reference decisions without needing external tools or documentation.  
3. **Future-proof:** Documentation of key decisions provides historical context and aids onboarding of new contributors, enabling long-term project sustainability.  
4. **Consistency:** Standardizing a format ensures decisions are clear, concise, and cohesive.  

### Implementation  
1. Introduce an ADR template (Title, Context, Decision, Consequences) within the Terraform Controller repository.  
2. Use Markdown files for simplicity and readability within `/docs/adr`.  
3. Encourage contributors to document any major decisions that affect functionality, workflows, or architecture.  
4. Educate contributors on when and how to add ADRs through the project’s contribution guidelines.  

### Consequences  
* Contributors will have a clear understanding of why certain decisions were made.  
* Future decisions will be informed by prior records, reducing redundancy or contradictory directions.  
* Project documentation will remain cohesive and centralized, improving long-term collaboration within the team.  ","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision

We will use **webhooks** to detect pull request changes.

### Rationale:
1. **Real-time Updates**: Webhooks provide near-instant notifications whenever a pull request event occurs, ensuring timely processing without delay.
2. **Reduced Resource Consumption**: Unlike polling, webhooks do not require continuous API requests, reducing server load and network usage.
3. **Scalability**: Webhooks allow the system to scale more effectively as there's no need to handle the overhead of repeated polling requests.
4. **Native GitHub Support**: GitHub natively supports webhooks for pull request events, making the setup straightforward and reliable.
5. **Cost Effective**: Polling could easily run into API request rate limits for high-frequency usage, whereas webhooks avoid such limitations, reducing potential costs.

### Trade-offs:
- Handling Webhook Downtime: Webhooks require additional mechanisms to handle situations where a webhook is missed or fails (e.g., through retries or periodic reconciliation).
- Initial Setup: Webhooks require configuration on the GitHub repository, which might introduce some upfront setup complexity compared to polling.

### Mitigation for Risks:
- Implement a backup periodic reconciliation mechanism using GitHub's API to identify any missed changes.
- Use logging and alerting to monitor the status of webhooks to ensure reliability.

This architectural decision ensures efficient and timely detection of pull request changes while minimizing resource usage and operational costs.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision

Allow cross-namespace references in tf-controller API with restrictive conditions that mitigate security and isolation concerns, while maintaining compatibility and usability for existing installations.

### Rationale:

1. **Namespace Isolation and Security:**
   - Cross-namespace references will be allowed but must be explicitly enabled. For example, this could be implemented using an annotation or a configuration option at a namespace level (e.g., `allowCrossNamespaceReferences: true`).
   - Default behavior will block cross-namespace references unless explicitly configured. This ensures that the API does not unintentionally weaken namespace isolation for users who have not opted into it.

2. **Granular Access Control:**
   - When enabling cross-namespace references, enforce fine-grained Role-Based Access Control (RBAC). Users must have explicit permissions to access referenced resources in other namespaces.
   - Document clear examples and guidelines on how to set secure RBAC rules for this use case.

3. **Backward Compatibility:**
   - Support existing use cases (e.g., Flux deployments with cross-namespace references to source objects) to ensure easy upgrades and minimal disruption to current workflows.
   - Existing configurations relying on cross-namespace references should work after the API changes, provided administrators opt-in with the appropriate namespace settings or permissions.

4. **Validation and Warnings:**
   - Introduce webhook validation that provides warnings or errors when cross-namespace references are detected without proper permissions or configurations.
   - Include the capability to log cross-namespace usage to help administrators audit and assess potential risks.

5. **Encourage Best Practices:**
   - Provide documentation and examples encouraging alternatives to cross-namespace references wherever feasible. Promote local-source definitions and dependencies at the namespace level to adhere to Kubernetes best practices.

By adopting these measures, the tf-controller API maintains compatibility and functionality for existing deployments while prioritizing security, namespace isolation, and manageable permissions.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision  

Implement a caching mechanism for Workspace BLOBs within the TF-Controller to enable efficient finalization and deletion of individual Terraform (TF) objects, including cleanup of dependent resources.

### Rationale  

1. **Dependency Identification and Cleanup:**  
   The caching mechanism for Workspace BLOBs will ensure that all relevant information (Source, Secrets, ConfigMaps) is readily available during the deletion process. This will enable the TF-Controller to efficiently identify and clean up dependent resources before proceeding with TF object deletion, preventing the object from getting stuck in an inconsistent state.

2. **Efficient Deletion Process:**  
   By storing Workspace BLOBs in a localized cache, the controller can minimize redundant download and upload cycles between the tf-runner and storage. This reduces overhead and speeds up the deletion process, making it more robust and predictable.

3. **Structured Priority Fix:**  
   Resolving single TF object deletion serves as the foundational problem for handling dependent resource deletion and namespace deletion. Implementing this caching mechanism addresses the immediate requirement while enabling future extensions to tackle broader deletion challenges.

4. **Scalability:**  
   A caching mechanism ensures scalability by optimizing performance in environments with large volumes of Terraform objects. It will streamline cleanup operations, even in complex dependency scenarios.

5. **Consistency and Robust Finalization:**  
   With Workspace BLOBs readily accessible in the cache, the TF-Controller can ensure consistent and thorough finalization of objects during deletion. This reduces the likelihood of partial deletions and maintains infrastructure integrity.

### Approach  

1. **Design a Cache:**  
   Create a lightweight and efficient internal or distributed caching system within the TF-Controller to store Workspace BLOB data. This cache should periodically update and evict entries based on a configurable lifecycle policy to ensure freshness and avoid stale data.  

2. **BLOB Lookup Workflow:**  
   Enhance the TF-Controller workflow to prioritize cached Workspace BLOBs during deletion processes. If the BLOB is unavailable in the cache, retrieve it from the storage and update the cache.

3. **Dependency Management Using Cached Data:**  
   Leverage cached Workspace BLOB information to identify and delete associated dependency objects (e.g., Secrets, ConfigMaps) before attempting deletion of the primary TF object.

4. **Validation:**  
   Introduce a validation step in the TF-Controller to confirm that all dependent resources have been successfully deleted using cached Workspace BLOB data before finalizing the TF object deletion.

### Consequences  

1. **Benefits:**  
   - Faster and more efficient single TF object deletion process.  
   - Reduced risk of inconsistent states due to stuck objects.  
   - Lays the groundwork for solving more complex deletion challenges like dependency-based and namespace-wide deletions.  
   - Improved reliability by ensuring comprehensive cleanup.  

2. **Trade-offs:**  
   - Increased implementation complexity due to cache management.  
   - Additional memory overhead for caching Workspace BLOBs.  
   - Potential need for scale-out solutions in high-volume scenarios.  

3. **Risks Mitigation:**  
   - To address cache expiration and stale data concerns, implement a robust update and eviction policy for Workspace BLOBs.  
   - Regular monitoring and logging of deletion workflows to ensure correctness and identify edge cases early.  
   - Testing with a variety of TF resource scenarios to validate the solution thoroughly before rolling it out.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision

### Change to a Multi-Environment E2E Testing Approach

Given the removal of the dependency on repository secrets for creating the zarf-init-{{arch}}.tar.zst and the desire to improve test coverage and developer efficiency, we will adopt the following approach for E2E testing:

1. **Implement Dynamic Testing Across Kubernetes Distributions:**
   - Configure the CI workflows to automatically provision test environments for targeted Kubernetes distributions (e.g., Minikube, K3s, Kind, etc.) during E2E testing. 
   - These tests will verify Zarf’s compatibility and behavior across different clusters.

2. **Expand Tests Across Linux Distributions:**
   - Include major Linux distributions (e.g., Ubuntu, CentOS, Fedora, Alpine) to ensure Zarf functions correctly in varied environments.
   - Use container-based runners or virtual machine images for simulating different Linux environments.

3. **Support Local Testing for Faster Feedback:**
   - Develop a lightweight E2E test suite that runs locally using Docker-based containers or local Minikube/K3d clusters. 
   - Provide documentation/guides to help developers spin up local test environments that align with CI conditions while omitting overly complex scenarios for faster iteration in local development.

4. **Optimize Cloud-Based CI for Comprehensive Tests:**
   - For full-scale testing, retain EC2-based or similar cloud environments for resource-intensive scenarios (e.g., large deployments or multi-node Kubernetes clusters).
   - This ensures scalability and testing of edge cases that are not feasible locally.

5. **Minimize Security Risks:**
   - Remove `slash-command-dispatch` from the workflow since repository secrets are no longer required for E2E tests.
   - Confirm that tests being executed without secrets can appropriately validate the overall integrity of PRs without exposing the repository to security vulnerabilities.

6. **Implement Modular CI Workflow:**
   - Split the CI workflow into distinct jobs targeting the following:
     - Local-like lightweight tests
     - Kubernetes distribution matrix (via cloud or containers)
     - Linux distribution matrix
   - Enable workflows to intelligently skip jobs based on the nature of changes detected in PRs for optimization.

This decision aligns with Zarf’s goals of improving flexibility and coverage in testing while enabling faster developer feedback loops. It also continues to protect the integrity of the project while maximizing efficiency.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision: Temporary Local Registry for Image Injection

After analyzing the context and requirements, the decision is made to implement a solution where Zarf will set up a **temporary local OCI-compliant registry** to inject images into the cluster when a standard injection mechanism or existing registry is unavailable.

### Rationale:
1. **Environment-Agnostic Solution**: Since Zarf must assume that no existing registry is available, creating a temporary local registry allows seamless operation across unknown and diverse environments without external dependencies.
2. **Avoid Host/Node Modifications**: Modifying root certificate authorities or the CRI configurations on all nodes in the cluster is complex and introduces a risk of breaking cluster behavior or causing security concerns. A temporary registry avoids this and keeps operations decoupled from the nodes' configuration.
3. **Leverage Kubernetes Pull Mechanisms**: By creating a temporary registry with the necessary images and distributing trust certificates (if the temporary registry is self-signed), the standard Kubernetes pull mechanisms can be used to retrieve the images. This maintains compatibility with Kubernetes' expected workflows without requiring advanced cluster modifications.
4. **Cluster-Wide Access**: By hosting the temporary registry and making it accessible from all nodes in the cluster, all images can be injected into the required nodes without the need for complex taint configurations or affinity rules. Once bootstrapping or workload deployment is complete, the temporary registry can be torn down.

### Implementation Steps:
1. **Temporary Registry Deployment**:
   - Use an OCI-compliant registry (e.g., `registry:2` or another lightweight alternative).
   - Deploy the registry either as a Kubernetes pod (if possible) or as a standalone process accessible at a predictable endpoint.

2. **Secure Access**:
   - Automatically generate a TLS certificate for the temporary registry if necessary.
   - Distribute the certificate to pods or configure Kubernetes secrets for accessing the registry during image pulls.

3. **Image Injection**:
   - Upload the required images to the temporary registry.
   - Use appropriate Kubernetes manifests to point workloads to the images in the temporary registry.

4. **Cleanup**:
   - Tear down the temporary registry and remove any temporary certificates or manifests once the workload has been successfully deployed.

This approach balances simplicity, security, and compatibility, ensuring images can be injected into Kubernetes clusters without requiring custom logic for every node or CRI configuration. It avoids dependency on vendor-specific CLI tools while maintaining a reliable workflow for diverse environments.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision

Adopt a Software Bill of Materials (SBOM) generation and management process to comply with EO14028 requirements. Use automated tools and workflows that can integrate seamlessly into the software development pipeline, ensuring that the SBOM is consistently generated and maintained for all software running on government hardware. Additionally, leverage standardized SBOM formats such as SPDX, CycloneDX, or SWID to align with industry best practices for compliance and interoperability.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision

Introduce a declarative approach in Zarf to handle image path and secret mutations, enabling compatibility with GitOps tools like Flux and ArgoCD, as well as CRDs referencing the git server. This will involve building a Zarf Mutating Webhook Admission Controller to dynamically handle these changes during runtime based on a set of predefined rules or configurations.

The admission controller will:

1. Intercept and modify manifests for Zarf-managed deployments in a Kubernetes cluster, ensuring image paths and secrets point to the internal Zarf Registry.
2. Account for resources like custom CRDs referencing the git server.
3. Provide a declarative configuration syntax for defining these mutation rules within Zarf packages, reducing manual effort for package authors.

This approach will shift the mutation burden away from package authors and onto Zarf’s automated processes, ensuring consistency and correctness across various deployment scenarios, even with GitOps tools or custom resource definitions.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision

Replace the use of the `stage2` binary and the `registry:2` image with a simpler, lightweight solution to reduce the overhead caused by the current implementation. Specifically, implement the following:

1. Remove the `stage2` binary entirely from the system and reduce the total binaries in the zarf ecosystem from 3 to 2.
2. Replace the full-featured `registry:2` Docker registry with a simpler, purpose-built solution, such as using an in-memory OCI image store or a lightweight container image registry library (e.g., `oras` for OCI registry interactions).
3. Eliminate the need to re-assemble a `registry:2` image, which would reduce the number of configmaps, simplify the init process, and cut down on cluster resource usage.
4. Use existing Zarf components or minimal tooling directly from the `stage1` binary or other existing binaries to handle pushing images to the cluster in the appropriate format. 

By consolidating these operations and removing the need for `stage2` along with the `registry:2` dependency, we can streamline the injector pod functionality, improve deployment and build times, and simplify overall system maintenance.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision

**Decision:** Implement Unit Testing in a defined and limited capacity within Zarf.

**Reasoning:**

Based on the outlined considerations, introducing unit testing in a defined and limited capacity provides a balanced approach that addresses the need for better test coverage without incurring excessive maintenance overhead. Specifically:

1. **Surgical Coverage Improvement:** By focusing unit tests on challenging-to-test areas (e.g., critical functions that aren't easily covered by end-to-end tests), the test suite can complement the existing e2e approach effectively without resulting in redundant tests.
   
2. **Targeted Risk Mitigation:** Limited unit testing ensures that core, high-priority interfaces and components—especially those under scrutiny as part of Zarf's evolution into a library—are properly vetted for quality and stability.
   
3. **Manageable Overhead:** Restricting the scope of unit testing avoids the downsides of maintaining a large and potentially fragile test suite. This also makes it easier to educate contributors about proper unit testing strategy, reducing the risk of misuse in pull requests.

4. **Alignment with Zarf’s Existing Approach:** Focusing on areas that are hard to test with end-to-end tests maintains harmony with Zarf's current testing philosophy, while still evolving its testing strategy incrementally.

5. **Adaptability:** This approach allows the team to optimize for areas with real, observed needs rather than attempting to cover the whole codebase, which mitigates the issues associated with diminishing returns on extensive unit testing as the application evolves.

**Implementation Notes:**

- Focus unit testing efforts on well-defined interfaces or specific areas that are not easily tested through e2e tests.
- Establish clear best practices and guidelines for writing unit tests (e.g., prioritizing interface-level tests over overly granular, implementation-specific tests).
- Incorporate a lightweight review process for unit tests included in PRs to ensure clarity, relevance, and maintainability.
- Periodically revisit this strategy as Zarf's usage and scope evolve to assess whether expanding its scope or transitioning to other testing strategies (e.g., integration tests) becomes necessary.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision

### Decision: Run Syft against files and dataInjections, while allowing limited flexibility for the user to specify additional SBOM targets.

### Justification:

To effectively address the need for a more complete representation of Zarf package contents without introducing unnecessary noise or significant complexity, the decision is as follows:

1. **Primarily focus on files and dataInjections**
   - By default, run Syft against files and dataInjections.
   - These represent known artifacts that are explicitly included in the Zarf package, ensuring high confidence in providing accurate and actionable SBOM results.

**Pros:**
   - Prevents noise and confusion from analyzing artifacts that might not be part of the final package (e.g., dependency files in git repos).
   - Aligns well with common user behavior, as files and dataInjections are the primary methods to include non-image artifacts.

**Cons:**
   - May not cover certain edge cases where other sources of artifacts exist (e.g., committed files in a git repo).

---

2. **Allow minimal user-configurable flexibility**
   - Introduce an optional configuration parameter that allows users to specify additional directories or artifact types to include in the SBOM analysis. 
   - For simplicity, default behavior will still restrict SBOM generation to files and dataInjections unless explicitly overridden by the user.

**Pros:**
   - Balances simplicity with flexibility for advanced users.
   - Ensures users wanting a broader SBOM scope still have a path to achieve that.

**Cons:**
   - Additional configuration may slightly increase complexity for users who want fine-grained control, but this is mitigated by keeping the default behavior focused and straightforward.

---

3. **Re-evaluate as necessary based on user feedback**
   - Monitor the adoption and effectiveness of the approach.
   - If a significant number of users need broader coverage (e.g., to include git repos or other dependencies), consider revisiting the decision for further enhancements.

### Summary:

The default strategy will focus on SBOMs for files and dataInjections, ensuring clarity, accuracy, and relevance for most use cases. For advanced needs, limited user-configurable flexibility will be provided.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision

Provide support in Zarf for declarative package deployments in connected environments without requiring a Zarf init package.

### Rationale

1. **Alignment with the Declarative Philosophy**: Enabling declarative deployments without requiring the Zarf init package aligns with Zarf's core philosophy of facilitating declarative Kubernetes (K8s) deployments, allowing users to focus on defining their desired state without extraneous prerequisites.

2. **Support for Connected Environments**: While Zarf was originally designed for disconnected environments, extending functionality to connected environments broadens its usability. Defaulting to the simpler deployment model in such scenarios aligns with the tool's goal of empowering a wide range of users.

3. **Improved UX**: Eliminating the need for a Zarf init package simplifies the workflow in connected environments. This reduces friction and makes Zarf more versatile across use cases.

4. **Flexibility Across Deployment Modalities**: Retaining the declarative features alongside simplification ensures Zarf remains robust while offering flexibility to both connected and disconnected environments.

### Implementation Actions

1. **Skip Init for Connected Environments**:
   - Detect if the Zarf deployment is in a connected environment.
   - Allow Zarf packages to be deployed without requiring Zarf init to be run, assuming necessary tools and dependencies are present in the environment.
   
2. **Dependency Validation**:
   - Implement mechanisms to validate dependencies (e.g., K8s cluster readiness, tools such as Helm or Kubectl) that would normally be installed via Zarf init.

3. **Configuration Options**:
   - Introduce a flag/config setting (e.g., `--skip-init`) to explicitly opt-out of using the Zarf init package during deployments in connected environments.
   - Allow users to define packages and initiate deployments without boilerplate setup.

4. **Documentation**:
   - Provide use cases and workflows illustrating the deployment process for connected environments without Zarf init.
   - Communicate clearly when skipping init is recommended and the associated trade-offs. 

5. **Backward Compatibility**:
   - Ensure that existing workflows using Zarf init are unaffected by the addition of this capability for connected environments. 

### Alternatives Considered
1. **Requiring Zarf Init for All Deployments**: This ensures consistency across disconnected and connected environments but limits adoption and simplifies workflows unnecessarily in connected use cases.

2. **Separable Deployment Model**: Fully splitting workflows into “disconnected” and “connected” modes of deployment. While conceptually clear, this adds complexity and diverges from the core idea of providing unified declarative deployments.

By prioritizing flexibility without sacrificing the declarative ethos, Zarf can more effectively address the needs of both disconnected and connected environments.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision  

We will redesign the `scripts` functionality to improve flexibility and scalability while addressing current limitations. Instead of the fixed sections (`before`, `after`, `prepare`), we will introduce a more versatile and modular approach, allowing teams to define custom script stages and specify their execution context and order more granularly. This will make the `scripts` feature adaptable to varied user scenarios and easier to extend in the future while maintaining backward compatibility for existing packages.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision

We will proceed with optimizing the `Crane` implementation as the primary solution for handling local image support in Zarf, while addressing its performance drawbacks as much as possible. The `Crane` approach strikes the best balance between performance, compatibility with future package schema changes, and avoidance of OOM conditions compared to other alternatives like the Docker CLI or Docker Go client.

### Justification:
1. **Performance Balance**: While `Crane` can lead to moderately slower image loading compared to existing solutions, it performs significantly better than many alternatives (e.g., Docker CLI). The performance benchmarks indicate `Crane` achieves usable speed even with large images, with room for optimization.

2. **Memory Efficiency**: The current naive `Crane` implementation that triggers OOM conditions for large images can be addressed by enabling Crane's streaming option, which avoids memory overload at the cost of speed. This ensures stability for users working with larger images, especially in memory-constrained environments.

3. **Future Compatibility**: The reliance on `Docker` for the save operation produces a legacy format that may not align well with upcoming changes for OCI imports. The `Crane` library better supports modernization and aligns with the project's forward-looking goals.

4. **Usability**: Despite a slight slowdown in certain workflows, resolving crashes and facilitating broader support for large images significantly improves the overall usability of Zarf for local development. User feedback consistently prioritizes stability and reliability over minor performance differences.

5. **Cost of Alternatives**: Other investigated methods, such as Skopeo or using the Docker Go client directly, either fail to meet performance and compatibility needs or introduce additional complexities that outweigh their benefits.

### Next Steps:
- **Streamlined `Crane` Integration**: Prioritize enabling Crane's streaming capabilities to eliminate OOM errors when handling large images.
- **Performance Tuning**: Conduct targeted optimizations for `Crane` operations to improve speed, especially for the most common scenarios such as cached images or frequently used workflows.
- **User Education**: Update documentation to clarify performance expectations and provide tips for mitigating speed trade-offs (e.g., by enabling caching or limiting large image sizes in local workflows).
- **Monitoring & Feedback**: Gather additional user input on the revised implementation in future Zarf releases and iterate based on real-world performance and stability metrics.

This decision ensures Zarf remains a robust and forward-compatible tool for local development while minimizing risks associated with large image handling.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision

To streamline the deployment of Big Bang within Zarf, we will implement a ""dynamic package preparation"" approach to address the complexities involved:

1. **Flux Integration Automation**: Zarf will include automated scripts or configurations to setup Flux and its CRDs as part of the Big Bang installation. This ensures that the required environment for deploying Big Bang is pre-configured, eliminating manual steps that may lead to errors.

2. **Selective Image Packaging**: Zarf will support an intelligent image packaging mechanism that dynamically determines the subset of images required based on the provided `values.yaml` file. This will reduce the storage requirements for Zarf packages, optimizing them to include only the necessary images for a specific deployment.

3. **Selective Git Repository Cloning**: Similar to the image packaging functionality, Zarf will provide a mechanism to identify and clone only the required git repositories for a specific deployment. This will further simplify the deployment process and avoid unnecessary bloat in Zarf packages.

4. **Enhanced `values.yaml` Integration**: Zarf will offer an intuitive configuration interface or tooling for users to inject `values.yaml` into the Big Bang deployment structure seamlessly. This includes mapping the values directly into the deployment's kustomization file to automatically discover and resolve the dependencies (e.g., images, repositories) specified in the values.

5. **Storage Optimization Strategies**: Zarf will implement compression strategies and deduplication techniques for packaging Big Bang-related images and repositories to further reduce package sizes.

By implementing these decisions, Zarf simplifies Big Bang deployments for end users, reduces resource requirements (storage, bandwidth), and minimizes deployment errors caused by manual configurations. This approach ensures that deployments are tailored and optimized for individual project needs without putting unnecessary overhead on the user or the system.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision

We have decided to implement native support in Zarf for publishing and retrieving Zarf packages to and from OCI (Open Container Initiative) compliant registries. This will involve creating functionality within Zarf to allow users to:

1. Publish a Zarf package directly to an OCI-compliant registry without requiring manual file transfers or local hosting.
2. Sign the Zarf packages during the creation process, using cryptographic signatures, to ensure both integrity and trustworthiness.
3. Verify package signatures during retrieval and deployment to confirm that the package has not been tampered with and originates from a trusted source.

This solution aligns with the priorities of enabling scalable package distribution, maintaining a high level of security, and ensuring trust in the software supply chain. 

Key considerations for this decision include:
- Using existing OCI registries as a proven, scalable, and widely supported distribution mechanism.
- Leveraging cryptographic signing standards (e.g., cosign) to provide integrity and authenticity.
- Preserving compatibility for users who may already be using OCI registries for other container distribution workflows.

This approach supports the primary objective of simplifying the package publishing and distribution process for users, while also fulfilling the secondary goal of enhancing supply chain security through trusted package signatures.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision 

### Extend Zarf to support native Git protocols and language-specific artifact repositories in air-gapped environments.

### Rationale:
To address the current gap in air-gapped development, where developers require broader access to programming language libraries and artifacts, and enhanced capabilities for Git workflows, Zarf will be updated to:

1. **Support native Git protocols**: Enable Zarf to facilitate more generic Git use cases beyond the current reliance on Flux `GitRepository` objects. This includes supporting native Git pulls, pushes, and clones that can interact with upstream Git hosts, allowing developers to work seamlessly with traditional Git workflows.

2. **Integrate language-specific artifact repositories**: Extend Zarf’s functionality to support a range of artifact repository types (e.g., Maven for Java, PyPI for Python, npm for JavaScript, etc.), enabling developers to fetch and use libraries required for software compilation and development in air-gapped environments.

### Trade-offs:
- **Implementation Complexity**: Adding support for multiple language-specific repository types introduces additional engineering complexity and maintenance overhead.
- **Potential Storage Overhead**: Hosting additional libraries and artifacts locally within the air-gap may require allocation of significant storage resources.
- **Increased Deployment Scope**: Supporting native Git protocols and securing additional repositories will require updated workflows and potential security considerations.

### Alternatives Considered:
1. **Do nothing**: Continue relying on current setups within Zarf, leaving users to handle air-gap development dependencies outside of Zarf. This was rejected due to the negative impact on developer productivity and adherence to air-gap requirements.
2. **Limited Language Repository Support**: Focus only on one or two commonly used programming languages. This was rejected as it does not comprehensively address the needs of diverse development environments.

### Next Steps:
1. **Research and Prototype**:
   - Investigate existing tools and APIs for creating and hosting language-specific artifact repositories in an air-gapped setup.
   - Prototype extensions for native Git workflows within Zarf.

2. **Implementation**:
   - Develop and test support for a subset of widely-used language repository types and workflows (e.g., Maven, PyPI, npm).
   - Add features to Zarf for hosting and facilitating native Git protocol workflows with upstream hosts.

3. **Documentation and Training**:
   - Create user guides and training materials for developers to use the new Zarf features effectively in air-gapped settings.

This decision enhances Zarf's functionality to better cater to air-gap development needs, improving usability and expanding its scope for real-world development workflows.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision

To ensure Zarf becomes a ""Generally Available"" v1.x.x product that can be relied upon for mission-critical operations, the following decision is made:

Implement an organized and structured roadmap to transition Zarf into a stable and reliable production-ready product. This includes:

1. **Embrace Semantic Versioning:** Commit to semantic versioning practices for all releases to clearly communicate breaking changes, new features, and patches to users.
  
2. **Extend Testing Frameworks:** Develop comprehensive automated testing coverage across all critical areas, including unit tests, integration tests, and end-to-end tests. Prioritize high-risk or frequently used features to enhance stability.

3. **Establish a Long-Term Support (LTS) Branch:** Introduce an LTS branch for users requiring stability and fewer breaking changes. This branch will only incorporate essential bug fixes and security updates.

4. **Strengthen Documentation:** Create detailed and user-friendly documentation to help users navigate Zarf’s lifecycle, updates, and features without requiring deep involvement or technical expertise.

5. **User Feedback and Validation:** Implement regular feedback cycles with users through surveys, beta testing, and direct engagement to validate high-stakes workflows and address pain points proactively.

6. **Define Backward Compatibility Practices:** Reduce or eliminate the introduction of breaking changes by formalizing compatibility guarantees. Develop tooling (e.g., migration scripts) and processes to support seamless upgrades.

By adhering to these measures, Zarf will transition into a robust product capable of fulfilling mission-critical demands while empowering regular consumers to use it effectively without deep involvement in its lifecycle.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision

To address the shortcomings of the current solutions and to ensure seamless integration of ECR as a supported remote registry within Zarf, we will introduce **standardized deployment lifecycle hooks** that extend Zarf's core functionality to handle these recurring challenges in a system-agnostic way.

### Key Points of the Decision:
1. **Lifecycle Hooks as Core Zarf Functionality:**
   - Introduce new lifecycle hooks (`pre-deploy`, `deploy`, `post-deploy`, etc.) that are executed at key points during Zarf package deployments.
   - These hooks can be used to automate the management of external dependencies in a consistent, standardized way without requiring invasive or overly complex `actions`.

2. **Registry-Specific Hook Implementation for ECR:**
   - Provide built-in hooks specific to the ECR registry that will:
     - Automatically refresh authentication tokens and manage token rotations seamlessly for the cluster.
     - Ensure the programmatic creation of image repositories during Zarf package deployment, eliminating the need for ""push-to-create"" support in ECR.
   - These hooks will be integrated into Zarf as part of its core functionality, making ECR behavior consistent and predictable without requiring package creators to implement custom solutions specific to ECR.

3. **System-Agnostic Package Design:**
   - Package creators will design their packages using core Zarf functionality, relying on lifecycle hooks to handle external registry nuances.
   - This ensures that packages remain system-agnostic, while Zarf transparently manages ECR-related complexities.

4. **Backward Compatibility and Flexibility:**
   - For users or packages that rely on `actions`, these will remain supported, but their usage will transition to primarily optional or supplemental functionality. Hooks will become the preferred method for handling deployment lifecycle events.
   - Users can register new or custom hooks (e.g., for other registries or infrastructure requirements) without modifying the core package deployment logic.

### Outcome:
This decision ensures that Zarf’s deployment lifecycle delivers a seamless experience for cluster maintainers and package developers, especially when working with ECR. By shifting the burden of solving ECR-specific problems to the platform itself through hooks, package creators can focus on building portable, system-agnostic solutions without needing advanced knowledge of registry quirks.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision

Leverage **gRPC** for synchronizing the Go-based Zarf transformation logic with the TypeScript-based Pepr while maintaining the centralized and reliable Zarf transformation library. 

### Rationale:
1. **Performance**: gRPC provides efficient serialization via Protocol Buffers, which ensures fast and compact communication between systems, making it a suitable choice for performance-critical scenarios like admission controllers.
   
2. **Consistency**: By exposing Zarf’s transformation capabilities via gRPC, we can reuse the existing Go transformation logic directly without rewriting it in TypeScript, avoiding duplication of logic and ensuring updates remain consistent.

3. **Typed Contracts**: gRPC enforces strongly typed interfaces through Protocol Buffers, making it easier to synchronize TypeScript and Go systems while reducing integration errors.

4. **Cross-Language Compatibility**: gRPC natively supports both Go and TypeScript (via the gRPC-Web library), allowing seamless interoperability without the need for separate codebases.

5. **Avoids Rewrite Overheads**: Rewriting the library in TypeScript may introduce bugs, require additional validation efforts, and consume significant developer time – using gRPC avoids this altogether.

6. **Modern Interface**: Compared to REST, gRPC allows for bi-directional streaming, better suited for evolving transformation needs in admission logic.

By choosing gRPC, we decouple transformation logic, keep a single source of truth in Zarf, and enable Pepr to interact seamlessly with the existing library for current and future requirements.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision

Standardize and enhance Zarf's package loading and handling capabilities to ensure consistent functionality across all supported package sources and actions.

### Key Actions

1. **Support Inspect, Deploy, and Remove for All Sources**:
   - Develop consistent handling mechanisms to ensure every package source type (e.g., Local Tarball, Split Tarball, OCI package, In-cluster package, etc.) can be inspected, deployed, and removed seamlessly.

2. **Expand Compatibility for Split Tarball**:
   - Enable inspection and removal functions for Split Tarball packages.

3. **Enhance In-Cluster Package Management**:
   - Implement the ability to inspect in-cluster deployed packages, in addition to their current removal functionality.

4. **Extend HTTPs URL Support**:
   - Enable inspection and removal actions for HTTPS URL packages to align with their deploy functionality.

5. **Abstract Package Source Management**:
   - Introduce a unified interface or API for package source handling that defines a common contract for all supported sources. This will ensure flexibility for extension as new package sources are added in the future.

6. **Improve Error Messaging**:
   - Provide clear and actionable error messages when actions are attempted on unsupported package source types to guide users effectively.

By standardizing and expanding Zarf's package handling capabilities, we can improve usability and eliminate inconsistencies in how various package sources are managed across deploy, inspect, and remove actions.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision

To address the challenges associated with composing components between packages and to improve scalability, maintainability, and clarity, the following decision has been made:

1. **Centralize Component Import and Filtering Logic:**  
   Refactor the package creation logic to establish a centralized mechanism for handling component imports and filtering. By creating a unified system to process imports and filters, complexities arising from scattered implementations will be eliminated, reducing bugs and improving maintainability.

2. **Enhanced Import Chain Handling:**  
   Redesign the import chain resolution to support deeper layering and guarantee robust testing for all import levels. Introduce unit and integration tests that evaluate multi-layer import scenarios, ensuring full coverage and scalability for larger package systems.

3. **Reassess and Refactor OCI Skeleton Integration:**  
   Update the OCI skeleton feature by rethinking its design in the context of composability. The integration will be reworked to align with the composable package concept, preventing it from being an isolated ""bolt-on"" feature. A seamless integration will ensure that OCI skeletons work fluidly within complex composable packages.

4. **Improve Test Coverage Across Layers:**  
   Develop a comprehensive testing strategy that includes edge cases, such as nested imports, `only` filtering impacts, and interactions with OCI skeletons. This strategy will ensure that the system supports the expected scaling behavior and remains reliable over time.

5. **Backward Compatibility Consideration:**  
   While implementing the above changes, ensure backward compatibility where possible. For any breaking changes, provide clear documentation and migration guides to help package creators transition smoothly.

This decision streamlines the package creation process, enhances reliability for large-scale composable packages, and lays a foundation for future growth and feature expansion.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision

### Title: Introduce Incremental Package Update Mechanism for Zarf Development Lifecycle

To alleviate the inefficiencies in the current package development cycle and streamline debugging, we will introduce an **Incremental Package Update Mechanism** within Zarf. This decision introduces an optional mode where instead of recreating the entire package from scratch, developers can rebuild only the modified parts of a package and selectively deploy updates to a cluster without re-initialization.

---

### Rationale
The current workflow is optimized for air-gapped deployments but imposes significant overhead during iterative debugging. Debugging errors in large packages requires frequent recreation and redeployment, which is tedious and time-consuming. The proposed mechanism removes the need to fully recreate or reinitialize a cluster when only small changes are made, especially for connected environments.

Key benefits of this approach include:
- **Speed**: Incremental updates reduce the need for redundant work, dramatically speeding up the debugging cycle, especially for larger packages or small changes.
- **Developer Productivity**: Developers can iterate faster with less downtime.
- **Flexible Compatibility**: The mechanism fits well in connected development environments while retaining compatibility with air-gapped deployments.

---

### Details

The implementation will involve the following adjustments:
1. **Incremental Rebuild Capability**:
   - Modify `zarf package create` to detect unchanged components in the `zarf.yaml`. It will only fetch and assemble dependencies for newly added or modified components, reducing redundant processing.

2. **Selective Deployment**:
   - Extend `zarf package deploy` to allow deploying only the modified/selected components of the package to the cluster without requiring a full package re-deployment.

3. **Cluster State Validation**:
   - Introduce checks to ensure that incremental deployments can properly validate compatibility with the cluster’s current state (e.g., avoid dependency mismatches or conflicts).

4. **Optional Mode for Connected Environments**:
   - Expose a new CLI flag, such as `--incremental` or `--fast-debug`, tied to features optimized for connected environments. This mode will include behaviors similar to `YOLO`, such as skipping re-fetching certain heavy dependencies if unchanged.

---

### Constraints
1. Changes must not compromise air-gapped environment workflows or security considerations.
2. Developers working in connected environments may need to occasionally refresh the full package or initialize the cluster to ensure consistency with production-like deployments.

---

### Alternatives Considered
1. **Enhancing YOLO Mode**:
   - While YOLO mode is designed for rapid development, it is not exposed as a CLI flag and lacks granularity for incremental updates at the package or component level.
   - Incremental updates provide finer control over debugging and partial deployments.

2. **Interactive Debugging Mode**:
   - A potential alternative was providing an interactive mode for development where errors could be resolved live during deployment. This was deemed infeasible given Zarf’s architecture and focus on package immutability.

---

### Risks
1. **Incremental Drift**: If selective updates are mismanaged, there is a risk of the deployed cluster deviating from the expected state of the full package. Strong validation mechanisms will mitigate this.
2. **Complex Debugging Behavior**: Introducing incremental workflows may add complexity to debugging for developers unfamiliar with the distinction between incremental and full deployment modes.

---

### Next Steps
1. Design the architecture for incremental package updates, leveraging checksum-based detection or other mechanisms to identify modified package components.
2. Update CLI commands to expose incremental modes (`--incremental`, `--fast-debug`, etc.) with documentation.
3. Build robust validation workflows to confirm cluster compatibility during incremental deployments.
4. Run user tests in connected environments to refine developer usability and ensure alignment with air-gapped workflows.

---

### Outcome
This decision will improve the development experience for Zarf package creation and deployment in iterative workflows while retaining core compatibility with production-grade air-gapped environments.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision

Migrate the documentation site to Astro.

### Rationale:
- **Performance**: Astro is designed for optimal performance, delivering only the necessary JavaScript to the browser. This can significantly improve the loading time compared to Docusaurus.
- **Modern architecture**: Astro provides a modern, flexible approach to static site generation, making it easier to manage dependencies and scale the site as needed.
- **Flexibility**: Astro allows integration with multiple front-end frameworks (React, Vue, etc.), giving us more control over components and design.
- **Community growth**: Astro has a rapidly growing ecosystem and strong community support, making it a future-proof choice.

### Key Considerations:
- Assess the migration effort, including refactoring components and rewriting documentation structure.
- Ensure compatibility with the current hosting setup.
- Plan for regression testing to ensure functionality and design consistency in the new system.

### Next Steps:
- Create a proof-of-concept for the documentation site using Astro to evaluate feasibility.
- Audit the existing content and identify pages requiring adjustments for migration.
- Define timeline and roadmap for migration and deployment.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision 
Replace the existing in-house logging solution with a standardized, thread-safe, and test-friendly logging library or framework, such as `logrus` or `zap`, to address the identified challenges.

### Rationale:
1. **Thread Safety**: By adopting a thread-safe logging library, issues with accessing shared global state (like the progress bar and spinner) can be mitigated, allowing cleaner multi-threading access patterns without causing race conditions.

2. **Testability**: Replace functions like `message.Fatal` with mechanisms that do not immediately exit the program but instead return errors or allow graceful handling within test scenarios. This improves unit testing across all modules by avoiding abrupt program termination and ensures clean-up operations are not skipped.

3. **Decoupling Logging from Business Logic**: Moving to a standardized library promotes separation of concerns and can align logging impacts such as writing to different destinations (STDERR vs debug logs) in a more structured way.

4. **Consistency**: A mature logging library provides clear state management, well-defined log levels, and configurable output options that lead to a more consistent user experience for CLI outputs.

5. **Extensibility**: Standard libraries often come with advanced features such as structured logging, adding tracing identifiers, and better error handling mechanisms. These additions enhance the overall debugging and development experience.

### Implementation Steps:
1. Evaluate and select a mature logging library (`logrus`, `zap`, or equivalent) based on performance and compatibility with Zarf's existing dependencies.
2. Replace all existing logging function calls with the API of the chosen library, including replacing progress bar and spinner functionality with similar features in the library.
3. Refactor the handling of program exits (`message.Fatal`) to leverage error-handling flows that allow graceful exits and proper clean-up operations.
4. Ensure configuration options are in place to allow toggling of various log destinations and verbosity levels (debug, error, warning, info).
5. Implement thread-safe mechanisms for any shared state interactions (e.g., progress bar management) during logging operations.
6. Validate changes through thorough testing, including unit tests and integration tests for multi-threading scenarios.

### Trade-offs:
- **Time to refactor**: There will be a significant development effort to replace all existing logging function calls and patterns across packages.
- **Learning curve**: Developers familiar only with the in-house solution will need to get familiar with the new library.
- **Dependency overhead**: Adding an external logging library introduces a dependency in the codebase.

Nonetheless, adopting a standardized solution ensures long-term maintainability, scalability, and robustness for Zarf.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision

**Establish a Versioned Schema Starting with v1**

To address the evolving schema issue and prepare Zarf for a stable, long-term support model, Zarf will adopt an explicitly versioned schema starting with version 1. This versioned schema will establish a clear and predictable contract with users regarding supported fields and features.

### Key Decision Points:

1. **Introduce a v1 Schema:**
   - A v1 schema will be defined and finalized, documenting supported fields and usage.
   - Deprecated fields and features will be reviewed, and those marked for removal in v1 will be permanently removed.

2. **Backwards Compatibility and Deprecation Handling:**
   - Deprecated keys will remain compatible for the duration of Zarf pre-v1 versions. 
   - With the introduction of the v1 schema, deprecated keys will no longer be supported, reducing technical debt.

3. **Explicit Versioning:**
   - The Zarf schema will explicitly include a version field (e.g., `schemaVersion: v1`) in its configuration to indicate the supported schema version.
   - Zarf binaries will validate the schema version of any configuration file or package. If the version is incompatible, clear and actionable error messages will be provided.

4. **Migration Tools and Warnings:**
   - For users who wish to migrate to the v1 schema, tooling will be provided to automatically convert deprecated fields wherever possible.
   - Any fields that cannot be automatically migrated will trigger a clear warning or error message with detailed migration guidance.
   - During the lead-up to v1, Zarf will continue to warn users when deprecated fields are detected, preparing them for breaking changes.

5. **Longevity of v1 Schema:**
   - The v1 schema will remain stable and will not introduce breaking changes. Deprecation of features within the v1 schema will be clearly marked and documented but will remain functional for the lifetime of v1.

6. **Future Schema Evolution:**
   - As new features and fields are introduced in subsequent schema versions (e.g., v2, v3), Zarf will maintain compatibility with the v1 schema unless explicitly noted.
   - Users will be able to opt in to newer versions at their own pace, ensuring a smooth transition path.

This decision ensures that Zarf’s schema evolves in a predictable and maintainable manner, while fostering user confidence and reducing confusion with breaking changes. By transitioning to a versioned schema, Zarf establishes a clear communication strategy for maintaining backward compatibility and supporting users during and after the v1 launch.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
