context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision  
We will implement native lazy loading for images and thumbnails inside the Storefront by using the `loading=""lazy""` attribute.  

This approach is based on the following considerations:  
1. Native lazy loading support is now widely available in modern browsers, as verified through compatibility checks such as ""Can I Use.""  
2. Relying on native lazy loading avoids the need for integrating third-party libraries like ""lazysizes,"" simplifying implementation and reducing dependencies.  
3. Native lazy loading is more performant in terms of resource usage since it is built into the browser, allowing it to optimize based on its internals.  

### Implementation steps:  
1. Update the image and thumbnail rendering logic in the Storefront to include the `loading=""lazy""` attribute for all non-critical images.
2. Exclude specific images (e.g., above-the-fold hero images) that should load immediately for a better user experience. These should explicitly use `loading=""eager""`.  
3. Ensure appropriate fallback behavior for older browsers that do not support native lazy loading, though their usage is diminishing.  
4. Test the feature in various browsers and under different network conditions to validate performance improvements and identify any edge cases.  

This decision ensures modern and efficient image handling while maintaining simplicity and leveraging built-in browser features.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision 
The `autoload` parameter of `OneToOneAssociationField` and `ManyToOneAssociationField` will be deprecated and default behavior will be changed to `autoload = false`. 

Specifically:
1. All future uses of these association fields will require explicit queries to load referenced data, eliminating unnecessary automatic data loading.
2. Developers will have to explicitly specify and control the data that needs to be loaded, improving application and database performance.
3. Existing codebases relying on `autoload = true` will need to be migrated to explicitly fetch associated data where necessary.
4. This decision will reduce unintended joins, unnecessary data hydration, and overly large API payloads, addressing the performance bottleneck.

This change ensures greater control over data access and encourages developers to optimize associations properly rather than relying on implicit data loading.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision  
We will enhance the organization of ADRs by introducing a supplementary index file in the root ADR directory. This index file will address the challenges posed by the current folder-based organization and improve the usability for different search scenarios. The index will operate as follows:

1. **Chronological Order**: The index file will list all ADRs in reverse chronological order (most recent first), allowing users to easily browse and read the latest ADRs without navigating the folder structure or relying on Git history.

2. **Headlines & Links**: Each entry in the index will include the ADR headline and a link to the corresponding file. This will make it easy to locate a specific ADR by its title or topic.

3. **Tags**: To improve discoverability for case 3 (searching by area), the index will include tags or categories alongside each ADR that indicate the area it belongs to (e.g., ""Database"", ""Security"", ""Infrastructure""). This will make it easier to cross-reference ADRs across different folders.

4. **Automated Updates**: To ensure the index remains up-to-date, we will script an automated process (or a Git pre-commit hook) that updates the index file whenever a new ADR is added or an existing one is modified. Developers will not need to manually maintain the index.

5. **Searchable Format**: The index will be stored in a simple markdown format or another easily searchable, readable format, ensuring compatibility with any text editor and web-based viewers like GitHub.

By adding this index file, we aim to retain the benefits of the existing folder structure for case 3 while significantly improving readability, usability, and discoverability for cases 1 and 2.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision 
We will discontinue the evaluation and development of the proposed Vue 3-based plugin extension system that relies on native Vue tools. Instead, we will retain and improve the current Component Factory-based plugin system.

### Rationale:
1. **Addressing Vue Compiler Limitations**: The proposed approach faced significant challenges due to Vue's compiler optimizations, which made it difficult to merge and manipulate data in a way that aligns with our extensibility goals for plugin developers.
   
2. **Stability Concerns**: The need to rely on internal, non-public Vue logic to overcome these challenges poses a high risk of fragility. Any updates to Vue could break the proposed implementation, compromising long-term maintainability.

3. **Developer Experience**: While the proposed system provided better static analysis and performance on paper, the complexity and unpredictability of its implementation would negatively impact developer experience, especially for plugin developers who need consistent and reliable extensibility mechanisms.

4. **Performance and Compatibility**: Transitioning to the new system without ensuring complete and reliable data accessibility for plugins diminishes the value of the extensibility and could hamper performance gains expected from adopting native Vue tooling.

### Next Steps:
- Retain the current Component Factory-based system but actively address its limitations:
  - Explore opportunities for partial pre-compilation to improve performance.
  - Investigate integrating Vue tools carefully within the existing factory-based approach to gain some benefits of linting, static analysis, and modern Vue features without a complete paradigm shift.
  - Optimize the runtime performance issues associated with Twig.JS template compilation.
  
- Reevaluate and iterate on the use of Vue 3 for future extensibility solutions, but only after addressing the identified challenges and ensuring compatibility, stability, and extensibility.

This decision prioritizes stability, extensibility, and developer experience while mitigating the risks and challenges posed by the proposed approach.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision  
We will adopt a ""changelog entry per change"" approach to improve compatibility with trunk-based development and eliminate merge conflicts. 

### Implementation:
1. Each technical change that requires a changelog entry will be accompanied by a new changelog file. This file will be:
   - Created as part of the branch or pull request containing the change.
   - Placed in a dedicated `changelog/` directory within the repository.
   - Named using a unique identifier, such as the Jira issue number or commit hash, to avoid naming collisions (e.g., `changelog/JIRA-1234.md`).

2. Each changelog file will:
   - Contain the necessary details about the change, including the description, affected areas, upgrade instructions if applicable, and any breaking changes.
   - Omit assigning a specific release version, as the version will be determined during the actual release process.

3. When a release is being prepared:
   - All changelog entries in the `changelog/` directory will be consolidated into the unified changelog for the release.
   - A script will assist with automating this process, ensuring entries are formatted and grouped consistently.

4. Benefits of this approach:
   - **Merge conflicts are avoided** because each change has its own individual changelog file.
   - **Uncertainty about release versions is reduced**, as the version assignment occurs during release preparation rather than during development.
   - **Alignment between Jira and code is ensured**, as the changelog entry creation becomes a part of completing the development and review process.

This process will provide clearer organization, streamline trunk-based development, and maintain an accurate release history with minimal friction for contributors.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision  
We will implement a mechanism to capture and store the relevant state of data and rules at the moment the delay is initialized to ensure consistent execution after the delay.

### Approach:
1. **Data Snapshot for Event Context**:  
   - When the action is delayed, take a snapshot of the data associated with the event context at that specific moment.  
   - Store this snapshot alongside the delayed action in an immutable store to ensure it remains unchanged during the delay period.  
   - The snapshot will be used to restore the exact state of the data when the delayed action is executed.

2. **Rule Evaluation State**:  
   - Capture the rules associated with the delayed action at the moment the delay is applied (e.g., their configuration or definitions).  
   - Store these rules as part of the delayed action metadata. If rules are dynamically sourced or updated during execution, ensure the rules associated with the snapshot are stored in a retrievable form (e.g., a versioned rules registry).  
   - Upon execution of the delayed action, re-evaluate the rules utilizing the captured rules and restored data snapshot.

3. **Fallback Mechanism for Missing or Changed Rules**:  
   - If the originally associated rules no longer exist when the action is executed, log a warning or error indicating the discrepancy.  
   - Optionally allow configuration to define fallback handling (e.g., skipping rule evaluation or applying default behavior).

### Benefits:  
- This approach ensures that delayed actions execute with the same context and rules as originally intended, maintaining consistency and correctness.  
- By storing discrete snapshots of data and rules, we avoid reliance on mutable or runtime-driven states that may change unpredictably during the delay period.  
- The immutable store ensures transparency in the processing of delayed actions and simplifies debugging.  

### Trade-offs:
- This decision introduces additional storage and processing overhead for capturing and maintaining snapshots of data and rules.  
- Dependency on the availability of a reliable storage system for snapshots and rule versions.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision 
The app system will be integrated directly into the Shopware platform as a first-class citizen, moving it out of its current status as a standalone plugin. 

### Rationale:
- By integrating the app system into the core platform, we signal to our partners and developers that it has reached a level of maturity and stability they can rely upon for long-term development.
- Integration with the Shopware 6 release cycle ensures that app manufacturers benefit from platform upgrades in a predictable and structured manner without having to manage a separate plugin lifecycle.
- Users will no longer need to install and configure an additional plugin to leverage apps, reducing barriers to entry and improving overall user experience.
- This approach aligns with the vision of providing a seamless and consistent experience across both cloud and on-prem environments, as apps are supported natively in the platform.

The migration process will ensure backward compatibility to allow existing app users to transition smoothly without interruptions. Following this integration, further improvements and adjustments to the app system will adhere to the regular platform development and release workflow.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision  
We will refactor the `FlowStorer` implementations to consolidate their functionality into a single, generic interface for scalar value storage and restoration logic. This unified interface will reduce boilerplate code and streamline the process of adding new storer implementations or bypassing certain events for plugins.

Key changes:  
1. Introduce a single generic `FlowStorer` interface with clearly defined methods for storing and restoring data.  
2. Decouple the identification logic for whether event data should be stored from individual storer classes by creating a shared, reusable mechanism for event filtering or bypass.  
3. Refactor all existing `FlowStorer` implementations to conform to the unified interface, ensuring consistent behavior across all storers.  
4. Reduce redundancy by reusing common logic in shared components or utility classes.  

By consolidating the logic, we make it easier for developers to extend functionality and enable plugins to interact with the storer system seamlessly. This decision aligns with reducing complexity, improving transparency, and encouraging modularity within the system.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision  
We will replace the current sw-text-editor in the administration with a third-party text editor library that meets the following criteria:  
1. **Easy to Maintain**: The chosen library should have clear documentation, an active development community, and a modular architecture to simplify debugging and updates.  
2. **Feature Set**: The editor should provide robust WYSIWYG features, such as formatting, hyperlinking, image embedding, and list management, with stable behavior.  
3. **Flexibility**: The editor needs to be customizable and extensible, allowing us to add features unique to our administration interface if required.  
4. **Stability**: The editor should have a proven track record of reliability, reducing low-level bugs and edge case issues encountered in the current implementation.

After assessing the available libraries, we will integrate the chosen third-party editor into the administration system. The first steps will involve evaluating potential options, conducting trials, and collecting feedback from developers and users. This approach will enable us to deliver a robust and maintainable solution while mitigating the challenges of the existing sw-text-editor system.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision  
Update the default salutation within the Shopware 6 platform to ""not_specified"" for cases where the salutation is not provided by the customer or administrator. 

This decision promotes inclusivity by avoiding assumptions about gender or preferred titles and ensures a consistent approach to handling unspecified salutations across the platform. The change will be implemented in all relevant areas of the platform, including customer profiles, order details, and administrative interfaces.

Additionally:
1. The documentation will be updated to inform users and developers about the default ""not_specified"" salutation.
2. Any existing logic relying on presumed salutations will be reviewed and modified to account for the possibility of ""not_specified.""
3. Testing will be conducted to ensure that the change does not break existing functionality and complies with the inclusivity goal.
4. Communication will be made to stakeholders to explain the rationale for the change and its implications. 

By making ""not_specified"" the default, Shopware strengthens its commitment to inclusiveness and respects a diverse range of customer identities.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision  
We will remove the use of the `padaliyajay/php-autoprefixer` package from the `theme:compile` process. 

### Rationale:
- **Performance Improvement**: Removing the auto-prefixer will significantly enhance the performance of the `theme:compile` process, which is important for our SaaS solution where compilation speed directly impacts user experience.
- **Redundancy**: With the updated browser support introduced through Bootstrap v5 and the modern `.browserslist` configurations, the majority of the vendor prefixes generated by the package are no longer relevant for supported browsers.
- **Maintenance**: The static, hard-coded behavior of `padaliyajay/php-autoprefixer` does not align with our need for dynamic adaptability based on `.browserslist`, making it unsuitable as a long-term solution.

### Implementation Plan:
1. **Remove Dependency**: The `padaliyajay/php-autoprefixer` package will be removed from the project.
2. **Modernize the Compilation Process**: Ensure that `.browserslist` is accurately configured and maintained to reflect the browsers we support.
3. **Document Browser Support**: Clearly communicate the supported browser versions and ensure this is documented alongside the deprecation of the auto-prefixer.
4. **Fallback Plan**: In the rare event that certain vendor prefixes are still required, a more modern and flexible tool such as `postcss` or an equivalent JavaScript-based pipeline could be explored for future use.

This decision eliminates unnecessary overhead from the compilation pipeline while aligning the project with modern web best practices and sustainable browser support configurations.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision  
We will avoid using pre-release package versions (e.g., `1.9.0-alpha1`) in production or in scenarios where security reporting by versioning tools like npm is critical.  

Pre-release versions will only be used for development, testing, and experimental purposes. When a version is meant for production or usage where dependency security is crucial, only stable, non-pre-release versions (e.g., `1.9.0`) will be utilized.  

### Rationale:
1. **Security Monitoring**: Tools like npm interpret pre-release versions as `<0.0.0`, potentially causing critical security vulnerabilities to go unnoticed. Avoiding pre-release versions ensures proper version comparisons and accurate security reporting.  
2. **Stability**: Pre-release versions often represent work-in-progress or experimental features. Using stable, fully released versions minimizes risks of unpredictable behavior in dependencies.  
3. **Semantic Versioning Compliance**: This approach adheres to expectations of semantic versioning, where pre-release versions should not typically be deployed in production.

### Implementation:
- Adjust workflows to standardize on non-pre-release versions for any production or critical use cases.
- Clearly document the impact and limitations of pre-release versions to ensure transparency and understanding among contributors.
- Review and adjust dependency management processes to prevent accidental use of pre-release versions in scenarios requiring reliable security monitoring.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision  
An index will be added to the `cart` table to optimize the deletion of outdated cart entries. This will enable the query to leverage the database index and significantly improve performance for high-traffic shops. 

The following steps will be taken:

1. **Create Composite Index:**  
   A composite index will be added on the `cart` table for the `updated_at` and `created_at` columns. This will allow the database to more efficiently search for rows matching the conditions in the query.

   Example:  
   ```sql
   CREATE INDEX idx_cart_updated_created ON cart (updated_at, created_at);
   ```

2. **Modify the Query to Leverage Index:**  
   The SQL query for deleting outdated cart entries will be updated to ensure it properly utilizes the index. To use the index efficiently, the `IS NULL` condition will be avoided where possible, and the query will be refactored to simplify condition checks.

   Example of the updated query:  
   ```sql
   DELETE FROM cart
   WHERE (updated_at <= '2023-02-01' OR (updated_at IS NULL AND created_at <= '2023-02-01'))
   LIMIT 1000;
   ```

3. **Test and Validate:**  
   After creating the index and modifying the query, we will validate that the query plan (`EXPLAIN`) shows the index being utilized (`possible_keys` referencing the newly added index). Performance improvements will be measured, ensuring query execution time is within acceptable limits.

4. **Implementation and Rollout:**  
   The index and optimized query will be deployed to production environments during low-traffic periods to minimize potential impacts during the schema change.

5. **Monitor Performance Post-Deployment:**  
   Post-deployment monitoring will be conducted to ensure the changes achieve the desired performance improvement without introducing regressions.

This solution aligns with database optimization best practices and will resolve the long query execution times observed on high-traffic shops.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision  
We will introduce the concept of ""experimental features"" to our release process. Experimental features will allow us to release early-stage functionality to gather feedback while explicitly communicating to users that the feature is not yet stable and is subject to change.

### Key Details:
1. **Designation**: Features will be marked as ""experimental"" until they reach a sufficient level of maturity and stability. This designation will serve as an explicit disclaimer that breaking changes or removals might occur without adhering to our usual backward compatibility guarantees.
2. **Clear Communication**: Experimental features will be clearly labeled in the documentation, UI, APIs, and any other relevant touchpoints to ensure users are aware of their experimental nature.
3. **Feedback First**: We will actively seek feedback from users during the experimental phase. The feedback loop will be instrumental in guiding which features are further developed and what improvements are prioritized.
4. **Incremental Development**: Releasing features in increments will allow us to provide foundational functionality early (e.g., basic employee management for B2B or foundational query support for Advanced Search) and build on top of that incrementally, based on user needs and feedback.
5. **Transition to Stable**: Once a feature has proven its utility, stability, and alignment with customer needs, it will be transitioned to a stable state. Once marked stable, it will be subject to our strict backward compatibility guarantees. The decision to transition or abandon the feature will be based on real-world usage and feedback.
6. **Opt-In by Default**: Experimental features will require an explicit opt-in (e.g., feature flags) so that users can make an informed decision to try them.

### Rationale:
- **Benefits for Users**: Customers can start using new features sooner, providing feedback and helping us build what they really need.
- **Reduced Risk**: We avoid the risk of overly investing in incorrect assumptions or building unnecessary functionality.
- **Agility**: Experimental features will enable quicker iterations and promote our ability to adapt based on data-driven insights.
- **Transparency**: By clearly labeling features as experimental, we maintain transparency with our ecosystem while avoiding premature stability commitments.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision  
We will refactor the stock handling system in Shopware 6 to make it more flexible, streamlined, and efficient.

### Key Decisions:
1. **Abstract Stock Management Logic:**
   - Introduce a dedicated abstraction layer (e.g., stock service interface) that allows for custom implementations of stock handling. This ensures that stock management logic can be customized or replacedfor example, by integrating with external systems like an ERP.

2. **Simplify Stock Representation:**
   - Replace the current dual stock representation (`stock` and `available stock`) with a single stock value. Stock will be reduced in real time as orders are placed, eliminating the complex calculation of ""available stock"" based on order status. This will simplify the handling process and reduce calculation overhead.

3. **Configurable Stock Increase/Decrease Logic:**
   - Provide flexible hooks or events in the order lifecycle to allow developers to modify how stock is increased or decreased. For example:
     - Stock reduction can occur at different stages (e.g., upon order creation or payment completion) based on the business needs.
     - Inventory restocking can be customized for canceled or returned orders.

4. **Optimize Stock Calculations:**
   - Improve performance for stock-related queries to handle large catalogs efficiently. This may include index optimization or caching mechanisms to minimize stock recalculation overhead.

5. **Backward Compatibility:**
   - Ensure that legacy implementations relying on the current stock model are not immediately broken. Introduce deprecation notices and migration tools so developers can transition to the new stock handling system.

This refactored stock approach will provide better flexibility, simplify stock management, and improve performance across large catalogs, while accommodating integration with external systems seamlessly.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision
We will implement a configurable mechanism to log specific exception classes at a reduced log level, such as `notice` or `info`, instead of the default `error` level. This will allow us to handle expected exceptions, such as `ShopwareHttpException` with HTTP-Status-Codes in the 40x range, appropriately without producing unnecessary noise in the error logs.

### Key Details:
1. **Configuration List**: A configuration list will define the exception classes that should be logged at a lower level. This list will be extendable to allow flexibility for future modifications or use cases.
2. **Default Behavior**: Exceptions not explicitly included in this list will continue to be logged at the default `error` level, preserving the expected behavior for unexpected or critical issues.
3. **Implementation**: The mechanism will be integrated with the `symfony/monolog-bridge` and evaluate exceptions against the configuration list before determining the log level.
4. **Benefits**:
   - Reduces noise in the logs caused by expected exceptions, improving the signal-to-noise ratio for actual errors.
   - Maintains the transparency and visibility of malformed requests without unnecessarily inflating error log entries.
   - Flexibility to adapt log handling for additional exceptions in the future without widespread code changes.

### Workflow:
- Expected exceptions, such as `ShopwareHttpException` with 40x status codes, will be logged at the `notice` level by default.
- Developers and admins retain the ability to modify the configuration list to adapt to specific needs or handling requirements.
- Logging levels for specific exceptions can still be adjusted globally or locally as needed to meet team or deployment-specific goals.

This decision ensures clearer prioritization and focus when analyzing application logs, reducing distractions caused by predictable, handled exceptions while preserving the information for debugging and auditing purposes.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision  
We will transition from using UUIDv4 to UUIDv7 as the default primary key type in our database.

This decision is motivated by the following:
1. **Performance Improvements**: UUIDv7 introduces a time-based prefix, which ensures a more sequential ordering compared to the completely random prefix of UUIDv4. This leads to more compact B-tree indexes, fewer page allocations, and improved database performance for insert operations.
2. **Integration Benefits of UUIDs**: UUIDs will continue to serve the purpose of easing the integration of multiple data sources due to their global uniqueness, with UUIDv7 retaining this advantage.
3. **Compatibility**: The move to UUIDv7 maintains compatibility with existing operations and libraries that leverage UUIDs as identifiers.

The migration plan will include:
- Updating the UUID generation logic to use UUIDv7 for all newly created entities.
- Planning for a gradual migration of existing UUIDv4 data where necessary, ensuring no disruption to current operations.
- Updating documentation to reflect the performance and structure advantages of UUIDv7 as part of our technical standards. 

This decision optimizes performance without sacrificing the benefits UUIDs offer as our primary key structure.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision  
We will introduce an extensible mechanism that allows applications to define and register their own custom events in the flow builder.  

This mechanism will include:  
1. A clear and standardized API or interface that developers can use to define custom events.  
2. A system for registering these custom events so they are seamlessly integrated into the existing flow builder alongside the pre-defined events.  
3. Proper documentation and examples to guide developers on how to create and register new events.  

By implementing this decision:
- Applications will achieve greater flexibility and customization in their workflows.  
- The structure will maintain modular and configurable principles, allowing the core flow builder to work seamlessly without tightly coupling with apps.  
- This will promote transparency for developers and maintain introspectability by clearly defining new custom events and their behavior in the flow structure.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision  
We will introduce a mechanism for apps to securely communicate with the App Server. This will involve issuing short-lived, signed tokens (e.g., JSON Web Tokens or JWTs) to apps via the Shopware backend. These tokens will include information such as the logged-in customer, shop context, and other relevant metadata.

The workflow will be as follows:
1. The app will initiate a request via the Shopware backend to acquire a signed token. This request will include authentication credentials (e.g., app API key) to validate the app's identity.
2. The Shopware backend will verify the app's credentials and, if valid, issue a signed token containing the required information (e.g., customer ID, shop context, expiration timestamp).
3. The app will use this token to make subsequent secure requests to the App Server.
4. The App Server will verify the token signature on every request to ensure that the token is authentic and untampered. Further, the token's expiration timestamp will be checked to validate it is still within the allowed time window.

This mechanism ensures that:
- The App Server can securely verify the origin of requests from apps.
- Sensitive and customer-specific operations can be handled securely when apps interact with the App Server.
- Tokens are short-lived, reducing the risk of misuse if stolen. They can be rotated or invalidated if needed.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision  
We will implement a ""Flow Builder Preview"" feature to allow users to preview and understand the steps and decisions within a flow without executing or simulating its actual actions. This feature will provide a transparent and comprehensible representation of the flow structure and its logic, empowering users to troubleshoot and validate their custom-built flows more effectively.

Key decisions and details:
1. **Visualization of Steps and Decisions**  
   - The preview will visually represent all the steps and decision points in the flow, in the order they occur.
   - It will provide clear indications of how decisions are made at various points in the flow (e.g., based on conditions or triggers).

2. **Non-execution Design**  
   - The preview is designed solely to outline the flow's structure and logic; it will not trigger any real actions or execute the flow's behavior.
   - This ensures safety and avoids unintended consequences during preview mode.

3. **Improved Debugging and Understanding**  
   - By providing a clear preview, we aim to make debugging and flow validation more intuitive for users, reducing the need for intervention by support teams.
   - Users will have a better understanding of why their flow behaves in a certain way before actual execution.

4. **Incremental Development**  
   - While this ADR focuses on the Flow Builder Preview, the preview will eventually complement other planned features, such as Flow Builder Logging, to create a comprehensive debugging and flow management experience.

By implementing the Flow Builder Preview, we are addressing existing challenges with debugging and improving user confidence in the reliability and transparency of their custom flows.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much real code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision 
We will refactor the media URL generation logic to decouple it from the `media.loaded` event subscriber and make it a more centralized system. The goal is to streamline and simplify the way media URLs and file paths are generated based on the chosen `PathnameStrategyInterface`.

### Key Points:
1. **Centralize URL and Path Generation**:
   - We will create encapsulated logic within the `UrlGeneratorInterface` and `PathnameStrategyInterface` for media URL generation instead of relying on event subscribers. This ensures consistency and reduces reliance on scattered mechanics for triggering URL generation.
   - The `UrlGeneratorInterface` will be responsible exclusively for building URLs (absolute or relative), while the `PathnameStrategyInterface` continues to provide path and cache-buster details.

2. **Delegate Responsibility to the Pathname Strategies**:
   - The `PathnameStrategyInterface` implementations will remain responsible for file and path-specific generation, including the `hash`, `cache buster`, and `physical filename`.
   - This ensures flexibility as new strategies can be defined and plugged in without additional modifications to the existing architecture.

3. **On-the-Fly URL and Path Generation**:
   - URLs will be generated on demand in a controlled way rather than at the time of the `media.loaded` event. This minimizes unnecessary processing during media entity loads when not required.

4. **Extendability and Customization**:
   - By adhering to well-defined interfaces, additional custom strategies or new features for media URL and path management can be implemented without disrupting existing functionality.
   - This will also lay a solid foundation for plugins or external modules to customize media behavior.

5. **Simplify Testing and Debugging**:
   - Centralizing the media URL generation logic reduces complexity, making it easier to test and debug scenarios involving different pathname and URL strategies.

6. **Backwards Compatibility**:
   - We will maintain compatibility with the current mechanism for generating URLs during the transition phase. However, the old event-driven behavior will be marked for deprecation in future releases.

### Implementation Plan:
- Update the `UrlGeneratorInterface` to delegate appropriate responsibilities to the `PathnameStrategyInterface`.
- Refactor the current event subscriber logic, moving URL generation to explicit method calls in services or controllers where required.
- Create better documentation and developer guides for extending and customizing media handling through `PathnameStrategyInterface` and `UrlGeneratorInterface`.
- Deprecate the use of `media.loaded` for triggering URL generation, providing deprecation warnings for developers relying on it.

This decision ensures a cleaner, more modular design for managing media URLs and pathnames in the system while supporting flexibility and future extensibility.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision  
We will introduce a dedicated ""one-time data migration"" functionality, separate from the indexer mechanism, to handle situations where data needs to be migrated or modified only once across Shopware versions.  

### Key Decisions:
1. **Separation of Concerns**:  
   - One-time migrations will no longer rely on the indexer.  
   - A clear distinction will be made between recurring indexing processes and one-time data transformations.

2. **Execution Context**:  
   - One-time migrations will be triggered during system updates or explicitly via a command-line interface or admin tool, ensuring precise control over their execution.  

3. **Idempotent Execution**:  
   - One-time migrations will be designed to be idempotent, ensuring that running the migration multiple times has no adverse effects. This will protect the system from errors in cases of accidental re-execution.  

4. **Version Tracking**:  
   - A versioning mechanism will be implemented to track executed migrations. This will prevent re-execution of the same migration, ensuring its one-time nature.  

5. **Implementation Example**:  
   - For cases like the migration of media path information, this functionality will provide reliable guarantees that the migration is executed once and only once, avoiding conflicts or errors with subsequent system behaviors.  

This dedicated ""one-time migration"" functionality will help improve the reliability and maintainability of data migrations in Shopware systems while addressing critical issues caused by repeated executions.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision  
We will introduce a structured and centralized tracking mechanism to effectively manage code associated with experimental features.

### Key Actions:

1. **Feature Identification and Tagging**:
   - Each experimental feature will be assigned a unique, consistent identifier (Feature ID).
   - All code related to a specific experimental feature must be annotated with this identifier using a standardized `@feature-id` annotation in code comments or metadata.
   - Expand on the current `@experimental` annotation to include the `Feature ID`, e.g., `@experimental(featureId = ""feature-123"", stableVersion = ""1.5.0"")`.

2. **Central Registry for Experimental Features**:
   - Create and maintain a centralized registry (e.g., as a configuration file or database) that maps `Feature IDs` to metadata like:
     - Feature name
     - Current status (e.g., active, extended, killed, deprecated)
     - Associated start date, stable version, and other tracking properties.

3. **Tooling and Automation**:
   - Build or extend existing tools to automatically:
     - Detect and list all instances of code annotated with `Feature ID`.
     - Highlight code fragments for experimental features marked as deprecated or scheduled for removal in the centralized registry.
     - Provide reports or notifications for features requiring updates to the `stableVersion` property.
   - Integrate these tools into code review processes and CI/CD pipelines to enforce compliance.

4. **Feature Lifecycle Management**:
   - Strictly adhere to the ""Killing Feature"" rule outlined in the [Experimental Features documentation](./2023-05-10-experimental-features.md).
   - When an experimental feature transitions to the killed state:
     - Annotate the relevant code sections with `@deprecated(featureId = ""feature-123"", removalVersion = ""2.0.0"")`.
     - Track its status and cleanup timeline in the centralized registry.

5. **Documentation and Training**:
   - Clearly document the process for annotating experimental features and updating metadata.
   - Train developers on how to use the new annotations and tools effectively.

### Consequences:
- **Positive**:
  - Streamlined tracking and maintenance of experimental feature code.
  - Improved ability to manage prolonged experiments and enforce deprecation/removal policies.
  - Clear, centralized visibility into the state and scope of all experimental features.
  - Reduced risk of missing updates or cleanup tasks tied to experimental features.

- **Negative**:
  - Initial development effort required to create the centralized registry and tooling.
  - Slight increase in the overhead for developers to consistently annotate and document experimental features.

This structured approach will address the detection and management challenges while aligning with our commitment to disciplined and transparent development practices.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision  
To enhance developer experience and simplify integrations, we will introduce unique human-readable identifiers (e.g., ""payment_stripe"", ""shipping_express"") for payment and shipping methods in addition to their existing IDs.  

These identifiers will be:
1. **Globally Unique**: Each identifier will be unique across all payment and shipping methods to avoid ambiguity.
2. **Stable**: Once assigned, identifiers will remain constant across updates, ensuring reliability for developers relying on them.

By implementing these identifiers, extension developers and app servers can directly reference payment and shipping methods without requiring API calls to look up IDs, thereby reducing overhead and improving efficiency.  

Additionally:
- Documentation will be updated to guide developers on how to define and use these identifiers.
- A validation mechanism will be put in place to ensure uniqueness and adherence to naming conventions for future extensions.  

This decision will maintain backward compatibility since existing IDs will remain functional, while the new identifiers provide an easier and more developer-friendly alternative.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision  
We will prioritize using Bootstrap's native utility classes for simple and repeatable styling needs, such as applying margins, padding, borders, and other basic layout changes, instead of creating custom SCSS rules. This approach will lead to a leaner, more maintainable stylesheet and reduce redundancy.  

### Guidelines:  
1. **Use Bootstrap Utilities Wherever Possible**  
   - Basic styles (e.g., spacing, borders, text alignment) should utilize Bootstrap utility classes in the HTML rather than custom SCSS.  
   - Example: Replace custom SCSS rules like `margin-bottom: $spacer;` with a Bootstrap utility class like `mb-3`.   

2. **Retain Semantic CSS Classes for Extensibility**  
   - Always ensure that semantically meaningful CSS class names (e.g., `shipping-modal-actions`, `shipping-abort`) exist in the HTML.  
   - These classes should not carry default styles but act as extension points for themes or specific overrides.  

3. **Use Custom SCSS Only for Complex Layouts**  
   - Custom SCSS should still be employed for advanced use cases where Bootstrap's utilities or components cannot achieve the desired layout cleanly, or relying solely on utility classes would result in convoluted HTML.  
   - Example: Multi-element components, grid breakpoints involving custom logic, or animations may require custom SCSS.  

4. **Avoid Overwriting Bootstrap Defaults**  
   - Refrain from overriding Bootstrap utilities or creating near-duplicates of them with custom SCSS. If an existing utility class is suitable, use it directly.  

5. **Documentation and Collaboration**  
   - Document this principle in the style guide to ensure all contributors are aligned in using Bootstrap utilities appropriately.  
   - Ensure code reviews enforce this principle by questioning custom SCSS additions where a Bootstrap utility could suffice.  

By leveraging Bootstrap utilities more effectively, we simplify code maintenance, reduce style duplication, and improve collaboration across teams, all while preserving flexibility for future theming through semantic class names.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision  
We will transition to using Playwright as our E2E testing framework to address the requirements and issues identified with the current Cypress test suite.  

**Key reasons for choosing Playwright:**  
1. **Deterministic and Reliable Tests:** Playwright provides context isolation and test fixtures that allow precise control over test environments, making tests independent of the external state and environment. This aligns with our goal for reproducible, deterministic test behavior.
2. **Cross-Environment Flexibility:** Playwright supports testing across different environments, including local, CI/CD pipelines, and cloud-based environments, enabling testing flexibility and scalability.
3. **Improved Speed:** Playwright's parallel test execution feature and support for headless browsers significantly improve test execution speed, reducing overall time required for regression testing.
4. **Readable Syntax and Accessibility to Non-Technical Users:** Playwright's API is clean and developer-friendly, supporting behavior-driven testing approaches like Cucumber/BDD plugins for team members, including non-technical stakeholders, to easily comprehend and contribute test scenarios.
5. **Alignment with Product Requirements:** Playwright enables us to write tests driven by product requirements by supporting robust functionality like descriptive selectors, API mocking/interception, and assertions. This ensures the tests validate real-world behavior effectively.  

**Next Steps:**  
1. **Setup the Playwright Framework:** Create a dedicated Playwright test repository and establish project scaffolding.  
2. **Develop Testing Guidelines:** Establish testing strategy documentation and standardize how tests are created. These guidelines will ensure consistency across teams and prioritize maintainability.  
3. **Migrate Existing Tests:** Gradually refactor and migrate critical Cypress tests to Playwright to minimize disruptions.  
4. **Train the Team:** Provide training and documentation for all team members on using Playwright effectively, ensuring ease of adoption.  
5. **Implement CI/CD Integration:** Integrate Playwright into the CI/CD pipeline to ensure automated and repeatable test execution against all environments, including the cloud.  

This decision ensures that we meet our goals for a more efficient, reliable, and maintainable test architecture while streamlining collaboration across technical and non-technical stakeholders.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision 
The flow dispatcher will be extended to handle transactions explicitly, ensuring consistent behavior and error handling during flow execution. 

### Key Changes:
1. **Transaction Management by Flow Dispatcher**: 
   - The flow dispatcher will start, manage, and commit transactions as part of its execution cycle.
   - Flow actions will no longer need to manage database transactions independently.

2. **Graceful Error Handling**:
   - If a transaction commit fails during flow execution, the flow dispatcher will catch the error, log a detailed and explicit error message, and mark the flow execution as failed to prevent subsequent undefined behavior.
   - The dispatcher will rollback the transaction automatically in case of failure, ensuring the connection is reset and safe for further operations.

3. **Save Points for Nested Transactions**:
   - Configure database connections to support save points by default. This will allow the use of nested transactions safely without marking the connection as rollback only when failures occur inside the flow.
   - Any inner transactional errors will be isolated to the inner block rather than affecting the outer transaction context.

### Benefits:
- Eliminates ambiguity and simplifies flow action implementation by removing the need for manual transactional management.
- Ensures consistency and reliability by treating transaction errors as critical failures rather than ignoring them or leaving them to be handled unexpectedly.
- Prevents cascading transaction errors caused by inner failures affecting outer commits, especially in default database configurations.

This approach will provide better error containment, transparent logging, and a safer execution state across flows, aligning execution behaviors with industry best practices for transactional systems.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision 
We will disable the Vue 3 compatibility mode in the administration after the transition period.

### Rationale:
1. **Migration Completeness**: Compatibility mode should only be temporary, as it exists to ease the migration process. Keeping it enabled long-term would mean we are not fully transitioning to Vue 3 and would continue to depend on deprecated features.
2. **Codebase Modernization**: Disabling compatibility mode allows us to take full advantage of Vue 3 features, such as the Composition API, improved performance, and overall modern tooling improvements.
3. **Plugin Updates**: By the time this change is made, plugin developers will have had sufficient time to migrate their plugins to be fully compatible with Vue 3. Disabling compatibility mode ensures plugins align with the latest standards.
4. **Future-proofing**: Relying on the compatibility mode indefinitely creates technical debt and risks compatibility issues with future Vue versions.

### Implementation Plan:
1. Communicate the plan to disable compatibility mode to plugin developers well in advance, giving them a specific timeline and resources to complete their migrations.
2. Update the administration codebase to remove any remaining Vue 2-specific code or features that would break without compatibility mode.
3. Release a migration guide for plugin developers that includes tips, examples, and potential issues to address.
4. Gradually test and remove compatibility mode in staging and ensure the application and plugins function as expected.
5. Disable compatibility mode in a major version release, clearly indicating in the release notes that compatibility mode is no longer enabled or supported.

This decision will allow us to provide a more modern, performant, and maintainable codebase in the long run while ensuring a reasonable transition period for plugin developers.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision  
We will implement a temporary patch to work around the getter reactivity bug in Vuex `4.1.0` until an official fix is released by the Vuex team. Downgrading to `4.0.2` is not a viable option due to the critical failures it introduces.

### Key Steps:
1. Investigate and incorporate the temporary patch specific to mitigating the getter reactivity issue in `4.1.0`. This may require either wrapping or modifying getter behavior in a non-intrusive way.
2. Regularly monitor the Vuex issue tracker and changelogs to stay updated about a permanent fix for the bug.
3. Once an official fix is released in a stable version of Vuex, upgrade to that version and remove the temporary patch, ensuring reactivity and module compatibility remain intact.
4. Document the temporary patch thoroughly, including details about the bug, the changes made, and how to identify the point at which it should be retired. 

### Rationale:
The temporary patch allows us to avoid both the reactivity issues in `4.1.0` and the more critical module failures in `4.0.2`. This ensures application stability and development continuity while waiting for an upstream resolution.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision  
We will transition from relying heavily on static class-based patterns like PHP Decoration, Adapter, and Factory, to leveraging a **modular event-driven architecture** as the foundation for extensions and customizations.

### Key Changes:  
1. **Event-Based Hooks for Extensions:**
   - Introduce an event-driven system where subprocesses emit specific events at defined points of extensione.g., `beforeProcess`, `afterProcess`, `validateProcess`, etc. 
   - Third-party developers can listen to and handle these events to customize behavior dynamically without modifying core classes or requiring excessive abstraction.

2. **Centralized Event Dispatcher:**
   - Implement a robust central event dispatcher that mediates between the application and third-party extensions.  
   - This will allow developers to register listeners or subscribers for specific events and define processing order via priorities.

3. **Reduce Class Proliferation:**
   - By embedding extension points in the form of events, we can remove the need to extract subprocesses into numerous interfaces and classes, helping simplify the codebase.

### Benefits:
- **Backward and Forward Compatibility:**
  - Event names and payloads can remain consistent, decoupling the internal workings of subprocesses from extensions. This ensures greater stability and reduces compatibility risks between releases.  
- **Simplified Codebase:**
  - Reduces the explosion of interfaces, abstract classes, and their implementations, resulting in leaner and more maintainable code.
- **Dynamic Extensibility:**
  - Enables runtime customizations and dynamic extensions by simply attaching listeners, avoiding reliance on static class hierarchies.  
- **Improved Modularization:**
  - Each subsystem or module can define its own event schema, making it easier to encapsulate concerns and isolate changes.

### Implementation Steps:
1. Define an initial core event system based on a lightweight dispatcher library or custom implementation.
2. Introduce key events in critical subprocesses of the current architecture.
3. Refactor a pilot subsystem to replace certain Adapter or Factory customizations with event hooks.
4. Deprecate excessive abstract classes and interfaces related to subprocesses gradually, with clear timelines.
5. Provide comprehensive documentation and migration guides for third-party developers to adopt the new event-driven extension model.

This shift will modernize the customization framework, enabling greater flexibility and scalability while reducing the long-term cost of maintaining compatibility and managing code complexity.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision 
We will introduce a new Jest runner configuration with compat mode explicitly disabled. This will enable us to run component tests without compat mode, ensuring the components work correctly without relying on deprecated or transitional features provided by the compat mode. 

The implementation plan is as follows:
1. Create a new Jest runner configuration specifically for non-compat mode tests.
2. Update the Jest test setup to include this additional runner.
3. Gradually migrate existing tests to run under the new non-compat mode runner, ensuring they still pass.
4. New component tests should default to using the non-compat mode runner unless there is a specific reason to test in compat mode.
5. Once all tests have successfully been migrated, the compat mode runner can eventually be deprecated and removed if no longer required.

This approach allows for a smooth transition away from compat mode while maintaining confidence in the functionality of the components.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision  
We will implement comprehensive system health checks to monitor the health of the software system and its underlying components.  

### Key Aspects of the Health Check Strategy:
1. **Granular Component Checks**: Each critical component of the system will have its own health check to verify its functionality. This includes services, databases, APIs, queues, and other dependencies.
2. **Aggregated System Health Status**: The health checks for all components will roll up into an aggregated system health status. This will help determine the overall state of the system while still allowing detailed investigation of individual components when necessary.
3. **Proactive Failure Detection**: Health checks will be executed regularly, preferably on a schedule or through reactive triggers, to detect issues before they impact end-users.
4. **Actionable Insights**: The health check reports will include clear and actionable details about any failed checks, making debugging faster and more efficient.
5. **Pre-Deployment Validation**: Before deploying to live environments, health checks will be conducted as part of the CI/CD pipeline to catch errors early and avoid rolling out faulty systems.

### Implementation Approach:
- **Health Check Framework**: A standardized health check framework will be selected (or developed if one does not meet our needs) that can test various system components and output standardized results.
- **Thresholds for Health**: A tolerable threshold for system uptime and healthy state vs component failures will be defined, recognizing that the system might remain operational if certain non-critical components experience partial degradation.
- **Alerting and Monitoring Tools**: Integrations with monitoring tools will be established to display real-time health information and send notifications/alerts to the engineering team for failures.
- **Documentation**: Clear documentation for all health check endpoints and methodologies will be provided to ensure ease of use, maintenance, and troubleshooting.

By implementing these health checks, we aim to detect issues early, reduce downtime, and ensure a seamless system experience for users.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision  
The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` will be updated to allow the inclusion of additional namespaces beyond `Shopware\Tests\Unit`.  

This change will provide greater flexibility for the unit testing suite by enabling feature flags to be turned on for tests outside the strict `Shopware\Tests\Unit` namespace. By introducing this broader namespace support, developers working in other logical components or modules of the codebase can benefit from testing major features without manually adjusting the subscriber or creating workarounds.  

Additionally, this decision aligns with the principles of modularity and configurability, ensuring all critical parts of the application share consistent functionality and runtime behavior during testing.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision

We will use an automated migration tool to assist in converting our Vue 2 Options API components to Vue 3 Composition API. Specifically, we will adopt [Vue Migration Helper](https://github.com/vuejs/vue-migration-helper) or a comparable tool to streamline the migration process. These tools are designed to detect and transform patterns of Options API usage and convert them into equivalent Composition API syntax where possible.

### Workflow:
1. **Evaluation**: First, we will evaluate the migration tool by running it on a subset of our codebase to verify its reliability and identify common edge cases.
2. **Documentation**: The team will collaboratively compose documentation that outlines best practices for Composition API usage and migration process, as well as notes on manual adjustments that might be needed.
3. **Automated Migration**: Use the migration tool to process the majority of components, reducing manual effort while increasing consistency.
4. **Manual Refinement**: After automated migration, developers will manually audit and refine the converted code, resolving ambiguous cases or adding missing annotations, especially where complex interactions are involved.
5. **Testing**: Comprehensive testing will be conducted for each migrated component to ensure functionality is maintained and that unintended side effects are avoided.
6. **Gradual Rollout**: Migration will proceed incrementally for manageable sections of the codebase. Each migrated section will only be pushed to production after verification and testing.
7. **Future Standards**: Post-migration, all new components will exclusively use the Vue 3 Composition API.

This approach will allow us to modernize our codebase while minimizing disruptions in the development workflow.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJss inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in Vues templating and reactivity model.","## Decision  
We will replace the current TwigJs-based block system with a Vue.js-based solution using the new `sw-block` and `sw-block-parent` components.  

### Key Points:
1. **Component-based Architecture:** Leveraging Vue.jss native capabilities will provide a more modern, dynamic, and reactive frontend architecture compared to the static template-based approach of TwigJs.  
2. **Improved Flexibility:** The introduction of `sw-block` and `sw-block-parent` components will allow developers to override and extend content dynamically, making it easier to customize and adapt compared to TwigJss inheritance mechanisms.  
3. **Reactivity Benefits:** Vue.js inherently provides reactivity within its components, enabling seamless state updates and dynamic behavior that would have been more complex or cumbersome to handle with TwigJs.  
4. **Seamless Migration Plan:** Plugin developers will be given clear migration guidelines to adapt their TwigJs components to the new Vue.js structure. Effort will be made to minimize disruption and ensure a smooth transition.

### Implementation Approach:
- Introduce `sw-block` as the main dynamic content container.
- Introduce `sw-block-parent` to allow structured and reusable parent-child relationships among blocks.  
- Apply these components project-wide, refactoring existing functionality gradually.

### Long-term Benefits:
Adopting Vue.js for the block system ensures that we future-proof our architecture, address limitations of TwigJs, and align ourselves with modern frontend development practices. This change positions us to maintain a scalable and maintainable codebase while enhancing developer efficiency and user experience.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the components internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: ` 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision  
We will remove the asterisk `*` from all product prices displayed in the default Storefront. 

Instead, the footer will still include the following text:  
`All prices incl. VAT, plus shipping costs and possible delivery charges unless otherwise stated.`  

This approach achieves the following benefits:  
1. **Improved UX:** Removing the asterisk simplifies visual presentation and improves readability for users.  
2. **Clarity:** The footer retains the essential pricing details, ensuring visitors have the necessary information without cluttering the UI.  
3. **Conformance to Standard Practices:** Many e-commerce websites follow a similar pattern without an asterisk and simply state price-related specifics in a footer or informational section.  

By removing the explicit symbol next to every price and maintaining detailed explanatory text in the footer, we strike a balance between simplicity, transparency, and compliance with regulations.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision  
We will replace DomPDF with an alternative PDF generation library that supports creating WCAG-compliant, accessible PDFs.  

### Rationale:  
1. **Accessibility Standards Compliance**: DomPDF does not support generating accessible PDFs, which is a vital requirement to meet WCAG guidelines.  
2. **Legal and Ethical Responsibility**: Ensuring accessibility is critical for meeting legal requirements and providing an inclusive experience for customers with disabilities.  
3. **Future-proofing**: Selecting a library with built-in accessibility features will allow us to maintain compliance without disproportionately increasing technical debt in the future.  

### Implementation Steps:  
1. Evaluate potential libraries that support accessible PDF generation (e.g., PDF/UA compliant).  
   - Candidates could include libraries like Apache PDFBox, mPDF, or wkhtmltopdf, depending on their capabilities and compatibility with our existing system.
2. Validate libraries against key criteria:  
   - Supports tagging structure for accessibility (e.g., semantic tags, alternative texts).
   - Integrates smoothly with Shopware 6.
   - Meets performance and scalability requirements.
3. Once a library is selected, integrate it into the workflow for document generation in Shopware 6.
4. Test the solution for compliance with accessibility standards, using tools and manual review.
5. Replace DomPDF completely and adapt documentation to reflect the new implementation.

### Follow-up:  
This decision will require ongoing maintenance to ensure compliance as guidelines and standards evolve. Regular testing and updates of accessible features should be incorporated into our development lifecycle.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision  
We will introduce a new system for rule management that restricts the unrestricted extension of rule classes while preserving enough configurability to meet user needs.  

- **Centralized Rule Definitions**: All rules will be centrally defined and registered in a single rule registry. This guarantees better visibility, discoverability, and avoids system-wide uncontrolled adhoc extensions.  
- **Modular Rule Extensions**: While unrestricted rule extension will no longer be allowed, modules or extensions must explicitly register their rule additions within the framework provided by the core system.  
- **Simplified Rule Structure**: Redesign the internal structure and API of the rule system to simplify extending or refactoring rules where needed while reducing overall complexity and coupling.  
- **Breaking Changes Communication**: Announce this change as a breaking change to ensure that users and developers relying on the previous unrestricted extension mechanic are aware of the migration path. Extend documentation to provide clear migration guidelines to the new system.  

This system balances evolution and maintainability in line with the expressed needs, while acknowledging backward compatibility challenges raised in discussions like the linked RFC.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision 
We will deprecate and eventually remove the `src/helper/iterator.helper.js` from the Storefront JavaScript codebase. Instead, developers should use native JavaScript constructs for iteration, such as `for`, `for...of`, `forEach`, or other appropriate iteration patterns available in modern JavaScript.

### Implementation Steps
1. **Documentation Update**:
   - Update the Storefront development guide to reflect this decision and emphasize the usage of standard JavaScript iteration methods.
   - Clearly communicate the deprecation of `iterator.helper.js` in future releases.

2. **Refactor Existing Code**:
   - Identify all occurrences where `Iterator.iterate()` is used.
   - Replace them with appropriate native iteration constructs based on the data type being iterated (e.g., `NodeList`, `Map`, `FormData`, plain arrays, etc.).
   - Ensure refactored code is consistent and follows best practices.

3. **Deprecation Process**:
   - Mark `iterator.helper.js` as deprecated in the codebase with a clear comment indicating its planned removal in the future.
   - Add a deprecation warning in the console whenever the helper is used, alerting developers to migrate to native solutions.

4. **Removal Timeline**:
   - Announce that the `iterator.helper.js` will be completely removed in a designated major release (e.g., the next major version).
   - Ensure sufficient time for developers to transition away from the helper.

5. **Encourage Standardization**:
   - Promote consistency across the codebase by enforcing the use of native iteration approaches and providing examples where needed.

By deprecating and removing this unnecessary abstraction, we aim to reduce complexity, improve code clarity, and simplify onboarding for developers working with the Storefront JavaScript.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision  
Flows will be decoupled from the immediate execution of the business process and executed asynchronously after the business process has been completed. This approach offers more robustness and scalability while addressing the outlined downsides:

1. **Error Isolation**: Any fatal errors occurring during the execution of a flow will no longer impact the business process. The decoupling ensures that even if a flow fails, the business process completes successfully.

2. **Performance Improvement**: Expensive operations, such as sending emails, will no longer hinder the performance of the business process since flows will be processed in an asynchronous manner.

3. **Improved Debugging and Maintenance**: Flows will be executed by a dedicated event listener instead of being triggered directly from Symfony's `EventDispatcher` decorator. This separation will reduce stack trace complexity and simplify debugging.

4. **Scalability for Flow Capabilities**: Moving flows to a dedicated execution environment lays the groundwork for future enhancements, making flows more flexible and potentially introducing retries, advanced scheduling, or distributed execution mechanisms.

**Implementation Notes**:
- A feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` will be used to toggle this new behavior, ensuring the change can be rolled out incrementally and tested on controlled environments.
- Flows execution will be orchestrated through a dedicated system, such as a message queue or task scheduler, ensuring reliable asynchronous processing.
- Logging and monitoring mechanisms will be enhanced to capture and handle flow execution failures independently of the main business process lifecycle.

With this decision, the business process will remain robust and performant, while flows gain greater flexibility and reliability through asynchronous handling.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision 
We will adopt the [MADR (Markdown Architecture Decision Record)](https://adr.github.io/madr/) format for recording architectural decisions in this project.

## Rationale
1. **Simple and Lightweight**: MADRs structure is easy to understand and implement, enabling contributors to quickly document decisions without excessive overhead.
2. **Markdown-based**: Markdown is widely used and supported across many tools, which ensures portability, ease of editing, and collaboration.
3. **Supports Key Elements**: MADR includes all the essential components for an architectural decision record, such as context, decision, consequences, and status.
4. **Encourages Consistency**: Using a standard format ensures that all ADRs within the project are consistent, making it easier for contributors to read, evaluate, and revisit decisions.

## Consequences
1. Future contributors will need to familiarize themselves with the MADR format (though this is minimal since the structure is straightforward and well-documented).
2. We will need to include a template of MADR in the project repository to guide contributors.
3. All ADRs will be version-controlled in a directory (e.g., `docs/adr/`) within the project for future reference.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision 

We will create a new repository tooling library as part of the TUF Metadata API implementation to support repository operations and improve coherence in the API. This new library will address the issues raised in the legacy python-tuf implementation, specifically by focusing on the following:

1. **Modularity and Maintainability**:
   - Split the repository tooling functionality into smaller, well-defined components to reduce the overall code complexity and improve maintainability.
   - Aim to significantly reduce the line count for key modules by eliminating redundancy and focusing on targeted, reusable functionality.

2. **Coherent API Design**:
   - Provide a clean and consistent API for repository tooling operations with well-defined methods that avoid overly complex, multi-argument signatures.
   - Ensure that repository functionality does not depend on global variables but uses explicit dependencies wherever required.

3. **Decouple Application-Level Decisions**:
   - Design the library to be generic and ""library-like,"" allowing users to make application-specific decisions (such as whether to load all metadata files) at the higher, application code level.

4. **Avoid Overloading with Application Behavior**:
   - Make the repository tooling library focused on repository-specific tasks and avoid incorporating logic that is better suited for applications or specific environments (e.g., large-scale servers or small CLI tools).

This modernized library approach will provide clarity and flexibility, allowing it to serve both small-scale tools and complex, large-scale repository implementations efficiently. The Metadata API will remain at the core, enabling access to metadata pieces, while the repository tooling library will provide additional structure and support for repository-specific operations. This separation will lead to a cleaner, more maintainable implementation.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision  
We will transition Primer React components to TypeScript to integrate type definitions directly with the implementation code rather than using a separate ambient declaration file (`index.d.ts`).  

### Rationale:  
1. **Maintainability**: Embedding type definitions within the implementation code will ensure that type definitions stay in sync with the underlying implementation. Any changes to the code will also update the corresponding types, reducing the risk of inaccuracies and TypeScript bugs.  
2. **Scalability**: As Primer React grows, manually maintaining diverging type definitions will become increasingly error-prone and resource-intensive. TypeScript adoption will streamline this process, providing type safety baked into the development of new features and components.  
3. **Developer experience**: Direct integration of TypeScript improves the developer experience by enabling improved IDE support, including inline documentation, autocompletion, and instant type-checking as engineers work with the library.  
4. **Community expectation**: TypeScript has become a standard for modern React libraries. This change will align Primer React with community expectations and keep the project competitive in the ecosystem.  

### Implementation Plan:  
1. Gradual Migration: 
   - Start with simpler components and progressively migrate more complex ones.
   - Maintain testing coverage to ensure no regressions are introduced during migration.  
2. Code Conversion:  
   - Convert `.js` files to `.tsx` to accommodate TypeScript syntax and definitions.  
   - Define type interfaces and props within the component files.  
3. Documentation Update:  
   - Update usage documentation to reflect the availability of TypeScript support.
   - Provide migration guides for teams using Primer React with TypeScript.  
4. Deprecate `index.d.ts`:  
   - Once migration is complete, remove the ambient declaration file in favor of inline definitions.  
5. Community Collaboration:  
   - Engage with the community through pull requests and discussions to validate type definitions and gather feedback during migration.  

By making this transition, Primer React will be better positioned for long-term maintainability, improved type accuracy, and an enhanced developer experience.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision 
We will adopt the **Box-based pattern** as the primary method for creating React components in Primer React. This decision is based on its consistency with the design system's principles, greater flexibility, and ability to integrate well with existing utilities in our codebase.

### Rationale:
1. **Consistency with Design System:**
   - The Box-based approach aligns closely with our system-first mindset. It promotes the use of existing `sx` utilities and adheres to the design tokens and primitives provided by Primer.

2. **Flexibility:**
   - The Box-based components are easier to compose and extend since they allow merging of styles (`sx`) in a clean and predictable way (e.g., merging local styles with `props.sx`).
   - This approach makes it easier to dynamically apply styles, increasing usability for developers and facilitating customization.

3. **Ease of Maintenance:**
   - Using Box simplifies maintainability by reducing the complexity of maintaining styled-components definitions and attributes mapping.
   - Styled-components require additional runtime resolution of props to generate styles, whereas the Box-based pattern leverages direct style application through composition.

4. **Alignment Across the System:**
   - The Box-based approach enforces a unified method of styling throughout the codebase, avoiding the pitfalls of diverging patterns (e.g., separate styled-components configurations).
   - Encouraging a single pattern minimizes cognitive overhead for contributors and new developers.

5. **Performance Considerations:**
   - Styled-components introduce a runtime overhead for generating component-specific styles, which can potentially impact performance.
   - By using Box and declarative styles (`sx`), we reduce reliance on runtime-heavy styled-component usage, ensuring more optimized styling and rendering.

### Implementation Guidelines:
- New components should be built using the Box-based pattern as described in the second example above.
- Existing components using `styled-components` should be migrated incrementally to align with the new standard, prioritizing components that frequently require updates or changes.
- Documentation should be updated to reflect this decision and provide guidance for working with the Box-based pattern.
- Adopting Box does not mean completely eliminating styled-components, but their usage should be limited to specific scenarios where they provide unique value that cannot be achieved more effectively through Box.

This decision ensures that our codebase remains flexible, maintainable, and consistent with the broader principles of Primer React's design philosophy.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision 
We will continue to use the `drafts` mechanism in `@primer/react` to address two key scenarios:

1. **Parallel Versions of Components**: When there is a need to develop, test, and maintain new versions of components while keeping the existing versions stable and available for production use. This ensures developers can gradually transition to the new version without disrupting current workflows.

2. **Work-In-Progress Components**: When components are still experimental or under active development, exporting them under `drafts` prevents their premature adoption in production environments and allows developers to collaborate with the team before full integration.

To ensure clarity and proper adoption:
- Draft components will be documented with clear labels indicating their experimental status or version differences.
- The team will manage draft components responsibly, ensuring they either graduate to production status or are deprecated when they are no longer needed.
- A robust feedback loop for developers using draft components will be established to capture insights and testing outcomes, aiding in their refinement.

By maintaining this approach, we preserve flexibility and control in the progression of our design system components.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision  
Snapshot testing will not be used as a primary testing strategy for Arachne's React components due to the challenges outlined in the context. Instead, we will adopt the following approach for testing:

1. **Behavior-Driven Testing**: Tests will focus on validating specific behaviors and business logic over capturing the entire rendered output of a component. These tests will ensure intentionality by verifying specific functionality and feature requirements.

2. **Targeted Unit Testing**: Unit tests will be written for reusable, modular logic embedded within components, ensuring the correctness and reliability of internal functionality.

3. **Integration Testing**: Components will be tested in the context of their interactions with other components and systems, ensuring that they function correctly within the application's ecosystem.

4. **Use of Snapshot Testing in Limited Scopes**: Snapshot testing may occasionally be used, but only when:
   - Testing static components with small, well-defined and stable render outputs.
   - The snapshot output is manually reviewed to ensure readability and clarity.
   - The snapshot explicitly tests a defined aspect of the component's expected visual structure.

5. **Clear Purpose in Tests**: All tests, including those that use snapshots, must explicitly document their purpose and scope in comments or as part of the test name. This ensures that future developers understand what the test is validating.

This decision prioritizes clarity, maintainability, and debugging efficiency over the broad usage of snapshot testing, ensuring that our tests serve to validate intentional behaviors and minimize confusion when failures occur.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision 
We will implement a utility function named `__DEV__()` to ensure that calls to `console.warn()` or other development-specific warnings are only included in the development environment. The utility will evaluate whether the application is running in a development context and will only execute development-specific logic under those conditions.

### Implementation Details:
1. Define a global variable or use an existing mechanism to detect the development environment (e.g., `process.env.NODE_ENV`).
2. Create a helper function `__DEV__()` to wrap and evaluate code intended for the development environment:
   ```javascript
   const __DEV__ = () => process.env.NODE_ENV === 'development';
   ```

3. Wrap all calls to `console.warn()` or other development-only warnings in a conditional check:
   ```javascript
   if (__DEV__()) {
       console.warn('This feature is deprecated...');
   }
   ```

4. During the build process, configure tools like Webpack or Vite to remove or dead-code-eliminate development-only logic based on the `process.env.NODE_ENV` value.

### Benefits:
- Ensures no unnecessary `console.warn()` calls or development-specific code can be executed in production.
- Reduces bundle size by removing unused development warnings.
- Enables clear separation between development and production behaviors.
  
### Enforcement:
- Create linting rules or guidelines to enforce the use of `__DEV__()` for development-only warnings and messages.
- Review existing warnings and refactor them to adhere to the `__DEV__()` pattern.

This approach will establish a clear and maintainable pattern for managing development-specific behavior and avoiding unintended production overhead.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision 

We will create a central utility for live region announcements in Primer React. This utility will provide a consistent and reliable pattern for using ARIA live regions across Primer and GitHub projects. By abstracting the implementation details, the utility will address known inconsistencies in live region behavior across screen readers and different dynamic scenarios. 

### Key Features of the Utility:
1. **Centralized Announcement Method**: 
   - The utility will provide a programmatic API for announcing messages, decoupled from specific DOM elements or roles in components.
   - Example: `announce(message, options)` where `options` can include `role`, `aria-live` (e.g., `polite`, `assertive`), and priority settings.

2. **Support for Multiple Use Cases**:
   - Announcing static messages on render (e.g., spinner displayed or form submitted).
   - Announcing dynamic content changes in elements (e.g., incrementing a button count).
   - Announcing messages programmatically (e.g., search results count or custom announcements).

3. **Predefined ARIA Roles**: 
   - The utility will include sensible defaults with predefined roles like `alert`, `status`, or `log` depending on the use case, ensuring semantic appropriateness.

4. **Dynamic Live Region Management**:
   - The utility will handle dynamically adding temporary live region elements to the DOM to ensure announcements are triggered properly, even when content is injected dynamically or loaded via React.
   - The live region elements will be programmatically removed after announcements to keep the DOM clean and reduce potential performance overhead.

5. **Configuration and Flexibility**:
   - Developers can override the default behavior and provide custom configurations if needed.
   - Support for customizable options like `aria-atomic`, `aria-relevant`, and role selection.

6. **Reliable Cross-Screen Reader Behavior**:
   - The utility's implementation will address known browser and screen reader quirks, ensuring announcements are consistent across a variety of user assistive tools.

### Implementation Plan:
1. Define the API and supported options for the announcement utility.
2. Research and account for edge cases and inconsistencies in live region announcements, using resources like the [Tetralogical blog](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/).
3. Implement the utility as part of the core Primer React library.
4. Refactor existing components in Primer React to use the centralized utility, replacing ad-hoc `aria-live` usage.
5. Conduct rigorous testing across major browsers and screen readers to ensure reliability and accessibility.

### Benefits:
- A unified approach eliminates fragmented and inconsistent implementation of live regions across components.
- Provides a pattern that is reusable, reducing the workload for developers and enabling efficiency.
- Improves accessibility by ensuring live regions work consistently across assistive technologies and dynamic content scenarios.

### Future Considerations:
- Regularly update the utility to accommodate new ARIA specifications, browser behavior changes, or user feedback.
- Document clear guidelines and examples for using the utility within Primer React and GitHub projects to foster adoption and understanding.

","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"## Decision
Since no additional context has been provided, no decision can be made at this time. To proceed effectively, more details or specific goals should be outlined to clarify the problem space and decision requirements.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision  
We will introduce automated interaction testing using the Storybook ""Stories"" as the source of truth for component behavior. This approach will address the challenges outlined in the context and unify our visual and interaction testing workflows.

### Changes to Testing Strategy:
1. **Leverage Storybook for Interaction Testing**:
   - Extend our existing Storybook setup by introducing tools like Storybook's **Test Runner** or **Playwright integration**, which can automate interaction testing directly on stories.
   - Use Storybook stories to simulate interactive scenarios, reducing redundancy between manual story verification and Jest tests for the same functionality.  
   - Storybook will serve as both a documentation tool and an automated testing platform.

2. **Use Storybook Stories as the Foundation**:
   - Automate interactions like clicks, form inputs, drag-and-drop, etc., within our existing stories using Storybook capabilities (e.g., `play` functions).
   - Rather than re-creating markup and scenarios in Jest, tests can reference component states and interactions defined in Storybook stories, ensuring consistency.

3. **Improve Debugging with Visual Feedback**:
   - Since Storybook provides a visual representation of components, it will be easier to observe the state of the component during automated interaction tests.
   - The ability to ""see"" the test run in the interactive Storybook canvas will simplify debugging and increase visibility for collaborators reviewing or enhancing tests.

4. **Combine Jest and Chromatic**:
   - Jest will continue to handle granular unit tests for logic-specific cases, while Storybook-driven interaction tests will handle more complex, visual scenarios.
   - Chromatic will remain responsible for snapshot-based visual regression testing, ensuring UI changes are tracked and approved.

### Benefits:
- **Reduced Redundancy**: By automating tests directly on Storybook stories, we remove the need to re-create scenarios in multiple places.
- **Stronger Collaboration**: Storybook provides a shared platform, with interactive feedback and visual clarity making it easier for designers and developers to collaborate on testing and debugging.
- **Enhanced Debugging**: Complex interactions can be observed visually, improving the speed and accuracy of debugging.
- **Better Test Coverage**: Combining Storybook interaction testing with Jest and Chromatic ensures comprehensive coverage of both logic and UI.

### Next Steps:
1. Evaluate and onboard Storybook tools like the **Storybook Test Runner** or external integrations (e.g., Playwright, Cypress).
2. Align existing Storybook stories with common test cases and augment them with `play` functions for interaction steps.
3. Train the team on using Storybook testing features and debugging using the interactive canvas.
4. Phase out redundant Jest tests where possible, consolidating them into Storybook-driven tests.

This strategy will streamline our testing efforts and align them with our broader goal of improving workflow efficiency and test visibility.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision 
We will adopt a monorepo management tool, specifically `npm workspaces`, to consolidate dependency management and facilitate better organization within the `primer/react` repository. 

### Key Actions
1. **Configure Workspace Management**: Transition the project into a fully managed monorepo using `npm workspaces`. All workspaces (`@primer/react`, `docs`, `examples/*`, etc.) will be explicitly defined in the root `package.json` using `workspaces` configuration. This will centralize dependency management, making it easier for contributors to set up the entire project with a single `npm install` at the root workspace.

2. **Inter-Workspace Dependencies**: Dependencies between workspaces (e.g., `docs` depending on `@primer/react`) will be defined directly within the workspace dependency configuration in `package.json`, using the `workspace:` protocol. This eliminates the need for relative paths, `file:` dependencies, or manual symlinks like `npm link`.

3. **Script Management**: Existing scripts in the `script` directory will be refactored and organized as workspace-specific scripts where applicable. Scripts that span multiple workspaces or apply globally will remain centralized at the top-level workspace. This will allow better modularity, dependency control, and contributors will have clearer understanding of script purpose and scope.

4. **Top-Level Commands**: Contributors will only need to use top-level commands like `npm install` to set up the repository. Higher-level commands, such as `npm run build` or `npm run test`, can be configured to work across all defined workspaces, ensuring a seamless and unified developer experience.

5. **Consistency Across Workspaces**: Dependency versioning will be standardized across workspaces wherever possible, with tools like `npm dedupe` or `npm install --workspaces` being leveraged to avoid redundancy or conflicts.

### Benefits
- **Unified Dependency Management**: Simplifies the contributor workflow by centralizing dependency installation and makes inter-workspace dependencies explicit and manageable.
- **Improved Script Organization**: By relocating workspace-specific scripts and streamlining top-level scripts, contributors can easily locate relevant tools/tasks.
- **Scalability**: Enables seamless addition of new workspaces and opens the door for future enhancements like workspace-specific build pipelines.
- **Developer Experience**: Reduces friction for contributors by eliminating the need to navigate individual directories and manually manage dependencies.

This decision aligns with the project's goal of improving maintainability and modularity by embracing modern workflows and tooling.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision  
We will restructure the codebase to clearly distinguish between the **public API** of the package and internal implementation details. Specifically, we will do the following:  

1. **Introduce a `src/internal/` folder**:  
   - Any code that is not intended to be part of the public API will be moved into this folder.  
   - Code in this folder will be excluded from being part of the exports in the `package.json`.  

2. **Clearly define the public API**:  
   - Only files and folders outside of the `internal` directory (e.g., files like `src/index.ts` and any specifically exported modules) will be considered part of the public API.  
   - This will ensure intentionality in what is exposed to consumers of `@primer/react`.  

3. **Update Rollup configuration**:  
   - Modify the Rollup build to exclude files under `src/internal/` from being included in the build output for any public paths, unless explicitly re-exported through public API definitions.  

4. **Add tooling and/or checks**:  
   - Use tools like ESLint or static analysis to enforce that internal code in `src/internal/` does not inadvertently import code from the public API unless absolutely necessary. This will ensure we maintain a separation of concerns.  

5. **Update documentation**:  
   - Document which parts of the codebase are meant to be used publicly and clarify the use of `src/internal/` for contributors.  

This restructuring ensures that our package has a well-defined and intentional public API while allowing room for internal implementation details that are not exposed to external consumers. It reduces the risk of breaking changes when modifying internal code and improves maintainability by making the separation of concerns explicit.  ","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision 
We will adopt a new styling architecture for Primer that prioritizes performance, leverages CSS variables, and moves away from runtime-injected styles in favor of pre-processed static `.css` files.

1. **Static CSS Files**  
   Styles will be pre-generated as static `.css` files instead of being dynamically injected at runtime. This improves performance in the following ways:  
   - **Faster Initial Page Loads:** Static files can be loaded in parallel with other resources, eliminating the delay caused by JavaScript-based style injection.  
   - **Improved Rendering Times:** Both lab and application benchmarks show significant improvements in rendering times compared to the runtime-injected style architecture.  

2. **Server-Side Rendering Optimization**  
   Moving to static styles eliminates the need to render styles dynamically on the server, which reduces the overhead associated with collecting styles during SSR. This reduces server-side rendering time by approximately 20%, as demonstrated in the issues-app experiment.

3. **Efficient Style Updates**  
   Dynamic updates to styles (e.g., via state or props) will leverage the improved static CSS architecture:
   - Guardrails will be introduced to minimize performance bottlenecks caused by runtime calculations or excessive dynamic styles.
   - Using static `.css` files for common interactions, combined with CSS variables for customization, ensures efficient re-rendering and theming.

4. **CSS Variables for Theming**  
   CSS variables will be adopted to manage theming across Primer components. This enables:  
   - Lightweight, dynamic customization without requiring runtime style re-calculation.  
   - Enhanced flexibility for theming and better alignment with emerging web standards.  

5. **Improved Guardrails for Primer/Primitives Consumption**  
   Updates to the architecture will include stronger guardrails around how Primer/Primitives are consumed. This ensures consistent usage and minimizes misuse of styling APIs that can degrade performance.  

By moving to a more efficient static CSS architecture and embracing CSS variables, this decision aligns with the goals of improving initial load performance, reducing SSR overhead, making runtime updates more efficient, and enabling robust theming capabilities across the board.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision 
We will no longer recommend using Storybook's `play` functions for authoring tests based on user interactions. Instead, we will consolidate our testing strategy by leveraging Playwright for all interaction-based tests. Playwright provides a more comprehensive and robust API for simulating user interactions and allows for easier debugging and maintenance of tests.

By focusing interaction tests in Playwright, we ensure a consistent and powerful approach to testing user behavior and application responsiveness. The `play` functions of Storybook will no longer be treated as a core part of our testing strategy, although they may still be used in specific cases where lightweight or quick checks within the Storybook environment are beneficial.

Existing tests written using `play` functions may be progressively migrated to Playwright as necessary, with priority given to complex or brittle tests. This approach ensures a smoother path toward a unified and maintainable testing strategy moving forward.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision  
To ensure that responsive props do not lead to layout shifts during server-side rendering, we will adopt the following approach:

1. **Allow authors to specify a server-side default**: When using responsive props, authors will be required to provide a default value that is used for server-side rendering. This default will act as the initial value for the responsive prop until the client-side code executes, and `matchMedia` can accurately determine the viewport size.

    Example:
    ```tsx
    <Stack gap={{ default: 'condensed', narrow: 'condensed', regular: 'normal', wide: 'spacious' }} />
    ```

    Here, the `default` key specifies the fallback value that will be used on the server, ensuring no layout shifts occur.

2. **Document best practices for responsive values with SSR**: We will provide clear guidance to developers on how to use responsive props in a way that avoids layout shifts:
    - Always specify a `default` value.
    - Ensure the `default` value is representative of what is likely seen by the majority of users to minimize perceived layout shifts.

3. **Enhance the `useResponsiveValue` hook to prioritize the `default` value during SSR**: We will enhance the implementation of `useResponsiveValue` to detect server-side rendering (e.g., using Node.js environment features or libraries like `is-server`) and prioritize the `default` value during this phase. During hydration, the hook will seamlessly resolve to the correct responsive value based on the `matchMedia` query.

4. **Provide warnings for missing `default` values**: To encourage authors to correctly specify a `default` value, we will introduce warnings during development when a responsive prop does not provide one.

By requiring a server-side fallback and enhancing the `useResponsiveValue` hook, this decision will ensure consistent styling and layout during server-side rendering, eliminating layout shifts caused by responsive props.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision  
We will adopt a phased deprecation strategy to ensure a smooth migration for authors while maintaining backwards compatibility for a defined time period. The strategy will include the following steps:

1. **Introduce the New API:**
   - Implement the new API alongside the existing (deprecated) API.
   - Clearly document the new API as the preferred approach in the documentation, with examples and use cases similar to the ones presented above.

2. **Deprecation Warning:**
   - Add runtime or compile-time (if applicable) warnings to alert developers when they are using the deprecated API. The warning message will:
     - Indicate which prop is deprecated.
     - Suggest the new API as an alternative.
     - Link to the documentation or migration guide for further help.

3. **Migration Guide:**
   - Publish a detailed migration guide that highlights the differences between the old and the new API.
   - The guide will include examples (like the ones provided) to show how to replace the deprecated prop with the new approach.

4. **Backward Compatibility Period:**
   - Maintain full support for the deprecated API for a specified period (e.g., 2 major versions or 6 months) to allow developers sufficient time to migrate.
   - During this period, our team will address any critical bugs or issues with the deprecated API.

5. **Removal of Deprecated API:**
   - After the defined deprecation period, we will remove the deprecated API entirely.
   - A major version update will be released to mark the breaking change. This will be communicated clearly in release notes, along with reminders about the previous deprecation notice.

6. **Communication and Community Support:**
   - Proactively communicate deprecation plans well in advance through multiple channels, including release notes, blog posts, and announcements.
   - Offer community support (through forums, GitHub issues, etc.) to assist developers during the migration process.

By following this structured deprecation approach, we aim to balance innovation with user experience, ensuring that authors have adequate resources and time to transition without disruption.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision  
Arachne will support proxy configuration via standard environment variables: `http_proxy`, `https_proxy`, and `no_proxy`.  

### Details:  
1. **Environment Variable Convention**:  
   - The lowercase variants (`http_proxy`, `https_proxy`, `no_proxy`) will be the primary convention followed, aligning with the majority of applications.  
   - Uppercase variants (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`) may also be supported for compatibility but will not be emphasized in documentation or guidance.

2. **Implementation Scope**:  
   - Proxy settings will be incorporated into Arachne's configuration mechanism. Wherever HTTP/HTTPS calls or connections are made within Arachne or its modules, these environment variables will be checked and respected.
   - Clear documentation will be provided to ensure that users understand how to configure proxy settings properly.

3. **Transparency and Configurability**:  
   - Proxy settings applied via environment variables will be reflected in Arachne's introspectable configuration data structure, ensuring users can see what proxy settings are being used at runtime.
   - Overriding proxy settings via other means (e.g., specific configuration files or module-level options) will also be supported, but environment variables will be the default mechanism.

4. **Scope for Modular Support**:  
   - Modules or plugins that involve network communication can extend their functionality to respect proxy configurations defined via these environment variables.  

Adopting this decision ensures compatibility with enterprise systems while maintaining Arachne's principles of transparency, introspectability, and configurability.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision 
We will change the default behavior of command echoing to be disabled unless explicitly opted into.  

- By default, command echoing in user logs will be disabled unless the `ACTIONS_STEP_DEBUG` secret is enabled.
- To provide flexibility, users will be able to explicitly control echoing via the commands `::echo::on` and `::echo::off`. This allows users to opt-in or opt-out of echoing as needed during their workflows.
- When command echoing is disabled, the logs will be less cluttered, improving readability.
- Command outputs will still be echoed in the event of errors to provide necessary context for debugging and troubleshooting. This ensures failure analysis remains effective and transparent.
- Documentation will be updated to clearly describe the new behavior and how to control echoing. This includes guidance on enabling `ACTIONS_STEP_DEBUG` and using the `::echo` commands. 

This approach maintains transparency where necessary while optimizing log readability and user control.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision  
To enhance the transparency and debugability of jobs, the runner will be updated to include a mechanism to log additional contextual information during the `Set up job` step. This mechanism will be designed to capture extra details, such as the operating system (OS) and software information available from the Hosted image.  

### Implementation Details:
1. **Integration with Hosted Images**:  
   - The runner code will have the capability to query the Hosted image for relevant metadata (OS version, installed software, etc.).
   
2. **Log Structure**:  
   - Ensure the extra information is formatted clearly in the log to allow users to quickly identify key details. Example:
     ```
     OS: Ubuntu 20.04.3 LTS  
     Installed Software: Docker 20.10.8, Node.js v14.17.6
     ```
   
3. **Extensibility**:  
   - The mechanism will be designed to allow flexibility for future use cases (e.g., including custom user-defined information).

4. **Performance Impact**:  
   - Careful consideration will be given to minimize the performance overhead when gathering and logging this extra information.

5. **Testing and Validation**:
   - Validate that the added logging works for various hosted image setups.  
   - Test against edge cases where metadata may be unavailable or incomplete.

This decision will lead to clearer, more informative logs, aiding contributors and users in diagnosing configuration-related issues more effectively.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision  
We will add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

- **`steps.<id>.outcome`** will indicate the result of the step execution at a granular level (e.g., success, failure, skipped, etc.). It provides clarity on how the step concluded.
  
- **`steps.<id>.conclusion`** will represent the overall conclusion or state (e.g., success or failure). This is more abstract and is useful for determining if subsequent steps should execute.

This change will enable conditional execution of downstream steps based on the outcome or conclusion of previous steps, improving configurability and flow control capability for users. 

Additionally, it aligns with existing conventions like `steps.<id>.outputs`, keeping the step context consistent and intuitive.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision  
We will add support for registering additional custom labels during the runner's configuration process via command-line arguments.  

### Details:
1. During the runner registration process, users will be able to specify additional, user-defined labels using a command-line argument (e.g., `--labels` or a similar approach).  
2. This enhancement will allow configuration scripts to dynamically add custom labels to runners at the point of registration without requiring manual intervention.  
3. The built-in labels (such as `os` and `arch`) will still be automatically added to every runner. Custom labels will complement these, providing additional flexibility for workflows requiring specific label-based targeting.  

### Justification:
1. **Automation and Flexibility**: This feature aligns with the common practice of automating self-hosted runner configuration via scripts, reducing the need for manual follow-up steps to add labels.  
2. **Improved User Experience**: Having the ability to include custom labels directly during configuration simplifies the process and streamlines the setup for users relying on large-scale self-hosted runners.  
3. **Alignment with Existing Practices**: This decision takes inspiration from ADR275 but expands on it by emphasizing the need for scriptable, command-line-based customization of labels.  

### Implementation Considerations:
- Proper validation will be added to ensure the labels supplied via the command line are valid and adhere to the expected format.
- Documentation and examples will be updated to clarify the new behavior for users.
- If providing invalid labels, the registration process should fail with a clear error message, guiding users to fix the issue.  

This decision improves configurability while maintaining the existing registration simplicity.  ","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision  
We will implement a **retry mechanism** with exponential backoff for all communication with the third-party authentication provider.  
This mechanism will include:  
1. **Retry Attempts:** A configurable number of retries after an initial failure to maximize chances of a successful request while avoiding unnecessary resource strain.  
2. **Exponential Backoff:** The time between retries will increase exponentially with each failed attempt to reduce the risk of overwhelming the provider and to gracefully handle rate limiting.  
3. **Jitter:** A small randomized amount of time will be added to backoff intervals to prevent synchronization issues or cascading failures during outages.  
4. **Fail-Fast Logic:** If a certain predefined error is detected (e.g., explicit ""rate limit exceeded"" or ""quota exceeded"" responses from the provider), we may choose to fail immediately, rather than retrying unnecessarily.  
5. **Circuit-Breaker Capability:** If persistent failures are detected over a prolonged period, a circuit breaker pattern will be implemented to temporarily halt attempts and prevent overwhelming the provider or exhausting system resources.  

This decision ensures fault-tolerance for transient issues while minimizing unintended negative impacts on both our system and the provider. Additionally, all retry attempts and circuit breaker states will be **logged and monitored** to provide visibility into authentication issues for debugging and operational insights.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision  
We will adopt the use of Architecture Decision Records (ADRs) to document all significant decisions made during the project's lifecycle. 

Key points of implementation:
1. Each decision will be documented in its own ADR file, following a simple, standardized format. These documents will include the context, decision, rationale, consequences, status, and any alternatives considered.
2. ADRs will be stored in a dedicated, version-controlled repository, ideally alongside the relevant project codebase, making them easily accessible for internal and external contributors.
3. Decisions will be recorded as they are made and updated or referenced whenever circumstances lead to revisiting an earlier decision.
4. Before a decision is finalized, it will undergo a review process where team members and external contributors can provide feedback, fostering collective ownership and promoting better decision-making.
5. These records will serve both as documentation for internal team members and as a transparent mechanism for communicating thought processes and strategic choices to outside contributors.
6. Old decisions that are revisited and updated will remain documented, with clear references to subsequent ADRs to preserve the history of the decision-making process.

This approach will encourage open collaboration, guide future work, and provide a historical record of the project's architecture and technical decisions.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up  with all relevant discussion summarized  in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision  
We will standardize an Extension Definition EDN (ExtDef) structure accessed via decentralized storage URIs.  
- **The EDN Structure:**  
  Extensions will follow a specific EDN schema that defines their metadata, dependencies, and configuration requirements. This ensures consistency and compatibility with Arachne's overarching configuration model.  

- **Decentralized Storage URIs:**  
  Extensions will be identified and retrieved using a specialized URI scheme that supports decentralized storage mechanisms (e.g., IPFS, Arweave). This ensures extensibility, security, and resilience against centralized points of failure.  

- **Validation:**  
  An Arachne module will handle the validation of these ExtDef files, confirming that they comply with the defined schema before integration into the application configuration. Any invalid ExtDef will provide detailed error feedback for developers.  

This approach ensures modularity, transparency, and a clean separation of configuration from runtime states while embracing decentralized principles.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision  
Group chats will be deprioritized and taken out of scope for the beta launch of the application. The focus for the beta release will remain on 1-1 chat and public chat, as these features are more critical to delivering value to users and achieving the goals of the minimum viable product (MVP).

### Rationale:
1. **Critical Path Focus**: Excluding group chats allows the team to concentrate on delivering core features that are essential for the beta launch, staying on track for the Q2 timeline.
2. **Impact**: 1-1 chat and public chat provide the most immediate and broad utility for the MVP, whereas group chats, while useful, are not as critical for early adoption.
3. **Technical Complexity**: Group chat introduces additional complexity, especially around encryption and security, which adds risk to meeting the beta timeline. Addressing regressions and ensuring a stable experience would demand significant resources.
4. **Past Overhead**: Removing group chat from scope puts an end to recurring discussions and the effort wasted on unresolved regressions, allowing the team to direct energy toward higher-priority tasks.

The team may revisit this decision post-beta launch when there are more resources and time available to explore group chat functionality in detail.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision  
We will implement the Discovery v5 protocol in our application to enable dynamic peer discovery for messaging.  

### Rationale:
1. **Scalability**: By adopting Discovery v5, peers will no longer need to be hardcoded. This allows the network to dynamically scale as the user base grows, without requiring manual configuration of nodes.  
2. **High Availability**: Discovery v5 ensures that users can find alternative peers in case of node failures or resource exhaustion, supporting better uptime and reliability.  
3. **Security**: The protocol helps in randomizing the selection of peers, reducing the risk of targeted attacks on specific hardcoded nodes or clusters.  
4. **Decoupling**: Server peers in the cluster can scale independently of the application since the discovery process will handle finding and connecting to new nodes automatically.  

### Implementation Plan:
1. Integrate the Discovery v5 protocol library into the application codebase.  
2. Replace the current hardcoded peer logic with dynamic peer discovery based on Discovery v5.  
3. Test the implementation in a controlled environment to verify behavior under different scaling and failover scenarios.  
4. Roll out the feature incrementally, with robust monitoring to validate its effects on scalability, availability, and security.  

By implementing Discovery v5, we align our system design with the goals of creating a scalable, resilient, and secure messaging infrastructure.  ","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision  
We will adopt the wallet compatibility approach outlined in Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README).

This means:
1. We will generate wallet keys and addresses using the BIP-39 and BIP-44 standards, which are widely used in the cryptocurrency ecosystem.
2. This change will align Status wallet functionality with industry norms, allowing users to import wallets created in other applications using the same 12 mnemonic words.
3. Existing users will retain their existing wallets to avoid disruption, but new wallets will follow the new standard.

By implementing this change, we improve user experience and interoperability without compromising security, aligning with both user expectations and industry best practices.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision  
We will prioritize the implementation of ULC (Ultra Light Client) as the preferred long-term solution for addressing the performance issues and decentralization concerns associated with LES (Light Ethereum Subprotocol) on Mainnet. The reasons for this decision include:

1. **Performance Improvement:** ULC has the potential to address existing performance bottlenecks such as slow/hot sync, disk usage, and bandwidth problems, which remain challenging to solve effectively with LES.

2. **Decentralization and Security:** Unlike relying on Infura or third-party Ethereum nodes, ULC enables users to connect directly to multiple upstream nodes. This enhances decentralization and reduces trust dependencies, aligning better with our principles of security and user sovereignty.

3. **Future Scalability:** ULC offers a more promising direction for supporting efficient and decentralized interaction with the Ethereum network in the future, especially as the application's user base grows.

In the short term, we acknowledge the reliance on Infura is suboptimal but a necessary compromise until ULC is ready for production use. To mitigate risks, we will allow users to optionally run their own upstream Ethereum nodes as an intermediary step while actively communicating the limitations of this trust model.

The following actions will be taken as part of this decision:
- **Implementation of ULC:** Focus development efforts on completing and optimizing ULC for integration into the application.
- **Fallback Options:** Maintain and improve support for LES in the interim to provide an alternative for users where ULC is not yet available or viable.
- **Transparency:** Clearly communicate with users about the current reliance on Infura, its trade-offs, and the steps being taken to achieve a more decentralized architecture.
- **Testing ULC:** Conduct extensive testing of ULC to ensure performance, security, and user experience before full deployment. 

This decision will guide our efforts moving forward, but we remain open to revisiting it if significant unforeseen challenges arise during ULC development or if LES performance improvements substantially exceed expectations.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision  
We will introduce a dedicated wrapper type for sensitive data, such as passwords, to prevent them from being logged accidentally.

### Key Details:
1. **Wrapper Type**:  
   A new type (e.g., `SensitiveData` or similar) will be created to encapsulate sensitive values such as passwords. This type will explicitly define restricted behaviors.

2. **String Conversion Prevention**:  
   The wrapper will override `toString()` or equivalent methods to prevent the sensitive data from being output as plain text. Instead, it could return a placeholder string such as `""[REDACTED]""`.

3. **Controlled Access**:
   The sensitive data will only be accessible programmatically through specific accessor methods, explicitly requiring developers to acknowledge they are working with sensitive data.

4. **Logging Enforcement**:  
   Logging mechanisms will be updated (if required) to recognize the wrapper type and enforce redaction or suppression of sensitive data in logs.

### Benefits:
- **Accidental Exposure Mitigation**: By encapsulating sensitive data in a dedicated type, we reduce the likelihood of it being logged accidentally.
- **Stronger Intent Signaling**: The use of a wrapper type makes it explicit to developers that they are handling sensitive information, encouraging more caution.
- **Customizable Behavior**: Using a wrapper ensures we can enforce consistent behavior (e.g., redaction, controlled access) across the codebase.

### Action Plan:
1. Implement the `SensitiveData` type wrapper.
2. Refactor existing code to use the wrapper for sensitive data like passwords.
3. Update logging systems to respect the wrapper and redact sensitive information.
4. Educate developers on the use of the new type and practices around handling sensitive data.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision  
We will deprecate and remove support for the JavaScript jailed execution environment for 3rd party status extensions (Jail).  

Instead, we will focus our efforts on building and adopting the new ""Pluto"" extensions framework, which provides a more robust and performant way to extend the application. This decision is driven by the following considerations:
1. **Performance**: The asynchronous nature of the Jail environment introduced significant performance penalties, particularly due to constant RN Bridge communication. The Pluto framework will offer more efficient extension mechanisms that are better integrated into the application and its workflows.
2. **Code Quality**: The Jail setup encouraged mutable semantics and introduced hard-coded logic into the app, leading to error-prone code and undermining maintainability. By moving to declarative extensions in Pluto, we expect to improve the codebase quality and avoid past pitfalls.
3. **Dogfooding**: The original API discouraged us from fully adopting and testing it ourselves (""eating our own dogfood"") due to its limitations. Pluto, on the other hand, provides a more versatile and reusable design that we can commit to using across the application.
4. **Technical Debt**: The Jail environment required frequent workarounds and hacks to maintain, given that commands relied on custom logic in the app and an inconsistent, outdated API. By transitioning to Pluto, we can eliminate this technical debt and start with a clean slate.
5. **Lack of Usage/Interest**: The current Jail-based implementation has not seen meaningful adoption by 3rd party developers, who primarily leverage standard web technologies for their DApps. As such, supporting the Jail environment does not align with real-world usage patterns.
6. **Future-Proofing**: Pluto offers enhanced possibilities for integrating extensions into the overall application ecosystem, enabling developers to extend not just command message logic but also other application features like wallets and contacts. This aligns more closely with our vision for extensibility.

### Steps:
1. Begin the deprecation process for the Jail-based system, and update associated documentation to indicate its planned removal.
2. Transition existing logic and commands that rely on Jail into the new Pluto extension framework, ensuring functionality is preserved with better performance and maintainability.
3. Inform and support any existing 3rd party developers relying on the Jail environment to migrate to alternative approaches that work with the Pluto framework.
4. Fully remove Jail-related code and dependencies from the codebase post-transition.

This change will help us align with our goals of better performance, improved code quality, and a more future-proofed extension ecosystem.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision  
We will implement Tribute to Talk (TtT) as described in the whitepaper, where users must stake a minimum amount of SNT for someone else to contact them directly. The recipient of a message will only claim the staked deposit if they choose to respond to the message.

The key design decisions for this implementation are as follows:

1. **Deposit Requirement**:  
   - A sender must deposit the minimum SNT amount specified by the recipient in order to initiate communication.
   - The recipient has full control to set and update their desired minimum deposit requirement.

2. **Conditional Forfeiture**:  
   - The sender's deposit is only transferred to the recipient's account if the recipient responds to the message.
   - If the recipient ignores or declines the message, the staked SNT will eventually be returned to the sender after a predefined timeout period.

3. **Transparency and Accountability**:  
   - Users should be able to transparently view configured deposit requirements and the status of sent or received messages.
   - Notifications will be implemented to inform users of incoming message requests and the status of their deposits.

4. **Spam Mitigation**:  
   - By requiring economic cost for unsolicited messages, the system discourages spam while allowing genuine communication by enabling sender intent.

5. **User Experience Considerations**:  
   - Adequate safeguards will be put into place to simplify the process for new users, including default deposit recommendations and user-friendly interfaces to configure or adjust settings.

6. **Decentralization**:  
   - The deposit transactions and message confirmations will be executed and verified on-chain to ensure transparency, trust, and resilience against tampering or abuse.

By adhering to this mechanism and leveraging the incentive model, Tribute to Talk will empower users to control their communication preferences while mitigating spam and ensuring a fair interaction model between parties.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision
We will establish a new role within the mobile development team, assigning a **Section Owner** for each of the following areas of the mobile app:  

1. Communities  
2. Syncing & Onboarding  
3. Activity Center & Shell  
4. Wallet  
5. Messaging  

### Responsibilities of Section Owners:
- **Oversight**: Ensure that their assigned section is well-polished in terms of design, functionality, and performance.  
- **Collaboration**: Act as the main point of contact between the development team and the design team for their section, ensuring alignment on vision and standards.  
- **Quality Assurance**: Provide regular feedback to developers working in their section and proactively identify areas for improvement.  
- **Documentation**: Maintain up-to-date documentation on their section's features, architecture, and ongoing issues to ensure transparency and a shared understanding among team members.  
- **Cross-Team Coordination**: Facilitate communication across sections to identify dependencies and avoid silos or fragmentation in the app.  

### Benefits of this Decision:
- Distributes responsibility to improve oversight and accountability for each app section.  
- Enhances collaboration between developers and designers.  
- Creates clear points of ownership, ensuring no area of the app is neglected.  
- Encourages alignment on technical and design standards across sections.  

Section Owners will not replace the current team lead but instead will act as delegates, collectively supporting the lead in maintaining the app's overall polish and quality.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision  
We will use the PATCH HTTP method to handle updates to the Product Option's values collection. This approach allows partial updates to be applied efficiently without requiring the entire collection to be replaced or re-sent in the request payload. This decision ensures flexibility and efficiency while adhering to RESTful principles.  

When modifying the collection:
1. Additions to the collection can be appended by including only the new entries in the PATCH request.
2. Updates to specific values can be made by including only the modified fields with their corresponding identifiers.
3. Deletions can be supported by including specific indicators or keys for removal within the PATCH payload.

This approach provides a clean mechanism for incremental customization while minimizing overhead for both client and server. To ensure consistency:
- Proper validation rules will be applied on the server to handle scenarios such as duplicated or conflicting entries in the request payload. 
- Detailed error responses will be provided to guide the client in case of incorrect payload structures or validation failures.
  
By leveraging the PATCH method, we maintain an efficient interface for managing Product Option values, improving user experience and system performance.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision 
We will introduce a consistent way of handling locale-aware resources in Sylius by embedding only the translation corresponding to the customers locale in the API responses. 

Specifically:
1. API endpoints will return only the translated content for the customer's requested locale, ensuring simplicity and usability for frontend developers.
2. A query parameter (`locale`) will be introduced to allow clients to explicitly specify the desired locale for the response. 
   - If no locale is specified in the request, the system will fall back to the default locale configured in the store.
3. This approach will be implemented consistently across all resources that have locale-aware attributes such as product descriptions, payment method names, shipping method names, etc.
4. Backwards compatibility will be ensured temporarily: endpoints that currently return all translations will maintain this behavior as a deprecation layer, but developers will be encouraged to switch to the new, more streamlined approach.
5. Documentation will be updated to reflect the changes, and examples will illustrate how to leverage the new locale handling features to enhance the customer experience.

This change ensures clarity, consistency, and improved user experience while keeping unnecessary complexity out of the frontend logic.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision  
We will use Architecture Decision Records (ADRs) to document significant decisions related to the development of the Terraform Controller. ADRs are a lightweight and structured way to provide context, record options considered, and explain the rationale behind the chosen decision.

The process to document decisions will be as follows:
1. When a significant architectural or technical decision needs to be made, a contributor will create a new ADR document with the status ""proposed.""
2. The ADR will include the context, any alternatives considered, the decision, and the reasons why the decision was made.
3. The ADR will be shared and reviewed by the team. Updates will be made to reflect discussions, additional context, and any agreed-upon changes.
4. When consensus is reached, the ADR will be marked as ""accepted"" and archived in a centralized, well-organized repository within the Terraform Controller project for future reference.
5. If the original decision is revisited due to new circumstances or insights, a new ADR will be created. It will reference the original ADR and document the updated decision and rationale. The older ADR will have its ""status"" updated to indicate that it has been superseded.

This approach will ensure decisions are transparent, well-documented, easy to discover, and beneficial for onboarding new contributors while fostering a consistent and unified development process.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision  
We will use **webhooks** to detect pull request changes instead of polling using GitHub's API.

### Rationale:
1. **Efficiency**: Webhooks allow for instant detection of changes, whereas polling involves repeatedly sending requests to GitHubs API, which can be both inefficient and prone to unnecessary usage of API quotas.
2. **Scalability**: As the system grows and the number of repositories or pull requests increases, webhooks will scale better compared to the overhead involved with frequent polling intervals.
3. **Lower Cost and Complexity**: Polling requires implementing logic to handle repeated API calls and managing intervals. Webhooks remove this burden by pushing events to us only when changes happen.
4. **Accuracy**: Polling introduces latency since updates might only be detected at the next polling interval, whereas webhooks guarantee real-time updates.

### Implementation Notes:
- Webhook listeners will be set up to handle GitHub's pull request events (such as `pull_request` and `pull_request_review`).
- Proper security measures (e.g., secret tokens) will be implemented to verify the authenticity of webhook payloads from GitHub.
- Retry mechanisms will be in place for handling failures when processing incoming webhook events.

### Caveat:
If users do not or cannot configure webhooks, a fallback mechanism using periodic polling may need to be considered in the future.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision  
The tf-controller API will allow cross-namespace references but enforce them with additional safeguards to address security and namespace isolation concerns.  

### Details:
1. **Cross-namespace References**  
   - `Terraform.spec.sourceRef`, `Terraform.spec.dependsOn[]`, and `.data.resources[]` will continue to support cross-namespace references as they are critical for certain Flux deployments and workflows.  
   - These references will include a namespace field to explicitly define the target namespace, making cross-namespace dependencies clear and intentional.

2. **Safeguards**  
   - **Restrictions via RBAC**:  
     Cross-namespace references will require explicit permissions. For example, the ServiceAccount used by the controller must be granted `read` access to objects in the referenced namespaces. This ensures that only authorized relationships between namespaces can exist.  
   - **Validation**:  
     The controller will validate that referenced resources exist and are accessible within the target namespace at runtime before proceeding with operations. If a referenced object does not exist or is inaccessible, the operation will fail with an informative error.  
   - **Optional Feature Flag**:  
     A feature flag will be introduced for operators to disable cross-namespace referencing entirely if they wish to enforce strict namespace isolation in their clusters.

3. **Guidance to Users**  
   - Encourage users to avoid cross-namespace references unless absolutely necessary.  
   - Provide documentation recommending best practices for namespace organization to minimize the security and operational impact of cross-namespace references.

By maintaining cross-namespace referencing with safeguards, we ensure backward compatibility for existing workflows while also addressing the security and isolation concerns.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision  
The TF-Controller will implement a caching mechanism for storing and retrieving Workspace BLOBs. This caching system will ensure that Workspace BLOBs are readily accessible during the finalization process, enabling efficient deletion of single TF objects by retaining all the necessary information from their associated Source, Secrets, and ConfigMaps.

### Key Implementation Details:  
1. **Workspace BLOB Creation and Storage**:  
   - The tf-runner will continue to generate the Workspace file system from a Source BLOB and its associated Secrets and ConfigMaps.  
   - Compressed Workspace file systems will be stored as Workspace BLOBs in the TF-Controller's storage, with clear mapping to their TF objects.

2. **Caching Mechanism**:  
   - A dedicated caching layer will be introduced in the TF-Controller to manage Workspace BLOBs efficiently.
   - The cache will be optimized for retrieval during finalization processes, ensuring that the data necessary for TF object deletion is immediately available.

3. **BLOB Retrieval for Deletion**:  
   - During deletion of a single TF object, the TF-Controller will fetch the corresponding Workspace BLOB (from cache or storage) to extract all required information to cleanly delete the object and its dependencies.

4. **Dependency Resolution**:  
   - The caching mechanism will streamline processes where TF objects have interdependencies, ensuring that associated resources like Secrets or ConfigMaps are deleted in the correct order without leading to inconsistent states.

5. **Scalability and Storage Optimization**:  
   - The caching solution will be designed to scale efficiently as the number of TF objects grows.
   - Periodic cleanup or expiration policies for outdated Workspace BLOBs will be introduced to prevent excessive storage consumption.

By implementing this caching mechanism, the TF-Controller will resolve the problem of single TF object deletion, laying the groundwork for resolving more complex deletion scenarios involving dependent resources and namespaces.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision  
We will update the approach to E2E testing for Zarf to align with the new capabilities provided by the removal of the need for repository secrets. The decisions below aim to improve security, increase flexibility, and streamline the development process:  

1. **Remove Dependency on Repository Secrets for E2E Tests**  
   - Since the initialization package no longer requires registry1 images or repository secrets, we will eliminate the use of `peter-evans/slash-command-dispatch@v2` for triggering tests.  
   - E2E tests will now run automatically for all PRs, without requiring maintainer intervention. This increases transparency and reduces the manual overhead for testing PRs.  

2. **Introduce Environment Matrix Testing**  
   - To increase robustness, we will configure GitHub Actions or the chosen CI tool to support matrix testing across different environments:  
     - Kubernetes distributions such as K3s, Minikube, and EKS.  
     - Linux distributions such as Ubuntu, CentOS, and Alpine.
   - The matrix setup will ensure that Zarf is tested against a diverse set of real-world scenarios and edge cases during CI.  

3. **Local Development E2E Support**  
   - We will provide lightweight local testing options to support faster developer feedback loops. This involves:  
     - Including scripts or Make targets to allow developers to run a subset of E2E tests on their local machines.  
     - Leveraging tools like KinD (Kubernetes in Docker) or Minikube for local Kubernetes testing.  
     - Making test dependencies (e.g., mock registries or configurations) portable and easy to set up locally.  

4. **Standardize and Document the Testing Process**  
   - Create clear documentation on running tests both locally and through CI, highlighting how to specify test configurations (e.g., distributions and versions).  
   - Define guidelines for contributors to write new E2E tests that are compatible with the matrix and local setups.  

This decision ensures a more secure, flexible, and developer-friendly testing process that can adapt to Zarfs evolving requirements. The new approach supports both rapid feature development and comprehensive testing to maintain high-quality standards.  ","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision  
Zarf will implement an embedded, temporary registry as the mechanism for injecting container images into a Kubernetes cluster in scenarios where no registry is available or accessible. This decision is made to ensure Zarf operates in unknown and dependency-free environments, while also respecting the challenges of interacting with CRIs and mitigating the burden of configuring nodes for TLS trust.

### Key Points of the Decision:
1. **Temporary Embedded Registry for Image Distribution**:
   - Zarf will start a temporary OCI-compliant registry within the cluster environment or locally, based on the specific context of operation. 
   - By embedding the registry, Zarf avoids reliance on external infrastructure or the need to modify existing environments significantly.
   - Kubernetes nodes can be configured to pull container images directly from the local registry, thus bypassing the need for external, network-accessible registries.

2. **Control Over TLS Trust Issues**:
   - When using the embedded registry, Zarf will generate self-signed certificates and inject the temporary registry's TLS root CA into each node's CRI configuration or root CA store, depending on the node's capabilities.
   - This ensures secure communication when Kubernetes nodes pull images from the temporary registry.
   - Zarf will automate the process of distributing and configuring trust for the registry certificates within the cluster to minimize user intervention.

3. **Cluster-Wide Image Availability**:
   - The approach accounts for multi-node clusters by ensuring all nodes either gain access to the temporary registry or are preloaded with the required images. 
   - For small or single-node clusters, this will be a straightforward operation. For larger clusters, Zarf will perform the necessary node-by-node configuration as part of its orchestration.

4. **Clean-Up Upon Completion**:
   - Zarf will terminate the temporary registry and remove any associated certificates or configurations from the nodes after use to avoid leaving persistent changes to the environment.
   - This ensures Zarf maintains an ephemeral and non-disruptive footprint in the target clusters.

5. **Fallback for Specialized Environments**:
   - For environments where modifying node-level configurations is not possible, Zarf will provide options such as tainting and affinity rules to leverage known, preconfigured nodes for image pulls during the bootstrap process.

### Rationale:
- **Flexibility**: This approach allows Zarf to operate across a wider range of unknown and restricted Kubernetes environments regardless of existing registries or configurations.
- **Security**: By automating TLS trust configuration and enforcing secure communication, the process will not compromise security standards or require manual intervention.
- **Modularity**: The embedded registry is entirely self-contained and ephemeral, aligning well with Kubernetes' principles of immutability and stateless design.
- **Ease of Use**: Users will not need to manually configure registries, TLS, or dependencies, maintaining Zarf's goal of simplicity and dependability in deployment.

### Trade-Offs:
- **Temporary Overhead**: There is some startup and teardown overhead involved with booting the temporary registry and automating node configurations.
- **Per-Node Configuration**: Modifying TLS trust or CRI configuration per node can be intrusive in certain environments where such actions are heavily restricted or undesired.
- **Resource Usage**: Depending on the size of the images being pushed, the temporary registry might consume additional memory and storage resources during its operation.

By creating an embedded, temporary registry and handling all configuration requirements programmatically, Zarf ensures it can function in even the most constrained or unknown Kubernetes clusters while maintaining ease of use, security, and adaptability.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision  
We will implement the generation and inclusion of Software Bill of Materials (SBOM) for all software components to comply with Executive Order 14028 requirements. SBOMs will provide a comprehensive, machine-readable inventory of software components, including libraries, dependencies, and their versions, which ensures transparency and security in our software supply chain.

To achieve this:

1. We will adopt a standardized SBOM format, such as SPDX or CycloneDX, for consistency and interoperability.
2. SBOM generation will be integrated into our CI/CD pipelines, ensuring that every build automatically produces an up-to-date SBOM for the resulting software artifact.
3. SBOMs will be stored alongside the corresponding software components in an accessible and versioned repository, enabling traceability and auditability.
4. Regular reviews of the SBOM will be conducted to verify that no critical vulnerabilities or unsupported components exist in the software stack. Security tools may be employed to automate parts of this process.
5. We will provide training for all developers to understand SBOM creation, its significance, and how to maintain compliance with EO14028.

This approach will ensure regulatory compliance while facilitating a more secure and transparent software development process.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision

We will implement a custom Kubernetes Admission Controller in Zarf to handle mutating image paths, secrets, and other references (e.g., git server URLs) in manifests dynamically at runtime. 

### Details of the Decision:
1. **Admission Controller**: A Kubernetes MutatingAdmissionWebhook will be developed and deployed as part of Zarf deployments. This controller will intercept requests to the Kubernetes API server, identify resources requiring mutation, and apply the necessary changes before the resource is persisted in the cluster.

2. **Dynamic Mutation**: 
   - The Admission Controller will mutate image paths to point to the internal Zarf Registry, ensuring compatibility regardless of the deployment tool (e.g., Flux, ArgoCD).
   - References to the Zarf Git server (e.g., in CRDs) will be dynamically updated to the correct internal URLs as part of the same mutation process.

3. **Automatic Workflow**: 
   - This approach will eliminate the dependency on Helm Post Rendering for image and secret mutation. 
   - It will also reduce the manual burden on package authors, as Zarf will handle all required mutations dynamically via the Admission Controller.

4. **Backward Compatibility**: To ensure the transition is smooth, we will maintain the existing `zarf prepare` command for package authors already accustomed to it. However, it will be clearly documented that the Admission Controller is the preferred and default mechanism moving forward.

5. **Security Considerations**: The Admission Controller will be designed with security in mind:
   - It will only mutate resources associated with Zarf packages.
   - Proper RBAC rules and isolation mechanisms will be in place to limit its scope of operation in the cluster.

6. **Future-proofing**: The Admission Controller will be built to be extensible, allowing additional mutating logic to be defined in the future for other use cases, as required by Zarf or its users.

### Outcome:
By leveraging an Admission Controller, Zarf will seamlessly integrate with secondary GitOps tools like Flux and ArgoCD without requiring package authors to implement manual hacks or complicated templating. This will make Zarf more user-friendly, robust, and extensible for complex Kubernetes environments.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision  
Replace the `stage2` Go binary and `registry:2` Docker registry construct with a simpler approach that eliminates unnecessary overhead while achieving the same functionality.

Instead of using a full-featured Docker registry, the process will be simplified to directly use the Rust-based `stage1` binary to handle the required registry operations. The updated workflow will:

1. Eliminate the need for the `registry:2` Docker registry image and the `stage2` Go binary entirely.
2. Modify the `stage1` Rust binary to handle the tasks previously covered in `stage2`. This includes:
   - Handling the `rw` and `r` modes required for the registry.
   - Managing the consumption of the `registry:2` crane tarball in a lightweight manner.
   - Pushing the image required by the `zarf-registry` package.
3. Reduce the number of required configmaps by consolidating operations into the modified `stage1` binary. 

This decision will:
- Reduce the operational and management overhead of additional binaries and configmaps.
- Optimize the initialization process by streamlining the injection workflow.
- Remove dependencies on a heavy, full-featured Docker registry when a lightweight, single-task solution suffices.
- Simplify the build process by eliminating the need to compile multiple binaries for multiple platforms.

As a result, this simplification aligns with the goal of improving maintainability, reducing complexity, and optimizing resource usage in the Zarf ecosystem.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision  
We will introduce **Unit Testing in a defined and limited capacity within Zarf**.

### Rationale:
- This approach allows us to strategically target areas of the codebase that are not easily covered by end-to-end testing or require higher scrutiny due to Zarf's transition into becoming a library for other products.
- By being selective and deliberate about where unit testing is applied, we can ensure meaningful test coverage for critical or hard-to-test sections without overwhelming the team with excessive maintenance for the tests.
- This balances the need for additional testing coverage with the risks of writing overly brittle or implementation-specific unit tests.
- The approach prevents a full pivot away from the current end-to-end testing strategy, leveraging integration testing only where absolutely necessary in complex scenarios.

### Implementation Guidelines:
1. **Define Clear Scope**: Identify the parts of the codebase:
   - That are challenging to test via end-to-end testing.
   - That handle significant logic or serve as critical interfaces for Zarf as a library.
2. **Write Interface-Focused Tests**: Unit tests will focus on validating well-defined interfaces rather than testing low-level, implementation-specific details.
3. **Review Criteria**: Establish strict PR review standards to ensure that any new unit tests are correctly applied and contribute to maintainable, meaningful test coverage.
4. **Leverage Existing Testing Strategy**: Use integration testing or partial end-to-end testing sparingly for scenarios that require orchestrating multiple application components.

This approach ensures we enhance Zarf's testing strategy to meet new scrutiny requirements while avoiding unnecessary complexity or burden.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision 
We will implement SBOM (Software Bill of Materials) generation for non-image dependencies in Zarf packages using the following approach:

1. **Run Syft against files and dataInjections (Option 2):**  
   - This strategy focuses specifically on the files and dataInjections components of a Zarf package since these are guaranteed to be part of the package and directly relevant.  
   - **Rationale:** This approach strikes the best balance by providing accurate and actionable information without introducing significant noise or confusion. It aligns with the goals of offering users a clear and complete understanding of what is actually inside their packages, avoiding extraneous data from unrelated contexts like git repos.  

2. **Future Flexibility for User-Defined SBOM Scope (Option 3):**  
   - In the future, we will consider adding a feature that allows users to define the scope of what should be included in SBOM generation.  
   - **Rationale:** While this adds complexity, it allows advanced users who may need to include additional components such as artifacts from git repos to tailor the SBOM process to meet their needs.

3. **Continuous Reassessment:**  
   - We will periodically reassess and adjust the SBOM generation scope as new use cases emerge and based on feedback from users.

This decision prioritizes providing a reliable and clean starting point with the most direct applicability (Option 2) while keeping the door open for user-driven customization (Option 3) in the long term. Option 1 is avoided due to its high potential to generate unnecessary and confusing noise, which could dilute the overall user experience.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision 
Zarf will introduce an optional declarative mode that allows users to deploy Zarf packages in connected environments without requiring a Zarf init package. 

To support this mode:  
1. Zarf will enhance its configuration system to allow packages to specify dependencies and settings that can be resolved dynamically in connected environments.  
2. The deployment process will detect the environment mode (connected vs disconnected) and adjust behavior accordingly, such as skipping dependencies and registry bootstrapping in connected environments.  
3. New validations will be added to ensure this declarative mode maintains the core principles of Zarf (e.g., immutability and reproducibility) while allowing for flexibility in connected deployments.  
4. Documentation will be updated to clearly define the setup and constraints for deploying in this mode, along with examples for leveraging Zarf without an init package in connected scenarios.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision  
We will refactor the `scripts` section of components to increase its flexibility and address the limitations revealed by broader use cases. Specifically:  

1. **Unified and Flexible Script Schema**  
   The `scripts` noun will be restructured to support a broader range of actions, not just `before`, `after`, and `prepare`. Scripts will be organized into distinct lifecycle phases, such as `on-package-create`, `on-deploy-start`, `on-deploy-finish`, or any additional phases deemed necessary for common workflows. These phases will be explicitly documented and associated with various stages of the application lifecycle.  

2. **Parameterized Execution**  
   Scripts will support parameterization, allowing dynamic inputs during execution. This will allow users to define reusable and context-sensitive scripts.  

3. **Conditional Execution**  
   Scripts will support conditional logic to define under what circumstances a particular script runs. This will enable users to streamline complex workflows where certain actions are contingent on environmental or runtime conditions.  
  
4. **Error Handling and Debugging**  
   Enhanced error-handling mechanisms will be introduced so that failures in one script phase can be logged with meaningful information and optionally configured to fail fast or continue with warnings, based on user-defined preferences.  

5. **Backward Compatibility**  
   To ensure a smooth migration, the existing `scripts` structure (`before`, `after`, `prepare`) will continue to be supported as a legacy format. However, the new schema will be recommended for all new development efforts, with clear migration guidelines provided.  

This refactoring will future-proof the `scripts` implementation, making it more robust for diverse use cases while maintaining transparency and clear application behavior. As adoption grows, the system will remain flexible enough to adapt to emerging requirements.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision  
We will continue using Crane for loading local Docker images, while implementing mitigations for the observed OOM issue.

### Rationale
1. **Performance Consideration**: 
   - Crane is consistently faster than other explored solutions such as the Docker CLI, skopeo, or the Docker Go client, as indicated in the provided benchmarks. The performance advantage is significant, particularly in scenarios with cached content or larger images, reducing overall development time.

2. **Future Compatibility**:
   - The images exported and saved using Docker produce a legacy format that could cause issues with planned changes to the package schema, specifically in regard to OCI imports. Crane avoids this compatibility concern.

3. **OOM Issue in Crane**:
   - Cranes current implementation leads to OOM crashes in certain scenarios due to how it handles tar streams in memory. However, Crane supports an option to avoid this issue, though it incurs a slower performance trade-off. By selectively applying this option in contexts where image size or memory usage could cause failures, the impact of the OOM issue can be mitigated.

4. **Further Strategies**:
   - The investigation revealed no entirely satisfactory alternative that offers both acceptable performance and reliability. As such, its most prudent to enhance the existing Crane-based solution until a fundamentally better approach is discovered or developed.

### Mitigation Actions
1. When images are loaded into the local Docker daemon, utilize the Crane option to avoid in-memory tar streaming for large images or high-memory-use scenarios, even if it is slower.
2. Implement a memory threshold detection mechanism to dynamically decide whether to enable the safer, slower Crane option for large images.
3. Clearly document the performance expectations and known limitations of the current implementation in the Zarf documentation.
4. Continue monitoring for advancements or changes in image management tools (e.g., improvements in Crane, Docker, or other third-party utilities).

### Trade-offs
- **Performance vs Reliability**: While prioritizing reliability for large images, there will be some performance degradation in specific scenarios. This trade-off is deemed acceptable to prevent crashes and to maintain a functioning developer experience.
- **Slower Development with Large Images**: Medium and large images will have slower loading times compared to an OOM-prone solution, but the trade-off is necessary to ensure stability and prevent crashes.

By addressing the OOM issue in this manner, Zarf will provide a balanced solution that optimizes for both developer usability and system reliability.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision
To simplify the deployment of Big Bang using Zarf, Zarf will implement the following strategies to address the outlined challenges:

1. **Flux Pre-installation**:
   - Zarf will ensure that Flux and its required CRDs are pre-installed as part of the package deployment process whenever including Big Bang is necessary.
   - Flux will be included as a dependency in any Zarf package that deploys Big Bang. This ensures compatibility and eliminates manual installation requirements for users.

2. **Selective Image Resolution**:
   - Zarf will analyze the `values.yaml` file provided for the deployment to determine the exact subset of images needed from the superset defined in Big Bang.
   - Only the required images identified from this process will be included in the Zarf package. This reduces the storage overhead and minimizes the size of the package.

3. **Efficient Image Packaging**:
   - Zarf will introduce a mechanism to download and package only the images needed for deployment based on the values provided, avoiding the need to package the entire Big Bang image suite.
   - Users will have the option to specify custom values, and Zarf will dynamically resolve and package the dependent images as part of the process.

4. **Git Repository Subsetting**:
   - Similar to the required images, Zarf will parse the deployment configuration to determine the subset of git repositories required for the deployment.
   - Only these necessary git repositories will be cloned or referenced in the Zarf package to streamline deployment.

5. **`values.yaml` Injection Support**:
   - Zarf will provide a dedicated mechanism or tooling to make injecting a `values.yaml` file into the Big Bang deployment straightforward.
   - This tooling will automate the process of merging provided `values.yaml` overrides into Big Bangs default deployment structure, ensuring compatibility and reducing user overhead.

6. **Transparent User Experience**:
   - Zarf will expose clear documentation and workflows to guide users through the Big Bang deployment process with proper examples for crafting `values.yaml` and understanding the packaging process.
   - Upon deployment, Zarf will inform the user of the resolved images and repositories being included, ensuring transparency in the deployment process.

By implementing these strategies, Zarf will streamline the deployment of Big Bang, reducing storage overhead, ensuring compatibility with Flux, and making custom deployments with specific `values.yaml` files feasible and user-friendly.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision 
Zarf will adopt the OCI (Open Container Initiative) Artifact Specification to enable native publishing and distribution of Zarf packages via OCI-compliant registries. 

1. **OCI Compliance for Distribution**:
   - Zarf packages will be encapsulated and published as OCI artifacts. This ensures compatibility with a wide range of existing OCI-compliant registries, making Zarf packages easily shareable and distributable at scale.
   - Users will be able to use Zarf commands to push packages to OCI registries, simplifying the deployment and retrieval process.

2. **Package Signing for Supply Chain Security**:
   - Zarf will include support for cryptographic signing of packages. This ensures that package deployers can verify the integrity and authenticity of packages before use.
   - Signing will be implemented using industry-standard methods to promote trust and ensure alignment with supply chain security best practices.

3. **Validation During Deployment**:
   - During the deployment process, Zarf will offer built-in verification of the package signature against trusted keys. This prevents the deployment of potentially tampered or malicious packages.

By implementing this approach:
- Users can seamlessly distribute and consume Zarf packages using widely supported OCI tools and registries.
- The package supply chain security of Zarf packages is bolstered by integrating signing and verification processes.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision
Zarf will expand its support for airgap development by including an artifact repository feature to host and serve programming language dependencies and libraries, allowing developers in airgapped environments to compile and develop software.

Additionally, Zarf will enhance its support for `git` by enabling more native use cases for interacting with upstream `git` hosts. This goes beyond flux `GitRepository` objects to accommodate broader `git` client requirements. This enhancement will enable developers to access and interact with `git` repositories in a manner more consistent with traditional workflows while adhering to airgap constraints.

These additions will ensure that Zarf becomes more versatile and better suited to the needs of airgapped development environments, while maintaining its existing functionality for production deployments.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision  
Zarf will adopt a formalized stability and versioning strategy to achieve a ""Generally Available"" v1.x.x release and become suitable for mission-critical operations. 

1. **Adoption of Semantic Versioning (SemVer):** 
   - Zarf will follow SemVer to clearly communicate changes in the platform. A `1.x.x` release will guarantee backward compatibility for all operations within the same major version, ensuring users can confidently use Zarf without concern about unexpected breaking changes.

2. **Introduction of Rigorous Testing Practices:** 
   - We will adopt a comprehensive suite of automated tests, including unit tests, integration tests, and end-to-end tests, to ensure that new features or changes do not introduce regressions or break critical functionality.

3. **Defined Stability Milestones:** 
   - Development will proceed through structured stability milestones (e.g., Alpha, Beta, RC, and GA). Each milestone will have specific criteria for user feedback, feature completeness, and testing coverage that must be met before advancing to the next stage.

4. **Long-Term Support (LTS) Concept Post-GA:** 
   - After the v1.x.x release, Zarf will support certain versions for an extended period through well-defined LTS releases. Users relying on mission-critical applications can rest assured of bug fixes and security patches for these versions over time.

5. **Improved Documentation:** 
   - To support a broad user base, Zarf will provide robust, consistently updated documentation to help new users and operators navigate the tool effectively.

6. **Community Feedback Integration:** 
   - Engage with users actively to gather feedback during the Beta and RC stages to identify and prioritize feature gaps or usability concerns before the GA release.

By adopting this stability and versioning strategy, Zarf can ensure users have a predictable, reliable foundation for their needs while building trust within the community and enterprise environments.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision  
We will introduce a formal concept of `hooks` within the Zarf framework. Hooks are distinct from `actions` and will provide cluster maintainers with a powerful mechanism to define custom functionality that operates during critical deployment lifecycle events. Unlike `actions`, which are package-specific and perform host-based operations, `hooks` will be cluster-aware, reusable, and inherently tied to the lifecycle of the cluster itself rather than individual packages.

### Implementation Details:
1. **Lifecycle Hooks for ECR Use Case:**
   - Hooks will address the recurring challenges associated with ECR registries:
     - A hook will be responsible for refreshing ECR authentication tokens periodically, ensuring seamless operation without user intervention.
     - Another hook will create necessary image repositories dynamically during package deployments, resolving the 'push-to-create' limitation of ECR.

2. **Separation of Concerns:**
   - Hooks will allow package creators to focus on building system-agnostic packages that leverage core functionality of Zarf, while cluster maintainers can implement hooks to bridge Zarf with the specific requirements of their backing infrastructure (e.g., ECR, GCR, custom registries).

3. **Composition and Reusability:**
   - Hooks will be designed as reusable modules built into the configuration of the cluster initializer (`zarf init`). Cluster maintainers can choose hooks that suit their deployment needs, ensuring that they are generalized for broader use cases beyond ECR.

4. **Transparency and Introspection:**
   - Hooks will be explicitly documented and easily discoverable within the cluster's configuration files. This aligns with Zarf's broader goals of transparency and introspectability.

5. **Preference over `Actions` for System Integration:**
   - Hooks will serve as the preferred mechanism for integrating core Zarf functionality into the cluster. While `actions` remain valid for implementing package-specific runtime operations, hooks will take precedence for solving infrastructure-level problems, reducing misuse of `actions` for such purposes.

6. **Default Hooks for Known Registry Types:**
   - Zarf will ship with predefined hooks for common registry types (e.g., ECR, Docker Hub, GCR) to simplify initial cluster setup. These hooks can be overridden or extended by maintainers for custom registries or unique requirements.

### Outcomes:
- Hooks will empower Zarf to become infrastructure-agnostic while still accommodating specific requirements seamlessly.
- The introduction of hooks will significantly simplify package creation, allowing developers to produce more portable and generic packages without embedding cluster-specific logic.
- Cluster maintainers will have finer control over deployment lifecycle events and can extend Zarf functionality to meet their operational needs without overloading package creators.
","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision  
We will use **WASM (WebAssembly)** to decouple transformations from Zarf.

### Rationale:
1. **Performance and Efficiency**: WASM offers near-native performance, making it an excellent choice for resource-intensive transformations. This is particularly valuable since the Zarf transformation library is battle-tested and performance-critical.
   
2. **Cross-Language Interoperability**: WASM enables seamless execution of the Go code within the Pepr framework without having to rewrite the existing, reliable Go library in TypeScript. 

3. **Code Reuse**: By embedding the Go code into WASM, we avoid maintaining two separate libraries (in Go and TypeScript), ensuring consistency and reducing maintenance overhead.
   
4. **Portability and Isolation**: WASM modules are portable and sandboxed, providing a secure and lightweight way to execute code within different runtimes, including Pepr.

5. **Future Proofing**: The decision to use WASM ensures the flexibility of integrating with other runtimes or services beyond Pepr while leveraging the same core transformation logic.

### Alternatives Considered and Rejected:
- **gRPC**: While gRPC is efficient for communication between services, it introduces complexity in maintaining the protocol definitions and runtime dependencies. It also doesnt address the need for reusing the existing Go code directly within Pepr.
- **REST**: REST introduces too much overhead and doesnt offer the same performance and efficiency guarantees. It is not ideal for low-latency, high-performance scenarios like transformations.
- **Rewrite in TypeScript**: Although this could simplify integration within Pepr, rewriting the battle-tested and reliable transformation library in TypeScript would be time-consuming, error-prone, and counterproductive.

### Implementation Notes:
- The Zarf transformation library will be compiled into a WASM target using Go's support for WebAssembly. 
- Pepr will embed and execute the WASM module for transformations, providing a bridge between its TypeScript-based architecture and the Go-based logic of Zarf.
- Thorough testing and validation will ensure that the WASM integration matches the reliability and correctness of the original Zarf library.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision  
Zarf will standardize and unify its package loading abilities to ensure consistent behavior across all supported actions (inspect, deploy, remove, and list). This decision will enhance reliability and usability for all package sources.

### Key Changes:
1. **Split Tarball Packages**
   - Will support all actions: `deploy`, `inspect`, and `remove`.
   - Existing functionality (creation and deployment) will be retained, and the inspection/removal logic will be added to allow consistent handling.

2. **In-Cluster (Deployed) Packages**
   - Users will be able to `inspect` packages in addition to already existing `remove` and `list` functionalities. The inspection will provide insights into the deployed package state.

3. **HTTPS Remote Packages**
   - Currently only supported for `deploy`. Functionality for `inspect` and `remove` will be added. This enables users to verify and manage packages hosted remotely via HTTP.

4. **OCI Packages**
   - OCI packages will maintain consistency, continuing to support all requested actions (`inspect`, `deploy`, and `remove`).

5. **Local Tarball Packages**
   - No changes requiredalready fully supported for `inspect`, `deploy`, and `remove`.

### Implementation Guidelines:
- Package loading functionality will be decoupled from specific actions (e.g., `deploy`, `inspect`) and will be abstracted into a unified package loader interface.
- Extend test coverage to ensure that packages behave as expected for any supported source across all relevant workflows.

### Benefits:
- Guarantees a consistent user experience, minimizing confusion.
- Facilitates clearer documentation by defining standardized behavior for all package types and actions.
- Improves maintainability by centralizing logic for package loading.

With these additions, Zarf's package system will become significantly more robust and predictable for end-users across diverse workflows.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision  
A new, unified approach for handling component composition and filtering will be implemented in Zarf to address scalability, maintainability, and improved testability. This will involve redesigning how components are imported, filtered, and composed during `zarf package create`, with specific attention to the following principles:

1. **Centralized Import Handling**: A single, centralized mechanism will be implemented to manage component imports and process import chains. This will ensure proper handling of deeply nested imports, eliminating issues with scaling to a larger number of layers and ensuring correctness across all levels.

2. **OCI Integration Redesign**: The handling of OCI skeletons will be reconsidered to natively integrate with the composability features of Zarf. This will eliminate the current ""bolt-on"" approach and ensure simpler, more predictable behavior when OCI skeletons are involved in package composition.

3. **Unified and Consistent Filtering**: Component filtering logic, including the `only` filter, will be centralized in a single location. This will prevent inconsistencies and bugs related to create-time filters by ensuring that all filtering logic is applied uniformly and tested comprehensively.

By addressing these challenges with a robust and holistic approach, this redesign will improve the reliability, scalability, and usability of Zarf package composition. The resulting implementation will include increased test coverage to ensure correctness for both deep import chains and edge cases.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision  
We will introduce a new mode for Zarf called ""Dev Mode"" to streamline the package development lifecycle by significantly reducing redundant operations during development. The goal is to optimize the iteration cycles for package creation and deployment while allowing developers to deploy changes efficiently without rebuilding the entire package.

### Key Features of ""Dev Mode"":
1. **Incremental Package Creation**:
   - Developers can skip re-fetching heavy dependencies (e.g., Docker images or large binaries) unless changes are detected, reducing the time spent on package creation in iterative development.
   - This will leverage caching mechanisms to reuse previously fetched dependencies.

2. **Direct Deploy without Recreating the Package**:
   - Enable direct deployment of package components without requiring a full tarball rebuild when making incremental changes to the `zarf.yaml`.
   - The deployment process will detect upgraded parts of the configuration and skip re-deploying unchanged layers/components.

3. **Cluster Reset Simplification**:
   - Implement a simplified way to reset or clean parts of the deployment (e.g., undeploy specific components or clean namespaces) without requiring a complete deletion of the cluster.
   - This aligns with reducing the need for re-initializing (`zarf init`).

4. **Command-line Exposure**:
   - Provide explicit CLI flags or commands for ""Dev Mode"" (e.g., `zarf dev build` and `zarf dev deploy`).
   - Avoid hiding this functionality behind internal or undocumented commands.

5. **Apply YOLO Mode Concepts Appropriately**:
   - Borrow sensible parts of YOLO mode, such as ignoring air-gap packaging requirements during dev loops, to prioritize fast feedback while retaining compatibility with standard Zarf deployments.

### Workflow with ""Dev Mode"":
1. Create the initial `zarf.yaml` and add components.
2. Build the package in ""Dev Mode"" with `zarf dev build`. This will use caching for dependencies.
3. Deploy the package with `zarf dev deploy <dir>` directly from the source directory without requiring tarball creation.
4. Debug deployment issues:
   - Update `zarf.yaml` as needed.
   - Re-run `zarf dev deploy <dir>` to reflect changes.
5. If cluster cleanup is needed, run `zarf clean` to reset components or namespaces, reducing the need for a full cluster reset.

By introducing ""Dev Mode,"" we aim to optimize the experience for developers iterating on Zarf packages while preserving core Zarf air-gap principles for production workflows.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision  
We will migrate the documentation site from Docusaurus 2 to Astro.  

Astro has shown strong performance characteristics, especially for static site generation, and has a modern architecture that emphasizes lightweight dependencies. By adopting Astro, we aim to:  
1. Improve site performance to deliver a faster, smoother experience for users.  
2. Simplify dependency management, reducing issues related to bloated or outdated dependencies.  
3. Maintain flexibility in supporting features like customization and responsive design.  

The migration will include:  
- Rebuilding the current site structure and bringing over all existing content.  
- Preserving features such as search, navigation, and any custom plugins.  
- Testing the migrated site to ensure parity with existing functionality in terms of usability and appearance.  

Docusaurus 2 documentation and configuration files will be archived for reference, but the project will fully transition to Astro upon completion of the migration. This decision aligns with the goal of having performant, maintainable, and future-proof documentation.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision  
We will refactor the existing logging solution in Zarf to achieve better separation of concerns, improve testability, ensure thread safety, and enhance proper context handling and cleanup processes. Specifically, the following changes will be implemented:

1. **Adopt a Structured Logging Framework**  
   Replace the in-house built logging solution with a more robust and widely adopted structured logging library. This will:
   - Provide more flexibility and standardization for logging functionality.
   - Allow for better management of log levels and outputs, reducing the need for workarounds like writing error messages to a debug log while printing messages to STDERR.
   - Ensure consistent formatting and improve readability across the codebase.

2. **Eliminate Global State for Progress Bars and Spinners**  
   Refactor progress bar and spinner implementations to be instance-based rather than relying on shared global state. This will:
   - Improve thread safety by avoiding race conditions and concurrency issues.
   - Allow for easier multi-threaded operations and predictable behavior.
   - Enable progress indicators to be toggled or disabled without global implications.

3. **Replace `message.Fatal` Usage with Context-Aware Error Handling**  
   Discontinue the use of the `message.Fatal` pattern, which abruptly exits the program. Instead:
   - Incorporate proper error propagation and context handling to ensure errors are managed without premature exits.
   - Use a structured error-handling approach, allowing for consistent program cleanup and the preservation of intended logic flows.
   - Make unit testing more effective and scalable by eliminating opaque and sudden process termination.

4. **Enhance Logging Abstraction for Separation of CLI-Specific and Package Logging**  
   Introduce an abstraction layer that separates CLI-specific user-facing logging from internal logging within packages. This will:
   - Prevent direct coupling between CLI UX concerns and internal package logic.
   - Enable changes to be made to user-facing output and internal logging independently.

5. **Maintain Compatibility with Existing Needs**  
   Ensure the new logging solution continues to support:
   - The current levels of logging (e.g., debug, info, error).
   - Features like writing debug logs to files and outputting messages to STDERR or STDOUT for user-facing feedback.

By adopting these changes, Zarf will achieve better modularity, improved thread safety, enhanced testability, and clearer error-handling patterns, leading to a more maintainable and robust codebase over time.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision  
Zarf will introduce an explicit schema version starting with v1. This schema version will serve as a stable contract between Zarf and its user base. The following decisions will govern the schema management moving forward:

1. **Schema Versioning**:  
   - A schema version will be embedded in the `zarf.yaml` file, starting with v1.  
   - Major versions will reflect potentially breaking changes, while backward-compatible changes will be handled within the same major version.

2. **Deprecated Features**:
   - All deprecated keys and features previously marked for removal will be fully dropped in v1.
   - Future deprecations in the v1 schema will remain functional but will trigger warnings to inform users of their planned removal in the next major release.

3. **Migration Mechanism**:
   - Zarf will continue to provide an automatic migration process for simple, subset or directly related schema changes, such as the migration of `setVariable` to `setVariables`.
   - For the lifetime of any schema version, the Zarf binary used in airgapped or delivery environments will be able to handle both current and deprecated keys from that schema version to ensure seamless backward compatibility.

4. **Schema Stability and Commitment**:
   - Once established, the v1 schema will remain stable throughout its lifecycle, allowing users to rely on long-term support until a v2 schema is introduced.
   - Any changes that break backward compatibility will only occur with a new major schema version (e.g., v2).

5. **Tooling for Transition**:  
   - Zarf will enhance tooling to facilitate schema migrations, providing automated or well-documented manual processes for users to transition their `zarf.yaml` files to the latest schema.

By introducing explicit schema versioning, Zarf will establish a clear and predictable path for users to follow as features are deprecated or updated, providing greater stability and ensuring smooth upgrades as Zarf evolves.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
