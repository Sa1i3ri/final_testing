context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision 
## Decision

* Implement native lazy loading for images in the Storefront by adding the `loading=""lazy""` attribute to img elements where appropriate.
* Utilize native browser support for lazy loading instead of relying on third-party libraries like ""lazysizes"" to reduce dependency on external code and improve performance.
* This will enable images to load only when they are near the viewport, decreasing initial page load time and improving perceived performance for users.
* For browsers that do not support native lazy loading, a fallback solution will not be implemented at this time, as modern browser support is sufficient and legacy support is deemed unnecessary for this use case.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision 
## Decision

To improve application performance and reduce unnecessary data transfer, the `autoload` parameter in the `OneToOneAssociationField` and `ManyToOneAssociationField` associations will be deprecated and eventually removed in a future version. 

Instead of relying on `autoload = true`, developers should explicitly load associations when needed using the DAL search criteria. This ensures only the required associations are loaded, avoiding unnecessary overhead during queries, hydration, and API payload processing.

By implementing this change:
- SQL queries will be more efficient as they won't include unnecessary joins.
- Application performance will improve due to reduced hydration and data processing workload.
- API payloads will be smaller and more efficient.

This change emphasizes better handling of associations by encouraging explicit usage, leading to cleaner, more performant applications. Documentation will be updated to guide developers on adapting their code and explicitly loading associations as needed.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision 
## Decision

We will implement a hybrid structure to address the different search approaches:

1. **Centralized Index File**  
   A central ADR index file will be created at the root directory for the project. This file will list all ADRs in chronological order (most recent first) with their respective titles and links to their location within the folder structure. This will address case 2 (reading the latest ADRs) and simplify navigation.

2. **Folder Structure by Areas**  
   The existing folder structure grouped by areas will remain in place. This structure will address case 3 (finding ADRs for a specific area) and provide focused access to ADRs.

3. **Search-friendly Metadata in ADR Files**  
   Each ADR file will include additional metadata fields at the top (e.g., tags, last updated timestamp, area, etc.). The metadata will allow users to search for ADRs easily using simple text search tools or future improvements, such as a search script.

This approach provides flexibility across all use cases without disrupting the existing workflow or creating unnecessary overhead. The `adr` tool may include optional subcommands or helpers in the future to facilitate access to this index or automate updates to it.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision 
## Decision
We will not pursue the new approach using native Vue components and the Composition API for our administration plugin extension system.

The main reasons for this decision are:

1. **Complexity of Data Merging**: The new approach faced significant challenges in merging component data because of Vue's compiler optimizations. These optimizations made passing and exposing all component data to the plugin developers challenging and unreliable.

2. **Reliance on Internal Vue Logic**: To fully realize the proposed system, we would need to use internal and potentially unsupported Vue logic to make data accessible in the desired way. This approach is not future-proof, as changes to Vue in future updates may easily break our implementation.

3. **Maintenance Risk**: Using internal Vue mechanics would create a high maintenance burden, making future updates to Vue much riskier and expensive to manage, potentially outweighing the benefits of the new system.

Given these drawbacks, we will continue using the current plugin extension system based on our Component Factory until a more suitable and feasible solution can be identified for extending Vue components. We acknowledge the current system's limitations but prioritize stability, update safety, and maintainability over the potential gains of the new approach. Additionally, we may continue exploring advancements in Vue or other alternatives to improve our extensibility model in the future.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision 
## Decision

We will adopt a new workflow for writing technical changelogs using individual changelog files for each change rather than a single file.

### Key points of the decision:
1. **Individual Changelog Files:**
   - Each pull request or change will include its own changelog file stored in a specific `changelogs` directory in the repository.
   - The file name should include a unique identifier (e.g., issue number, date, or meaningful descriptor) to avoid conflicts.

2. **Structure of Changelog Files:**
   - Files should follow a standardized format, e.g., category (`Added`, `Changed`, `Deprecated`, `Removed`, `Fixed`, or `Security`) along with a concise description of the change.

3. **Linking to Jira Issues:**
   - Developers will reference any relevant Jira ticket IDs or commit references in the changelog files to connect the development state with the issue state.

4. **Release Process:**
   - During the release process, individual changelog files will be aggregated into a release-specific changelog.

### Advantages:
- **Reduces Merge Conflicts:** Individual files eliminate the likelihood of conflicts in a single changelog file.
- **Improved Release Management:** Changes can be associated with the intended release version during aggregation, resolving version uncertainty beforehand.
- **Enhanced Traceability:** Clear links between changelog files, code changes, and Jira tasks ensure better tracking throughout the development lifecycle.
- **Maintains Flexibility:** The approach accommodates trunk-based development and branching workflows seamlessly.

### Next Steps:
- Define the format and directory structure for the individual changelog files.
- Communicate the new workflow to all developers along with examples and guidelines.
- Adjust the release process to include aggregation of individual changelog files.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision 
## Decision

To handle the challenges associated with the `DelayAction` in Flow Builder, the following decisions have been made:

1. **Persist Business Event Data at the Time of Execution:**
   - When a `DelayAction` is triggered, take a snapshot of the Business Event's data at that moment.
   - This snapshot will persist all necessary event data in a serialized form so that it can be restored later.

2. **Reconstruct Event Context Post-Delay:**
   - After the delay period is over, reconstruct the original event context using the persisted snapshot data, ensuring that the event uses the state it had when the action was initiated.

3. **Reload and Re-Evaluate Rules:**
   - Before executing the delayed action, reload the latest version of the rules to ensure they use the most up-to-date definitions.
   - Re-evaluate the rules against the reconstructed event context to validate their applicability at the time of execution.

4. **Handle Missing or Changed Rules Gracefully:**
   - If a rule no longer exists after the delay, log a warning and skip the execution of the action associated with that rule.
   - If a rule has changed, apply the latest version of the rule to maintain consistency with current logic.

5. **Design System for Flexibility and Extensibility:**
   - The serialization and deserialization of event data should be flexible to accommodate evolving event schemas without breaking existing functionality.
   - Logging mechanisms should track issues like missing rules, updated rule evaluations, or discrepancies between old and current data.

By implementing these strategies, the `DelayAction` will be reliable, ensuring actions are executed with the correct context while adapting to changes that might occur during the delay period.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision 
## Decision

The app system will be integrated directly into the Shopware platform instead of continuing as a standalone plugin. This integration will:

- Ensure consistency across cloud and on-premise environments for both partners and app manufacturers.  
- Signal to partners, developers, and manufacturers that the app system is stable and fully supported.
- Align the app system with the Shopware 6 release lifecycle, providing partners and users the benefits of structured upgrades and streamlined process management.
- Eliminate the need for users to install an additional plugin, simplifying the setup and improving the overall user experience.

The standalone app-system plugin will no longer be maintained or updated after integration. All future development and improvements for the app system will happen directly within the platform.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision 
## Decision

Refactor the `FlowStorer` implementation to use a unified and generic interface for storing scalar values. The new interface will reduce boilerplate code and simplify the process of adding new storer implementations or bypassing specific events.

### Key Changes:
1. **Unified Storer Interface**: Introduce a single `FlowStorer` interface across all storer implementations with methods for storing data and optional restore logic. This simplifies the architecture by reducing specialized, disparate interfaces.
   ```go
   type FlowStorer interface {
       Store(event Event, value interface{}) error
       Restore(event Event) (interface{}, error) // Optional for storers with restore requirements
   }
   ```

2. **Generic Functionality for Scalar Data**: Create a base implementation for handling common scalar storage needs. This base implementation can be extended or reused by all specific storer classes.
   ```go
   type ScalarFlowStorer struct {
       storage map[string]interface{}
   }

   func (s *ScalarFlowStorer) Store(event Event, value interface{}) error {
       key := event.GetUniqueID()
       s.storage[key] = value
       return nil
   }

   func (s *ScalarFlowStorer) Restore(event Event) (interface{}, error) { 
       key := event.GetUniqueID()
       value, exists := s.storage[key]
       if !exists {
           return nil, fmt.Errorf(""event data not found"")
       }
       return value, nil
   }
   ```

3. **Pluggable Extensions**: Allow plugins to easily extend or bypass events by using the standardized interface. This can be achieved by applying decorators or middleware wrappers over the unified `FlowStorer`.

4. **Minimal Event Storage Checks**: Replace custom checks in storers with one centralized logic for determining whether a specific event type should be stored. This avoids unnecessary duplication across individual storer implementations.
   ```go
   type EventFilterFunc func(event Event) bool

   type FilteredFlowStorer struct {
       Filter EventFilterFunc
       BaseStorer FlowStorer
   }

   func (fs *FilteredFlowStorer) Store(event Event, value interface{}) error {
       if fs.Filter(event) {
           return fs.BaseStorer.Store(event, value)
       }
       return nil
   }
   ```

### Benefits:
- **Streamlined Developer Experience**: Developers can focus on business logic instead of implementing redundant interfaces and boilerplate code.
- **Modular and Reusable**: New storers can readily adopt the standardized interface with minimal effort.
- **Extensibility**: Plugins and extensions can easily integrate into the storer lifecycle.
- **Reduced Code Complexity**: Centralizing scalar storage logic and event filtering eliminates duplication and enhances maintainability.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision 
## Decision

We will replace the current `sw-text-editor` with a new text editor built on [TipTap](https://tiptap.dev/). TipTap is a modern, highly customizable WYSIWYG editor based on ProseMirror. It has a good feature set, is actively maintained, and provides flexibility for extending its functionality, making it a suitable replacement that addresses the stability and maintainability issues of the current editor.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision 
## Decision

We have decided to set the default salutation to ""not_specified"" in Shopware 6 for cases when the salutation is not explicitly provided by the customer or administrator. 

This decision ensures inclusivity and accommodates customers who prefer not to disclose their salutation or for situations where it is not relevant. The implementation will:

1. Update the default salutation value within the system to ""not_specified"".
2. Ensure that ""not_specified"" is visible in relevant interfaces and APIs to highlight its presence as the default option.
3. Remain backward-compatible by not affecting user records that already have a salutation configured.
4. Allow administrators to override the default salutation if needed while ensuring the inclusivity principle remains intact.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision 
## Decision

* Remove the usage of `padaliyajay/php-autoprefixer` from the `theme:compile` process.
* Stop relying on automatic prefixing functionality in PHP for storefront CSS.
* Ensure that the browser compatibility list is respected by delegating vendor prefixing processes to appropriate tools within the SCSS pipeline.

### Implementation Plan:
1. Use the `.browserslist` configuration file already present in the project to define browser support consistently.
2. Recommend using a Node.js-based solution like `postcss` in combination with `autoprefixer` as part of the SCSS compilation process. This will be a more sustainable and efficient alternative since it dynamically respects the `.browserslist` configuration.
3. Advocate for local development setups and CI pipelines to ensure the correct tooling is in place with minimal runtime impact.
4. Document the newly expected SCSS compilation process, including integrating `postcss` with `autoprefixer`. Provide guidelines for setting up the development environment to incorporate Node.js tooling.

### Benefits:
- Avoid unnecessary CSS prefixes, reducing output size and improving maintainability.
- Drastically improve the performance of `theme:compile` by removing the heavyweight `padaliyajay/php-autoprefixer`.
- Use modern tools (`autoprefixer`) that dynamically support what is defined in `.browserslist`, ensuring compatibility with the officially supported browsers.
- Align with industry standards for CSS compilation and vendor prefixing.

This approach eliminates reliance on outdated and hard-coded PHP libraries, provides better maintainability, and aligns the front-end development workflow with modern practices around SCSS compilation.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision 
## Decision 

Avoid using pre-release versions (`1.9.0-alpha1`) for any packages that are critical and may introduce security vulnerabilities. 

Opt for regular release versions for such packages where npm's tools can appropriately monitor and report any insecurities. If pre-release versions must be used, implement additional monitoring mechanisms to catch critical vulnerabilities beyond npm's scope.

This ensures dependencies marked insecure are caught and addressed promptly, maintaining a secure system.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision
## Decision

To optimize the deletion of outdated cart entries, we will introduce a composite index to the `cart` table for the `created_at` and `updated_at` columns. The index will facilitate faster lookups for the conditions in the query, significantly improving query performance.

The following changes will be made:
1. Add a new composite index to the `cart` table:
   ```sql
   CREATE INDEX idx_cart_cleanup ON cart (created_at, updated_at);
   ```
2. Rewrite the SQL query to use the `OR` condition more efficiently while leveraging the index:
   ```sql
   DELETE FROM cart
   WHERE (created_at <= '2023-02-01' OR updated_at <= '2023-02-01') LIMIT 1000;
   ```
   This query structure ensures the database can leverage the composite index for both `created_at` and `updated_at` fields.

By adding the index and optimizing the query, we can ensure that the cleanup process runs more efficiently on high-traffic shops, reducing query execution times drastically. This solution will allow operations to scale better without introducing performance bottlenecks in the database.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision 
## Decision

Introduce an ""experimental"" flag or label for new features to enable early feedback and iterative development, while communicating clearly that these features are subject to change and do not fall under the backwards compatibility promise.

### Key details of the experimental feature approach:
1. **Labeling and Communication**:
   - Mark features explicitly as ""experimental"" both in the documentation and the UI/UX where applicable.
   - Clearly communicate to the end-users and developers that ""experimental"" features may have significant changes, be deprecated, or removed entirely.
   - Ensure users are opting into experimental features knowingly.

2. **Feedback Collection**:
   - Provide clear channels for users to share feedback and suggestions for experimental features.
   - Use this feedback to inform feature iterations or extensions and ensure alignment with real-world use cases.

3. **Scope of Experimental Features**:
   - Enable incremental delivery of features that can start with minimal viable functionality.
   - Examples include foundational increments like Multi Account handling or Advanced Search.

4. **Transition Process**:
   - Define criteria for experimental features to graduate into stable releases, such as sufficient user adoption, feedback incorporation, and technical stabilization.
   - Upon graduation, updates will fall under the backwards compatibility promise.

By introducing this experimental flag, we aim to reduce the time-to-market for new features, increase collaboration with users and ecosystem partners, and enhance overall feature development flexibility while balancing risks related to backwards compatibility.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision 
## Decision

- Introduce a new **Stock Handling Service** that encapsulates all stock-related logic. This service will serve as the central point for stock management, making it easier to implement custom stock handling logic or integrate with external systems such as ERPs.
  
- **Standardize Stock Representation**: Replace the dual representation of stock (stock and available stock) with a single stock-related property. A ""Reserved Stock"" column will be introduced to represent stock reserved for open orders, enabling a more straightforward calculation of available stock (`stock - reservedStock`). This will reduce complexity and improve performance for large catalogs.

- **Make Stock Handling Extensible**: Allow storefront developers and integrations to replace or extend stock loading and stock modification logic, so that custom workflows such as real-time ERP communication or special stock allocation algorithms become feasible.

- **Optimize Performance**: Adjust the stock calculation to query directly from dedicated stock tables rather than relying on inefficient joins and calculations for large catalogs, significantly improving the available stock calculation speed.

- **Simplify Order Lifecycle Stock Adjustments**: Introduce integration points into the order lifecycle to allow for easier customization of how stock is increased or decreased at specific points (e.g., when an order is placed, paid, cancelled, or completed). These hooks will allow for tailored stock handling logic in different projects.

- Deprecate direct stock manipulation throughout the core. All stock adjustments must go through the centralized Stock Handling Service to ensure data consistency and future-proof extensibility. 

This restructuring will enhance flexibility, reduce complexity, and prepare the platform for more sophisticated stock handling scenarios.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision 
## Decision

We will implement a configuration-based approach to manage the logging levels of specific exceptions, like the one used for the cloud product. Specifically:

- Exceptions such as `ShopwareHttpException` with HTTP-Status-Codes in the range of 40x, which arise from client-side errors (e.g., malformed API requests), will be logged with a `notice` level instead of `error`. This approach reduces noise in the logs and makes it easier to identify actual errors that require attention.
  
- A configurable list will determine which exception types or categories should have custom logging levels. This allows flexibility and adaptability over time as requirements evolve or new exception scenarios arise.

- Any other exceptions outside this configuration that are not caught will continue to be logged by the `symfony/monolog-bridge` on the `error` level, ensuring critical issues receive proper attention.

This approach will help streamline the logging process while maintaining the ability to analyze and act on real errors efficiently.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision 
## Decision

We propose to switch from using UUIDv4 to UUIDv7 as the primary keys in the database. UUIDv7 provides a time-based prefix, which improves the efficiency of B-tree indexes in the database. It allows for a more compact and optimized index layout by making inserts more sequential, thus reducing fragmentation, minimizing the need for new pages, and improving query performance.

This change ensures better performance without sacrificing the uniqueness guarantees provided by UUIDs.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision 
## Decision

Introduce a mechanism to allow apps to define and register custom events in the flow builder. This will enable developers to extend the list of available events based on the specific needs of their app.

**Implementation Steps:**

1. **Define Event Interfaces:** Create a common interface for events in the flow builder, ensuring all custom events adhere to a consistent structure. 

2. **Event Registration System:** Implement a central registry where apps can register their custom events. The registry should support:
   - Assigning unique identifiers to each event.
   - Storing metadata such as event names, descriptions, and required parameters.

3. **Integration with Flow Builder:** Update the flow builder to dynamically load events from the registry, allowing registered custom events to appear alongside predefined ones.

4. **UI Adaptations:** Ensure the flow builder UI is capable of displaying custom events properly, including their specific details and triggers.

5. **Validation Mechanism:** Introduce a validation layer to check that custom events comply with the expected structure before they are registered and exposed in the flow builder.

6. **Developer Documentation:** Provide clear documentation for app developers detailing how to define, register, and utilize custom events, as well as best practices for implementation.

By enabling the extensibility of events in the flow builder, apps can introduce tailored functionality while maintaining the integrity of the framework's core features.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision 
## Decision

To enable secure communication between the App Server and apps when apps perform direct requests to the App Server, we will implement token-based authentication. This approach allows the App Server to verify the origin and the identity of the app making the request, ensuring reliable customer verification and access to sensitive data.

### Key Points:
1. **Token Generation and Validation:**  
   - The Shopware Backend will issue secure tokens that include necessary information, such as the logged-in customer context and the secret for app communication.  
   - The token will be signed to ensure its authenticity and validity.

2. **Token Scope:**  
   - Tokens will include scopes to limit their access to specific domains or functionalities (e.g., customer data access, order updates).

3. **App Server Verification:**  
   - When receiving a request from the app, the App Server will validate the token to verify its origin and associated context. This ensures that the request is legitimate and originates from a trusted app instance. 

4. **Token Expiration and Refresh:**  
   - Tokens will have expiration times to reduce security risks from stolen or misused tokens. Apps can implement a refresh mechanism to request new tokens via the Shopware Backend when required.

By implementing tokenization, we provide a secure and efficient way for apps to communicate directly with the App Server while maintaining reliability and customer context verification.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision 
## Decision

We will implement a **Flow Builder Preview** feature to allow users to visually preview and understand the steps and decisions within a flow without executing or simulating the real flow. This feature will empower users to design reliable flows and better comprehend the logic theyve built. It addresses the need for clarity and transparency in flow execution, making debugging and configuration less time-consuming for users.

Key Implementation Points:
- The preview will provide a step-by-step representation of the flow structure, including conditional decisions and branching paths.
- It will not execute real operations within the flow (e.g., sending emails or triggering actions); it will focus purely on flow visualization.
- Interactive elements may be introduced to allow users to expand particular steps/details during the preview to better analyze the process.
- This preview will serve as an educational and debugging tool, helping merchants design flows with predictability and confidence.

By delivering a clear, user-friendly preview, merchants can save time and reduce frustration, enabling them to focus on building efficient processes rather than struggling with debugging or troubleshooting unclear flows.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much real code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision
## Decision

The generation of URLs for media entities and thumbnails will be refactored to decouple it from the `media.loaded` event subscriber and align closely with the `PathnameStrategyInterface`.

### Key Changes:
1. **Centralize URL Generation Logic**:
   - Move URL generation logic to implementations of the `UrlGeneratorInterface`. 
   - `UrlGeneratorInterface` will explicitly depend on `PathnameStrategyInterface` to ensure that consistent paths are generated for media and thumbnails.

2. **Responsibility Shift**:
   - Decouple URL generation from event listeners (`media.loaded`) and instead execute it either lazily when accessed or as part of the `MediaEntity`/`MediaService` lifecycle, depending on the use case.

3. **Explicit Handling of Cache Buster and Path Hash**:
   - Ensure `generatePathHash` and `generatePathCacheBuster` in the `PathnameStrategyInterface` are fully utilized to produce consistent and performant URLs.
   - Configuration for enabling or disabling hashes and cache busters should be made customizable based on deployment needs.

4. **Uniform Configuration**:
   - Offer a unified place to configure `PathnameStrategyInterface` and its dependencies within the service container.
   - Introduce validation to enforce consistent settings between path generation for storage and URL generation.

Advantages:
   - Decoupling: Removing the logic from event subscribers makes the system easier to test and reduces dependency on specific event sequences.
   - Consistency: Leveraging `PathnameStrategyInterface` ensures that the same logic governs path and URL generation.
   - Extensibility: Scoped, centralized logic makes it easier to introduce new pathname or URL strategies in the future.
   - Performance: Lazily generating URLs reduces unnecessary computation during the `media.loaded` event if the URL is not immediately required.

Disadvantages:
   - Refactoring existing implementations may take some effort, and some features will need to be validated for backward compatibility in existing implementations. However, this introduces a cleaner, more maintainable design.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision 
## Decision

We will introduce a dedicated mechanism for one-time data migrations to separate them from indexer logic. This new mechanism will ensure that data migrations are only executed once and do not interfere with regular indexer executions.

Key points of the decision:
- Develop a new `DataMigration` system that runs one-time data migrations and stores the migration state (e.g., completed or pending).
- Provide a unique identifier for each data migration to track its execution status.
- Integrate the `DataMigration` system into existing update processes for Shopware versions.
- Ensure that migrations are idempotent to prevent critical issues if a migration is accidentally re-executed.
- Add a CLI command to list, execute, or rollback specific data migrations for better control during updates.
  
This solution ensures data consistency and prevents system errors caused by repeated execution of data migrations tied to indexers. It also provides better maintainability and separation of concerns between indexing and migration logic.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision 
## Decision

Introduce a feature-specific tagging mechanism to associate all experimental code with its corresponding experimental feature. This tagging mechanism will ensure effective identification, tracking, and management of experimental features within the codebase. 

**Implementation approach:**
1. Enhance `@experimental` annotation:
   - Add a `featureId` (or similar) property to the `@experimental` annotation to uniquely associate experimental code with its designated experimental feature.
   - Example: `@experimental(featureId = ""feature-name"", stableVersion = ""1.5.0"")`

2. Ensure consistent feature identification:
   - All experimental code must include the `featureId` in its annotation for every associated feature. This provides immediate visibility into which feature the code belongs to and simplifies searchability.

3. Streamline feature lifecycle processes:
   - **Prolongation of Experiments**:
     - Easily locate all code associated with an experimental feature by searching for the `featureId`. Update the `stableVersion` property efficiently.
   - **Deprecation of Killed Features**:
     - Identify and deprecate all relevant code for an abandoned experimental feature by its `featureId`. This ensures adherence to the ""Killing Feature"" rule while minimizing the risk of missing components.

4. Tooling support:
   - Develop or integrate tools to automate:
     - Detection of all occurrences of a specific `featureId` in the codebase.
     - Generating reports of experimental features and their code coverage status.
     - Highlighting outdated or improperly annotated code for experimental features.
  
5. Documentation and best practices:
   - Update the 'Experimental features' documentation ([2023-05-10-experimental-features.md](./2023-05-10-experimental-features.md)) to include details on using the `featureId` tagging mechanism.
   - Educate developers regarding best practices to ensure consistent application of feature tags.

**Benefits:**
- Enables granular tracking and management of experimental features across the codebase.
- Simplifies tasks such as updating, prolonging, or deprecating features.
- Reduces the likelihood of errors or omissions when working with experimental code.
- Improves overall clarity and maintainability of the platform.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision 
## Decision
Introduce unique, developer-friendly, and human-readable identifiers (e.g., strings like ""credit_card"" or ""express_shipping"") for payment and shipping methods. These unique identifiers will serve as stable references for developers and app servers, eliminating the need to rely solely on the IDs returned by the Shopware Admin API. 

This approach ensures that app servers and extension developers can directly recognize payment and shipping methods without additional API requests and reduces complexity in app implementations.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision 
## Decision

* Replace custom SCSS rules for basic utilities (e.g., spacing, borders, colors) with Bootstrap utility classes wherever possible. Avoid introducing custom CSS rules for cases covered by Bootstrap utilities.
* Keep semantic and extendable custom class names in the HTML structure for developers to add specific styling if needed later.
* For simpler UI adjustments like spacing or borders, prioritize Bootstrap utilities to reduce the need for additional SCSS.
* Apply a hybrid approach for more complex layouts, using custom SCSS where Bootstrap utilities would result in overly complicated or less maintainable templates.
* Provide clear separation between utility usage for default styles and added custom classes for semantic and extensibility purposes.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision 
## Decision

We will transition to a Playwright-based E2E test suite to address the issues with the current Cypress setup and meet our defined goals for test automation.

## Key Actions:
1. **Switch to Playwright:**
   - Use Playwright for deterministic, fast, and reliable browser-based testing that supports multiple environments, including cloud.

2. **Define a Test Strategy:**
   - Establish a clear and uniform test strategy that focuses on product requirements and validates behavior.
   - Categorize the tests into critical workflows, edge cases, and regressions.

3. **Make Tests Environment-Agnostic:**
   - Ensure tests are written in a way that they dont rely on a specific test environment or predefined states, reducing flakiness.

4. **Readable and Easy Syntax:**
   - Utilize Playwright's modern syntax to make tests declarative and readable, enabling collaboration with non-technical stakeholders.

5. **Test Performance Optimization:**
   - Leverage Playwright's parallel testing capabilities and fast execution to improve test performance, ensuring quick feedback loops.

6. **Comprehensive Documentation and Training:**
   - Create guides and training sessions to onboard engineers and non-technical QA members to the new framework.

This transition will enable a much-needed shift toward a scalable, manageable, and reliable E2E testing approach aligned with our automation goals.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision 
## Decision

To address the transactional issue caused by handling the database interactions solely within flow actions, we propose the following changes:

1. **Transaction Context Enforcement for Flow Actions**:
   Flow actions should not independently handle transactions. Instead, they should rely on the transaction context provided by the outer calling process. This ensures that nested transactions do not corrupt the transaction state, avoiding scenarios where the connection is marked as rollback-only due to nested commit failures. Flow actions should primarily execute their operations within the scope of the outer transaction.

2. **Error Handling for Transaction Failures**:
   Modify the flow dispatcher to properly surface and handle errors when a transaction fails to commit. A clear and actionable error message should be logged instead of being vaguely ignored. Additionally, if a transaction-related failure occurs during the execution of a flow action, the dispatcher should halt further flow execution and ensure proper rollback handling to prevent cascading failures.

3. **Configure Savepoint Usage**:
   To mitigate nested transaction issues further, configure the database connection to enable savepoint usage as a default when interacting with flows. This enables precise rollback management at the appropriate levels without affecting the outer transaction. If savepoints are disabled by default in Shopware, this configuration change should be considered mandatory when flows are expected to interact transactionally with the database.

4. **Documentation and Guidelines**:
   Provide clear documentation and guidelines for developers to understand how flow actions interact with transactions. This includes cautioning against starting transactions inside flow actions and emphasizing adherence to the provided transaction context from the calling code.

With these decisions implemented, transactional integrity will be preserved, error handling will be improved, and the reliability of flows that interact with the database will be enhanced.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision 
## Decision

We will keep the Vue 3 compatibility mode enabled during the current major version. This decision aims to ease the transition for plugin developers by reducing the impact of breaking changes in a single major release. The compatibility mode allows the administration to use Vue 3 while still supporting most Vue 2 features, creating a smoother upgrade path for plugin migrations.

However, we plan to disable the compatibility mode in the following major release to fully transition to Vue 3. This ensures future development is aligned with Vue 3 standards and best practices. During this major, we will begin phasing out deprecated Vue 2 features and preparing the administration to run without the compatibility mode. Plugin developers are encouraged to migrate their code to Vue 3 compatibility as early as possible to avoid potential disruptions once the mode is disabled.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision 
## Decision 
We decided to remain on Vuex `4.1.0` despite its bug that destroys getter reactivity under specific circumstances. The reasoning is that downgrading to `4.0.2` introduces other unfixable issues that compromise critical functionality, causing modules to fail entirely. In addition, the getter reactivity bug in `4.1.0` can be mitigated with workarounds in the implementation, making it the lesser of two evils compared to the severity of errors in `4.0.2`.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision  
## Decision  

Adopt the **Pipeline Pattern** as a new architectural structure for handling extensions and subprocesses.  

### Rationale:  
The Pipeline Pattern addresses the challenges identified in our current architecture:  
1. **Improved Compatibility Management:**  
   - By structuring process logic into stages within a pipeline, it becomes easier to introduce changes incrementally.  
   - Each pipeline stage is independent, so changes to one stage are less likely to have ripple effects on others.  

2. **Simplified Process Extension:**  
   - The pipeline model allows subprocesses to naturally slot into clearly defined stages.  
   - Third-party developers can easily extend functionality by adding or modifying specific pipeline stages, reducing the need for complex decoration or class hierarchies.  

3. **Reduced Code Proliferation:**  
   - Subprocess logic is consolidated into pipeline stages, reducing the need for a large number of interfaces and abstract classes.  
   - This makes the codebase more clean, readable, and maintainable.  

### How we'll implement the transition:  
1. **Introduce iteratively:**  
   Start by adopting the Pipeline Pattern for new features or refactoring isolated sections. Gradually transition other modular areas of the system.  

2. **Stage-based design:**  
   Define clear contracts and responsibilities for each pipeline stage, with minimal coupling between stages.  

3. **Backward compatibility:**  
   - Maintain the current approach for existing features, and provide adapters as necessary during the transition period.  
   - Actively engage with third-party developers to ensure smooth adoption of the new architecture for their extensions.  

By moving toward the Pipeline Pattern, we aim to create a more flexible, maintainable, and developer-friendly architecture while keeping the codebase lean and manageable.  ","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision 
## Decision

We will create a new Jest runner dedicated to testing components with compat mode disabled. This runner will ensure that all component tests are executed without compat mode enabled, allowing us to identify and address any issues caused by the removal of compat mode while maintaining isolated testing scenarios.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision 
## Decision

To improve the detection of errors and ensure system robustness, we will implement detailed health checks for the software system. These health checks will monitor the health of individual components in addition to the overall system health. 

The health check implementation will include the following:

1. **Component-Level Health Checks:** Each individual component will have its own health check to ensure its functionality is working as expected. This will allow us to pinpoint issues within specific components rather than relying solely on overall system health.

2. **Aggregated System Health Check:** The system's overall health will be calculated based on the aggregated statuses of all components. This aggregated status will take into account the importance of each component and its impact on overall system availability.

3. **Pre-Rollout Health Validation:** Before deploying the system live, health check validations will be performed to ensure that all critical components and functionalities are working correctly.

4. **Real-Time Monitoring in Production:** Health checks will run continuously in production to detect issues early. If a critical component fails, alerts will be triggered for immediate action.

5. **Communication of Results:** Health check results will be documented and shared with relevant stakeholders to assist with debugging, support, and maintenance.

By instituting thorough system health checks, potential errors can be detected earlier during development, testing, or deployment phases, reducing the likelihood of encountering undetected issues post-rollout.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision 
## Decision

To maintain a consistent and streamlined testing approach, we decide to limit the major feature flag enablement in the unit test suite solely to the `Shopware\Tests\Unit` namespace. This restriction ensures clear separation of concerns and avoids unintended side effects when running unit tests.

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` behavior will remain as-is, as it aligns with this decision and simplifies test configuration by keeping feature-flag-related changes isolated within the `Unit` namespace.

If in future, other namespaces are required to support major feature flags, we will reevaluate and extend the functionality with proper consideration and thorough discussion.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision 
## Decision

We will adopt an automated tool or framework to assist in the migration of our Vue.js components from the Options API to the Composition API. Specifically, we will integrate and use the official Vue Migration Build, along with tools like [vue-codemod](https://github.com/vuejs/vue-codemod) and ESLint plugins where necessary to accelerate the migration process and reduce the risk of errors.

### Plan:
1. **Use Vue Migration Build**: This tool will help us identify deprecated features and ensure compatibility during the migration phase, enabling a gradual transition from Vue 2 to Vue 3.
2. **Leverage vue-codemod**: Automate the conversion of components from the Options API to the Composition API where applicable. It will provide a baseline conversion that can be refined manually as needed.
3. **Add ESLint rules**: Introduce ESLint plugins such as `eslint-plugin-vue` with migration-specific rules. These will assist in flagging issues and enforcing Composition API best practices.
4. **Gradual Conversion**: Convert components in phases. Priority will be given to shared and reusable components to maximize the immediate benefits of the Composition API.

### Rationale:
- Manual migration would be highly time-consuming and prone to mistakes, especially given our codebase size.
- Combining automated tools with manual oversight ensures a smooth and consistent transition while minimizing technical debt.
- Transitioning to the Composition API will modernize our application and bring improved TypeScript support, clearer logic organization, and better maintainability.

### Tradeoffs:
- Minor adjustments or rewriting may still be required post-automation, particularly for highly complex or unconventional components.
- Incorporating tools like vue-codemod requires set-up time and learning curve for the team but will save significant time on migration in the long run.

By utilizing these tools and following a structured migration plan, we can modernize our codebase efficiently and prepare it for future scalability and Vue 3 best practices.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJss inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in Vues templating and reactivity model.","## Decision 
## Decision
We will introduce `sw-block` and `sw-block-parent` components as a replacement for the existing TwigJs block system to fully leverage Vue.js's modern frontend capabilities and ensure a more dynamic and reactive architecture.

### Key Features and Benefits:
1. **Dynamic Overrides with Vue:** 
   - The `sw-block` component serves as the placeholder for content that can be overridden or extended.
   - The `sw-block-parent` acts as the registration context, providing the capability to dynamically inject or override content.

2. **Improved Reusability:** 
   - These components provide a consistent and component-based solution to ensure templates are reusable and maintainable while eliminating the constraints caused by TwigJs.

3. **Enhanced Reactivity:**
   - Leveraging Vue's reactive nature, the system will now support efficient updates to dynamic content, making it more performant for modern use cases.

4. **Backward Compatibility Until Migration Complete:**
   - During the migration phase, an additional compatibility layer will be provided to ensure existing TwigJs blocks function correctly. This avoids breaking changes in plugins or modules until they adapt to the `sw-block` model.

### Example Implementation:

```vue
<template>
  <sw-block>
    <template #default>
      <div>Default content</div>
    </template>
    <template #override>
      <div>Custom plugin content</div>
    </template>
  </sw-block>
</template>
```

### Migration Plan:
1. Migrate critical areas of the application incrementally from TwigJs blocks to `sw-block` and `sw-block-parent`.
2. Provide clear documentation and examples for developers to transition their plugins/extensions to the new system efficiently.
3. Deprecate the TwigJs block system gradually and eventually remove it in a major release to ensure a clean transition.

The introduction of `sw-block` and `sw-block-parent` components signifies a shift towards a more Vue-centric architecture, aligning with modern frontend development practices while maintaining the flexibility needed for extension and customization.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the components internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: ` 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision 
## Decision

We decided to remove the asterisk `*` next to all product prices in the default Storefront. Instead, a clearer and more user-friendly approach will be implemented to communicate the tax and shipping information to customers.

### Details:
1. **Footer Information Update**: The footer will retain the tax and shipping cost information but will no longer reference the asterisk. It will clearly state that all prices include VAT and any extra costs.
   
2. **Category- and Product-Specific Notices**: If products or product categories require specific pricing details (e.g., custom delivery charges), a tailored notice will be added in a relevant location, such as on the product detail page or during checkout.

3. **Improved Clarity**: By removing the asterisk, the price representation will become cleaner without overloading the user interface with symbols, improving readability and user experience.

This approach balances legal compliance and visual clarity while making the information accessible in a user-friendly and modern manner.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision 
## Decision

Replace DomPDF with an accessibility-compliant PDF generation library to ensure that Shopware 6's document solution meets WCAG standards. The selected library must support the creation of accessible PDFs, including features like proper tagging, reading order, alternative text for images, and other A11y-compliant attributes. This change will enable compliance with legal requirements and improve usability for customers with disabilities.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision 
## Decision

To improve maintainability and simplify the evolution of the rule system, we will restrict the extension of core rule classes. Existing rule classes will be marked as `final`. This measure ensures a clear and predictable behavior of the rule system while facilitating future improvements and refactoring.

For extensibility needs, rule-specific events and interfaces will be introduced, allowing developers to implement custom behavior without modifying the core. Additionally, the use of app-specific rules (e.g., via the generic script rule) will be encouraged as the preferred method for customization, further reducing direct dependencies on core rule classes. 

To support backward compatibility, we will provide deprecation notices for developers currently extending core rules and include detailed migration paths and documentation for transitioning to the new approach.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision 
## Decision

The helper `src/helper/iterator.helper.js` will be deprecated and entirely removed in the next major version of the Storefront JavaScript.

* All occurrences of `Iterator.iterate` will be replaced with native JavaScript loops like `forEach` or `for...of` depending on the iteration context.
* Developers will be encouraged to use native JavaScript loops for clarity, simplicity, and maintainability.
* No custom abstractions will be necessary for working with already standardized objects like `NodeLists`, `Maps`, `FormData`, or arrays.
* This will improve code consistency and readability across the codebase.
* By adhering to standard JavaScript solutions, we simplify onboarding and reduce the need for additional documentation, as developers are already familiar with these constructs. 
* Files that currently import the helper will be updated to remove unnecessary dependencies.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision 
## Decision

Flows will be executed **asynchronously** after the completion of the business process. This ensures that errors during flow execution do not disrupt or cancel the core execution of the business process. By decoupling flow execution from the main process, we can also significantly improve the performance and reliability of the business process.

### Implementation Details:

1. **Asynchronous Execution**  
   - Flows will no longer be executed directly within the context of the business process.
   - Instead, a dedicated event listener will handle the execution of flows by dispatching flow-related jobs to a message queue. This allows flows to run in the background without impacting the response time or reliability of the main process.

2. **Error Handling**  
   - Flow execution failures will be isolated from the main business process. Any errors that occur during flow execution will be logged and retried if appropriate, without interfering with the ongoing user-triggered process.

3. **Performance Improvements**  
   - Flows such as sending emails, making HTTP requests, or other resource-intensive tasks will no longer block the main business process, greatly improving user experience and transaction speeds.

4. **Simplified Debugging**  
   - Flows will be executed by a dedicated event listener, decoupling their execution from Symfony's `EventDispatcher` and resulting in cleaner stack traces. This simplification will make debugging and tracing flow-related issues easier.

5. **Feature Flag**  
   - These changes will initially only take effect if the `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` feature flag is enabled. This allows for gradual adoption and safe rollouts, minimizing potential regressions or disruptions in the production environment.
   
6. **Extensibility**  
   - Moving flows to their own execution environment opens the door for expanded capabilities. For instance, flows can leverage advanced scheduling, retry mechanisms, or monitoring features provided by the messaging/queueing system in the future.

### Advantages:
- Prevents disruption of business processes due to flow-related errors.
- Improves response time and overall performance of user-triggered events.
- Simplifies debugging and tracing of flow executions.
- Paves the way for future enhancements in flow handling.

### Trade-offs:
- Introduces a dependency on a queueing system for handling asynchronous operations. Proper monitoring will be required to ensure flows execute reliably in the background.
- Adds slight operational complexity, as administrators/developers must monitor both the business process and the flow execution systems independently.

This architecture ensures a cleaner separation between core business logic and flow handling, increasing reliability, maintainability, and performance.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision 
## Decision Outcome

Chosen option: ""MADR 4.0.0"", because

- MADR provides a clear, consistent, and structured format for documenting architectural decisions.
- It is lightweight and fits well into agile and iterative development practices.
- The format encourages recording the context, alternatives, and the reasoning behind decisions, which improves long-term clarity and understanding.
- MADR is supported by a community and is actively maintained.
- The structure is simple and easy to adopt by the team, making it an excellent choice for our project needs.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision 
## Decision

A new repository API will be designed and implemented that offers a coherent and maintainable approach to managing TUF repositories. Key principles guiding the new implementation include:

1. **Separation of Concerns**:
   - The repository API will separate ""library-like"" functionality from ""application-like"" decisions, ensuring flexible usage across different scenarios.
   
2. **Simplified and Coherent Design**:
   - APIs will be streamlined to reduce unnecessary complexity, providing intuitive methods with fewer arguments and avoiding reliance on global state.

3. **Context-Specific Operations**:
   - Decisions regarding operations like metadata loading will be customizable and context-specific, ensuring the API works well for both small repositories and large-scale distributed systems.

4. **Maintainability**:
   - The implementation will avoid bloated codebases by modularizing functionality and focusing on maintainability, reducing the large overhead seen in legacy implementations.

5. **Modern Python Practices**:
   - The new API will leverage modern Python constructs and practices to deliver a clean, Pythonic integration, aligning with the style and modern approaches supported by the Metadata API.

This decision ensures that developers have a better, more robust tool in the repository API without being bogged down by legacy design issues or unnecessary complexity.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision 
## Decision

We will migrate the Primer React components library to TypeScript. This involves rewriting the components and their implementation from JavaScript to TypeScript, integrating type definitions directly into the implementation code. This approach will ensure the type definitions remain up-to-date and accurate, reducing the maintenance burden and minimizing TypeScript-related bugs. 

The `index.d.ts` file will be deprecated and removed as part of this migration process.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision  
## Decision  

We will standardize using `Box` for creating React components in Primer React, instead of `styled-components`.  

### Rationale:  
1. **Consistency across Primer React**: Using `Box` simplifies component creation by aligning with our design system. It also uses the system props directly, unifying the styling approach across components.  
2. **Better developer and user experience**: `Box` integrates well with SX props and other utilities while allowing consumers to extend styles more efficiently with consistency and flexibility.  
3. **Reduced styled-components dependency overhead**: Moving away from `styled-components` decreases reliance on third-party libraries, reducing complexity and potential compatibility concerns.  
4. **Improves performance**: `styled-components` introduces runtime overhead due to its dynamic CSS injection. `Box`, on the other hand, leverages a CSS-in-JS approach, which is lighter and faster.  
5. **Easier debugging**: With system props and `Box`, styles are easier to debug, and the code is more readable for consumers by adhering strictly to the design system principles.

By transitioning to the `Box` pattern, we align tighter integration with Primer's design system, improving portability and code maintainability.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision 
## Decision

Continue using the `Drafts` namespace for components that are either experimental or have multiple versions being developed concurrently. 

This approach enables clear separation between stable components and components under iteration, ensuring developers can easily identify which components are production-ready and which require collaboration or are subject to change.

This decision reinforces the practice introduced in ADR-006 and ensures consistency in development and usage patterns.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision 
## Decision

We will adopt snapshot testing for React components judiciously, ensuring adherence to best practices:

1. **Use snapshots selectively**: Snapshots will only be applied to smaller or isolated components with simple, well-defined outputs. Avoid snapshotting large or deeply nested components, as this creates bloated, difficult-to-debug snapshots.

2. **Pair snapshots with explicit unit tests**: Snapshot tests alone often do not communicate intent clearly. Whenever snapshot testing is used, complementary unit tests will also be written to verify behaviors or dynamic aspects of the component. This will ensure clarity around what is being tested.

3. **Maintain snapshot hygiene**: Regularly review and update snapshots. Developers must carefully assess whether changes to snapshots are intentional and appropriate during review cycles.

4. **Document test intent**: Add comments or descriptions to test cases to clarify what the snapshot captures and why it exists. This will make the test purpose more transparent during debugging or review.

5. **Avoid reliance on snapshots for dynamic content**: Do not use snapshots for components whose output changes frequently or unpredictably, such as those driven by external APIs or runtime data.

By following this approach, we aim to balance the utility of snapshot testing with the need for maintainable, meaningful tests.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision 
## Decision

A utility function, `deprecationWarning`, will be implemented to standardize how warnings and deprecation messages are provided to developers. This function will wrap `console.warn()` but ensure that it is only executed in a development environment.

### Implementation Details:
- The `deprecationWarning` utility will check if the environment is development (e.g., through `process.env.NODE_ENV` or equivalent). If the environment is not development, the warning will not be logged.
- All instances of `console.warn()` related to deprecations, unsupported features, or messages specific to development will be replaced with calls to `deprecationWarning`.
- During the build process, tools like Terser or similar will be used to remove development-specific code (including calls to `deprecationWarning`) from the production bundle.

### Outcome:
- Warnings will only appear in development, improving the developer experience without cluttering production logs.
- This approach will ensure that no unnecessary code related to development checks is included in production builds, reducing bundle size and potential performance impact.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision 
## Decision

Introduce a **centralized utility for live region announcements** in Primer React to ensure consistency and reliability across all components. This utility will handle the announcement of dynamic messages and regions, providing a standard, predictable user experience for screen reader users. The new utility will be designed to work reliably across different screen readers and browsers, addressing the common pitfalls of live region implementations.

### Key Implementation Details:

1. **Centralized Announcement Function**:
   - A single function (e.g., `announce(message: string, options?: AnnounceOptions)`) will expose live region announcements programmatically.
   - Options will allow customization, such as:
     - `politeness`: Defines the `aria-live` priority, e.g., `""polite""` (default) or `""assertive""`.
     - `role`: Optional ARIA live region roles such as `alert`, `status`, or `log`.

2. **Dedicated Hidden Live Region**:
   - A hidden `div` element will be appended to the DOM as the dedicated, centralized live region for announcements.
   - This `div` will have an `aria-live` attribute that updates dynamically based on the announcement type (e.g., `polite` or `assertive`).
   - The `div` will include attributes like `aria-atomic=""true""` to ensure atomic updates are announced correctly.

3. **Coverage for All Scenarios**:
   - **Initial Content Announcements**: Components like spinners or success messages can use the utility when rendered or updated.
   - **Content Changes**: Changes to dynamic elements such as counters can trigger announcements.
   - **Programmatic Announcements**: Messages like search results can be announced programmatically.

4. **Integration in Components**:
   - Components in Primer React that use live regions will leverage the centralized utility instead of custom implementations.
   - Examples include updating spinners, alerts, or dynamically loaded counters.

5. **Testing Across Screen Readers**:
   - The utility will be tested across major screen readers (e.g., NVDA, JAWS, and VoiceOver) and browsers to ensure compatibility and reliability.

6. **Fallbacks for Dynamic Issues**:
   - Mitigate issues caused by dynamically injecting/re-rendering content by removing and re-adding the live text to force announcements.
   - Handle visibility toggles gracefully to avoid skipped announcements.

---

### Benefits of the Decision:

- **Consistency**: Ensures all live region implementations in Primer React follow a predictable, unified pattern.
- **Reliability**: Addresses common pitfalls with dynamic and programmatic live region announcements, improving screen reader behavior.
- **Ease of Use**: Simplifies the burden on contributors by providing a flexible and reusable utility for all live region scenarios.
- **Accessibility**: Enhances the overall accessibility of Primer React and its components, creating better user experiences for all users.

--- 

### Follow-Up Actions:

1. **Develop the Live Region Utility**:
   - Implement the `announce` function and the centralized live region.
   - Add options for flexibility, such as specifying `politeness` and `role`.

2. **Document the Approach**:
   - Create detailed documentation for contributors, including when and how to use the new utility.

3. **Retrofit Existing Components**:
   - Update existing components that rely on live regions to use the new utility.
   - Identify components with inconsistent behavior and align them with the new strategy.

4. **Testing and Validation**:
   - Conduct testing across screen readers and browsers to validate announcements.
   - Collect feedback from users and contributors to refine the utility as needed.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"## Decision  
No decision can be provided without additional context. Please provide more information or details about the scenario or issue.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision 
## Decision

1. Adopt [Storybook testing-library](https://storybook.js.org/docs/react/writing-tests/interaction-testing) to automate interactive test scenarios directly in Storybook. This will help reduce the repeated markup and effort of constructing test cases separately in Storybook and Jest, ensuring that stories and tests stay in sync.

2. Leverage Storybook as a visual test debugging tool by using Storybook's addon for Jest ([@storybook/addon-jest](https://storybook.js.org/addons/@storybook/addon-jest)) to display Jest test results in the Storybook UI. This will provide a more visual and intuitive way to debug and understand test failures while improving accessibility for others who want to enhance tests.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision 
## Decision

Adopt and integrate `npm workspaces` into the `primer/react` repository to streamline dependency management and inter-workspace relationships. 

Key considerations:
- All workspaces within the repository will be managed from a single top-level `package.json` via `npm workspaces`.
- Dependencies across workspaces will be simplified and managed natively with `npm workspaces`, allowing for linking without the need for symlinks or manual `npm link` commands.
- Running `npm install` at the root will install dependencies for all workspaces, removing the need to visit each directory or maintain a custom script like `npm run setup`.
- The `script` directory and any related development utilities will also be refactored into a dedicated workspace for better dependency management and maintainability.
  
This approach centralizes dependency management, reduces complexity, and aligns with modern `npm` capabilities for monorepos.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision 
## Decision 

To ensure that only intended parts of the codebase are exposed as part of the public API, we will move files that are not meant to be part of the public API into a new directory called `internal` under the `src` directory. Files and folders within the `internal` directory will not be included in the exports specified in the package's `package.json`.

This approach will keep the public API clean and ensure that internal implementation details remain private to the package, preventing unintended usage by consumers.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision 
## Decision

* Transition away from runtime-injected CSS and adopt static `.css` files instead to address performance issues and improve rendering speeds, both in client-side and server-side scenarios. 
* Begin utilizing CSS variables to handle theming more effectively, enabling dynamic theme management without the need for redundant style recalculation or re-rendering.
* Update styling architecture to rely on static CSS files for better parallelization during page loads, which reduces initial render time and application benchmarks significantly.
* Ensure that Primer primitives and components, wherever applicable, are updated to enforce the new architecture and improve consumption guardrails, making it easier for developers to adopt and follow best practices.
* Phase out the uncontrolled use of properties like `sx` for styling in dynamic runtime CSS and replace them with classes defined in static `.css` files. This will drastically improve performance during style updates and state changes without compromising flexibility. 
* Implement a migration plan for existing applications to shift to the new styling approach without breakage, ensuring backward compatibility during the transition phase.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision 
## Decision

We decided to no longer use Storybook's `play` functions for interaction tests. Instead, the recommended approach is to use Playwright for any tests that require user interactions. This decision ensures consistency in our testing stack and leverages Playwright's robust capabilities for handling user interaction tests.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision 
## Decision  

We will adopt an approach wherein responsive values for props are resolved on the server whenever server-side rendering (SSR) is happening. To enable this, components using responsive props will accept both a static value and a responsive object for their props but will treat the `regular` breakpoint as the default fallback for server-side rendering.  

### Implementation  

1. **API**:
   Components will support passing both static values and responsive objects as props, e.g.,  
   ```tsx
   <Stack gap=""normal"" /> // Static  
   <Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }} /> // Responsive  
   ```

2. **Server-Side Behavior**:  
   During SSR, the value for the `regular` breakpoint will be resolved as the fallback value, ensuring layout stability even before hydration. 

3. **Hydration Behavior**:  
   Once hydration occurs, the client will resolve and potentially update the prop value to account for the true viewport size using the `useResponsiveValue` hook, but this will not affect the initial layout of the component rendered on the server.

4. **Best Practices with Documentation**:
   - Encourage authors to always define a `regular` breakpoint value in responsive objects for consistent behavior.  
   - Provide documentation and examples for responsive props, clearly explaining server-side and client-side behavior.

5. **Testing**:
   - Add integration tests for SSR and hydration to ensure there are no layout shifts when using responsive props.  

This approach minimizes layout shifts and offers a flexible yet stable API for handling responsive props, while still allowing graceful degradation during server-side rendering.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision
## Decision

We will implement a deprecation strategy for props that ensures a smooth transition path for users to migrate to the new API. 

### Details:
1. **Deprecation Warnings**:
   - Introduce runtime warnings in development mode when the deprecated prop is being used.
   - The warning will provide clear instructions on how to migrate to the new API, including a before-and-after code example.

2. **Backward Compatibility**:
   - Support the deprecated prop alongside the new API for at least one major version cycle to give users sufficient time to migrate.
   - Ensure that the old and new APIs work seamlessly alongside each other during this transition period.

3. **Documentation Update**:
   - Update the documentation to clearly mark the deprecated prop.
   - Provide detailed migration guides and examples showing how to move from the old API to the new one.

4. **Codemod Utility**:
   - Provide an optional codemod script to help users automatically migrate their code from the deprecated prop to the new API.

5. **Final Removal**:
   - Remove the deprecated prop and its associated runtime warning in the subsequent major version following the deprecation announcement.

### Example Implementation:
#### During the Deprecation Period:
```tsx
const ActionListGroup = ({ title, children }: ActionListGroupProps) => {
  if (process.env.NODE_ENV === 'development' && title) {
    console.warn(
      'The `title` prop in `ActionList.Group` is deprecated. Replace it with `<ActionList.GroupHeading>`. For example: \n\n' +
      '<ActionList.Group>\n' +
      '  <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>\n' +
      '  {children}\n' +
      '</ActionList.Group>\n'
    );
  }

  return (
    <div>
      {title && (
        <ActionList.GroupHeading as=""h2"">{title}</ActionList.GroupHeading>
      )}
      {children}
    </div>
  );
};
```

#### After Deprecation Period (Next Major Version):
- The `title` prop will be fully removed, and usage will result in an error if a user attempts to use it.

By adhering to this strategy, we aim to provide a clear, accessible, and structured pathway for migration, ensuring minimal disruption for developers.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision 
## Decision

We will implement proxy support for self-hosted runners by introducing the use of environment variables `http_proxy`, `https_proxy`, and `no_proxy`. 

- The system will adhere to the lowercase forms of these environment variables for consistency, as they are the more widely-accepted standard.
- The self-hosted runner will automatically detect and respect these variables when making HTTP or HTTPS requests, ensuring compatibility with enterprise proxy infrastructure.
- This decision allows flexibility, aligning with industry standards and minimizing setup steps for users already accustomed to these conventions.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision 
## Decision

We will change the default behavior to have command echoing disabled unless explicitly enabled by the user. Users can turn echoing on or off using the new `::echo::on` and `::echo::off` commands.

When errors occur, command outputs will automatically be echoed regardless of the current echoing state to provide better context for troubleshooting.

If the `ACTIONS_STEP_DEBUG` secret is enabled, echoing will default to being enabled to aid in debugging workflows. Otherwise, echoing will remain disabled by default to reduce log clutter.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision 
## Decision

Enhance the runner to include an optional mechanism for injecting additional contextual information in the `Set up job` step's log. 

This will allow important metadata such as OS version or software details from the Hosted image to be displayed. The mechanism should adhere to the following constraints:
- The feature should be entirely optional and configurable for use cases where additional logging is needed.
- The implementation will extract and display the OS/Software metadata from Hosted image configuration without impacting the overall job execution performance.
- Minimal interference with existing logging structure while ensuring readability, scalability for adding new metadata in the future.

This approach provides teams and users with greater transparency of foundational setup details for debugging or ensuring alignment with expected runner configurations.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision  
## Decision Outcome

Chosen option: Add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context, because:

- It provides increased flexibility for downstream steps by explicitly indicating success or failure of prior steps.
- Enables conditional logic without relying solely on `outputs`, which might not always represent step integrity or status.
- Simplifies development and promotes clear workflows by directly incorporating actionable statuses for steps.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision 
## Decision Outcome

Labels provided via a new command-line argument (`--labels`) during registration will be added to the set of labels registered with the GitHub Actions service.

### Rationale

- Automating the configuration of self-hosted runners requires easy customization without manual intervention.
- Extending label assignment via command-line arguments supports scripting workflows and simplifies setup processes.
- It maintains compatibility while enriching existing functionality for built-in labels (`os`, `arch`) already handled during registration. 

### Consequences

- Self-hosted runners can now dynamically register custom labels during their configuration/registration step, improving automation capabilities.
- The `--labels` flag allows packagers and users to streamline custom configurations without additional manual label assignment logic.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision  
## Decision  
To ensure robustness and reliability, we will implement a retry mechanism with exponential backoff when communicating with third-party authentication providers like Google OAuth2. This mechanism will:

1. **Handle Transient Faults:** Automatically retry the request to the provider in the event of expected transient faults, such as rate limits or temporary outages.

2. **Exponential Backoff Strategy:** Use an exponential backoff strategy with randomized jitter to prevent overloading the provider and mitigate thundering herd issues.

3. **Retry Policy Limits:** Apply a retry policy with a maximum number of retries and a reasonable timeout to prevent indefinite waiting and enhance user experience.

4. **Fallback Mechanism:** In cases where retries fail, provide a fallback mechanism (e.g., displaying an error message to the user or suggesting an alternative action such as retrying later).

5. **Logging and Metrics:** Log retry attempts and failures to provide observability and insights into provider reliability and potential bottlenecks in the application.

This strategy ensures that SS?? can handle temporary disruptions with minimal impact on the user experience, while also allowing for proper monitoring and handling of more persistent issues.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision  
## Decision  

We will record architectural and significant project decisions using **Architecture Decision Records (ADRs)**.  

Chosen structure and format: **MADR (Markdown Architectural Decision Records) 2.1.2**, due to its simplicity, readability, and suitability for open collaboration.  

### Justifications:  
- Documenting decisions ensures transparency and helps new team members and contributors understand the reasoning behind decisions, promoting seamless onboarding.  
- MADR is lightweight and aligns with an open collaboration philosophy.  
- Written decisions encourage constructive discussions, leading to improved decision-making.  
- This approach inherently avoids stale documents, as ADRs are point-in-time records that do not require frequent updates.  

### Implementation:  
- ADRs will follow the [MADR 2.1.2 template](https://adr.github.io/madr/) for consistency.  
- All project contributors will be encouraged to write ADRs for significant decisions across architecture, processes, and development.  
- ADRs will be stored in a dedicated `docs/adr` folder within the repository for easy access and visibility.  ","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up  with all relevant discussion summarized  in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision 
## Decision

Use the `did:ext:` URI scheme to reference extension EDN files stored on decentralized storage.

- The `did:ext:` scheme will allow clear identification of extension files. For example, `did:ext:<hash|path>` where `<hash|path>` represents the decentralized storage identifier (e.g., IPFS CID or a readable path).

- The use of a structured URI schema centralizes the reference mechanism to make it easier for developers and systems to resolve and validate extensions effectively.

This decision ensures consistent, extensible, and recognizable identification of extension metadata, providing alignment with decentralized storage conventions.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision 
## Decision

Group chat will officially be deprioritized for the beta launch. The focus will remain on 1-1 chat and public chat, as they provide the most essential user experience for the MVP and align with the goal of staying on the critical path for the Q2 launch.

By deprioritizing group chat, we will free up resources to address outstanding regressions and ensure a high-quality release for the prioritized chat features. Group chat can be revisited as a future enhancement after the beta launch.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision 
## Decision

We have decided to integrate Discovery v5 protocol to enable dynamic peer discovery for the messaging app. This decision was made to address the current limitations of hardcoded peers, which inhibit scalability and lead to cluster bottlenecks as the user base increases.

By implementing Discovery v5, we can:
- Dynamically discover and connect to new peers in the network.
- Scale server peers independently from the app to ensure high availability and reliability.
- Improve fault tolerance by avoiding reliance on a static set of peers.
- Enhance security by distributing load and reducing a single point of failure.

The implementation will use the Discovery v5 specification and align with the protocol's features, ensuring seamless integration into the existing system while maintaining compatibility with current messaging functionality. Further research and testing will determine optimal parameters, such as peer selection strategies and network performance considerations.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision  
## Decision

To ensure wallet compatibility with other applications and provide users with the ability to seamlessly create and import wallets between Status and external systems, we will update the key generation process according to industry-standard practices.

Specifically, Status will adopt the **BIP-39 mnemonic standard** in conjunction with **BIP-44 paths for key derivation**. This will align Status's key generation and wallet functionality with widely-used practices across decentralized ecosystems.

The updated process will work as follows:
1. **Mnemonic Generation**: When a new wallet is created, 12 mnemonic words will be generated using the BIP-39 standard. These words will represent a deterministic seed.
2. **Key Derivation**: From the mnemonic, we will derive keys using the BIP-44 hierarchical deterministic (HD) derivation scheme. The path structure will follow industry norms: 
   - `m / purpose' / coin_type' / account' / change / address_index`
   - For Ethereum, the specific path will be: `m/44'/60'/0'/0/0`.
3. **Address Deterministic Behavior**: Using the above derivation standard, the wallet address created in Status will match the address created in other compatible wallets (e.g., MetaMask, Trust Wallet) when the same 12 mnemonic words are imported.

### Benefits of this Decision:
- **Improved Wallet Interoperability**: Users will be able to create wallets in other apps and import the corresponding wallet into Status without discrepancy in addresses.
- **Enhanced User Experience**: The change removes user friction when interacting with external decentralized applications and wallets, promoting easier on-boarding and interactions.
- **Conformance to Industry Standards**: By aligning with widely recognized standards (BIP-39 and BIP-44), Status stays up-to-date with decentralized ecosystem norms, ensuring better compatibility and security.

### Implementation Notes:
- **Migration Handling**: Existing Status wallets will continue to function as-is with their current derivation method to avoid breaking changes. A clear distinction will be made between legacy-generated wallets and BIP-44-compatible wallets.
- **User Education**: As part of releasing this feature, we will inform users about the new mnemonic compatibility and the advantages it brings for interoperability.
- **Default Configuration**: For Ethereum wallets in Status, the BIP-44 path will default to `m/44'/60'/0'/0/0`. For other currencies in the future, the corresponding coin type (as specified in [SLIP-44](https://github.com/satoshilabs/slips/blob/master/slip-0044.md)) will be used.

This decision supports interoperability and improves the end-user experience while aligning Status with best practices in the blockchain ecosystem.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision 
## Decision

We will prioritize transitioning from LES to ULC for the Status app as the path forward to address performance issues and decentralization concerns. ULC offers a more promising route, allowing for improved decentralization, increased security, and more efficient synchronization with Ethereum nodes. 

While ULC is not yet ready, our temporary solution will be to rely on Infura for node connectivity, with an option for users to configure an upstream node to mitigate decentralization risks. This is a short-term compromise, and our primary focus will remain on finalizing ULC implementation as soon as possible.

Additionally, we will monitor and document LES performance-related bottlenecks and conduct further experiments to better understand their impact, ensuring that any transition pain points are minimized. Once ULC is ready, we will provide support for connecting to multiple nodes, enhancing decentralization, security, and future scalability of the app.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision 
## Decision

To avoid sensitive data, such as user passwords, from appearing in logs or being inadvertently exposed, we will introduce a specialized `SensitiveData` wrapper type to encapsulate such data. The aim is to ensure sensitive information is handled securely and cannot be logged accidentally.

### Design

1. **`SensitiveData` Wrapper**:
   - A new type, `SensitiveData`, will be created to encapsulate any sensitive information. This type will provide stricter guarantees that its contents are never logged or output unless explicitly allowed.
   - For example:
     ```go
     type SensitiveData struct {
         data string
     }

     func (s SensitiveData) String() string {
         return ""[REDACTED]""
     }

     func (s SensitiveData) Reveal() string {
         // Access to the actual value should require an intentional method call for safety
         return s.data
     }
     ```

2. **Redacted Display by Default**:
   - By overriding the `String()` or equivalent representation methods for `SensitiveData`, any attempt to log or print the wrapped object will result in `[REDACTED]` or another placeholder text instead of the actual sensitive content.

3. **Explicit Handling for Access**:
   - Sensitive data will only be accessible through explicit methods like `Reveal()`. Developers are advised to avoid casual use of such methods in situations where the data could be exposed.

4. **Data Propagation**:
   - Any functions or operations that can potentially output logs should be updated to recognize and handle `SensitiveData` types appropriately. This will ensure the internal mechanism of the `SensitiveData` type prevents accidental leakage.

5. **Application-wide Integration**:
   - All potentially sensitive values currently passed around as standard `string` types will be migrated to `SensitiveData`. This includes passwords, access tokens, and any other confidential data.

### Benefits
- Reduced risks of sensitive data exposure in logs or unintended outputs.
- A clear, enforced standard for handling confidential information across the application.
- Developers are forced to critically assess how sensitive data is accessed and propagated, encouraging safer coding practices.

This decision ensures sensitive information is shielded at the structural level of the code, minimizing human errors that could lead to accidental data leaks.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision 
## Decision

We have decided to **deprecate and remove 3rd party status extensions via the Jail execution environment and corresponding JavaScript API** in favor of the upcoming Pluto extensions system.

### Rationale:
1. **Simplified Architecture**:
   - The Jail execution environment has proven to introduce significant architectural complexity and maintainability issues, including stateful, asynchronous, and hard-coded logic spread throughout the application.
   - Moving to Pluto extensions, which focus on a clean, declarative approach, provides a cohesive model for extending application functionality without the pitfalls of Jail.

2. **Poor Adoption**:
   - There has been little interest or uptake of 3rd party bots or commands using the Jail mechanism. Most submitted DAPPs rely on standard web technologies (`web3.js`), and the Jail approach has not delivered meaningful value for building chat commands or bots.

3. **Improved Developer Experience**:
   - Pluto offers a declarative and modular way for developers to extend Status functionality. This approach is easier to adopt, less error-prone, and does not impose the messy, stateful semantics of the current Jail API.

4. **Performance Optimization**:
   - The constant back-and-forth communication via the RN Bridge for Jail commands introduces performance overhead, which is counterproductive given our resource constraints. Pluto extensions will provide greater efficiency through more streamlined execution models.

5. **Consistency and Stability**:
   - Maintaining a half-functional and fragmented Jail API was neither sustainable nor desirable. Pluto extensions will offer a more stable and well-documented approach that aligns with the broader direction of the application.

6. **Future-Proof Approach**:
   - Pluto extensions are designed to support modular hooks beyond just chat commands, encompassing wallet, contacts, and other host integrations. This broader capability ensures a unified path forward for future application extensibility.

### Action Plan:
- The existing Jail execution environment and its API will be phased out entirely.
- The remaining hard-coded ""magic"" in the application related to Jail commands will be refactored and replaced with a direct integration into the app or adapted to Pluto's declarative extension model.
- Documentation and support for the Jail API will be retired, and developers are encouraged to explore Pluto extensions as the new standard for building custom functionality.
  
By deprecating the Jail system and embracing Pluto, we aim to provide a clearer, more robust, and forward-looking extension framework that enhances both developer and user experience.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision 
## Decision

We will implement the Tribute to Talk (TtT) feature as described in the whitepaper with the following mechanics:

1. **Stake Requirement**: Users must stake a minimum amount of SNT (defined by the receiving stakeholder) to send a cold message or contact request. Each user can configure their own staking threshold.

2. **Message Handling**: The staked amount will act as a deposit for sending the message. This deposit ensures economic deterrence against spamming and guarantees the quality of requests received by the stakeholders.

3. **Deposit Forfeiture Upon Response**: The deposit will only be forfeited to the recipient when they explicitly reply to the message. Until a response occurs or the message is ignored, the staked deposit remains held in escrow.

4. **Escrow Management**: Deposits are securely held in an escrow contract to ensure fairness and immutability until an event (such as a reply or a refund) triggers the flow of funds.

5. **Refunds for Ignored Messages**: If the recipient does not respond within a predefined time limit (configurable by the platform or recipient), the deposit can be refunded back to the sender. This prevents indefinite holding of funds for unanswered messages.

6. **Transparency**: To maintain trust, users will be provided a clear interface showing their staked amounts, pending responses, forfeited deposits, and refunded stakes.

This implementation aligns with the original proposal, creating a reliable economic anti-spam mechanism while promoting meaningful interactions within the network.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision 
## Decision

To improve oversight and ensure better polish and quality within the mobile app, a new roleSection Leadwill be established within the mobile development team. Each Section Lead will take ownership of a specific section of the mobile app (e.g., communities, syncing & onboarding, activity center & shell, wallet, messaging). 

Responsibilities of Section Leads include:
- Ensuring the overall quality, usability, and polishness of the assigned section.
- Acting as the point of contact between design and development for their section.
- Reviewing pull requests and ensuring adherence to design and development guidelines for their section.
- Coordinating with other Section Leads and the team lead to ensure consistency and integration across sections.
- Evaluating and prioritizing issues or feedback related to their section.

This new role will distribute responsibilities more effectively, improve focus on key areas of the app, and address the identified lack of oversight. The team lead will continue to oversee the overall mobile app and guide Section Leads to ensure harmony between sections.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision 
## Decision

We decided to integrate Product Option's values as a sub-resource within the new Sylius API using API Platform's `@ApiSubresource` annotation. 

### Rationale

* **Efficiency**: Using sub-resources enables efficient manipulation and retrieval of values directly associated with a Product Option without requiring additional queries or manual operations.
* **Ease of Use**: API consumers are provided with a clean and standard way of navigating and managing the relationship between Product Options and their values.
* **Consistency**: API Platform has built-in support for sub-resources, which fits well with our overall API design framework and avoids custom implementation overhead.
* **Flexibility**: Allows for separation of logic and data management by ensuring the values are accessible as a part of the Product Option while maintaining the ability to expand or version API endpoints in the future.

By adopting this approach, we ensure our API aligns with current best practices for resource relationships in RESTful API design while simplifying its usage for third-party developers.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision 
## Decision

We will provide a clear and standardized mechanism for delivering localized resources in Sylius APIs to enhance user experience and ensure consistency.

### Approach:
1. **Locale Detection**:  
   For endpoints returning translatable resources, the API will detect the customer locale from:
   - The `Accept-Language` header in the request.
   - A specific query parameter (e.g., `locale`) to override the header if needed.

2. **Single Translation per Locale**:  
   Resources returned via the API will only contain content for the detected or explicitly requested locale, instead of embedding all translations. This simplifies the frontend implementation by removing the burden of localization.

3. **Fallback Mechanism**:  
   If the requested locale is unavailable, the API will use the store's default locale as a fallback, ensuring a seamless user experience.

### Consistency:
- Endpoints that deal with translatable resources will conform to this new standard.
- Internal handling of translations within the API will respect the same locale detection and fallback logic described above.

This decision promotes clarity and usability for frontend developers, enabling efficient and consistent handling of localized content.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision 
## Decision

We will document architectural and technical decisions for Terraform Controller using Architecture Decision Records (ADR) as a lightweight method to record and communicate decisions. These records will be stored in the project's repository to ensure they are easy to discover and accessible to current and future contributors. Each decision will follow the format [described by Michael Nygard](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) to provide consistent and clear records of decisions. This approach will allow developers to understand the rationale behind decisions and align with the project's long-term goals.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision 
## Decision

We will use GitHub webhooks to detect pull request changes instead of polling.

This decision is based on the following considerations:
1. **Efficiency**: Webhooks provide real-time notifications whenever pull request events occur, avoiding the need for periodic API calls and reducing unnecessary requests.
2. **Performance**: Webhooks minimize the load on GitHub's API and our infrastructure by eliminating continuous polling.
3. **Scalability**: As our repository and usage grow, webhooks will provide a more scalable and maintainable solution compared to polling.
4. **Reduced Rate-Limiting Concerns**: Polling is bound by GitHub API's rate limits, while webhooks can handle changes promptly within GitHub's infrastructure.

We will configure GitHub webhooks to trigger an action whenever a relevant pull request event, such as creation, update, or closure, occurs. This keeps our system responsive and minimizes unnecessary overhead.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision 
## Decision

We will allow cross-namespace references but enforce strict control measures to mitigate the risks associated with breaking namespace isolation and encouraging unnecessary permission proliferation.

Here are the key decisions made:

1. **Namespace-scoped access via explicit references**:
   - Cross-namespace references will be explicitly allowed only for certain fields such as `Terraform.spec.sourceRef`, `Terraform.spec.dependsOn[]`, and `.data.resources[]` where their use is justified. Examples include objects relying on Flux sources defined in separate namespaces.

2. **Require explicit referential configuration**: 
   - Cross-namespace references must be explicitly configured with fields for both the Namespace and the Name of the object being referred to. For example:
     ```yaml
     sourceRef:
       namespace: namespace-a
       name: source-object
     dependsOn:
       - namespace: namespace-b
         name: dependency-object
     ```

3. **Permission model enforcement**:
   - Any cross-namespace access will require specific permissions, enforced through Role and RoleBinding configurations, ensuring that only authorized namespaces (or the objects within them) are allowed to communicate with each other.
   - Namespaces must explicitly grant permissions for cross-namespace interaction.

4. **Validation Webhook**:
   - Introduce a validation mechanism (e.g., a Kubernetes Admission Webhook) to validate and enforce the correctness and security of cross-namespace references, including:
     - Whether the referenced object exists in the specified namespace.
     - Whether the appropriate permissions are in place for the cross-namespace access.

5. **Auditing and Monitoring**:
   - Implement logging and metrics to track cross-namespace references, enabling users to identify potentially insecure configurations.

6. **Documentation and Communication**:
   - Provide clear documentation to educate users about the risks of cross-namespace references, best practices, and how to securely configure them.

By striking a balance between usability and security, these mechanisms ensure that legacy and existing installations relying on cross-namespace references remain functional while mitigating risks of abuse and misconfiguration.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision 
## Decision

The decision is to implement a caching mechanism for Workspace BLOBs in the TF-Controller to ensure efficient and reliable deletion of single TF objects.

The caching mechanism will store Workspace BLOBs that contain all the necessary configuration and state information generated by the tf-runner. These cached BLOBs will enable proper finalization of TF objects by ensuring all associated Source, Secrets, and ConfigMap data is available during the deletion process, even if those resources are deleted or altered before the TF object itself is finalized. 

Key implementation details:

1. **Workspace BLOB Management**:
   - The TF-Controller will maintain a cache of Workspace BLOBs locally.
   - These BLOBs will be kept synchronized with the state and lifecycle of the TF objects.

2. **Dependency Awareness**:
   - Before deleting a TF object, the TF-Controller will retrieve and utilize the cached Workspace BLOB to finalize and clean up all associated resources (e.g., Source, Secrets, ConfigMaps).
   - This ensures proper deletion of dependent resources in the correct order.

3. **Automatic Updates**:
   - Whenever the tf-runner generates a new Workspace BLOB, it will replace any older version in the cache.
   - This keeps the cache up-to-date with the latest configuration and state information.

4. **Resource Cleanup**:
   - Upon deletion of the TF object, the cached Workspace BLOB will be referenced to ensure complete cleanup.
   - Once finalization is complete, the associated Workspace BLOB will also be removed from the cache, freeing up space.

By introducing this caching approach, the TF-Controller can efficiently address challenges with single TF object deletion, setting the stage for resolving more complex deletion scenarios involving dependencies and namespaces. Additionally, this mechanism enhances system reliability and improves user experience by preventing inconsistent states during deletion operations.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision 
## Decision

We will rework our E2E testing approach with the following changes:

1. **Decouple E2E Testing from Repository Secrets:** Since Zarf no longer requires images from `registry1` to create the `zarf-init-{{arch}}.tar.zst`, repository secrets are no longer necessary for E2E testing. This allows us to remove the use of the `slash-command-dispatch` workflow, enabling automated E2E testing to run by default for all PRs. This enhances contributor experience and ensures tests run consistently on all submitted changes without requiring maintainer intervention.

2. **Incorporate Testing Across Kubernetes Distributions:** We will expand test coverage to include various Kubernetes distributions. This ensures compatibility and reliability across different platforms.

3. **Include Testing Across Linux Distributions:** We will test Zarf on different Linux distributions to verify its behaviors in a wider range of environments. This will help identify potential issues related to OS-specific configurations.

4. **Enable Local Testing for Quicker Feedback Loops:** To improve developer feedback loops during feature development, we will create local E2E testing capabilities. Developers should be able to run tests locally, eliminating the need to rely on EC2 instances except for specific, cloud-dependent workflows.

5. **Integrate Scalable Test Infrastructure:** We will migrate the testing infrastructure to a more scalable and adaptable platform that can accommodate multiple OS and Kubernetes variations. Running tests on platforms such as GitHub Actions, combined with Docker or Kubernetes-in-Docker (e.g., KinD), will allow us to handle diverse configurations.

By implementing these changes, our E2E testing strategy will remain secure, efficient, and capable of handling the growing demands of the project while improving the developer experience.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision 
## Decision

To support Zarf's requirement for pushing images into a K8s cluster without assuming the presence of an existing registry and ensuring compatibility with unknown environments, we will implement a strategy that includes the following:

1. **Temporary Registry Deployment**: Zarf will create a temporary, ephemeral OCI-compatible registry directly on the cluster to serve the required images during bootstrapping. This ensures Zarf does not rely on external registries and provides full control of image delivery.

2. **Cluster-wide TLS Trust Setup**: For clusters pulling images from the temporary registry, Zarf will inject a trusted TLS certificate authority into every nodes CRI configuration or root certificate authorities. This guarantees that the registry connection is trusted without external dependencies.

3. **Affinity/Taint Management for Node-Specific Control**: To simplify bootstrapping operations, Zarf will establish affinity rules or taints that direct initial workloads (such as setup pods) to a specific node, ensuring consistent handling of single-node modifications.

4. **Fallback Mechanism for Direct Push**: If no registry option is viable (e.g., CRI does not support registry pulls), Zarf will use local node injection via CRI-native approaches (containerd, Docker, etc.) when supported by the underlying K8s distribution.

This strategy ensures Zarf accommodates clusters of varying configurations, maintains compatibility with unknown environments, and avoids reliance on external infrastructure. Additionally, the ephemeral registry deployment avoids lingering dependencies and simplifies the cleanup process post-bootstrapping.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision 
## Decision

We have decided to implement the use of Software Bill of Materials (SBOM) for all software running on government hardware in compliance with Executive Order 14028. To achieve this, we will:

1. **Choose an SBOM Standard**:
   - Use widely adopted SBOM standards such as SPDX (Software Package Data Exchange) or CycloneDX to generate and manage SBOMs.

2. **Integrate SBOM Generation**:
   - Incorporate SBOM generation as part of our CI/CD pipeline to ensure SBOMs are created automatically during the build process.
   - Use tools like Syft, Trivy, or Snyk to generate SBOMs efficiently.

3. **Maintain and Distribute SBOMs**:
   - Store SBOMs in a version-controlled repository to maintain an accurate historical record.
   - Provide SBOMs alongside software deliverables to meet government contractual requirements.

4. **Monitor Vulnerabilities**:
   - Use the information in SBOMs to monitor and address vulnerabilities in dependencies and components proactively.

5. **Compliance and Documentation**:
   - Ensure all SBOMs are reviewed and updated periodically to align with EO14028 requirements.
   - Provide training to relevant teams about SBOM usage and the importance of compliance with government regulations.

By implementing SBOMs, we fulfill the requirements of EO14028, enhance transparency, and improve the security posture of the software running on government infrastructure.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision 
## Decision

We will implement a Kubernetes Mutating Admission Webhook within Zarf to dynamically mutate and manage image paths, secrets, and other required configurations for workloads deployed on clusters using Zarf, including those managed by gitops tools like Flux and ArgoCD.

### Details:
1. **Webhook-based Dynamic Mutation:**
   - Introduce a Kubernetes Mutating Admission Webhook that will automatically intercept resource manifests at the cluster level, modifying image paths, secrets, and any other necessary Zarf-specific configurations (e.g., references to the internal Zarf registry or Zarf git server).
   - This enables seamless integration without requiring manual mutations or separate preprocessing steps by package authors.

2. **CRD/Agnostic Handling:**
   - Extend functionality to work not only on standard Kubernetes manifests but also on Custom Resource Definitions (CRDs) that reference the Zarf git server or other resources to ensure consistent behavior across workloads.

3. **Consistency Across Deployment Tools:**
   - The webhook ensures uniform mutational logic for deployments regardless of whether they are applied directly by Zarf, managed via Flux/ArgoCD, or other external deployment pipelines.

4. **Simplified Experience for Package Authors:**
   - Eliminate the need for package authors to use `zarf prepare` or rudimentary templating within their manifests for mutations, reducing complexity and improving developer ergonomics.
   - Package authors can focus on providing clean, standard manifests while Zarf transparently handles Zarf-specific adjustments at runtime.

5. **Security and Lifecycle Management:**
   - The Mutating Admission Webhook will be securely deployed as part of the Zarf setup process and managed lifecycle within the Zarf context.
   - It will follow Kubernetes best practices for admission controllers, including proper role-based access control (RBAC).

6. **Backward Compatibility with Current Workflows:**
   - Existing workflows using Zarf with Helm Post Rendering and `zarf prepare` will not break. Users can onboard the new Webhook-based solution when appropriate for their use cases.

By implementing this solution, we reduce burdens on package authors, improve compatibility with secondary gitops tools, and minimize customization required for manifests when working with Zarf-managed clusters.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision  
## Decision  

Eliminate `stage2` and the need for `registry:2`. Instead, use a slimmed-down solution that avoids introducing another binary and minimizes overhead. Replace the `docker registry` functionality with an alternative image-hosting mechanism, such as directly using a lightweight container image loader (e.g., `crane` or `imgpkg`) to push the required image into the cluster.  

This decision simplifies the architecture by:  
1. Reducing the number of binaries in the ecosystem to 2.  
2. Minimizing the number of configmaps, improving initialization speed.  
3. Decreasing the build time and CI complexity by removing platform-specific compilation for `stage2`.  
4. Avoiding overuse of a full-featured docker registry for hosting a single image, optimizing resource usage.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision  
## Decision  

We will adopt **Option 2: Introduce Unit Testing in a defined and limited capacity within Zarf.**  

We will focus on implementing unit tests specifically in areas that are:  
1. Difficult or infeasible to test with end-to-end approaches.  
2. High-value interfaces where we anticipate greater scrutiny or high potential for breakage as Zarf evolves into a library for other products.  

### Rationale:
- **Targeted Coverage:** This approach allows us to increase the coverage of the codebase without introducing excessive testing or maintenance overhead.  
- **Maintainability:** By limiting unit tests to well-defined critical areas, the risk of reduced test efficacy over time is mitigated.  
- **Balance:** It strikes a balance between increasing confidence in code correctness and not over-burdening the development process with unnecessary testing constraints.  
- **Flexibility:** We maintain adaptability as the codebase evolves, avoiding premature over-commitment to large-scale testing strategies.  

### Implementation Notes:
- Establish clear guidelines on where and how unit tests should be applied to ensure consistency and alignment with our goals.  
- Perform rigorous reviews of PRs with unit tests to ensure the correct application of this strategy.  
- Reevaluate the effectiveness and scope of unit testing regularly to align with Zarf's evolving needs as a library for other products.  

Integration and partial end-to-end testing will remain as complements to this unit testing strategy but will not be the primary focus of this decision.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision 
## Decision

Option 2 - Run Syft against files and dataInjections

This approach strikes a balance between completeness and practical utility. By targeting files and dataInjections, we ensure only artifacts that are guaranteed to be part of the Zarf package are included in the SBOM, reducing noise and confusion. Although this means we might miss artifacts like dependencies embedded in git repos, this tradeoff is acceptable given that files and dataInjections are the most common way users include artifacts in packages.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision 
## Decision

To support connected environments without requiring Zarf init packages, we will introduce an option for declarative Zarf deployments in these scenarios. This will allow users to deploy and manage Zarf packages without the constraints imposed by disconnected environments while still leveraging Zarf's declarative approach.  

### Key Changes:
1. **Optional Initialization**: Enable deployments of Zarf packages without requiring a Zarf init package for connected environments. Users can directly deploy packages that define their application or upgrade process.
   
2. **Declarative Deployment Configuration**: Create a streamlined way for users to define and apply their deployments using Zarf's declarative package specification, without assumptions about an offline or disconnected setup.

3. **Context-Aware Deployment Mode**: Zarf will differentiate between ""connected"" and ""disconnected"" environments. Connected environments will skip validation requiring airgap configurations (e.g., ensuring images are pulled from local registries).

### Impact:
- Extending Zarf's usability to connected environments expands its scope beyond its original design for disconnected environments, while maintaining its core declarative deployment ideology.
- Users can standardize deployments for connected environments in a similar way they do for disconnected environments, bolstering adoption and flexibility.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision 
## Decision

We decided to expand the `scripts` functionality into a more robust system to address the limitations and support more extensibility. The enhanced `scripts` will now support additional hooks and a more structured object model. This includes:

- Introducing a full lifecycle hook system (`before`, `after`, `prepare`, `onError`, and `onSuccess`) to enable more granular script execution based on the package state and actions. 
- Allowing scripts to support additional metadata, such as descriptions, dependencies, and conditions for execution.
- Adding conditional execution logic to the scripts, enabling users to specify certain preconditions (e.g., OS type, environment variables) before running a script.
- Supporting more complex arrays of commands for each lifecycle hook, including sequence control and fallback logic.
  
This decision ensures `scripts` are more powerful and flexible, enabling teams to express more diverse initialization and business logic directly through the package system.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision 
## Decision

We will address the issue by providing a **configurable option** in Zarf to allow users to toggle between the faster but memory-intensive `Crane` method (which may lead to OOM issues for large images) and a slower, more memory-efficient method when loading local Docker images into the docker daemon. This will give users the flexibility to prioritize performance or resource usage based on their environment and needs.

By default, Zarf will continue to use the current `Crane` implementation as it is faster for the majority of use cases. However, for environments where large images are common and memory constraints exist, users can opt to enable the alternative, slower method using a configuration flag.

Additional implementation steps:
1. Clearly document both options within Zarf to ensure users understand the trade-offs between speed and memory usage.
2. Optimize the slower method (if possible) for competitive performance while avoiding OOM conditions.
3. Continue to evaluate and plan for future package schema changes to ensure compatibility with both methods while adhering to the upcoming OCI import standard.

This decision balances the usability gap for local development while accommodating users' diverse system capabilities.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision 
## Decision

To simplify the deployment process for Big Bang via Zarf, the following approach will be implemented:

1. **Dedicated Flux Installation**: Bundle Flux as part of the Zarf package to ensure the necessary CRDs are in place for Big Bang deployments. This eliminates manual installation steps for the end user.

2. **Image Optimization**:
   - Implement a dynamic image scanning mechanism that analyzes the `values.yaml` provided by the user to identify the specific subset of images required for their deployment. 
   - Only include these necessary images in the Zarf package, significantly reducing storage requirements and eliminating the over-provisioning of images.

3. **Git Repository Optimization**:
   - Using the configuration provided in `values.yaml`, identify and bundle only the specific git repositories needed for the deployment, instead of including all repositories present in Big Bang's definition. This reduces complexity and package size.

4. **Simplified Values Injection**: Develop tooling or scripts within the Zarf package to automate the injection of the user's `values.yaml` into Big Bang's default Helm chart structure. This ensures minimal manual intervention and simplifies the customization process.

5. **Dynamic Discovery of Resources**:
   - Enhance the deployment process with automated discovery and validation of required resources (images, repositories) based on the provided configuration.
   - Integrate this process into Zarf so that it enriches the deployment package with only tailored resources.

6. **Testing and Validation**: Ensure rigorous testing with various `values.yaml` files to guarantee the accuracy of dynamic resource provisioning and the elimination of redundant resources.

This decision aims to make Big Bang deployments via Zarf more efficient, tailored, and user-friendly by addressing the challenges of resource superset provisioning and complex configuration.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision 
## Decision

Zarf will implement support for publishing and pulling packages from OCI-compliant registries. This will enable scalable distribution of Zarf packages and align with existing industry standards for container and artifact publishing.

### Key aspects of the decision:
1. **OCI-Compliant Registries**: Zarf packages will be structured to fit within the OCI Artifact Specification, allowing the use of popular container registries (e.g., Docker Hub, GitHub Container Registry, etc.) for distribution.

2. **Native Publishing**: Users will be able to natively publish Zarf packages using the Zarf CLI. This eliminates the need for manual file transfers and provides a seamless, standardized process.

3. **Package Signing for Security**:
   - Package creators will have the ability to sign packages during creation using cryptographic signatures.
   - Zarf will incorporate functionality to validate the integrity and authenticity of packages before deployment. This ensures a trusted package supply chain.

4. **Backward Compatibility**: Existing workflows using local builds and manual file transfers will remain functional for users who prefer those methods.

This decision aligns with our goals of enabling secure and scalable distribution while maintaining Zarf's commitment to supply chain security and trust.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision 
## Decision

1. **Expand Artifact Support for Development**  
   Zarf will introduce support for additional artifact types and libraries commonly used in software development to cater to airgap development needs. The solution will include:  
   - Support for language-specific package registries such as npm, pip, Maven, etc.  
   - Local hosting of these artifacts within the airgap, enabling developers to fetch and use the necessary resources without reaching external servers.

2. **Enhance Git Functionality**  
   Zarf will improve its `git` capabilities to support broader use cases beyond flux `GitRepository` objects. This includes:  
   - Allowing clients to fetch and interact with upstream `git` repositories via protocols like HTTPS and SSH, mirroring native `git` workflow functionality.  
   - Providing configuration and tooling to cache `git` repositories within the airgap for reuse.

3. **User Experience Improvements**  
   - Implement intuitive workflows for developers to import and manage artifacts and repositories in the airgap environment.  
   - Document clear instructions and best practices for using Zarf in development workflows.  

These enhancements will empower developers to compile and build software efficiently in airgap development environments, while also expanding Zarf's utility for broader `git` use cases.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision 
## Decision

To prepare Zarf for a ""Generally Available"" v1.x.x release that can be reliably used in mission-critical operations, the following steps will be taken:

1. **Initiate Semantic Versioning**:
   - Adopt semantic versioning (major.minor.patch) to clearly communicate breaking changes in major versions while providing guarantees of stability within minor and patch updates.

2. **Establish Rigorous Testing Framework**:
   - Enhance and expand automated testing across the codebase, including integration and end-to-end tests, to ensure functionality and minimize regressions.
   - Introduce testing for edge cases, high-stress scenarios, and mission-critical workflows to validate reliability under varied conditions.

3. **Improve Documentation and User Guidance**:
   - Strengthen documentation to ensure users can easily understand, deploy, and operate Zarf in production environments without heavy reliance on internal developers or special expertise.
   - Include migration guides for breaking changes.

4. **Stabilize Core Features**:
   - Review and prioritize stabilization of key features upon which users depend for mission-critical operations.
   - Defer new feature experimentation to minor releases. 

5. **Backward Compatibility and Change Management**:
   - Minimize breaking changes in future releases and communicate planned changes transparently to users with sufficient transition/support timelines.
   - Create tooling or processes for seamless migration when breaking changes cannot be avoided.

6. **Community Engagement**:
   - Actively engage with the Zarf user community for feedback on production use, feature priorities, and potential issues.
   - Incorporate feedback into the roadmap to better align with user needs.

7. **Release Candidate Process**:
   - Introduce release candidates (RC versions) for major updates to allow early adopters to test in controlled environments, reducing surprises with the final GA version.

By implementing these measures, Zarf can evolve into a robust, production-ready v1.x.x product capable of supporting mission-critical operations with user confidence and minimal operational friction.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision  
## Decision  

We will introduce a `hooks` mechanism in Zarf to provide a structured and system-agnostic way to handle deployment lifecycle functionality, enabling cluster maintainers to respond to specific lifecycle events transparently. This replaces overloading the `actions` feature, which is being misused to work around missing functionality.

### Key Details:  

1. **Adding Lifecycle Hooks:**  
   Zarf will support the concept of `hooks` that operate during specific lifecycle events, such as `pre-deployment`, `post-deployment`, and potentially others. These hooks can include functionality provided by Zarf core or custom functionality registered by maintainers.

2. **Solving the ECR Problem:**  
   - **Token Refreshing:** Zarf will include a built-in `post-init` hook for ECR-based clusters to manage token refreshing transparently. This hook will continuously refresh ECR authentication tokens on the cluster without requiring user or package-specific intervention.  
   - **Repository Creation:** Pre-deployment and pre-package hooks will be added to handle repository creation for ECR. When a package includes images, Zarf will ensure the required repositories are created on ECR before any image push is attempted.

3. **System-Agnostic Packages:**  
   To ensure packages remain system-agnostic:
   - Packages will not be required to carry any ECR-specific logic within `actions`.
   - Instead, Zarf hooks will handle ECR-related tasks automatically based on the cluster configuration (e.g., determining if ECR is set as the external registry during `zarf init`).  
   - This makes packages portable and eliminates the need for conditional logic within the packages themselves.

4. **Core vs. Custom Hooks:**  
   - **Core Hooks:** Zarf core will provide built-in hooks for standard scenarios like external registries (e.g., handling token refreshing and repo creation for ECR).  
   - **Custom Hooks:** Maintainers can register custom hooks to extend functionality based on their specific cluster requirements. These hooks will be managed outside of packages, ensuring that the functionality is implemented and controlled by cluster administrators rather than package creators.

5. **Implementation Considerations:**  
   - **Hook Registration:** Hooks will be defined declaratively within Zarf's cluster configuration. For example:
     ```yaml
     hooks:
       post-init:
         - name: ecr-token-refresh
           type: built-in
       pre-deploy:
         - name: ecr-repo-creation
           type: built-in
     ```
   - **Execution:** Hooks will run automatically during specific lifecycle stages without requiring manual user input for standard scenarios.

6. **Backward Compatibility:**  
   - Existing `actions` will remain functional for the time being, but their usage should be limited to scenarios that are external to Zarf's core lifecycle concerns.
   - Users will be guided to transition functionality to `hooks` as the new mechanism becomes the standard.

### Outcome:  

This decision enables Zarf to solve cluster-specific problems like ECR integration transparently and flexibly without forcing package creators into overly complex and specific design patterns. By introducing `hooks`, we provide a more structured and extensible way to handle lifecycle events while keeping packages system-agnostic and focused on their core functionality. ","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision 
## Decision

We will use **gRPC** to decouple transformations from Zarf while synchronizing the transformation library used by Pepr. 

This decision is motivated by the following considerations:

1. **gRPC Offers Strong Performance and Efficiency**:
    - gRPC is lightweight and efficient, making it suitable for scenarios where performance matters.
    - It enables binary communication using Protocol Buffers, which reduces overhead compared to REST.

2. **Compatibility with Existing Go Code**:
    - By using Protocol Buffers, we can maintain compatibility with Zarf's existing Go transformation library.
    - The Go library will act as the server, which Pepr (written in TypeScript) will interact with via gRPC calls.

3. **Minimal Rewrite and Code Duplication**:
    - This approach avoids a complete rewrite of the Zarf transformation library into TypeScript, preserving the reliability and battle-tested nature of the existing Go code.
    - Instead, gRPC allows Pepr to leverage the existing Go code base directly through remote procedure calls.

4. **Clear Interface Definitions**:
    - gRPC's reliance on Protocol Buffers ensures that the contract between the Go server and the TypeScript client is explicitly defined and versioned.
    - This makes the system more maintainable and reduces risks associated with disparate implementations.

5. **Avoiding Complexity of WASM**:
    - While WASM would allow embedding the transformation library, it introduces complexity in bridging Go and TypeScript, particularly around complex data types and serialization.
    - By contrast, gRPC provides a cleaner, more standard mechanism for cross-language communication.

6. **REST Shortcomings**:
    - REST, while simple, introduces inefficiencies with JSON (e.g., larger payload sizes and slower processing times).
    - It lacks some features like native bi-directional streaming, which could be useful for potential future use cases.

By adopting gRPC, we achieve the goals of:
- Decoupling the transformation logic from Zarf.
- Allowing Pepr to directly use the matured transformation library without duplicating logic or introducing inconsistencies.
- Establishing a scalable and maintainable architecture for future growth.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision  
## Decision

To address the inconsistencies in Zarf's package loading abilities and streamline the user experience, we will implement the following changes:

1. **Standardized Package Loading**:
   - Introduce a unified loading mechanism for all package types (local tarball, split tarball, OCI packages, in-cluster packages, and HTTPs URLs) to ensure consistency across all package lifecycle actions: creation, deployment, inspection, and removal.

2. **Inspection for All Package Types**:
   - Enable the ability to inspect all supported package types, including split tarballs, in-cluster packages, and HTTPs URLs. This ensures users can verify package contents before deployment or removal.

3. **Removal for All Package Types**:
   - Allow for the removal of any deployed or referenced package sources in a consistent manner, including in-cluster packages and packages specified by HTTPs URLs.

4. **Enhanced Error Handling**:
   - Implement robust error handling for unsupported operations or improperly formatted package sources, providing clear guidance to users on correcting their input.

5. **Documentation Updates**:
   - Update the Zarf documentation to reflect these enhancements, including clear explanations and examples for creating, deploying, inspecting, and removing each supported package type.

This decision ensures that Zarf's package management capabilities are consistent, predictable, and user-friendly, improving the overall experience and reducing confusion caused by current inconsistencies.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision
## Decision

To address the issues with scaling, composability, and filtering, Zarf will implement the following changes:

1. **Centralized Import Handling**: Refactor the import logic to a central module that can handle deep import chains across multiple layers effectively while ensuring consistent behavior and avoiding duplication of logic. This will improve scalability and provide better test coverage for multi-layer imports.

2. **Redesign OCI Skeleton Integration**: Reevaluate and refactor the integration of OCI skeletons with composable components. This will ensure that OCI skeleton usage is seamless and properly supports composability without layering issues introduced by prior modifications. The redesigned architecture will maintain flexibility while improving stability.

3. **Centralized Component Filtering**: Move the `only` filter logic to a single, central location where all create-time filtering is managed. Centralization will eliminate bugs caused by scattered filter implementations and ensure consistent behavior across all operations.

By implementing these changes, Zarf will enhance its composability features, improve performance for complex packages, and provide a more robust and reliable system for filtering and composition.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision 
## Decision

To improve the developer experience, especially during package iteration and debugging, we will introduce a new CLI flag called `--dev` for `zarf package deploy`. This flag aims to streamline the package development lifecycle without compromising air-gapped deployment requirements.

### Changes to be Implemented:

1. **`--dev` Mode**:
   - The `--dev` mode will only affect package deployment (`zarf package deploy`), allowing developers to redeploy directly from the package directory without requiring a new `zarf init` or the recreation of the package tarball.
   - In `--dev` mode:
     - Dependencies like Docker images and chart repositories will not be re-fetched. Instead, previously fetched dependencies will be reused.
     - It will skip structural changes to the cluster, assuming the cluster is already initialized and compatible with the package.
     - It will redeploy the components specified during the command execution, making the process faster and avoiding redundant repetitions of upstream steps.

2. **Behavioral Differences in `--dev` Mode**:
   - The `--dev` mode is not meant for air-gapped or production environments, as it assumes an already initialized cluster and doesn't build the package as a complete, deployable tarball.
   - Warnings will be displayed to users indicating that `--dev` should only be used in connected environments or for development purposes.

3. **Benefits**:
   - Developers will no longer need to rebuild the package on small changes.
   - Testing iterations will be faster by bypassing redundant cluster initialization and dependency fetching.

4. **Leverage Existing Functionality**:
   - Leverage the logic from `YOLO` mode to bypass certain heavyweight operations, but integrate it cleanly with the new `--dev` flag to improve accessibility and usability.
   - Unlike `YOLO` mode, `--dev` mode will allow selective specification of components for redeployment or specific changes in the debug phase.

This decision will provide developers with a more efficient and developer-friendly approach during package development while maintaining full integrity for production and air-gapped scenarios.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision 
## Decision

Migrate the documentation site from Docusaurus 2 to Astro.

Rationale:
- **Performance**: Astro is optimized for performance and provides faster page load times by shipping less JavaScript.
- **Dependency Management**: Astro has a more modern and streamlined approach to dependency management, leading to easier updates and maintenance.
- **Flexibility**: Astro provides a framework-agnostic setup, allowing easier integration of various tools and libraries.
- **Future-proofing**: Astro's architecture aligns well with emerging practices in web development, ensuring long-term maintainability.
  
Steps:
1. Create a feature parity checklist to ensure all features from the Docusaurus site are replicated in the new Astro setup.
2. Plan the migration to prioritize core content first, followed by advanced features (e.g., search and versioning).
3. Document new processes for contributing to the Astro-based documentation site for developers and maintainers.

Timeline to be determined based on the scope of the migration and resource allocation.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision  
## Decision  

To address the issues with the current in-house logging solution in Zarf, we will move to a more robust, testable, and thread-safe logging approach while maintaining a good CLI user experience.  

### Key Changes  
1. **Adopt a Standard Logging Library**  
   We will replace the custom logging solution with a widely-recognized logging library, such as `zerolog` or `logrus`. These libraries are well-supported, thread-safe, and provide structured logging capabilities, which can help ensure cleaner log outputs and better integration with external logging pipelines if needed.  

2. **Centralize Context Handling**  
   - Instead of calling `message.Fatal` to output a message and abruptly terminate the program, we will refactor error handling to propagate the context back to the main execution thread or entry point of the program.
   - By using proper context handling, cleanup tasks such as releasing resources or shutting down gracefully will run even on errors. This improves reliability and avoids disruptions caused by early exits.  

3. **Decouple Progress UI from Logging**  
   - The progress bar and spinner components will be abstracted into a separate package/module for CLI UX, such as `ux.Progress` or similar. These components will no longer operate on a shared global state but instead accept instance-level configurations (e.g., progress bar state per thread).
   - This ensures thread safety and reduces complexities when logging operations occur in parallel threads.  

4. **Unit Testing-Friendly Logging**  
   - We will implement a logger that accepts an underlying log destination (e.g., file, buffer, or output stream). For unit tests, we can redirect logs to a custom in-memory writer, letting us validate outputs without the tests being affected by actual program exits or file system dependencies.
   - The `Fatal` functionality will be replaced with proper error-handling mechanisms that return errors to the caller rather than forcing an immediate program exit. This enables unit tests to validate behavior without premature termination.  

5. **Maintain CLI User Experience**  
   - To keep the user-friendly CLI experience, logs intended for CLI output (such as spinners, progress bars, and user-facing messages) will be distinct from internal debug/structured logs.
   - The logging system will support multiple log levels (`INFO`, `DEBUG`, `ERROR`, etc.) and output them to appropriate destinations, such as `STDOUT`, `STDERR`, or a debug log file.  

6. **Configuration of Logging at Runtime**  
   - Users of the Zarf CLI will have the ability to configure logging behavior via CLI flags (e.g., `--debug` or `--quiet`) or configuration files. For instance:
     - `--debug` would enable verbose logs, including debug-level logs in a separate file.
     - Progress spinners or bars will automatically be disabled in `--quiet` mode or in non-TTY environments, improving automation or scripting contexts.  

### Benefits  
- Ensures thread-safe operations for progress/stateful components.  
- Eliminates the challenges of abrupt program termination with better error propagation.  
- Decouples user-facing CLI outputs from structured logs, enhancing clarity for end users and developers.  
- Enhances testing by enabling mock logging destinations and removing reliance on global states.  
- Sets the foundation for integrating with structured log analysis or monitoring tools in the future.  

","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision 
## Decision

Zarf will introduce an explicit `v1` schema to ensure a stable and well-defined structure that users can rely on. This schema will establish a contract with users that guarantees long-term support for all fields and features within the `v1` schema. The following actions will be taken as part of this transition:

1. **Removal of Deprecated Keys**:
   - All deprecated keys that have been marked for removal in `v1` will be permanently removed in this schema.
   - This includes keys such as `setVariable`, `scripts`, `group`, and `cosignKeyPath`.

2. **Static Schema Definition**:
   - A well-documented and explicitly versioned `v1` schema will be established and maintained. 
   - Any changes to the schema will require careful consideration and follow proper deprecation processes.

3. **Backward Compatibility via Warnings**:
   - Any additional schema changes in the future that require deprecation will follow structured deprecation notices within the `v1` schema lifecycle.
   - Deprecated fields will remain functional but will be flagged with warnings during use to inform users of their upcoming removal in a future major schema version.

4. **Strict Enforcement**:
   - When the `v1` schema is implemented, Zarf will reject configurations that do not adhere to the schema, ensuring invalid fields or structures are not unintentionally utilized.

5. **Automated Migration Tools**:
   - Automated tools and commands will be provided to help users migrate configurations to the `v1` schema, minimizing friction during the upgrade.

6. **Documentation**:
   - Full documentation of the `v1` schema will be included, providing clear guidance on supported features, deprecated keys, migration strategies, and examples for compliant usage.

The introduction of `v1` schema establishes trust and predictability for Zarf users. It ensures a structured and smooth path to maintaining compatibility while providing a mechanism to sunset legacy features in a transparent way.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
