context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision

We will implement native lazy loading for images and thumbnails in the Storefront. This approach leverages the `loading=""lazy""` attribute, which is now supported by most modern browsers. By using this native feature, we can improve page load times and reduce initial data usage without the need for a third-party extension.

### Rationale
- **Performance Improvement**: Native lazy loading will only load images as they enter or are about to enter the viewport, reducing the initial page load time and improving the overall performance of the Storefront.
- **Simplicity**: Using a built-in browser feature eliminates the need for additional dependencies and reduces the complexity of the codebase.
- **Browser Support**: The `loading=""lazy""` attribute is supported by all major browsers, including Chrome, Firefox, Safari, and Edge. For older or unsupported browsers, images will fall back to their default loading behavior, ensuring that the Storefront remains functional.
- **Cost-Effective**: Implementing native lazy loading is a cost-effective solution as it does not require any additional licensing or maintenance costs associated with third-party extensions.

### Implementation Steps
1. **Update HTML Templates**: Add the `loading=""lazy""` attribute to all `<img>` tags in the Storefront's HTML templates.
2. **Testing**: Conduct thorough testing across different browsers and devices to ensure that the lazy loading works as expected and does not introduce any new issues.
3. **Fallback Mechanism**: Ensure that a fallback mechanism is in place for browsers that do not support the `loading=""lazy""` attribute.
4. **Documentation**: Update the developer documentation to reflect the changes and provide guidance on how to use the new lazy loading feature.

### Monitoring and Evaluation
- **Performance Metrics**: Monitor the impact of lazy loading on page load times and user experience using tools like Lighthouse and Google Analytics.
- **User Feedback**: Collect feedback from users to ensure that the change has a positive impact on their experience.
- **Ongoing Support**: Continuously monitor browser updates and adjust the implementation if necessary to take advantage of new features or address any deprecated functionality.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision

We will deprecate and eventually remove the `autoload` parameter for `OneToOneAssociationField` and `ManyToOneAssociationField` associations. Instead, we will encourage the use of explicit loading strategies such as lazy loading or eager loading based on the specific needs of each query.

### Rationale:
- **Performance Optimization**: By removing the `autoload` feature, we prevent unnecessary data transfer, reduce the number of SQL joins, and minimize the overhead of hydrating and processing unused data.
- **Flexibility**: Explicit loading strategies allow developers to fine-tune their queries, ensuring that only the necessary data is loaded, which can lead to more efficient and performant applications.
- **Best Practices**: Lazy loading (loading associated data only when needed) and eager loading (loading associated data in a single query) are well-established patterns in ORM (Object-Relational Mapping) that provide better control over data retrieval and performance.

### Implementation Steps:
1. **Deprecation Notice**: Add a deprecation notice to the documentation and codebase for the `autoload` parameter, indicating that it will be removed in a future release.
2. **Documentation Update**: Update the documentation to guide users on how to use lazy loading and eager loading effectively.
3. **Code Refactoring**: Provide tools and scripts to help users migrate their existing code to use the new loading strategies.
4. **Removal Timeline**: Set a clear timeline for the removal of the `autoload` parameter, allowing users sufficient time to update their applications.
5. **Testing and Validation**: Ensure that the changes do not break existing functionality and that the new loading strategies meet the performance and usability requirements.

By making this change, we aim to improve the overall performance and maintainability of the application.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision
To address the different approaches to searching for ADRs and improve the overall user experience, we will implement a multi-faceted solution that includes both organizational and technological improvements:

1. **Maintain Area-Based Folders**: Continue to organize ADRs into area-based folders to facilitate easy access for users looking for ADRs in specific areas (case 3).

2. **Implement a Centralized Index File**: Create a centralized index file (e.g., `adr-index.md`) at the root of the ADR repository. This file will list all ADRs with their titles, brief descriptions, and links to the full ADR documents. The index will be updated whenever a new ADR is added or an existing one is modified.

3. **Add a Search Tool**: Integrate a search tool or script (e.g., using a simple Python or JavaScript script) that allows users to search for ADRs by keyword, title, or content. This tool will provide a more refined search experience and reduce duplicate matches.

4. **Automate the Latest ADRs List**: Implement a script that automatically generates a list of the latest ADRs (e.g., the last 10 ADRs) and updates it regularly. This list can be included in the `adr-index.md` file or in a separate `latest-adrs.md` file.

5. **Enhance Git History Access**: Provide clear documentation on how to access the git history for those who prefer to use shell or IDE. Additionally, consider integrating a web-based interface (e.g., a GitHub Action or a custom web app) that allows users to view the commit history and latest ADRs directly from a web browser.

By implementing these changes, we aim to support all the different use cases for searching and accessing ADRs, making the process more efficient and user-friendly.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision

Given the context and the challenges identified, we have decided to **not proceed with the proposed native Vue 3 approach for the plugin extension system**. Instead, we will continue to use the existing Component Factory and Twig.JS template compilation, while exploring alternative solutions that can address the limitations without compromising on the update safety and maintainability of the system.

### Rationale

1. **Update Safety**: The proposed solution relies on internal Vue logic, which is not guaranteed to be stable across Vue updates. This could lead to significant maintenance overhead and potential breakages in the future.

2. **Complexity in Data Merging**: The difficulty in merging data and passing it to the block system would require extensive custom workarounds, which could introduce bugs and reduce the overall quality of the system.

3. **Performance and Tooling**: While the current approach has some performance and tooling limitations, it provides a more stable and predictable environment for plugin developers. The benefits of using native Vue tools and better performance are outweighed by the risks and complexities introduced by the new approach.

### Next Steps

1. **Evaluate Alternative Solutions**: We will explore other ways to improve the current system, such as:
   - Investigating if there are any updates or changes in Vue 3 that can be leveraged to enhance the current system.
   - Researching third-party libraries or tools that can provide better static analysis and linting capabilities for our existing setup.

2. **Incremental Improvements**: We will focus on making incremental improvements to the current system, such as:
   - Optimizing the performance of the Component Factory and Twig.JS template compilation.
   - Enhancing the documentation and tooling support for plugin developers to make the current system more user-friendly.

3. **Community Feedback**: We will gather feedback from the community and plugin developers to understand their pain points and requirements better. This will help us prioritize the improvements and ensure that they align with the needs of our users.

By taking these steps, we aim to balance the need for a robust and flexible plugin extension system with the practical constraints of maintaining a stable and update-safe codebase.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision
We will adopt a decentralized changelog approach, where each pull request (PR) or feature branch includes its own changelog entry. This method aligns better with trunk-based development and reduces the issues identified in the current workflow. Hereâ€™s how it will work:

1. **Changelog Entries per PR/Feature Branch:**
   - Each developer will be responsible for adding a changelog entry to their PR or feature branch. The entry should include a summary of the changes, any breaking changes, and upgrade instructions if necessary.
   - Changelog entries will be stored in a `changelogs` directory within the repository, with each entry named after the PR or feature branch (e.g., `changelogs/PR-123.md`).

2. **Automated Changelog Aggregation:**
   - A script or CI/CD pipeline step will aggregate all the changelog entries from merged PRs into a single, versioned changelog file during the release process.
   - This aggregated changelog will be included in the release notes and tagged with the appropriate version number.

3. **Versioning and Tagging:**
   - The version number will be determined by the release process, ensuring that the changelog entries are associated with the correct version.
   - Tags will be created in the repository to mark the release, and the aggregated changelog will be updated accordingly.

4. **Jira Integration:**
   - To address the discrepancy between Jira and code state, we will integrate Jira with our CI/CD pipeline. This integration will automatically update the Jira issue status based on the PR's merge status.
   - Developers will be required to link their PRs to the corresponding Jira issues, ensuring that the Jira board accurately reflects the state of the codebase.

5. **Documentation and Training:**
   - We will provide clear documentation and training sessions for developers to ensure they understand the new changelog process.
   - Best practices and templates for writing changelog entries will be provided to maintain consistency.

By adopting this decentralized and automated approach, we aim to reduce merge conflicts, ensure accurate versioning, and maintain a consistent state between Jira and the codebase.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision

To address the issues with the `DelayAction` in Flow Builder, we will implement a mechanism to ensure that the data and rules are up-to-date when the delayed action is executed. The solution involves the following steps:

1. **Data Snapshot on Delay**: When an action is set to be delayed, we will create a snapshot of the current state of the data. This snapshot will be stored along with the delayed action. This ensures that the data used in the action is consistent with the state at the time the delay was initiated.

2. **Rule Versioning and Re-evaluation**:
   - **Versioning**: Each rule will have a version number. When a rule is updated or deleted, its version number will change.
   - **Re-evaluation on Execution**: When the delayed action is executed, we will re-fetch the latest version of the relevant rules. If the rule has been updated, the new version will be used. If the rule has been deleted, the action will either fail gracefully or be skipped, depending on the configuration.

3. **Data Refresh Mechanism**:
   - **Data Refresh API**: Implement an API that can refresh the data snapshot with the latest data from the source. This API will be called before the delayed action is executed to update the snapshot with the most recent data.
   - **Conditional Data Refresh**: Provide a configuration option to decide whether the data should be refreshed before executing the delayed action. This allows for flexibility based on the use case.

4. **Error Handling and Logging**:
   - **Error Handling**: Implement robust error handling to manage cases where the data or rules cannot be refreshed or reloaded. This includes logging the errors and providing fallback mechanisms.
   - **Logging**: Log the execution of the delayed action, including any data and rule updates, to help with debugging and auditing.

By implementing these steps, we ensure that the `DelayAction` in Flow Builder operates with the most up-to-date and consistent data and rules, thereby reducing the risk of using outdated information.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision

We will integrate the app system directly into the Shopware 6 platform as a core feature, rather than maintaining it as a separate plugin. This integration will be done in a way that ensures compatibility and consistency across both cloud and on-premises environments. 

### Rationale
- **Stability and Reliability**: By integrating the app system into the core, we signal to partners and potential app manufacturers that the app system is stable and a key part of the platform.
- **Release Cycle Alignment**: Partners and app manufacturers can rely on the established release cycle and upgrade process of Shopware 6, ensuring they have a predictable and consistent development environment.
- **Simplified User Experience**: Users will no longer need to install an additional plugin to use apps, making the process more seamless and reducing the overhead for them.

### Implementation Steps
1. **Code Integration**: Migrate the app system codebase from the plugin repository to the main Shopware 6 repository.
2. **Testing and Validation**: Conduct thorough testing to ensure the app system works seamlessly within the core platform, including compatibility with existing features and plugins.
3. **Documentation and Communication**: Update documentation to reflect the new integrated app system and communicate the changes to the community, partners, and users.
4. **Release and Support**: Plan the release of the integrated app system in a future version of Shopware 6, ensuring it aligns with the platform's release schedule and support processes.

### Implications
- **Development Effort**: The initial effort to integrate the app system into the core will require significant development and testing resources.
- **User Impact**: Users will benefit from a more streamlined and integrated experience, but there may be a transition period as they adapt to the new system.
- **Community and Partner Engagement**: Clear communication and support will be essential to ensure a smooth transition and to build confidence in the integrated app system.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision
To address the current issues with the `FlowStorer` implementations, we will introduce a more generic and flexible approach. We will create a unified `FlowStorer` interface that all storer implementations will adhere to. This interface will include methods for storing and restoring data, as well as a method to determine if the storer should be applied to a given event.

### Key Changes:
1. **Unified Interface**: Define a common `FlowStorer` interface with the following methods:
   - `boolean shouldStore(Event event)`: Determines if the storer should store the data for the given event.
   - `void store(Event event, Object data)`: Stores the data associated with the event.
   - `Object restore(Event event)`: Restores the data associated with the event (if applicable).

2. **Generic Storer Implementation**: Create a default implementation of the `FlowStorer` interface for scalar values that do not require any specific restore logic. This will reduce boilerplate code for simple cases.

3. **Plugin-Friendly Design**: Allow plugins to easily bypass or extend the behavior of the storer by providing a mechanism to register custom `FlowStorer` implementations. This can be done through a service registry or a similar pattern.

4. **Configuration and Discovery**: Implement a configuration-driven approach to manage which `FlowStorer` implementations are active. This can be done via a configuration file or a dynamic discovery mechanism.

### Benefits:
- **Reduced Boilerplate**: By using a common interface and a default implementation, the amount of boilerplate code required for new storer implementations will be significantly reduced.
- **Flexibility**: The unified interface and plugin-friendly design will make it easier to add, remove, or modify storer implementations without changing the core logic.
- **Maintainability**: A more structured and consistent approach will make the codebase easier to maintain and understand.

### Example:
```java
public interface FlowStorer {
    boolean shouldStore(Event event);
    void store(Event event, Object data);
    Object restore(Event event);
}

public class ScalarFlowStorer implements FlowStorer {
    @Override
    public boolean shouldStore(Event event) {
        // Logic to determine if this storer should store the data
    }

    @Override
    public void store(Event event, Object data) {
        // Logic to store the data
    }

    @Override
    public Object restore(Event event) {
        // Default no-op for scalar values
        return null;
    }
}
```

This decision will streamline the management of `FlowStorer` implementations and make the system more extensible and maintainable.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision
We will replace the current sw-text-editor with a modern, well-maintained, and widely-used WYSIWYG editor such as TinyMCE or CKEditor. These editors are known for their robust feature sets, ease of maintenance, and flexibility in terms of customization and extension. 

### Rationale
- **Stability and Reliability**: Both TinyMCE and CKEditor have a strong track record of stability and reliability, which will help to reduce the number of low-level bugs experienced with the current editor.
- **Ease of Maintenance**: These editors are well-documented and have large, active communities, making it easier for our development team to maintain and update the editor.
- **Feature Set**: They offer a comprehensive set of features out-of-the-box, including rich text editing, media embedding, and collaboration tools, which meet our requirements for a good feature set.
- **Flexibility and Extensibility**: Both editors provide APIs and plugins that allow for easy extension and customization, ensuring that we can adapt the editor to our specific needs.
- **Community and Support**: The active community and commercial support options available for both editors will be beneficial for long-term maintenance and troubleshooting.

### Implementation Plan
1. **Evaluation and Selection**: Conduct a detailed evaluation of TinyMCE and CKEditor to determine which one best fits our specific needs.
2. **Integration**: Integrate the selected editor into the administration system, ensuring that all existing features are supported and any new features are added as needed.
3. **Testing**: Perform thorough testing to ensure that the new editor is stable, reliable, and meets all functional and non-functional requirements.
4. **Training and Documentation**: Provide training and documentation for the development team to ensure they are comfortable with the new editor and can maintain it effectively.
5. **Rollout and Monitoring**: Roll out the new editor to the production environment and monitor its performance and user feedback to make any necessary adjustments.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision
We will update the default salutation setting in Shopware 6 to ""not_specified"" for any cases where a salutation is not provided by the customer or administrator. This change aims to ensure that all users, regardless of their preference or oversight in selection, are addressed appropriately and inclusively within the platform. The ""not_specified"" option will be seamlessly integrated into the system, ensuring it does not disrupt existing user experiences or data processing workflows. Additionally, we will provide clear documentation and guidelines on how this new default can be customized or managed by administrators if needed.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision

We will discontinue the use of `padaliyajay/php-autoprefixer` for prefixing CSS in the storefront. Given that our supported browsers, as defined in `.browserslist`, no longer require most of the vendor prefixes, and considering the performance impact on the `theme:compile` process, this change is necessary to improve both build times and maintainability.

### Rationale:
1. **Performance Improvement**: The removal of the auto-prefixer will significantly reduce the time taken by the `theme:compile` process, which is crucial for our SaaS solution.
2. **Modern Browser Support**: With the update to Bootstrap v5, the need for many vendor prefixes has diminished. The `.browserslist` configuration ensures that we only target modern browsers, making the auto-prefixer redundant.
3. **Maintainability**: The hard-coded nature of `padaliyajay/php-autoprefixer` makes it inflexible and difficult to maintain. By removing it, we can rely on more dynamic and up-to-date tools or methods if needed in the future.
4. **Future-Proofing**: This decision aligns with best practices in modern web development, where vendor prefixes are becoming less necessary due to improved browser standards and support.

### Implementation Steps:
1. **Remove `padaliyajay/php-autoprefixer`** from the project dependencies.
2. **Update Build Process**: Modify the `theme:compile` script to exclude the auto-prefixing step.
3. **Review and Test**: Ensure that the compiled CSS still works correctly across all targeted browsers as defined in `.browserslist`.
4. **Documentation**: Update the project documentation to reflect the changes and the rationale behind them.
5. **Monitor Performance**: After the change, monitor the performance of the `theme:compile` process to ensure the expected improvements are realized.

### Alternatives Considered:
- **Alternative Auto-Prefixers**: Investigating other auto-prefixers that dynamically consider `.browserslist` and are more performant. However, given the reduced need for vendor prefixes, this was deemed unnecessary.
- **Manual Prefixing**: Manually adding any required vendor prefixes, but this is not needed as the current set of supported browsers do not require them.

This decision will streamline our build process, improve performance, and ensure that our CSS remains modern and maintainable.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision
Given the context, we will adopt a policy to avoid using pre-release versions for critical or production dependencies. This decision is based on the following considerations:

1. **Security Reporting**: Pre-release versions, such as `1.9.0-alpha1`, are not properly recognized by npm, which means that security vulnerabilities in these versions may not be reported. This poses a significant risk, especially if the package is used in a production environment.

2. **Stability and Reliability**: Pre-release versions (e.g., alpha, beta, rc) are typically used for testing and may contain bugs or incomplete features. Using them in a production environment can lead to unexpected issues and instability.

3. **Version Management**: To ensure that security patches and updates are applied correctly, we will use only stable, non-pre-release versions. This will help in maintaining a consistent and secure software environment.

### Implementation Steps:
- **Update Dependency Versions**: Ensure that all dependencies in our project are updated to stable, non-pre-release versions.
- **Automated Checks**: Implement automated checks in our CI/CD pipeline to prevent the use of pre-release versions in production.
- **Documentation and Training**: Update our development documentation and provide training to the team on the importance of avoiding pre-release versions in production environments.

By making this change, we aim to improve the security and reliability of our software, ensuring that all security patches and updates are properly applied and reported.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision

To improve the performance of the SQL query that deletes outdated cart entries, we will create a composite index on the `cart` table. The index will include the `created_at` and `updated_at` columns, as these are the fields used in the WHERE clause to filter out the entries that need to be deleted.

The new index will allow the database engine to efficiently locate the rows that match the conditions specified in the DELETE statement, significantly reducing the time required to execute the query.

### Steps to Implement:

1. **Create the Index:**
   - Add a composite index on the `created_at` and `updated_at` columns.
   - The SQL command to create the index will be:
     ```sql
     CREATE INDEX idx_cart_created_updated ON cart (created_at, updated_at);
     ```

2. **Verify the Index Usage:**
   - After creating the index, run the `EXPLAIN` command again to ensure that the index is being used.
   - The `EXPLAIN` output should now show the new index in the `possible_keys` and `key` columns.

3. **Test the Performance:**
   - Execute the DELETE query and measure the execution time to confirm that the performance has improved.
   - Ensure that the query now completes within an acceptable timeframe, ideally much less than 30 seconds.

4. **Monitor and Optimize:**
   - Continuously monitor the performance of the query and the overall database.
   - If necessary, further optimize the index or consider additional indexing strategies based on the specific access patterns and data distribution.

By creating and using this composite index, we expect to significantly reduce the query execution time, thereby improving the overall performance of the system, especially during high traffic periods.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision

We will introduce an ""Experimental Features"" framework to allow for the early release of features in a non-backwards-compatible, feedback-gathering state. This approach will enable us to:

1. **Release Early and Gather Feedback**: Publish features in an initial, potentially incomplete state, clearly marked as experimental. This will allow us to collect user feedback, validate assumptions, and make necessary adjustments before finalizing the feature.
   
2. **Temporary Backwards Compatibility Exemption**: Experimental features will not be subject to our standard backwards compatibility promise. This means we can make significant changes to the feature based on feedback without the constraints of maintaining backwards compatibility.

3. **Clear Communication and Opt-In**: Experimental features will be clearly labeled and communicated to users. Users will need to explicitly opt-in to use these features, acknowledging that they are subject to change and may not be fully stable.

4. **Feedback Loop Integration**: We will establish a structured process for collecting and integrating user feedback into the development process. This will include regular updates and communication with users about the progress and changes to the experimental features.

5. **Phased Rollout and Gradual Stabilization**: As experimental features mature and stabilize, we will gradually transition them to a stable, backwards-compatible status. This will involve a phased rollout, ensuring that the feature is thoroughly tested and refined before it becomes part of our standard offering.

### Implementation Steps

1. **Define Experimental Feature Criteria**: Establish clear criteria for what qualifies as an experimental feature, including the level of completeness and the types of changes that may be made.
   
2. **Develop User Interface and Documentation**: Create a user interface and documentation that clearly identifies experimental features and provides instructions on how to opt-in and provide feedback.

3. **Set Up Feedback Collection Mechanisms**: Implement tools and processes for collecting and analyzing user feedback, such as surveys, user testing sessions, and a dedicated feedback channel.

4. **Monitor and Adjust**: Continuously monitor the performance and feedback of experimental features, making iterative improvements and adjustments as needed.

5. **Transition to Stable Release**: Once an experimental feature has been sufficiently refined and validated, transition it to a stable release, ensuring it meets our backwards compatibility standards.

By adopting this decision, we aim to accelerate the development and delivery of valuable features, while also fostering a more collaborative and responsive relationship with our users.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision

To address the limitations and inflexibility in the current stock handling system of Shopware 6, we will implement a more modular and extensible architecture. The following changes will be made:

1. **Introduce a Stock Provider Interface:**
   - Define a `StockProvider` interface that abstracts the process of fetching and updating stock levels.
   - This interface will include methods for loading stock, increasing stock, and decreasing stock.
   - Implement a default `DefaultStockProvider` that uses the existing database-driven approach.
   - Allow developers to register custom `StockProvider` implementations, such as one that communicates with an ERP system.

2. **Refactor Stock Calculation:**
   - Simplify the stock model by storing only one value: `stock`. 
   - Remove the `available_stock` field to reduce complexity.
   - Update the stock calculation logic to account for open orders at runtime, rather than storing a separate `available_stock` value.
   - Optimize the stock calculation process to handle large catalogs efficiently. This can be achieved by:
     - Using indexed database queries to speed up the retrieval of stock data.
     - Implementing caching mechanisms to store frequently accessed stock data.
     - Utilizing background jobs or asynchronous processing to update stock levels without blocking the main application flow.

3. **Event-Driven Stock Updates:**
   - Introduce events (e.g., `StockIncreased`, `StockDecreased`) that are triggered when stock levels change.
   - Use these events to allow plugins or custom implementations to react to stock changes, such as updating an external system or performing additional business logic.
   - Ensure that stock updates are atomic and consistent, even in high-concurrency scenarios, by using database transactions or other concurrency control mechanisms.

4. **Order Lifecycle Integration:**
   - Modify the order lifecycle to use the `StockProvider` interface for stock adjustments.
   - Ensure that stock is decreased when an order is placed and increased if the order is canceled or returned.
   - Provide hooks or events in the order lifecycle to allow for custom stock adjustment logic, such as partial shipments or backorders.

5. **Documentation and Developer Support:**
   - Provide comprehensive documentation on how to create and register custom `StockProvider` implementations.
   - Include examples and best practices for optimizing stock handling in large catalogs.
   - Offer support and guidance to developers who need to integrate with external systems or implement complex stock management rules.

By implementing these changes, we aim to make the stock handling in Shopware 6 more flexible, efficient, and adaptable to a wide range of use cases.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision
We will implement a logging configuration that differentiates the log level based on the type of exception thrown. Specifically, we will adjust the `symfony/monolog-bridge` configuration to log `ShopwareHttpException` and other client-side error exceptions (those with HTTP status codes in the 40x range) at the `notice` level instead of the `error` level. This change will help in reducing the noise in our logs, making it easier to identify true system errors that require attention.

To achieve this, we will:
1. Update the existing configuration list used for our cloud product to include `ShopwareHttpException` and potentially other similar exceptions.
2. Modify the Monolog configuration to check against this list and set the log level to `notice` for these specific exceptions.
3. Ensure that the new configuration is applied across all environments to maintain consistency in logging behavior.

This approach will ensure that expected client-side issues are logged appropriately, while still capturing and highlighting more critical system errors.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision

We will switch from using UUIDv4 to UUIDv7 as the primary key for our database entities. This change is expected to improve the performance of B-tree index operations by leveraging the time-ordered nature of UUIDv7, which will help in keeping the indexes more compact and efficient. The transition will be planned to ensure minimal disruption to the existing system, with a phased approach to update the primary keys in the database and adjust any dependent systems accordingly.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision
To address the limitation where apps cannot extend the list of available events in the flow builder, we will implement a plugin architecture that allows third-party developers to define and register custom events. This approach will provide flexibility and extensibility to the flow builder, enabling it to support a wider range of use cases.

### Key Points:
1. **Plugin Architecture**: Develop a plugin system that allows for the dynamic registration of custom events.
2. **Event Definition API**: Create an API that third-party developers can use to define new events, including their properties and triggers.
3. **Validation and Security**: Implement validation and security measures to ensure that custom events are safe and do not introduce vulnerabilities.
4. **Documentation and SDK**: Provide comprehensive documentation and an SDK to guide developers on how to create and integrate custom events.
5. **User Interface Integration**: Ensure that the flow builder's user interface is updated to seamlessly include and display custom events alongside the built-in ones.

This decision will enhance the functionality and adaptability of the flow builder, making it more versatile and valuable to users.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision

To enable secure communication between the App and the App Server, we will implement a token-based authentication mechanism. This mechanism will allow the App Server to verify the origin of the request and ensure that it is coming from a trusted source, even when the request is initiated directly by the app.

### Implementation Details:

1. **Token Generation:**
   - The Shopware Backend will generate a unique, time-limited, and signed token for each authenticated user session.
   - This token will be securely transmitted to the app (e.g., via an API call or through a secure channel).
   - The token will include information such as the customer ID, session ID, and other relevant metadata, all encrypted and signed with a secret key known only to the Shopware Backend and the App Server.

2. **Token Transmission:**
   - The app will include this token in the `Authorization` header of each request made to the App Server.
   - The token will be transmitted over HTTPS to ensure its confidentiality and integrity.

3. **Token Verification:**
   - The App Server will verify the token by checking its signature and expiration time.
   - If the token is valid, the App Server will extract the customer ID and other necessary information from the token.
   - If the token is invalid or has expired, the App Server will reject the request.

4. **Token Revocation:**
   - The Shopware Backend will maintain a list of revoked tokens to handle cases where a token needs to be invalidated before its expiration (e.g., if a user logs out or if a security breach is detected).
   - The App Server will check the revocation list as part of the token verification process.

### Benefits:
- **Enhanced Security:** The App Server can now verify the origin of the request, ensuring that it is coming from a trusted source.
- **Reliable User Information:** The App Server can reliably access the logged-in customer's information, enabling more personalized and secure interactions.
- **Scalability:** This solution can be extended to support additional security features, such as role-based access control and fine-grained permissions.

### Considerations:
- **Performance:** Token generation and verification should be optimized to minimize latency.
- **Security:** The secret key used for signing the tokens must be kept secure and rotated periodically.
- **Usability:** The app and backend must handle token management seamlessly to avoid disrupting the user experience.

By implementing this token-based authentication mechanism, we can ensure that the App Server can securely communicate with the app, even when the request is initiated directly by the app.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision
We will implement a **Flow Builder Preview** feature that allows users to visualize and understand the execution path of their flows without actually executing or simulating the flow. This preview will provide a clear, step-by-step representation of the flow, including all steps and decision points, enabling users to better comprehend how their flow is structured and how it would execute in a real scenario.

### Key Features of the Flow Builder Preview:
1. **Step-by-Step Visualization**: The preview will display each step and decision point in the flow, showing the order in which they would be executed.
2. **Conditional Logic Highlighting**: For decision points, the preview will highlight the conditions and the possible paths that can be taken based on those conditions.
3. **Interactive Elements**: Users will be able to interact with the preview to expand and collapse sections, making it easier to navigate complex flows.
4. **Read-Only Mode**: The preview will be read-only, ensuring that users cannot accidentally modify the flow while previewing it.
5. **Real-Time Updates**: Any changes made to the flow will be immediately reflected in the preview, allowing users to see the impact of their modifications in real-time.

### Implementation Approach:
- **UI/UX Design**: The preview will be integrated into the existing Flow Builder interface, providing a seamless experience for users. It will be accessible via a dedicated ""Preview"" button or tab within the Flow Builder.
- **Data Representation**: The preview will use the same data model as the actual flow, but will render it in a visual, non-executable format. This ensures consistency and accuracy in the representation.
- **Performance Considerations**: To ensure the preview remains responsive even for complex flows, we will optimize the rendering process and use lazy loading techniques where necessary.

### Benefits:
- **Enhanced Understanding**: Users will have a clearer understanding of how their flows are structured and how they will execute, reducing the likelihood of errors and misconfigurations.
- **Faster Debugging**: By providing a visual representation of the flow, users can more easily identify potential issues and areas for improvement, leading to faster debugging and troubleshooting.
- **Improved User Experience**: The preview feature will enhance the overall user experience by providing a powerful tool for flow design and validation, making the Flow Builder more intuitive and user-friendly.

### Next Steps:
- **Design and Prototyping**: Create wireframes and prototypes for the Flow Builder Preview feature.
- **Development**: Implement the preview functionality in the Flow Builder.
- **Testing and Feedback**: Conduct user testing to gather feedback and make necessary adjustments.
- **Documentation and Training**: Provide documentation and training materials to help users understand and effectively use the new preview feature.

This decision aligns with our goal of empowering users to build reliable flows and reduces the time and effort required to debug and troubleshoot issues.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â°realÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision

To improve the current media system and ensure a more robust, maintainable, and flexible URL generation process, we will refactor the `UrlGeneratorInterface` and `PathnameStrategyInterface` to decouple the responsibilities of generating URLs and file paths. This will allow for better separation of concerns and easier extension in the future.

### Changes:

1. **Refactor `UrlGeneratorInterface`**:
   - The `UrlGeneratorInterface` will be simplified to focus solely on generating URLs based on the provided path and hash.
   - The `generatePathHash` and `generatePathCacheBuster` methods will be moved to a new interface, `MediaPathGenerationStrategyInterface`, which will handle the generation of the path components.
   - The `generatePhysicalFilename` method will remain in the `PathnameStrategyInterface` as it is specific to the filename generation.

2. **Introduce `MediaPathGenerationStrategyInterface`**:
   - This new interface will be responsible for generating the path hash and cache buster.
   - Implementations of this interface will be used by the `UrlGeneratorInterface` to generate the full URL.

3. **Update Event Subscriber**:
   - The event subscriber that listens to the `media.loaded` event will be updated to use the new interfaces.
   - The `UrlGeneratorInterface` will be injected with an instance of `MediaPathGenerationStrategyInterface` to generate the necessary path components before generating the URL.

4. **Configuration**:
   - The configuration for the media system will be updated to allow for the selection of both a `PathnameStrategyInterface` and a `MediaPathGenerationStrategyInterface`.
   - This will provide flexibility in choosing different strategies for path generation and URL generation independently.

### New Interfaces:

```php
interface MediaPathGenerationStrategyInterface
{
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;
}

interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media, MediaPathGenerationStrategyInterface $strategy): string;

    public function getRelativeMediaUrl(MediaEntity $media, MediaPathGenerationStrategyInterface $strategy): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail, MediaPathGenerationStrategyInterface $strategy): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail, MediaPathGenerationStrategyInterface $strategy): string;
}
```

### Example Implementation:

```php
class DefaultUrlGenerator implements UrlGeneratorInterface
{
    private PathnameStrategyInterface $pathnameStrategy;

    public function __construct(PathnameStrategyInterface $pathnameStrategy)
    {
        $this->pathnameStrategy = $pathnameStrategy;
    }

    public function getAbsoluteMediaUrl(MediaEntity $media, MediaPathGenerationStrategyInterface $strategy): string
    {
        $pathHash = $strategy->generatePathHash($media);
        $cacheBuster = $strategy->generatePathCacheBuster($media);
        $filename = $this->pathnameStrategy->generatePhysicalFilename($media);

        // Construct the full URL
        return ""https://example.com/media/{$pathHash}/{$cacheBuster}/{$filename}"";
    }

    // Other methods...
}
```

### Benefits:
- **Separation of Concerns**: The responsibilities of generating paths and URLs are clearly separated.
- **Flexibility**: Different strategies can be easily swapped or extended.
- **Maintainability**: The codebase becomes more modular and easier to maintain.

This decision will enhance the extensibility and maintainability of the media system while ensuring that the URL generation process remains robust and flexible.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision
We will implement a dedicated one-time data migration service to handle version-specific data updates. This service will be triggered only once during the upgrade process and will not be coupled with the indexer. The steps for this decision are as follows:

1. **Create a One-Time Migration Service:**
   - Develop a new service that encapsulates the logic for the one-time data migration.
   - Ensure this service is idempotent, meaning it can be safely executed multiple times without causing issues.

2. **Trigger the Migration via Upgrade Process:**
   - Integrate the one-time migration service into the Shopware upgrade process.
   - Use a flag or a database entry to track whether the migration has already been executed.
   - If the migration has already been executed, skip it in subsequent upgrades.

3. **Decouple from Indexer:**
   - Remove the one-time migration logic from the indexer.
   - Ensure the indexer continues to function as intended without the one-time migration logic.

4. **Testing and Validation:**
   - Thoroughly test the one-time migration service to ensure it handles all edge cases and does not cause system errors.
   - Validate that the migration is only executed once and does not interfere with the normal operation of the indexer.

5. **Documentation:**
   - Document the new one-time migration service and its integration into the upgrade process.
   - Provide clear instructions for developers on how to add new one-time migrations if needed in the future.

By implementing this dedicated one-time migration service, we can ensure that critical data migrations are handled correctly and do not cause issues when the indexer is executed.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision
To address the challenges related to codebase fragmentation and the management of experimental features, we will implement a structured tagging and documentation system. This system will help in clearly identifying and managing experimental feature code, making it easier to update, deprecate, and track these features throughout their lifecycle.

### Key Components of the Decision:

1. **Feature Tagging:**
   - **Tag Naming Convention:** Establish a consistent naming convention for tags that includes the feature name and a version identifier (e.g., `feat-experimental-feature-v1.0`).
   - **Code Comments:** Add comments in the code to indicate the start and end of experimental feature code blocks, along with the corresponding tag.
   - **Git Tags:** Use Git tags to mark specific commits related to the introduction or significant updates of experimental features.

2 . **Centralized Feature Documentation:**
   - **Feature Registry:** Create a centralized feature registry (e.g., a markdown file or a dedicated database) that documents each experimental feature, including its description, status (active, extended, deprecated), and associated tags.
   - **Documentation Updates:** Ensure that the feature registry is updated whenever changes are made to an experimental feature, such as extending the experiment or deprecating the feature.

3. **Automated Tooling:**
   - **Code Analysis Tools:** Implement or configure code analysis tools to automatically detect and report on the presence of experimental feature tags and their associated code blocks.
   - **Version Update Script:** Develop a script or tool that can automatically locate and update the `stableVersion` property in `@experimental` annotations when an experiment is extended.
   - **Deprecation Helper:** Create a tool or script that can identify and mark components associated with a deprecated experimental feature, ensuring they are appropriately documented and flagged in the codebase.

4. **Development Workflow Enhancements:**
   - **Branching Strategy:** Adopt a branching strategy that includes branches specifically for experimental features, allowing for better isolation and tracking.
   - **Pull Request Templates:** Introduce pull request templates that require developers to specify the experimental feature and any relevant tags, ensuring that changes are properly documented and reviewed.

5. **Training and Onboarding:**
   - **Developer Training:** Provide training sessions and documentation for developers to understand the new tagging and documentation system, ensuring consistent adoption across the team.
   - **Onboarding Process:** Update the onboarding process for new developers to include information about the experimental feature management system.

By implementing this structured approach, we aim to improve the maintainability and traceability of our experimental features, making it easier to manage updates, deprecations, and overall codebase organization.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision
To address the challenge of uniquely identifying payment and shipping methods, we will introduce a standardized and unique identifier system for both payment and shipping methods. This system will be designed to be easily accessible and consistent across the platform, reducing the need for extension developers and app servers to make additional calls to the Shopware Admin API.

### Key Points of the Decision:
1. **Unique Identifiers**: Each payment and shipping method will be assigned a unique, immutable identifier (UUID) that will be generated upon creation and remain constant throughout the lifecycle of the method.
2. **API Enhancements**: The Shopware Admin API will be updated to include endpoints that allow retrieval of payment and shipping methods by their unique identifiers. This will enable app servers and extension developers to fetch the necessary information with a single, efficient API call.
3. **Documentation and Best Practices**: We will provide comprehensive documentation and best practices for using these unique identifiers, ensuring that developers understand how to integrate and utilize them effectively.
4. **Backward Compatibility**: To ensure backward compatibility, the existing ID-based system will be maintained, and the new unique identifier system will be introduced as an optional but recommended approach. Over time, we will encourage the transition to the new system through deprecation notices and support.

This decision aims to streamline the process of identifying payment and shipping methods, improving the efficiency and reliability of the integration process for extension developers and app servers.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision

We will adopt a strategy that prioritizes the use of Bootstrap utility classes for common styling needs such as spacing, margins, and padding. This approach will help in reducing the amount of custom SCSS code, making the project more maintainable and aligning with best practices for using utility-first CSS frameworks like Bootstrap.

### Key Points:
1. **Use Bootstrap Utility Classes**: For simple styling tasks, such as adding margins, padding, or borders, we will use Bootstrap's built-in utility classes. This will reduce the need for custom SCSS and ensure consistency across the application.
   
2. **Semantic Class Names**: We will continue to add semantic class names (e.g., `register-login-collapse-toogle`, `shipping-modal-actions`, `shipping-abort`, `shipping-submit`) to HTML elements. These classes will not have any default styles but will be available for theme-specific or future customizations.
   
3. **Custom SCSS for Complex Layouts**: Custom SCSS will still be used for more complex layouts or when it is not feasible to achieve the desired design with utility classes alone. This ensures that the templates remain clean and maintainable.

4. **Documentation and Guidelines**: We will update our documentation to include guidelines on when to use utility classes and when to write custom SCSS. This will help developers make informed decisions and maintain consistency in the project.

By following this decision, we aim to streamline the development process, reduce the overhead of maintaining custom SCSS, and improve the overall maintainability of the Storefront.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision
We will adopt a new end-to-end (E2E) testing strategy and framework that addresses the current limitations of our Cypress-based test suite. The following decisions will guide the transformation:

1. **Framework Selection:**
   - **Cypress with Best Practices:** We will continue to use Cypress, but we will implement best practices to improve the reliability and maintainability of our tests. This includes:
     - **Isolating Tests from Environment State:** We will refactor our tests to minimize dependencies on the state of the test environment. This can be achieved by using fixtures, stubs, and mocks where appropriate.
     - **Page Object Model (POM):** We will adopt the Page Object Model to encapsulate the interactions with the UI. This will make our tests more readable, maintainable, and aligned with the application's structure.
     - **Test Data Management:** We will implement a robust test data management strategy to ensure that tests are deterministic and not dependent on the state of the test environment.

2. **Test Strategy:**
   - **Behavior-Driven Development (BDD):** We will adopt BDD to ensure that our tests are derived from real product requirements and validate behavior. This will involve writing tests in a way that reflects the expected behavior of the application, making it easier for non-technical stakeholders to understand and contribute.
   - **Test Scenarios and Steps:** We will define clear test scenarios and steps that are based on user stories and acceptance criteria. This will help in creating a more structured and consistent approach to writing tests.

3. **Environment Agnostic Testing:**
   - **Configuration Management:** We will implement a configuration management system that allows us to run the same test suite against different environments (e.g., local, staging, production). This will involve using environment variables and configuration files to manage different settings.
   - **Cloud Environment Support:** We will ensure that our test suite is capable of running against our cloud environment. This may require additional setup, such as configuring access to cloud resources and handling cloud-specific configurations.

4. **Performance and Reliability:**
   - **Optimization Techniques:** We will apply optimization techniques to make our tests faster and more reliable. This includes:
     - **Parallel Test Execution:** We will enable parallel test execution to reduce the overall test run time.
     - **Retry Mechanism:** We will implement a retry mechanism for flaky tests to reduce false negatives.
     - **Headless Browsers:** We will use headless browsers for faster test execution, especially in CI/CD pipelines.

5. **Ease of Use and Readability:**
   - **Readable Syntax:** We will ensure that our tests are written in a readable and understandable syntax. This will make it easier for both technical and non-technical team members to comprehend and contribute to the test suite.
   - **Documentation and Training:** We will provide comprehensive documentation and training sessions to help the team get up to speed with the new testing framework and best practices.

By implementing these changes, we aim to create a more robust, reliable, and maintainable E2E test suite that supports our goals of reducing manual testing and improving the overall quality of our software.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision
To address the issues identified in the context, we will implement a more robust transaction management strategy for flow actions. The following changes will be made:

1. **Centralized Transaction Management**: 
   - Introduce a centralized transaction management service that will handle the start, commit, and rollback of transactions. This service will be responsible for ensuring that all database interactions within a flow action are transactional.
   - Flow actions will no longer need to manage transactions themselves. Instead, they will interact with the new transaction management service.

2. **Error Handling and Propagation**:
   - When a transaction fails to commit, the error will not be ignored. Instead, the error will be propagated to the flow dispatcher, which will then handle it appropriately.
   - The flow dispatcher will log a detailed error message and, if necessary, roll back the entire transaction to ensure data consistency. This will prevent the connection from being marked as ""rollback only"" and avoid subsequent commit failures.

3. **Nested Transactions Support**:
   - Ensure that the transaction management service supports nested transactions. If a nested transaction fails, the outer transaction should be aware of this and handle it accordingly.
   - If the database connection does not support savepoints, the transaction management service will emulate nested transactions by managing the state of the transaction and rolling back the entire transaction if any part of it fails.

4. **Logging and Monitoring**:
   - Enhance logging to include detailed information about transaction states and any errors that occur during the transaction process.
   - Implement monitoring to track and alert on transaction failures, ensuring that issues are identified and addressed promptly.

By implementing these changes, we will ensure that flow actions can interact with the database in a transactional manner without the risk of silent failures or inconsistent data states.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision

We will keep the Vue 3 compatibility mode enabled in the administration for the current major release cycle. This decision is made to facilitate a smoother transition for plugin developers, allowing them to adapt their plugins to Vue 3 at a more manageable pace. By enabling compatibility mode, we can reduce the immediate impact of breaking changes, thus splitting the migration effort into two phases: first, the update of the core administration, and second, the subsequent updates of the plugins. This approach ensures that both the core and the ecosystem can evolve with less disruption. However, it is critical to set a clear timeline and plan for deprecating and eventually removing the compatibility mode in future releases to fully leverage the new features and performance improvements of Vue 3.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision
We will not downgrade to Vuex `4.0.2` due to the known issues with module failures. Instead, we will:

1. Temporarily pin our dependency to the last stable version of Vuex that does not contain the reactivity bug, if such a version is available.
2. If no suitable version is found, we will maintain our current version and implement a workaround for the reactivity bug in our codebase.
3. We will closely monitor the Vuex project for updates and patches that address the reactivity issue. Once a new version is released that fixes the bug, we will update our dependency to that version.
4. In parallel, we will consider evaluating alternative state management solutions (e.g., Pinia) to ensure we have a robust and reliable state management system in place.

This approach allows us to avoid the immediate issues while keeping an eye on long-term stability and future-proofing our application.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision

To address the challenges posed by the current heavy reliance on PHP decoration, Adapter, and Factory patterns, we will adopt a more modular and flexible approach using a combination of the following strategies:

1. **Service-Oriented Architecture (SOA) with Microservices:**
    - Transition to a service-oriented architecture where each core functionality is encapsulated in a microservice. This approach allows for better isolation of changes, making it easier to maintain backward and forward compatibility. Each microservice can be versioned independently, reducing the risk of breaking existing extensions.

2. **Event-Driven Architecture (EDA):**
    - Implement an event-driven architecture to handle subprocesses and extensions. By decoupling processes through events, third-party developers can extend or modify behavior without altering the core code. This approach reduces the need for extensive interface and abstract class proliferation.

3. **Domain-Driven Design (DDD):**
    - Adopt Domain-Driven Design principles to better organize the codebase around business domains. This will help in creating a more coherent and understandable structure, reducing the cognitive load on developers. Bounded contexts within DDD will help in managing dependencies and ensuring that changes in one part of the system do not unintentionally affect others.

4. **Dependency Injection (DI) and Inversion of Control (IoC):**
    - Leverage Dependency Injection and Inversion of Control to manage dependencies more effectively. This will make the code more testable and maintainable, and will allow for easier swapping of implementations without changing the core logic.

5. **Plugin/Module System:**
    - Develop a robust plugin or module system that allows third-party developers to add or modify functionality without directly modifying the core code. This system should include a well-defined API and clear extension points, enabling developers to hook into the application at specific points.

6. **Versioning and Semantic Versioning:**
    - Implement strict versioning and adhere to semantic versioning practices. This will help in managing changes and ensuring that third-party developers can understand and adapt to new versions without breaking their existing work.

By adopting these strategies, we aim to create a more maintainable, extensible, and scalable architecture that addresses the challenges of backward and forward compatibility, process extension, and code proliferation.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision

We will introduce a new Jest runner configuration that disables the compat mode for running component tests. This new configuration will be used specifically for components where we want to ensure that the tests are executed without the legacy compatibility settings. 

### Implementation Steps:
1. **Create a New Jest Configuration File**: We will create a new Jest configuration file, e.g., `jest.config.no-compat.js`, which will include the necessary settings to disable the compat mode.
2. **Update Test Scripts**: Modify the test scripts in the `package.json` or the build tool configuration to use the new Jest configuration when running specific component tests.
3. **Gradual Migration**: Migrate components to the new configuration one by one, ensuring that all tests pass and the behavior remains consistent.
4. **Documentation**: Update the testing documentation to include instructions on how to use the new Jest runner and the benefits of disabling the compat mode.

### Benefits:
- **Future-Proofing**: Ensuring that our tests are aligned with the latest standards and practices, making it easier to adopt future updates.
- **Performance**: Potentially improving the performance of the tests by removing unnecessary compatibility layers.
- **Consistency**: Maintaining consistency in the testing environment, which can help in identifying and fixing issues more efficiently.

### Risks and Mitigations:
- **Breaking Changes**: Disabling the compat mode may introduce breaking changes in some tests. We will mitigate this by migrating components gradually and thoroughly testing each one.
- **Complexity**: Adding a new configuration might increase the complexity of the testing setup. We will document the process and provide clear guidelines to minimize confusion.

This decision will help us move towards a more modern and efficient testing environment while maintaining the integrity and reliability of our component tests.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision

We will implement a comprehensive system health check mechanism that allows for the continuous monitoring of all critical components of our software system. This health check system will be designed to provide real-time feedback on the operational status of each component, and it will be capable of detecting both complete failures and degraded performance. The health checks will be integrated into our deployment pipeline, ensuring that any issues are identified before the system goes live. Additionally, we will establish a post-deployment monitoring phase where the health of the system is closely monitored to catch any issues that may only appear in a live environment. This approach will help us to proactively address potential problems, reduce downtime, and maintain a high level of service availability.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision

We will expand the `TestPreparationStartedSubscriber` to allow additional namespaces, beyond just `Shopware\Tests\Unit`, to be considered for enabling the major feature in the unit tests suite. This change will support a broader range of testing scenarios and ensure that all relevant test cases, regardless of their namespace, can take advantage of the major feature during the execution of the unit tests. 

To implement this, we will modify the `TestPreparationStartedSubscriber` to include a configurable list of allowed namespaces. This list can be set via an environment variable or configuration file, allowing for flexibility and easy updates. Additionally, we will update the documentation to reflect this new capability and provide clear instructions on how to configure the allowed namespaces.

This approach ensures that the `TestPreparationStartedSubscriber` remains flexible and can adapt to future changes in the testing structure without requiring further code modifications.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision

We will adopt the `vue-codemod` tool to automate the migration of our Vue 2 Options API components to the Vue 3 Composition API. This tool provides a set of codemods that can transform the existing codebase, reducing the manual effort required and minimizing the risk of errors.

### Rationale

- **Efficiency**: Automating the migration process with `vue-codemod` will significantly reduce the time and effort needed to convert our components.
- **Consistency**: The tool ensures that the transformation is consistent across all components, maintaining a uniform code style.
- **Error Reduction**: By using an automated tool, we minimize the risk of human error during the conversion.
- **Community Support**: `vue-codemod` is a well-maintained tool with community support, which means it is likely to be updated and improved over time.
- **Flexibility**: While the tool automates much of the process, it still allows for manual adjustments where necessary, ensuring that the final code meets our specific requirements.

### Implementation Steps

1. **Install `vue-codemod`**: Add the `vue-codemod` tool to our development environment.
2. **Run the Codemod**: Execute the codemod scripts on our existing Vue 2 components to convert them to the Composition API.
3. **Review and Refine**: Manually review the transformed components to ensure they meet our quality standards and make any necessary adjustments.
4. **Testing**: Thoroughly test the migrated components to ensure they function as expected and integrate correctly with the rest of the application.
5. **Documentation**: Update our internal documentation to reflect the changes and provide guidelines for future development using the Composition API.

By following these steps, we can efficiently and effectively migrate our Vue.js application to the Composition API, taking full advantage of the new features and improvements offered by Vue 3.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¯s templating and reactivity model.","## Decision
We will replace the current TwigJs block system with a native Vue.js approach by introducing two new components: `sw-block` and `sw-block-parent`. This decision is based on the following considerations:

1. **Modern Frontend Development**: Vue.js provides a more dynamic and reactive component-based architecture, which is better aligned with modern frontend development practices. This will allow for a more seamless and efficient user experience.

2. **Reusability and Extensibility**: The `sw-block` and `sw-block-parent` components will enable plugin developers to override or extend content in predefined blocks, similar to the functionality provided by TwigJs. However, this will be done within the context of Vue's templating and reactivity model, offering enhanced flexibility and maintainability.

3. **Component-Based Architecture**: By leveraging Vue's component-based architecture, we can ensure that the new block system is modular and easier to manage. This will also facilitate better separation of concerns and improve the overall code quality.

4. **Dynamic Content Management**: The new components will support dynamic content overriding and extension, allowing for more flexible and interactive UIs. This will enable developers to create more engaging and responsive user interfaces.

5. **Ecosystem and Community Support**: Vue.js has a robust ecosystem and a large community, which means better support, more resources, and a wider range of tools and libraries to enhance our development process.

### Implementation Plan

1. **Design and Develop Components**:
   - Design the `sw-block` and `sw-block-parent` components to replicate and improve upon the existing TwigJs block system.
   - Ensure that these components provide the necessary hooks and slots for dynamic content overriding and extension.

2. **Migration Strategy**:
   - Develop a migration plan to transition existing TwigJs templates to the new Vue.js components.
   - Provide documentation and examples to guide plugin developers through the migration process.

3. **Testing and Validation**:
   - Conduct thorough testing to ensure that the new components function as expected and do not introduce any regressions.
   - Validate the new system with a subset of existing plugins before a full rollout.

4. **Documentation and Training**:
   - Update the developer documentation to include detailed information about the new `sw-block` and `sw-block-parent` components.
   - Offer training sessions or webinars to help developers understand and adopt the new system.

By making this transition, we aim to leverage the strengths of Vue.js to create a more robust, flexible, and maintainable block system.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â€ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision

We will remove the asterisk `*` from all product prices in the default Storefront and instead display the tax and shipping costs information more clearly and contextually. Specifically, we will implement the following changes:

1. **Remove Asterisk from Prices**: The asterisk `*` will be removed from all price displays, making the prices cleaner and more straightforward for users.

2. **Display Tax Information Inline**: We will add a small, unobtrusive note next to each price that states ""incl. VAT"" or ""excl. VAT"" depending on the configuration. This will provide immediate clarity on whether the displayed price includes tax.

3. **Shipping Costs Information**: We will include a clear and concise message about shipping costs at a more prominent location, such as below the product price or in a dedicated section on the product page. For example, ""Shipping costs will be calculated at checkout.""

4. **Footer Update**: The footer will still contain a comprehensive statement about tax and shipping, but it will be rephrased to be more user-friendly and less dependent on the asterisk. For example: ""All prices include VAT. Shipping costs and possible delivery charges are calculated at checkout.""

5. **User Experience Enhancements**: To ensure that users do not miss this important information, we will also consider using tooltips or pop-ups that appear when the user hovers over or clicks on the price or the tax/shipping information.

By implementing these changes, we aim to improve the user experience by providing clearer and more contextual information, reducing confusion, and making the pricing more transparent.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision

We will migrate from DomPDF to a more A11y-compliant PDF generation library, such as PDF.co or FPDF2, which support the creation of accessible PDFs. Additionally, we will implement a process to ensure that all generated PDFs are tagged and structured correctly, including the use of headings, lists, and alternative text for images. This will involve:

1. **Library Selection**: Evaluate and select a new PDF generation library that supports accessibility features.
2. **Implementation**: Integrate the new library into our existing codebase, ensuring that all generated documents are compliant with WCAG standards.
3. **Testing**: Conduct thorough testing to ensure that the new PDFs are fully accessible, using tools like Adobe Acrobat's Accessibility Checker and involving users with disabilities in the testing process.
4. **Training and Documentation**: Provide training and documentation for developers to understand and implement the necessary changes for generating accessible PDFs.
5. **Ongoing Compliance**: Establish a process for regular audits and updates to ensure ongoing compliance with WCAG and other relevant accessibility standards.

This decision ensures that Shopware 6's document solution not only meets legal requirements but also provides a better user experience for all customers, including those with disabilities.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision
To address the issues with the existing rule system, we will implement a more structured and controlled approach to rule extensions. This involves:

1. **Defining a Clear Rule Interface**: We will define a clear and concise interface for rules that all rule classes must adhere to. This will standardize the way rules are implemented and used, making it easier to understand and maintain.

2. **Introducing a Rule Factory**: A factory pattern will be introduced to manage the creation of rule instances. This factory will enforce the use of the defined rule interface and provide a centralized point for managing and instantiating rule classes.

3. **Restricting Direct Rule Class Extensions**: To prevent the unrestricted extension of rule classes, we will restrict direct subclassing. Instead, developers will be required to register their custom rules through the rule factory. This ensures that all custom rules are properly vetted and integrated into the system in a controlled manner.

4. **Documentation and Best Practices**: We will provide comprehensive documentation and best practices for extending and using the rule system. This will help guide developers on how to create and integrate custom rules without compromising the system's integrity.

5. **Review and Approval Process**: Implement a review and approval process for new rule extensions. This process will ensure that any new rules meet the required standards and do not introduce unnecessary complexity or performance issues.

By implementing these changes, we aim to reduce the complexity of the rule system, improve maintainability, and streamline the process of adding new rules.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision

We will deprecate and eventually remove the `src/helper/iterator.helper.js` in favor of using native JavaScript iteration methods (e.g., `forEach`, `for...of`, `map`, etc.). This decision is based on the following considerations:

1. **Simplicity and Clarity**: By removing the unnecessary abstraction, we simplify the codebase. Developers can use well-known and widely understood native iteration methods, which are already part of the JavaScript standard and do not require additional documentation.

2. **Performance and Efficiency**: Native iteration methods are optimized by browser engines and generally perform better than custom abstractions. Using these methods directly can lead to more efficient and performant code.

3. **Consistency**: Adopting a consistent approach across the codebase by using native loops will reduce confusion and make the code more predictable and maintainable. This will also help new developers onboard more easily, as they will be working with familiar constructs.

4. **Reduced Dependencies**: Removing the dependency on the `iterator.helper.js` will reduce the number of imports in each file, leading to a cleaner and more modular codebase. This can also have a positive impact on build times and bundle sizes.

5. **Standardization**: Relying on web standards aligns with best practices and ensures that the codebase remains up-to-date with the latest developments in JavaScript. This also makes it easier to leverage new features and improvements in the language.

### Implementation Plan

1. **Deprecation Phase**:
   - Mark the `iterator.helper.js` as deprecated in the code and documentation.
   - Add warnings or console logs when the helper is used, suggesting the use of native iteration methods.
   - Update the development guidelines to recommend the use of native iteration methods.

2. **Refactoring Phase**:
   - Gradually refactor the codebase to replace uses of `iterator.helper.js` with native iteration methods.
   - Prioritize refactoring in areas of the codebase that are most critical or frequently modified.

3. **Removal Phase**:
   - Once all instances of `iterator.helper.js` have been replaced, remove the file from the codebase.
   - Update any remaining references or documentation to reflect the removal.

4. **Testing and Validation**:
   - Ensure that all changes are thoroughly tested to maintain the existing functionality and performance.
   - Validate that the refactored code behaves as expected and that no regressions have been introduced.

By following this plan, we aim to improve the overall quality and maintainability of the Storefront JavaScript codebase while adhering to modern web development best practices.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision

To address the issues with flow execution during the business process, we will implement a new approach where flows are executed asynchronously after the completion of the business process. This decision is based on the following considerations:

1. **Error Isolation and Business Process Continuity:**
   - By decoupling the flow execution from the business process, any errors that occur during flow execution will not disrupt the business process. The business process can complete successfully, and any issues in the flows can be handled separately.

2. **Performance Improvement:**
   - Executing flows asynchronously will improve the performance of the business process. For example, expensive operations like sending emails can be offloaded to a background job, allowing the business process to complete more quickly.

3. **Simplified Debugging and Maintenance:**
   - Moving flow execution to a dedicated event listener will simplify debugging and reduce the complexity of stack traces. This will make it easier to identify and fix issues in the flow execution.
   - A dedicated execution environment for flows will also make it easier to expand their capabilities in the future, as it will be more modular and isolated from the core business process.

4. **Implementation Details:**
   - **Event Listener:** We will create a dedicated event listener that listens for the completion of the business process. This listener will then trigger the asynchronous execution of the flows.
   - **Message Queue:** We will use a message queue (e.g., RabbitMQ, Kafka) to handle the asynchronous execution of flows. When the business process completes, a message will be sent to the queue, and a worker will pick up the message and execute the corresponding flows.
   - **Feature Flag:** The new approach will be controlled by the feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS`. If this flag is enabled, flows will be executed asynchronously; otherwise, the current synchronous execution will be used.

5. **Fallback Mechanism:**
   - In case the message queue or the worker fails, we will implement a fallback mechanism to ensure that the flows are eventually executed. This could include retry logic, dead-letter queues, and monitoring to alert on failures.

By implementing this decision, we aim to improve the robustness, performance, and maintainability of the system, while also providing a better experience for both users and developers.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision

We will use the Architectural Decision Records (ADR) format for documenting architectural decisions. The ADRs will follow the structure outlined by Michael Nygard in his article and further refined by other practitioners, which includes:

1. **Title**: A short, descriptive title that succinctly captures the essence of the decision.
2. **Status**: Indicates the current status of the ADR, such as ""Proposed,"" ""Accepted,"" ""Rejected,"" ""Deprecated,"" or ""Superseded.""
3. **Context**: Provides the background information and context leading up to the decision. This section should explain why the decision is necessary and what problem it aims to solve.
4. **Decision**: Clearly states the decision that was made. This should be a concise, unambiguous statement.
5. **Consequences**: Outlines the implications of the decision, including both positive and negative outcomes. This section should also include any trade-offs that were considered.
6. **Date**: The date when the ADR was created or last updated.
7. **Author(s)**: The person or people who authored the ADR.

### Example Structure

```markdown
# [Title]

## Status
[Proposed | Accepted | Rejected | Deprecated | Superseded]

## Context
[Explanation of the context and problem statement]

## Decision
[The decision that was made]

## Consequences
[Implications and trade-offs of the decision]

## Date
[YYYY-MM-DD]

## Author(s)
[Names of the author(s)]
```

This format ensures that all relevant information is captured and that the decisions are easily understandable and traceable. Additionally, we will store these ADRs in a dedicated directory within the project repository, such as `docs/adr/`, to keep them organized and accessible.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision

To address the issues with the legacy python-tuf implementation and to provide a more robust, maintainable, and scalable solution for TUF repository management, we will develop a new Python library focused on TUF repository operations. This new library, tentatively named `tuf-repo`, will be designed with the following principles in mind:

1. **Modularity and Separation of Concerns**:
   - The library will be structured into distinct modules, each responsible for a specific aspect of TUF repository management (e.g., metadata generation, key management, snapshot management).
   - Each module will have a clear, well-defined API that is easy to use and understand.

2. **Reduced Codebase**:
   - The new library will aim to significantly reduce the amount of code by refactoring and removing redundant or unnecessary functionality.
   - We will leverage modern Python features and best practices to write more concise and efficient code.

3. **Improved API Design**:
   - The API will be designed to be intuitive and consistent, with methods that have a minimal number of arguments and clear, descriptive names.
   - We will avoid the use of global variables and instead promote the use of context-specific objects and state management.

4. **Scalability and Performance**:
   - The library will be designed to handle large repositories efficiently. For example, it will not load all metadata files into memory at once but will provide mechanisms to manage and access metadata in a more scalable manner.
   - We will include performance optimizations and provide guidelines for best practices in managing large-scale TUF repositories.

5. **Documentation and Examples**:
   - Comprehensive documentation will be provided, including detailed API documentation, usage examples, and best practices.
   - We will also include a set of tools and scripts to help users quickly set up and manage TUF repositories.

6. **Testing and Reliability**:
   - The library will be thoroughly tested with a comprehensive suite of unit tests, integration tests, and end-to-end tests.
   - We will also implement continuous integration and automated testing to ensure the reliability and stability of the library.

By following these principles, the `tuf-repo` library will provide a modern, maintainable, and scalable solution for TUF repository management, addressing the shortcomings of the legacy implementation and providing a better experience for developers and users.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision

We will migrate the Primer React components to TypeScript, integrating type definitions directly with the component implementation. This approach will ensure that the type definitions are always in sync with the actual code, reducing the likelihood of type-related bugs and improving the overall developer experience for teams using Primer React in TypeScript applications.

### Rationale
- **Maintainability**: By having type definitions alongside the implementation, we can automatically update types as the code evolves, reducing the maintenance burden.
- **Accuracy**: Direct integration ensures that the type definitions accurately reflect the functionality of the components, leading to fewer type-related issues.
- **Developer Experience**: Improved type support will provide better autocompletion, inline documentation, and compile-time checks, enhancing productivity and reducing runtime errors.
- **Future-Proofing**: As the library grows, this approach will scale more effectively, supporting a larger and more complex codebase.

### Implementation Steps
1. **Incremental Migration**: Start by converting a small, manageable set of components to TypeScript. This will help in identifying any challenges and refining the process.
2. **TypeScript Configuration**: Set up a TypeScript configuration (`tsconfig.json`) that enforces strict type checking and other best practices.
3. **Testing and Validation**: Ensure that all existing tests pass and add new tests to cover type-specific scenarios.
4. **Documentation and Training**: Provide documentation and training for contributors to understand the new TypeScript setup and best practices.
5. **Community Involvement**: Engage with the community to gather feedback and contributions, ensuring a smooth transition and leveraging the collective expertise.

### Impact
- **Short-term**: There may be an initial overhead in terms of time and effort to convert the components. However, this will be offset by the long-term benefits.
- **Long-term**: The project will benefit from more robust, maintainable, and accurate type definitions, leading to a better developer experience and fewer type-related issues.

By making this decision, we aim to create a more sustainable and reliable development environment for both the Primer React library and the applications that use it.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚ÂƒÃƒÂ‚Ã‚Âº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚ÂƒÃƒÂ‚Ã‚Âº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision

After evaluating the two patterns for creating React components in Primer React and consuming applications, we have decided to standardize on using the **Box** component pattern for the following reasons:

1. **Flexibility and Reusability**: The Box component is more flexible and can be used as a foundational building block for many different types of components. It allows for easy composition and extension, making it simpler to create new components or modify existing ones.

2. **Consistency with Design Tokens**: The Box component aligns well with the use of design tokens, which are a key part of our design system. This ensures that the styling is consistent across the application and adheres to the design system's guidelines.

3. **Simpler Syntax and Readability**: The Box component approach results in cleaner and more readable code. The separation of styles into a `styles` object and the use of `merge` for combining styles with props makes the code more maintainable and easier to understand.

4. **Performance Considerations**: The Box component approach avoids the overhead of styled-components, which can sometimes lead to performance issues, especially in large-scale applications. By using a more straightforward JSX and CSS-in-JS approach, we can potentially improve the performance of our components.

5. **Easier Prop Management**: The Box component pattern allows for more straightforward management of props. The destructuring and spreading of props make it easier to handle and pass down props, ensuring that all necessary properties are correctly applied to the underlying DOM element.

6. **Community and Ecosystem Support**: The Box component is a widely adopted pattern in the React community, and there is a wealth of resources and best practices available. This can help us leverage community knowledge and tools, making it easier to onboard new developers and maintain the codebase.

By standardizing on the Box component pattern, we aim to achieve a more consistent, maintainable, and performant codebase. This decision will be reflected in our coding guidelines and enforced through code reviews and linters.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision

We will continue to use the `drafts` directory for developing and testing new or updated components outside of the main bundle. This approach allows us to:

1. **Parallel Development**: Maintain multiple versions of a component in parallel, which is useful for testing and iterating on new features or major changes without affecting the stable version.
2. **Work in Progress Components**: Provide a space for components that are still under development and not yet ready for production use. This helps prevent premature adoption by developers.
3. **Collaboration and Feedback**: Facilitate collaboration with other teams or stakeholders by allowing them to test and provide feedback on the draft components before they are finalized and moved to the main bundle.

### Implementation Guidelines

- **Naming Convention**: Components in the `drafts` directory should follow a consistent naming convention to clearly indicate their status (e.g., `UnderlineNavDraft`, `TreeViewDraft`).
- **Documentation**: Clearly document the purpose and status of each component in the `drafts` directory, including any known issues or limitations.
- **Versioning**: Ensure that the `drafts` directory is included in the package but not in the main export, so that users must explicitly import from `@primer/react/drafts` to use these components.
- **Testing**: Implement thorough testing for components in the `drafts` directory to ensure they meet the same quality standards as components in the main bundle.
- **Promotion to Main Bundle**: Establish a clear process for promoting components from the `drafts` directory to the main bundle, including code review, testing, and documentation updates.

By following these guidelines, we can effectively manage the development and testing of new and updated components while minimizing the risk of premature adoption in production environments.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision

Given the context and the challenges associated with snapshot testing, especially in large and complex React applications, we will adopt a more nuanced approach to snapshot testing. The decision is to **limit the use of snapshot testing for React components** and instead focus on more targeted and meaningful tests that clearly define the intent and behavior being tested.

### Rationale

1. **Clarity and Intent**: Snapshot tests can sometimes obscure the intent behind what is being tested. By moving towards more explicit tests (e.g., unit tests, integration tests), we can ensure that the purpose of each test is clear and that it is testing specific, well-defined behaviors.

2. **Maintainability**: Large and deep component trees can result in snapshots that are difficult to manage and debug. Smaller, more focused tests are easier to maintain and update as the application evolves.

3. **Debugging**: When a snapshot test fails, it can be challenging to pinpoint the exact cause of the failure. More granular tests provide better visibility into what has changed and why, making debugging more straightforward.

4. **Performance**: Large snapshots can slow down the test suite, especially if they involve rendering complex components. Focused tests can be more efficient and faster to run.

### Implementation

- **Refactor Existing Snapshot Tests**: Gradually refactor existing snapshot tests to more specific and targeted tests. This may include:
  - Unit tests for individual components.
  - Integration tests for component interactions.
  - Visual regression tests for critical UI elements.

- **Use Snapshots Judiciously**: Continue to use snapshot testing for simple, stable components where the output is unlikely to change frequently. For example, small, stateless components or components with very predictable outputs.

- **Documentation and Guidelines**: Update the project's testing documentation to include guidelines on when and how to use snapshot testing. Emphasize the importance of writing clear, intent-driven tests.

- **Review and Refinement**: Regularly review the test suite to ensure that it remains effective and maintainable. Encourage team members to question the value of each test and to refine the test strategy as needed.

By adopting this approach, we aim to improve the clarity, maintainability, and effectiveness of our test suite, while still leveraging the benefits of snapshot testing where appropriate.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision

We will use a combination of environment variables and build-time transformations to ensure that `console.warn()` calls and related development checks are excluded from the production build. Specifically, we will:

1. **Introduce an Environment Variable**: Use an environment variable (e.g., `NODE_ENV`) to differentiate between development and production environments. This is a common practice in many JavaScript projects.

2. **Conditional Warnings**: Wrap `console.warn()` calls and any other development-specific code in conditional statements that check the value of `NODE_ENV`. For example:
   ```javascript
   if (process.env.NODE_ENV !== 'production') {
     console.warn('This feature is deprecated and will be removed in the next major version.');
   }
   ```

3. **Build-Time Transformations**: Utilize a build tool like Babel or Webpack to strip out the development-only code during the production build. For instance, Babel's `@babel/plugin-transform-react-constant-elements` and `@babel/plugin-transform-react-inline-elements` can help, but more specifically, `@babel/plugin-transform-react-remove-prop-types` and similar plugins can be configured to remove such warnings.

4. **Documentation and Guidelines**: Update our project documentation to include guidelines on how to properly add development-only warnings and checks. This will ensure that all developers follow the same pattern and maintain consistency across the codebase.

By following these steps, we can ensure that `console.warn()` calls and other development-specific code do not make it into the production build, thereby improving performance and reducing unnecessary noise in the production logs.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision

To address the inconsistencies and challenges with live region announcements in Primer React, we will adopt a centralized pattern for handling live region announcements. This approach will ensure that live region announcements are consistent, reliable, and accessible across different components and screen readers.

### Centralized Live Region Manager

We will implement a **Centralized Live Region Manager** (CLRM) as a utility within the Primer React library. The CLRM will provide a set of functions and components that can be used to manage and announce live regions in a standardized way. This will include:

1. **Live Region Component**: A reusable `LiveRegion` component that can be used to wrap content that needs to be announced. This component will handle the necessary ARIA attributes and ensure that the content is announced correctly.

2. **Announcement Functions**: Utility functions such as `announce`, `announcePolite`, and `announceAssertive` that can be used to programmatically announce messages. These functions will manage the creation and updating of live regions to ensure consistent behavior.

3. **Visibility Management**: The CLRM will handle the visibility of live regions to ensure that they are only visible when necessary, reducing visual clutter and ensuring that screen readers can focus on the relevant content.

4. **Consistent Behavior Across Screen Readers**: The CLRM will be tested and validated across a range of screen readers to ensure that announcements are consistent and reliable.

### Implementation Details

- **LiveRegion Component**:
  - The `LiveRegion` component will accept props such as `role` (e.g., `alert`, `status`), `politeness` (e.g., `polite`, `assertive`), and `atomic` (e.g., `true`, `false`).
  - It will automatically manage the ARIA attributes and ensure that the content is announced when it changes or is rendered.

- **Announcement Functions**:
  - `announce(message: string, politeness: 'polite' | 'assertive' = 'polite')`: Announces a message with the specified politeness level.
  - `announcePolite(message: string)`: A convenience function for announcing a message with `politeness` set to `polite`.
  - `announceAssertive(message: string)`: A convenience function for announcing a message with `politeness` set to `assertive`.

- **Visibility Management**:
  - The CLRM will use CSS to control the visibility of live regions, ensuring that they are not visually distracting but are still accessible to screen readers.

- **Testing and Validation**:
  - The CLRM will be thoroughly tested with a variety of screen readers, including NVDA, JAWS, and VoiceOver, to ensure consistent and reliable behavior.

### Example Usage

```jsx
import { LiveRegion, announce } from 'primer-react';

// Using the LiveRegion component
<LiveRegion role=""status"" politeness=""polite"">
  <p>Form submitted successfully.</p>
</LiveRegion>

// Using the announcement functions
const handleSubmit = () => {
  // Form submission logic
  announce('Form submitted successfully.');
};
```

### Benefits

- **Consistency**: Ensures that live region announcements are handled consistently across all components.
- **Reliability**: Reduces the likelihood of announcements being missed or inconsistent across different screen readers.
- **Simplicity**: Provides a simple and intuitive API for developers to use, reducing the complexity of implementing live regions.
- **Maintainability**: Centralizes the logic for live regions, making it easier to maintain and update in the future.

By adopting this centralized pattern, we will improve the accessibility and user experience of Primer React components, ensuring that live region announcements are reliable and consistent.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"It seems like the context is missing from your request. Could you please provide more details about the specific situation or problem that needs to be addressed in the Architectural Decision Record? This could include information on the current system, any challenges faced, and what you aim to achieve with the decision.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision

To address the issues with repeated markup and the lack of visual debugging in our testing infrastructure, we will integrate Storybook with Jest for interactive testing. This approach will allow us to leverage the visual and interactive capabilities of Storybook while maintaining the robustness and automation of Jest tests.

### Key Points:
1. **Shared Markup**: We will create a shared setup that allows the same markup to be used in both Storybook and Jest tests. This can be achieved by exporting the components and their states from a common file or directory. This way, any changes made to the component or its state will be reflected in both the Storybook and Jest tests, reducing duplication and ensuring consistency.

2. **Visual Debugging with Storybook**: We will use Storybook's `addon-jest` or `addon-actions` to display Jest test results and interactions within the Storybook interface. This will provide a visual and interactive way to debug complex tests. Developers can see the actual UI and how it behaves during the test, making it easier to identify and fix issues.

3. **Automated Interaction Testing**: We will explore tools like `@storybook/test-runner` or `@testing-library/react` to automate interaction testing within Storybook. This will allow us to write and run interaction tests directly in Storybook, which can then be integrated with Jest for more comprehensive testing.

4. **Documentation and Visibility**: By integrating Storybook and Jest, we will also improve the visibility and documentation of our tests. The interactive nature of Storybook will make it easier for developers to understand and enhance the tests, as they can see the visual and behavioral aspects of the components.

### Implementation Steps:
1. **Refactor Shared Markup**: Create a common directory or module for shared component markup and states.
2. **Integrate Storybook Addons**: Install and configure `addon-jest` or `addon-actions` in Storybook to display Jest test results.
3. **Automate Interaction Tests**: Set up `@storybook/test-runner` or `@testing-library/react` to handle interaction testing in Storybook.
4. **Update Documentation**: Update the project documentation to reflect the new testing workflow and provide guidelines for using the integrated setup.

By implementing these changes, we aim to streamline our testing process, reduce redundancy, and improve the overall quality and maintainability of our tests.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision

To address the challenges of managing dependencies and connecting workspaces, as well as organizing scripts in the `primer/react` repository, we will adopt the following strategies:

1. **Monorepo Management with Lerna:**
   - **Rationale:** Lerna is a popular tool for managing JavaScript projects with multiple packages. It allows us to manage interdependencies between workspaces, handle installations, and run scripts across the entire project.
   - **Implementation:**
     - Initialize Lerna in the top-level of the `primer/react` repository.
     - Configure Lerna to use `npm` as the package manager.
     - Update the `lerna.json` configuration to include all relevant workspaces (e.g., `packages/*`, `docs`, `examples/*`).
     - Use Lerna's `bootstrap` command to install dependencies and link interdependent packages.

2. **Unified Dependency Management:**
   - **Rationale:** Centralizing dependency management ensures that all workspaces can be installed and updated consistently.
   - **Implementation:**
     - Create a `package.json` at the root level to manage global dependencies and scripts.
     - Use Lerna's `bootstrap` command to install dependencies for all workspaces in one step.
     - Ensure that workspace-specific dependencies are managed within their respective `package.json` files.

3. **Workspace Dependencies:**
   - **Rationale:** Properly managing inter-workspace dependencies ensures that changes in one workspace are reflected in others.
   - **Implementation:**
     - Use Lerna's `--hoist` option to hoist common dependencies to the top level, reducing duplication.
     - Replace relative paths and `npm link` with Lerna's `@primer/react` package references in the `package.json` files of dependent workspaces.

4. **Script Organization and Management:**
   - **Rationale:** Organizing scripts in a dedicated directory and using a consistent approach for managing their dependencies improves maintainability and reusability.
   - **Implementation:**
     - Move all scripts into a `scripts` directory at the root level.
     - Use a build tool like `esbuild` or `ts-node` to handle script dependencies and execution.
     - Add a `package.json` in the `scripts` directory to manage script-specific dependencies.
     - Update the root `package.json` to include scripts that delegate to the `scripts` directory, e.g., `npm run <script-name>`.

5. **Convenience Scripts:**
   - **Rationale:** Providing convenience scripts simplifies the development process for contributors.
   - **Implementation:**
     - Add a `setup` script in the root `package.json` that runs `lerna bootstrap` to set up the entire monorepo.
     - Include additional scripts for common tasks such as building, testing, and linting, which can be run from the root level and delegate to the appropriate workspaces.

By adopting these strategies, we will streamline the development process, improve dependency management, and enhance the overall maintainability of the `primer/react` repository.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision

To address the issue of unintended public exposure of internal code, we will restructure the project to clearly separate the public API from internal implementation details. We will move all internal files and folders that are not intended for external use out of the `src` directory and into a new `internal` directory. The `src` directory will then only contain files and folders that are part of the public API.

### Steps to Implement
1. **Create a New Directory**: Create a new directory named `internal` at the root level of the project.
2. **Move Internal Files**: Move all files and folders that are not intended for external use from the `src` directory to the `internal` directory.
3. **Update Import Paths**: Update all import paths in the codebase to reflect the new location of the moved files.
4. **Update Build Configuration**: Modify the Rollup configuration to exclude the `internal` directory from the build process. This ensures that only the contents of the `src` directory are included in the final package.
5. **Update npm Exports**: Update the `exports` field in the `package.json` to reflect the new structure and ensure that only the `src` directory is exposed as part of the public API.

### Updated `package.json` Exports
```json5
{
  ""exports"": {
    ""./lib-esm/*"": {
      ""import"": [
        ""./dist/esm/src/*.js""
      ],
      ""require"": [
        ""./dist/cjs/src/*.js""
      ]
    }
  }
}
```

### Benefits
- **Clarity**: Clearly separates the public API from internal implementation details.
- **Security**: Reduces the risk of consumers relying on internal code, which may change without notice.
- **Maintainability**: Simplifies the maintenance of the public API by keeping it isolated from internal changes.

### Drawbacks
- **Initial Effort**: Requires an initial effort to move files and update import paths.
- **Potential Breakage**: If any internal files were being used externally, this change could break existing consumer code. To mitigate this, we will provide clear documentation and potentially a deprecation period if necessary.

By implementing this decision, we ensure that the public API of `@primer/react` is well-defined and that internal implementation details are not inadvertently exposed to consumers.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision

To address the performance issues and to leverage CSS variables for theming, we will transition from a runtime CSS injection approach to a static CSS file approach. This decision is based on the significant performance improvements observed in both lab and application benchmarks, as well as the benefits of using CSS variables for theming.

### Key Points of the Decision:

1. **Static CSS Files**:
   - **Performance Improvement**: Static CSS files will be used instead of injecting styles at runtime. This will allow the browser to parallelize the download and processing of CSS, leading to faster initial page loads.
   - **Server-Side Rendering (SSR)**: The use of static CSS files will eliminate the need for double rendering on the server, reducing the overall rendering time and improving the first meaningful paint (FMP) and time to interactive (TTI).

2. **CSS Variables for Theming**:
   - **Dynamic Theming**: CSS variables will be utilized to enable dynamic theming. This will allow for more flexible and maintainable theming without the need for recompiling or reloading the entire stylesheet.
   - **Customization**: Consumers of the Primer/Primitives library will have more control over the theme, allowing for easier customization and better adherence to design guidelines.

3. **Improved Guardrails**:
   - **Consistency and Maintainability**: By moving to static CSS files and CSS variables, we can enforce better guardrails around the usage of styles. This will help maintain consistency across different parts of the application and reduce the likelihood of style conflicts.
   - **Documentation and Best Practices**: We will provide clear documentation and best practices for using CSS variables and static CSS files, ensuring that developers can easily adopt and follow these new standards.

### Implementation Steps:

1. **Audit and Refactor**:
   - Conduct a thorough audit of the current styling architecture to identify all instances of runtime CSS injection.
   - Refactor these instances to use static CSS files, ensuring that the new approach is consistent and adheres to best practices.

2. **CSS Variables Integration**:
   - Define a set of CSS variables for theming, including colors, fonts, and other design tokens.
   - Update the existing components to use these CSS variables for theming, ensuring that they can be easily customized by consumers.

3. **Testing and Validation**:
   - Perform comprehensive testing to ensure that the transition to static CSS files and CSS variables does not introduce any regressions or performance issues.
   - Validate the performance improvements through benchmarking and real-world application testing.

4. **Documentation and Training**:
   - Update the documentation to reflect the new styling architecture, including how to use CSS variables and static CSS files.
   - Provide training and support for developers to help them understand and adopt the new approach.

By making this change, we aim to significantly improve the performance of our applications, enhance the flexibility and maintainability of our theming, and provide better guardrails for consuming Primer/Primitives.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision

After revisiting the decision made in `ADR 009`, we have decided to shift our approach for authoring tests that require user interactions. Instead of using Storybook's `play` functions, we will adopt a more integrated and robust testing strategy that leverages both Jest and Playwright.

### Rationale

1. **Unified Testing Framework**:
   - **Consistency**: Using Playwright for interaction tests, alongside Jest for unit and integration tests, provides a consistent and unified testing framework.
   - **Ecosystem and Community Support**: Playwright is actively maintained and has a strong community, which ensures better long-term support and frequent updates.

2. **Separation of Concerns**:
   - **Storybook for Development**: Storybook remains an excellent tool for developing and documenting UI components. It is not, however, the best tool for comprehensive interaction testing.
   - **Playwright for End-to-End (E2E) Tests**: Playwright is designed for E2E testing and can handle complex user interactions, navigation, and cross-browser testing more effectively than Storybook's `play` functions.

3. **Test Coverage and Reliability**:
   - **Better Test Coverage**: Playwright allows us to write more comprehensive and reliable E2E tests, ensuring that our application works as expected in various scenarios.
   - **Visual Regression Testing**: Playwright can also be used for visual regression testing, providing a single tool for multiple types of tests.

4. **Maintenance and Scalability**:
   - **Simplified Maintenance**: Having a single, powerful tool for interaction and E2E testing simplifies the maintenance of our test suite.
   - **Scalability**: As our application grows, Playwright's capabilities will scale with it, allowing us to handle more complex testing requirements.

### Implementation

- **Migrate Existing Tests**: We will migrate existing interaction tests from Storybook's `play` functions to Playwright.
- **Update Documentation**: Update our testing documentation to reflect the new approach and provide guidelines for writing interaction tests with Playwright.
- **Training and Onboarding**: Provide training and resources for the team to get up to speed with Playwright, ensuring a smooth transition.

### Next Steps

- **Task Assignment**: Assign tasks for migrating existing tests and updating documentation.
- **Timeline**: Set a timeline for the migration and ensure that all team members are aware of the changes.
- **Review and Feedback**: Conduct a review of the new testing approach and gather feedback from the team to make any necessary adjustments.

By adopting this new approach, we aim to improve the quality and reliability of our tests, while also streamlining our testing process.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision

To address the issue of layout shifts during server-side rendering (SSR) when using responsive props, we will implement a solution that ensures consistent prop values between the server and the client. The decision is to use a combination of static extraction of media queries and critical CSS injection for responsive styles.

### Implementation Details:

1. **Static Media Query Extraction:**
   - Introduce a build-time step that statically extracts the media query breakpoints used in the `useResponsiveValue` hook.
   - This step will generate a map of media queries and their corresponding values, which can be used to determine the appropriate value for each breakpoint.

2. **Critical CSS Injection:**
   - During SSR, inject the critical CSS required for the initial render based on the extracted media queries.
   - This ensures that the initial styles are consistent with the responsive values, preventing layout shifts when the component hydrates on the client.

3. **Enhanced `useResponsiveValue` Hook:**
   - Modify the `useResponsiveValue` hook to use the pre-extracted media queries and provide a fallback mechanism for SSR.
   - On the server, the hook will use the pre-extracted values to ensure the correct prop value is set.
   - On the client, the hook will re-evaluate the media queries and update the prop value if necessary, ensuring responsiveness.

4. **Server-Side Fallback:**
   - Use a default viewport size (e.g., ""regular"") as a fallback for SSR.
   - Ensure that the fallback value is consistent with the most common or expected viewport size to minimize the likelihood of layout shifts.

5. **Client-Side Hydration:**
   - When the component hydrates on the client, the `useResponsiveValue` hook will re-evaluate the media queries and apply the correct responsive value.
   - This ensures that the component remains responsive and the layout is updated correctly without causing a shift.

### Benefits:
- **Consistent Initial Render:** Ensures that the initial render on the server matches the client, preventing layout shifts.
- **Improved Performance:** Reduces the need for reflows and repaints on the client, leading to better performance.
- **Simplified Authoring:** Provides a straightforward way for authors to use responsive props without worrying about SSR issues.

### Example Usage:
```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }} />
```

By implementing this solution, we provide a robust and reliable way to handle responsive props, ensuring a smooth and consistent user experience across both server and client renders.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision

To facilitate a smooth transition from the deprecated `title` prop to the new, more flexible and accessible `ActionList.GroupHeading` component, we will implement the following steps:

1. **Deprecation Warning**: When the `title` prop is used, a deprecation warning will be logged in the console, informing developers of the change and directing them to the new API documentation.

2. **Temporary Support with Migration Path**: For a defined period (e.g., 2 major releases), the `title` prop will continue to work, but it will render the content using the `ActionList.GroupHeading` component internally. This ensures that existing codebases do not break immediately while providing a clear migration path.

3. **Documentation Update**: The official documentation will be updated to reflect the new recommended usage, including examples and best practices. A migration guide will be provided, detailing the steps to update from the old API to the new one.

4. **Removal of Deprecated Prop**: After the defined period, the `title` prop will be removed, and any remaining usages will result in a runtime error, prompting developers to update their code.

5. **Testing and Validation**: Provide a set of test cases and validation tools to help developers ensure that their migrations are correct and that the new API is being used as intended.

By following these steps, we aim to minimize disruption for our users while ensuring that they can take advantage of the new, more accessible, and flexible API.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision

We will support proxy configuration for self-hosted runners by utilizing the commonly recognized environment variables `http_proxy`, `https_proxy`, and `no_proxy`. This approach aligns with the conventions used by a wide array of applications, ensuring broad compatibility and ease of use for our users. To maximize compatibility, we will also check for the presence of `HTTPS_PROXY` (uppercase) as an alternative to `https_proxy` (lowercase), but prioritize the lowercase variant in our documentation and implementation guidance. This decision aims to provide a straightforward and familiar method for enterprises and organizations to configure their proxies, thereby facilitating the adoption of self-hosted runners.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision

We will implement a new system where command echoing is disabled by default, and users can opt-in to see the command outputs in the user logs. This change aims to reduce log clutter and provide a cleaner user experience. 

To enable command echoing, users can use the `::echo::on` command. Conversely, to disable it, they can use the `::echo::off` command. This gives users explicit control over when and where they want to see the command outputs.

However, in the event of an error during command processing, the command output will still be echoed in the logs. This ensures that users have the necessary context to understand and troubleshoot the issue effectively.

Additionally, if the `ACTIONS_STEP_DEBUG` secret is enabled, command echoing will be turned on by default. This provides a convenient way for users who are debugging or need more detailed logs to automatically see the command outputs without needing to manually enable it. 

This approach balances the need for a clean, uncluttered log with the requirement for detailed information when needed, while also providing flexibility and control to the users.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision

To address the need for including extra information, such as OS and software details from the hosted image, in the `Set up job` step's log, we will implement a feature that allows the runner to automatically detect and log this information. This can be achieved by:

1. **Adding a Script or Command Execution**: Introduce a script or command that runs at the beginning of the `Set up job` step. This script will gather the necessary system and software information using built-in commands (e.g., `uname`, `cat /etc/os-release`, `dpkg -l` for Debian-based systems, or `brew list` for macOS).

2. **Logging the Information**: The output of the script or command will be captured and logged as part of the `Set up job` step. This ensures that the relevant information is available in the job logs for review and debugging purposes.

3. **Configuration Option**: Provide a configuration option in the runner settings to enable or disable this feature. This allows users to control whether they want to include this additional information in their logs.

4. **Documentation**: Update the documentation to include instructions on how to enable and use this feature, along with examples of the types of information that can be logged.

By implementing this decision, we ensure that the `Set up job` step provides comprehensive and useful information about the environment, which can be valuable for troubleshooting and ensuring the correct setup of the job.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision

We will add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context. This enhancement will provide more granular control over the execution flow, allowing subsequent steps to be conditional based on the success or failure of previous steps. The `outcome` field will indicate whether the step succeeded or failed, while the `conclusion` field will provide a more detailed status, such as ""success"", ""failure"", ""cancelled"", or ""skipped"". This will enable more flexible and robust workflows, improving the overall reliability and maintainability of our processes.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision

We will implement a new command-line argument for the runner registration process that allows users to specify additional labels. This approach will enable more flexibility and automation in configuring self-hosted runners, as it will allow administrators to define custom labels directly through scripts or configuration management tools.

### Implementation Details
- **Command Line Argument**: Add a new optional parameter `--labels` (or similar) to the registration command. This parameter will accept a comma-separated list of label values.
- **Label Validation**: Ensure that the provided labels meet any necessary validation criteria (e.g., no special characters, length limits).
- **Integration with Existing Labels**: The custom labels provided via the command line will be added to the set of built-in labels (os, arch) during the registration process.
- **Documentation**: Update the documentation to include examples and best practices for using the new `--labels` parameter.

### Example Usage
```sh
./config.sh --url https://github.com/your-org/your-repo --token YOUR_TOKEN --labels ""custom-label1,custom-label2""
```

### Benefits
- **Automation**: Simplifies the setup and management of self-hosted runners by allowing labels to be set via scripts.
- **Flexibility**: Enables users to define custom labels that are specific to their environment or use case.
- **Consistency**: Ensures that all relevant labels are registered at the time of runner setup, reducing the need for manual updates later.

### Drawbacks
- **Complexity**: Introduces an additional command-line option, which may add some complexity to the registration process.
- **Validation**: Requires robust validation to ensure that the provided labels are valid and do not conflict with existing labels.

### Alternatives Considered
- **Configuration File**: Instead of a command-line argument, we could have considered a configuration file where users can specify labels. However, this would add an extra step and might be less convenient for automated setups.
- **API Endpoint**: Provide an API endpoint to add labels after registration. This was deemed less user-friendly and would require additional steps and potentially more complex scripting.

This decision aligns with the goal of improving the automation and flexibility of self-hosted runner configurations, as highlighted in the referenced GitHub issue.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision
We will implement a resilient authentication mechanism that can handle transient faults from the third-party provider. This will include the following strategies:

1. **Circuit Breaker Pattern**: To prevent overwhelming the system with failed requests, we will use a circuit breaker. If the number of consecutive failures exceeds a threshold, the circuit breaker will ""open,"" and subsequent requests will not be sent to the provider for a specified timeout period. Instead, these requests will receive an immediate failure response or a cached response if available. After the timeout, the circuit breaker will ""half-open"" and allow a limited number of requests to test if the provider is back online. If these requests succeed, the circuit breaker will ""close,"" and normal operation will resume.

2. **Retry Mechanism**: For transient errors, we will implement a retry mechanism with exponential backoff. This means that if a request fails, it will be retried after a delay that increases exponentially with each attempt. This helps to avoid overloading the provider during a temporary outage.

3. **Rate Limiting Handling**: We will monitor and handle rate limits imposed by the provider. If we detect that we are approaching the rate limit, we will throttle our requests or queue them for later processing. Additionally, we will implement a mechanism to gracefully handle rate limit errors by providing appropriate feedback to the user and retrying the request after the reset time specified in the rate limit header.

4. **Fallback Mechanisms**: In case of a prolonged outage, we will have fallback mechanisms in place. For example, we can use cached tokens or provide a degraded mode of service where users can still access some functionalities without full authentication.

5. **Monitoring and Alerting**: We will set up monitoring and alerting to detect and notify us of any issues with the authentication provider. This will help us to quickly identify and respond to any problems, ensuring minimal disruption to our service.

By implementing these strategies, we aim to build a robust and fault-tolerant authentication system that can handle transient faults and maintain a high level of service availability.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision

We will adopt the use of Architectural Decision Records (ADRs) to document significant project decisions. ADRs will be stored in a dedicated directory within our version control system, ensuring they are easily accessible and remain part of the project's history. This practice will help in maintaining transparency, fostering community involvement, and providing a clear historical context for both current and future team members. Each ADR will follow a standardized template to ensure consistency and clarity, covering the context, decision, status, and any consequences or implications of the decision.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Âª with all relevant discussion summarized ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Â¡ÃƒÂƒÃ‚Â‚ÃƒÂ‚Ã‚Âª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision

We will adopt a standardized URI schema to uniquely identify and access the EDN files that define extensions. This URI schema will be designed to incorporate the decentralized storage system's addressing mechanism, ensuring that each extension can be reliably located and retrieved. The URI will include necessary components such as the protocol (e.g., `ipfs://` for IPFS, `ar://` for Arweave), the unique identifier of the file in the decentralized storage, and any additional parameters required for accessing or interpreting the EDN file. This approach will facilitate a consistent and interoperable method for referencing and integrating extensions across different systems and environments.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision

Given the context and constraints, we will **de-prioritize group chat functionality** for the beta launch in Q2. The focus will be on ensuring that 1-1 chat and public chat are fully functional, stable, and secure. This decision aligns with our goal of launching an MVP that provides a solid and engaging chat experience, while also allowing us to stay on the critical path and meet our timeline.

### Rationale
- **Scope Management**: By de-prioritizing group chat, we can maintain a tight scope and avoid scope creep, which is essential for meeting the Q2 launch deadline.
- **User Experience**: 1-1 and public chat are more critical for the initial user experience, as they provide the core communication features and the ability to interact with a broader community.
- **Resource Allocation**: Addressing the outstanding regressions and security concerns related to group chat would divert significant resources away from other critical tasks. Focusing on 1-1 and public chat ensures that we can deliver a high-quality, secure, and stable product.
- **MVP Requirements**: For an MVP, it is more important to have a robust and reliable 1-1 and public chat experience. Group chat can be added in a future release once the core functionalities are well-established and the product has gained traction.

### Next Steps
- **Reassign Resources**: Redirect the team's efforts from group chat to 1-1 and public chat, focusing on resolving any remaining issues and enhancing the user experience.
- **Communicate Decision**: Clearly communicate this decision to all stakeholders, including the development team, to ensure everyone is aligned and focused on the same priorities.
- **Future Planning**: Plan for the inclusion of group chat in a subsequent release, possibly after gathering feedback from the initial beta launch and identifying any additional features or improvements needed.

This decision will help us stay on track for the Q2 beta launch and ensure that we deliver a high-quality product that meets the needs of our users.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision

We will implement the Discovery v5 (Discv5) protocol to dynamically discover and connect to peers, replacing the current hard-coded peer list. This decision is based on the need for better scalability, high availability, and enhanced security as our user base grows.

### Rationale
- **Scalability**: Discv5 allows the app to find and connect to new peers dynamically, which means we can scale the number of server peers in our cluster without modifying the app's configuration.
- **High Availability and Reliability**: By dynamically discovering peers, we can ensure that if some nodes become unavailable, the app can automatically find and connect to other available nodes, maintaining service continuity.
- **Security**: The Discv5 protocol includes features like encryption and authentication, which enhance the security of the peer discovery and communication process.

### Implementation Steps
1. **Research and Familiarization**: Conduct a detailed review of the Discv5 specification and existing implementations to understand the best practices and potential challenges.
2. **Development**: Implement the Discv5 protocol in the app, ensuring that it integrates seamlessly with the existing messaging system.
3. **Testing**: Thoroughly test the implementation to ensure it meets the requirements for scalability, reliability, and security.
4. **Deployment**: Gradually roll out the new peer discovery mechanism, monitoring its performance and making adjustments as necessary.

### Implications
- **Short-term**: There will be an initial investment in development and testing resources to implement the Discv5 protocol.
- **Long-term**: The app will be more scalable, reliable, and secure, which will support a growing user base and improve the overall user experience.

### Alternatives Considered
- **Maintaining Hardcoded Peers**: This option was rejected due to the limitations in scalability and the inability to handle a large number of active users effectively.
- **Using a Centralized Peer Discovery Service**: This was considered but ultimately rejected because it introduces a single point of failure and does not align with our goal of a decentralized and highly available system.

This decision will position our app to handle future growth and maintain high standards of service and security.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision

We will implement the BIP-39 standard for mnemonic phrase generation and BIP-44 for hierarchical deterministic wallet derivation. This approach ensures that the 12-word mnemonic generated by Status is compatible with other cryptocurrency wallets and applications that adhere to these standards. By adopting BIP-39 and BIP-44, users will be able to import their existing mnemonic phrases into Status and generate the same wallet addresses, thereby enhancing interoperability and user experience across different platforms.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision

Given the context and the challenges faced with LES, especially on Mainnet, and the interim reliance on Infura which poses decentralization and security concerns, we will adopt the following strategy:

1. **Short-term Solution:**
   - Continue using Infura as a temporary solution to provide a stable and usable beta release.
   - Implement the ability for users to connect to their own or other trusted upstream nodes to mitigate the centralization and trust issues associated with Infura.

2. **Mid-term Strategy:**
   - Prioritize the development and integration of ULC (Universal Light Client) as it offers a more decentralized and secure approach.
   - Develop and integrate features that allow the Status app to connect to multiple Ethereum nodes, enhancing both decentralization and resilience.
   - Optimize the use of CHT (Canonical Header Tree) and Bloom Trie to improve sync performance and reduce bandwidth usage, making the app more efficient and user-friendly.

3. **Long-term Vision:**
   - Phase out the dependency on Infura and transition fully to ULC, ensuring that the Status app operates in a fully decentralized and secure manner.
   - Continuously monitor and improve the performance of the app, focusing on Whisper and client-side code optimizations to ensure a smooth and energy-efficient user experience.

This decision aligns with our goal of providing a high-performing, secure, and decentralized application, while also addressing the immediate need for a functional and reliable beta release.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision

To prevent sensitive data, such as user passwords, from being inadvertently logged, we will implement a strategy that involves the following steps:

1. **Data Masking**: Any function or method that receives sensitive data (e.g., passwords) will automatically mask the data before any logging occurs. This can be achieved by creating a utility function that replaces the actual content with a masked version (e.g., `****`).

2. **Use of Custom Data Types**: We will introduce a custom data type (e.g., `SensitiveString`) for handling sensitive information. This data type will override the default string representation to return a masked value, ensuring that if it is accidentally logged, the sensitive data is not exposed.

3. **Logging Interceptor**: Implement a logging interceptor or middleware that scans log messages for patterns that match sensitive data and masks them before the log entry is written. This can be done using regular expressions or more sophisticated pattern matching depending on the logging framework in use.

4. **Developer Guidelines and Training**: Update our developer guidelines to emphasize the importance of handling sensitive data carefully and provide training sessions to ensure all developers are aware of the best practices for logging and handling sensitive information.

5. **Code Reviews and Static Analysis**: Enforce code reviews to check for any instances where sensitive data might be logged. Additionally, integrate static analysis tools into the CI/CD pipeline to detect and flag potential issues related to logging sensitive data.

By implementing these measures, we can significantly reduce the risk of sensitive data being exposed in logs, thereby enhancing the security and privacy of our application.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision

Given the context and the issues identified with the current implementation of 3rd party status extensions using a JavaScript jailed execution environment, we will deprecate and remove the existing jail-based extension mechanism. Instead, we will focus on developing and adopting the new concept of extensions (Project Pluto) for extending Status functionalities. This decision is based on the following considerations:

1. **API Design and Code Quality**: The current API design is poor and leads to hard-coded, untestable, and error-prone logic in the application. By moving to a more declarative and well-designed extension system, we can improve code quality and maintainability.

2. **Performance**: The current jail-based approach introduces unnecessary performance overhead due to constant communication over the React Native Bridge. Removing this will help in optimizing the application's performance.

3. **Third-Party Interest**: There has been no significant interest from third-party developers in creating bots or extensions using the jail-based system. The new extension system (Project Pluto) is more aligned with modern web technologies and is likely to attract more developer interest.

4. **Future-Proofing**: Project Pluto offers a more flexible and powerful way to extend the application, not just for command messages but also for other parts of the application like wallet assets and chat contacts. This aligns better with our long-term goals and provides a more robust foundation for future development.

### Action Plan

1. **Deprecation and Removal**:
   - Announce the deprecation of the jail-based extension system to the community.
   - Remove all existing jail-based extension code from the application.
   - Update the documentation to reflect the changes and provide guidance on the new extension system.

2. **Development of Project Pluto**:
   - Prioritize the development and stabilization of Project Pluto.
   - Ensure that the new extension system is well-documented and easy to use.
   - Provide migration paths and support for any existing extensions that need to be ported to the new system.

3. **Community Engagement**:
   - Engage with the community to gather feedback and ensure that the new extension system meets their needs.
   - Promote the new extension system through workshops, tutorials, and developer outreach programs.

By making this decision, we aim to improve the overall quality, performance, and extensibility of the Status application, while also providing a better experience for both users and developers.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision

We will implement the Tribute to Talk (TtT) mechanism as an optional feature for users within the Status platform. This feature will allow users to set a minimum staking requirement of SNT tokens that must be deposited by another user before they can send a direct message or contact request. The deposit will act as a deterrent against spam and non-essential messages, ensuring that only meaningful interactions are initiated.

### Key Points:
1. **User Opt-In**: TtT will be an opt-in feature, allowing users to decide if they want to activate this anti-spam measure.
2. **Stake Requirement**: Users can set a minimum SNT stake amount that others must meet to initiate contact.
3. **Deposit Forfeiture**: The deposited SNT will only be forfeited to the recipient if they respond to the message or contact request. If the recipient does not reply, the deposit will be returned to the sender.
4. **User Experience**: The interface will clearly indicate the stake requirement and the process for depositing SNT, ensuring a smooth and transparent experience for both the sender and the recipient.
5. **Security and Transparency**: The smart contract handling the deposits and forfeitures will be audited to ensure security and transparency, maintaining trust in the system.

This decision aligns with the goals of reducing spam and enhancing the quality of interactions on the platform, while also providing a fair and balanced approach to communication.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision:

We will introduce a new role, ""Section Lead,"" within the mobile development team. Each Section Lead will be responsible for one of the following sections: communities, syncing & onboarding, activity center & shell, wallet, and messaging. The responsibilities of the Section Leads will include:

1. **Oversight and Quality Assurance**: Ensuring that the section they are responsible for meets high standards of quality and polish. This includes reviewing code, UI/UX, and overall functionality to ensure it aligns with the design team's vision and user expectations.

2. **Collaboration with Design Team**: Working closely with the design team to understand and implement the design specifications and feedback. This will help in maintaining a consistent and polished look and feel across the app.

3. **Coordination and Communication**: Coordinating with other developers, designers, and stakeholders to ensure that the work is aligned and integrated smoothly. They will also be responsible for communicating updates, challenges, and progress to the team lead and other relevant parties.

4. **Mentorship and Support**: Providing guidance and support to other team members working on their section. This includes helping to resolve technical issues, providing feedback, and fostering a collaborative and productive environment.

5. **Documentation and Best Practices**: Ensuring that best practices and documentation are maintained for their section. This will help in maintaining consistency and making it easier for new team members to get up to speed.

By introducing these Section Leads, we aim to improve the overall quality and polish of the mobile app, while also enhancing collaboration and oversight within the team.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision

We have decided to use a dedicated sub-resource for managing Product Option's values collection. This approach will allow us to leverage the full power of API Platform, including its built-in pagination, filtering, and validation capabilities. The sub-resource will be linked to the main Product Option resource, enabling clients to easily fetch, add, update, or remove option values while maintaining data integrity and consistency.

### Rationale
- **Separation of Concerns**: By using a sub-resource, we separate the management of option values from the main Product Option, making the API more modular and easier to understand.
- **Efficiency**: Sub-resources in API Platform are optimized for performance, especially when dealing with large collections. This ensures that operations on option values are efficient and scalable.
- **Ease of Use**: Clients can interact with the option values collection directly, without needing to modify the entire Product Option resource. This simplifies the client-side logic and reduces the risk of errors.
- **Consistency and Validation**: API Platform's built-in features ensure that all operations on the sub-resource are consistent and validated, maintaining the integrity of the data.

### Implementation
- **Sub-Resource Definition**: Define the sub-resource in the API Platform configuration, linking it to the main Product Option resource.
- **Serialization Groups**: Use serialization groups to control the exposure of the sub-resource in the main Product Option representation.
- **Documentation**: Update the API documentation to include the new sub-resource and provide examples of how to interact with it.

This decision aligns with best practices for RESTful API design and leverages the strengths of API Platform, ensuring a robust and user-friendly solution for managing Product Option values.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision

We will implement a consistent and streamlined approach to handle locale-aware resources in Sylius. This will involve the following steps:

1. **Locale Header for API Requests:**
   - Introduce a `Accept-Language` header in API requests to specify the desired language. This will allow clients to request resources in a specific locale.
   - If the `Accept-Language` header is not provided, the system will default to a predefined fallback locale (e.g., `en_US`).

2. **Localized Resource Endpoints:**
   - Modify existing endpoints to return only the translation that matches the requested locale. For example, if a client requests a product with `Accept-Language: fr_FR`, the response will include only the French translations.
   - Ensure that all locale-aware resources (e.g., product descriptions, payment method names, shipping method names) follow this pattern.

3. **Consistent Embedding of Translations:**
   - Standardize the way translations are embedded in responses. For example, always include a `translations` key in the JSON response, which contains the localized content.
   - Example response structure:
     ```json
     {
       ""id"": 1,
       ""name"": ""Product Name"",
       ""description"": ""Product Description"",
       ""translations"": {
         ""fr_FR"": {
           ""name"": ""Nom du produit"",
           ""description"": ""Description du produit""
         }
       }
     }
     ```

4. **Fallback Mechanism:**
   - Implement a fallback mechanism to handle cases where a translation for a specific locale does not exist. The fallback should first check for a more general locale (e.g., `fr` if `fr_FR` is not available) and then fall back to the default locale (e.g., `en_US`).

5. **Documentation and Developer Guidelines:**
   - Update the API documentation to clearly explain how to use the `Accept-Language` header and how the responses will be structured.
   - Provide examples and best practices for frontend developers to handle and display localized content.

6. **Testing and Validation:**
   - Add comprehensive tests to ensure that the new localization mechanism works as expected across all relevant endpoints.
   - Validate that the fallback mechanism correctly handles missing translations and that the responses are consistent and predictable.

By implementing these changes, we will provide a clear, easy-to-use, and consistent way of handling customer locales, enhancing the user experience and simplifying the development process for frontend developers.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision

We will adopt the use of Architectural Decision Records (ADRs) to document significant decisions made during the development of the Terraform Controller. ADRs are lightweight, easy to create, and provide a clear, structured way to capture the reasoning, context, and outcomes of architectural and design decisions. This approach will help in maintaining a historical record of decisions, which can be easily referenced by current and future contributors. The ADRs will be stored in a dedicated directory within the project's repository, ensuring they are easily discoverable and accessible. Each ADR will follow a standardized format, including sections for status, context, decision, and consequences, to ensure consistency and clarity.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision

We will use webhooks to detect pull request changes. Webhooks provide a more efficient and real-time method for receiving updates about repository events, including pull requests, as they trigger an HTTP POST request to a specified URL whenever an event occurs. This approach is preferred over polling because it reduces the need for continuous API calls, which can be resource-intensive and may hit rate limits. Additionally, webhooks ensure that we receive immediate notifications of changes, allowing for faster response and processing times.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision

We will implement a controlled and limited support for cross-namespace references in the tf-controller API, while also providing mechanisms to mitigate the security concerns. Specifically, we will:

1. **Allow Cross-Namespace References with Explicit Permissions:**
   - Enable `Terraform.spec.sourceRef` and `Terraform.spec.dependsOn[]` to reference objects in other namespaces.
   - Require explicit RBAC (Role-Based Access Control) permissions to be set up for these cross-namespace references. This ensures that only authorized users or service accounts can create or modify such references.

2. **Namespace Isolation by Default:**
   - By default, restrict cross-namespace references unless explicitly allowed via RBAC.
   - Provide clear documentation and examples on how to set up the necessary RBAC rules to allow cross-namespace references.

3. **Audit and Monitoring:**
   - Implement logging and auditing for cross-namespace references to track and monitor their usage.
   - Provide alerts or notifications for any unauthorized attempts to create or modify cross-namespace references.

4. **Configuration and Best Practices:**
   - Encourage best practices for namespace management and isolation.
   - Provide guidance on how to structure Flux deployments to minimize the need for cross-namespace references, such as using shared namespaces for common sources and dependencies.

5. **Future Deprecation Plan:**
   - Develop a long-term plan to gradually deprecate cross-namespace references, with a clear timeline and migration path for existing installations.
   - Engage with the community to gather feedback and provide support during the transition period.

By implementing these measures, we aim to balance the need for flexibility in Flux deployments with the security and isolation requirements of Kubernetes.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision

To address the challenges related to the deletion of Terraform resources, particularly focusing on the single object deletion, we will implement a caching mechanism for the Workspace BLOBs. This decision is based on the following key points:

1. **Caching Mechanism for Workspace BLOBs**:
   - **Implementation**: We will introduce a caching layer within the TF-Controller to store and manage the Workspace BLOBs. This cache will be designed to efficiently handle the retrieval and storage of BLOBs, ensuring that the necessary information is readily available when needed.
   - **Benefits**:
     - **Reduced Latency**: By caching the Workspace BLOBs, we can reduce the time required to fetch and process the BLOBs, leading to faster and more efficient deletion operations.
     - **Consistency**: The cache will ensure that the TF-Controller has consistent and up-to-date information about the resources, reducing the likelihood of the TF object getting stuck in an inconsistent state.
     - **Resilience**: The caching mechanism will provide a fallback in case of temporary network or storage issues, ensuring that the deletion process can proceed even if there are transient problems.

2. **Enhanced Finalization Procedures**:
   - **Refinement of Finalization Logic**: The finalization procedures in the TF-Controller will be enhanced to utilize the cached Workspace BLOBs. This will involve updating the logic to first check the cache for the necessary BLOBs before attempting to delete the TF object.
   - **Dependency Handling**: The finalization procedures will be updated to handle dependencies (e.g., Source objects, Secrets, ConfigMaps) more effectively. This will include ensuring that all dependent resources are properly cleaned up before the TF object is deleted, thus preventing the TF object from entering an inconsistent state.

3. **Integration with Existing Workflow**:
   - **Seamless Integration**: The caching mechanism will be integrated into the existing workflow of the TF-Controller, ensuring that it does not disrupt the current processes. The tf-runner will continue to generate and compress the Workspace file system, and the TF-Controller will store the BLOBs in the cache.
   - **Performance Considerations**: The caching mechanism will be designed to minimize performance overhead. This includes using efficient data structures and algorithms for cache management, as well as implementing appropriate eviction policies to manage the cache size.

4. **Testing and Validation**:
   - **Comprehensive Testing**: Before rolling out the changes, we will conduct thorough testing to ensure that the caching mechanism and the enhanced finalization procedures work as expected. This will include unit tests, integration tests, and end-to-end tests.
   - **Validation in Staging Environment**: The changes will be validated in a staging environment to ensure that they do not introduce any new issues and that the deletion process is robust and reliable.

By implementing this decision, we aim to resolve the single object deletion issues, which will pave the way for addressing the more complex problems related to the deletion of resources with dependencies and namespace deletions.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision

Given the context and the new capabilities of Zarf v0.15, we will adopt a more flexible and secure approach to our E2E testing. The decision is to:

1. **Remove the Secret Dependency for E2E Tests**: Since the creation of the `zarf-init-{{arch}}.tar.zst` no longer requires repository secrets, we will remove the dependency on these secrets in our E2E tests. This change will allow non-maintainers to run the E2E tests, thereby broadening the scope of contributors who can test their changes.

2. **Implement a Matrix Testing Strategy**: To support testing against different Kubernetes distributions and Linux distributions, we will implement a matrix testing strategy in our CI/CD pipeline. This will involve setting up multiple jobs that each run the E2E tests on a different combination of Kubernetes and Linux distributions. This approach will ensure that our software is robust across various environments.

3. **Enable Local E2E Testing**: To facilitate quicker feedback loops during development, we will provide a mechanism for developers to run (at least some of) the E2E tests locally. This could be achieved by:
   - Providing a local setup script that sets up a minimal Kubernetes environment (e.g., using Minikube or Kind).
   - Creating a Docker-based environment that simulates the necessary conditions for running the E2E tests.
   - Documenting the steps required to set up and run the E2E tests locally, including any prerequisites and dependencies.

4. **Enhance CI/CD Pipeline with Slash Commands (Optional)**: While the need for repository secrets has been removed, we may still use the `peter-evans/slash-command-dispatch@v2` action to trigger specific E2E test runs. This can be useful for targeted testing or for running tests on demand without waiting for the full CI/CD pipeline to complete.

By implementing these changes, we aim to improve the security, flexibility, and efficiency of our E2E testing process, making it easier for both maintainers and contributors to ensure the quality of Zarf.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision

Given the context, Zarf will implement a solution that leverages a temporary, in-cluster registry to push and manage images for bootstrapping purposes. This decision is driven by the need to support unknown environments with minimal dependencies and ensure that Zarf can function without assuming the presence of an external or pre-existing registry. Here's a detailed breakdown of the approach:

1. **Temporary In-Cluster Registry**:
   - **Implementation**: Utilize a lightweight, in-cluster registry (e.g., `registry:2` Docker image) that can be deployed as part of the Zarf initialization process.
   - **Advantages**: This approach ensures that the registry is available within the cluster, eliminating the need for external dependencies. It also simplifies the management of TLS trust, as the registry is internal to the cluster.

2. **TLS Trust Management**:
   - **Node-Level Configuration**: For clusters where node-level configuration is possible, Zarf will provide a mechanism to inject the necessary TLS certificates into the nodes' root certificate authorities or the CRI's configuration.
   - **CRI Configuration**: If the CRI supports custom TLS root CA configurations, Zarf will modify the CRI configuration to include the necessary certificates.
   - **Automation**: Zarf will automate the process of distributing and configuring the TLS certificates across all nodes in the cluster, ensuring that each node can securely pull images from the in-cluster registry.

3. **Pod Affinity and Taints**:
   - **Affinity and Taints**: To handle the scenario where not all nodes can be configured immediately, Zarf will use pod affinity and taints to ensure that critical pods (e.g., those required for bootstrapping) are scheduled on nodes that have been properly configured.
   - **Bootstrap Node**: Designate a specific node as the ""bootstrap node"" and apply taints and tolerations to ensure that only essential pods are scheduled on this node during the initial setup phase.

4. **Cleanup and Teardown**:
   - **Temporary Nature**: The in-cluster registry will be designed to be temporary. Once the bootstrapping process is complete, Zarf will provide a mechanism to tear down the temporary registry, ensuring that the cluster returns to a clean state.
   - **Image Management**: Zarf will manage the lifecycle of the images, ensuring that they are pushed to the in-cluster registry and then, if needed, moved to a more permanent storage solution or retained within the cluster as required.

By implementing this decision, Zarf will be able to handle the complexities of image management in K8s clusters, even in environments with no pre-existing infrastructure, while maintaining security and operational simplicity.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision

We will implement a process to generate and maintain Software Bill of Materials (SBOM) for all software that runs on government hardware, in compliance with Executive Order 14028. This will involve:

1. **Selecting SBOM Standards**: We will use the SPDX (Software Package Data Exchange) and CycloneDX formats, which are widely adopted and supported by various tools and platforms.

2. **Automating SBOM Generation**: We will integrate SBOM generation into our CI/CD pipelines to ensure that SBOMs are automatically created and updated with each build. This will help in maintaining up-to-date and accurate SBOMs.

3. **Storing and Managing SBOMs**: We will store SBOMs in a secure, centralized repository. This repository will be accessible to authorized personnel and will be regularly audited to ensure compliance and accuracy.

4. **Validation and Verification**: We will implement checks to validate the completeness and correctness of the generated SBOMs. This will include automated scans and manual reviews as necessary.

5. **Training and Documentation**: We will provide training and documentation to our development and operations teams to ensure they understand the importance of SBOMs and how to work with them effectively.

6. **Compliance Monitoring**: We will establish a monitoring system to track compliance with SBOM requirements and to alert us to any issues or non-compliance.

By implementing these steps, we will ensure that our software meets the SBOM requirements set forth in EO14028, thereby enhancing the security and transparency of our software supply chain.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision

To address the limitations of Helm Post Rendering when used with secondary GitOps tools and to reduce the burden on package authors, we will implement a more robust and automated solution for mutating image paths, secrets, and other references. This will involve the following steps:

1. **Enhance `zarf prepare` Command:**
   - Extend the `zarf prepare` command to automatically detect and mutate image paths, secrets, and references to the Zarf Registry and Git server.
   - Integrate advanced templating capabilities into `zarf prepare` to handle more complex scenarios, ensuring that it can be used seamlessly with various GitOps tools.

2. **Introduce a Custom Mutator:**
   - Develop a custom mutator that can be integrated into the CI/CD pipeline or directly into the GitOps tool (e.g., Flux, ArgoCD).
   - The custom mutator will apply the necessary transformations to the Kubernetes manifests before they are applied to the cluster, ensuring that all references are correctly updated.

3. **Provide Documentation and Examples:**
   - Create comprehensive documentation and examples to guide package authors on how to use the enhanced `zarf prepare` command and the custom mutator.
   - Include best practices and common use cases to help package authors understand how to integrate these tools into their workflows.

4. **Automate Testing and Validation:**
   - Implement automated testing and validation to ensure that the mutations are applied correctly and do not introduce any errors or inconsistencies in the Kubernetes manifests.
   - Use CI/CD pipelines to validate the mutated manifests before they are deployed to the cluster.

By implementing these changes, we aim to provide a more seamless and automated experience for package authors, reducing the need for manual intervention and ensuring that the Zarf Registry and Git server references are consistently and correctly managed.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision

Given the context, we will simplify the current approach by removing the `stage2` go registry binary and instead use a more lightweight and efficient method to achieve the same goals. The new approach will involve:

1. **Using a Lightweight Docker Registry Alternative**: Instead of using a full-featured Docker registry, we will use a minimal, in-memory registry like `registry:2` in a read-write (rw) mode. This will be sufficient for the temporary storage and immediate consumption of the `registry:2` crane tarball.

2. **Simplifying the Process**:
   - **Step 1**: Start the minimal in-memory registry in `rw` mode.
   - **Step 2**: Push the `registry:2` crane tarball into the in-memory registry.
   - **Step 3**: Switch the in-memory registry to read-only (r) mode.
   - **Step 4**: Consume the `registry:2` image from the in-memory registry and proceed with the `zarf-registry` package.
   - **Step 5**: Destroy the injector pod once the image is consumed.

3. **Reducing ConfigMaps and Overhead**:
   - **ConfigMaps**: We will reduce the number of configmaps by consolidating the necessary configurations into fewer, more efficient configmaps.
   - **Binary Management**: By eliminating the `stage2` binary, we will only need to manage two binaries (`stage1` and the minimal in-memory registry), reducing the complexity and overhead of the system.
   - **Build Time and CI**: Removing the `stage2` binary will also reduce the build time and CI pipeline complexity, as there will be no need to compile and manage an additional binary for each platform.

This decision aims to streamline the process, reduce the overhead, and maintain the required functionality while improving the overall efficiency and simplicity of the system.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision

After considering the potential strategies for improving our testing coverage and ensuring that Zarf's interfaces are robust and maintainable, we have decided to **introduce Unit Testing in a defined and limited capacity within Zarf**. This approach will allow us to target specific areas of the codebase that are difficult to test with end-to-end (e2e) tests, while also keeping the maintenance burden manageable.

### Rationale

1. **Coverage of Critical Areas**: By selectively introducing unit tests, we can focus on the parts of the codebase that are most critical and challenging to test with e2e tests. This ensures that these areas are well-validated and reduces the risk of introducing bugs.

2. **Balanced Maintenance Burden**: While unit tests can become a maintenance burden if overused, by limiting their scope, we can ensure that the tests remain relevant and effective. This approach allows us to be more selective about where and how we apply unit tests, thus minimizing the overhead.

3. **Interface Scrutiny**: As Zarf is moving into being a library for other products, it is crucial to have a high level of confidence in the defined interfaces. Unit tests can help ensure that these interfaces are stable and reliable, which is essential for external consumers of the library.

4. **Vigilance in PRs**: We will need to be vigilant during the code review process to ensure that new unit tests are correctly implemented and do not introduce unnecessary complexity. This can be managed through clear guidelines and peer reviews.

### Implementation Plan

1. **Identify Critical Areas**: Conduct a codebase review to identify the most critical and hard-to-test areas. These should be prioritized for unit testing.

2. **Define Unit Test Guidelines**: Establish clear guidelines for writing unit tests, including best practices and standards. This will help ensure that the tests are effective and maintainable.

3. **Integrate with CI/CD**: Integrate the new unit tests into the continuous integration and continuous deployment (CI/CD) pipeline to ensure they are run automatically and consistently.

4. **Training and Documentation**: Provide training and documentation for the team on how to write and maintain unit tests effectively. This will help ensure that everyone is aligned and can contribute to the testing effort.

By following this decision, we aim to improve the overall quality and reliability of Zarf, while also ensuring that the testing strategy remains practical and sustainable.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision

We will implement option 2: **Run Syft against files and dataInjections**. This approach strikes a balance between providing meaningful and actionable SBOM information and avoiding the inclusion of extraneous or misleading data.

### Rationale

- **Relevance**: By focusing on files and dataInjections, we ensure that the SBOM includes only the artifacts that are actually part of the Zarf package. This reduces the risk of including unnecessary or irrelevant dependencies, such as index files from git repositories.
- **Common Use Cases**: Files and dataInjections are the most common ways for users to include additional artifacts in Zarf packages. This approach aligns with the typical use cases and ensures that the SBOM is useful and relevant to most users.
- **Simplicity and Usability**: This method is straightforward to implement and understand, making it more likely that users will find it valuable and easy to use. It avoids the complexity and potential confusion that could arise from allowing too much flexibility or including too much data.

### Implementation Plan

1. **Integrate Syft**: Integrate Syft into the Zarf build process to scan files and dataInjections.
2. **Generate SBOM**: Generate an SBOM for the scanned artifacts and include it in the Zarf package.
3. **Documentation**: Update the documentation to inform users about the SBOM generation process and the types of artifacts that are included.
4. **Testing**: Conduct thorough testing to ensure that the SBOM generation is accurate and does not miss any critical dependencies.

### Future Considerations

- **User Flexibility**: If there is a strong demand from users for more flexibility in what gets SBOMed, we can consider implementing a more configurable approach in the future. However, this should be done carefully to avoid adding unnecessary complexity.
- **Feedback Loop**: Collect feedback from users to continuously improve the SBOM generation process and ensure it meets their needs.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision

We will introduce a new feature in Zarf that allows for the deployment of Zarf packages in connected environments without the requirement of a Zarf init package. This will be achieved by extending the Zarf CLI to support a new command, `zarf package deploy`, which will enable users to directly deploy Zarf packages in a declarative manner. The new command will automatically handle the necessary Kubernetes resources and configurations required for the deployment, ensuring that the experience remains consistent with the principles of declarative K8s deployments. Additionally, we will update the documentation to include examples and best practices for using this feature in connected environments, thereby providing a seamless and flexible deployment option for both disconnected and connected scenarios.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision

To address the limitations of the current `scripts` implementation and to provide more flexibility and control, we will introduce a new structured script system. This system will support the following enhancements:

1. **Script Phases**: Expand the existing phases (`before`, `after`, `prepare`) to include additional lifecycle events such as `install`, `uninstall`, `upgrade`, and `pre-check`. This will allow for more granular control over the execution of scripts at different stages of the package lifecycle.

2. **Script Types**: Introduce support for different types of scripts, such as shell scripts, Python scripts, or any other executable format. This will enable users to write scripts in the language they are most comfortable with.

3. **Environment Variables**: Allow the definition of environment variables that can be used within the scripts. This will make it easier to pass configuration and context information to the scripts.

4. **Conditional Execution**: Implement a mechanism to conditionally execute scripts based on certain conditions (e.g., environment, platform, or user-defined conditions). This will provide more flexibility in how scripts are run.

5. **Error Handling and Logging**: Enhance error handling and logging capabilities to provide better visibility into the execution of scripts. This will help in debugging and troubleshooting issues.

6. **Documentation and Examples**: Provide comprehensive documentation and examples to guide users on how to use the new script system effectively. This will ensure that users can take full advantage of the new features.

By implementing these changes, we aim to provide a more robust and flexible scripting system that can better meet the needs of our users and accommodate a wider range of use cases.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision

Given the context and the trade-offs between performance and stability, we will adopt a hybrid approach to address the local image loading issue in Zarf. The decision is to use Crane for its speed and compatibility with future package schema changes, while also implementing safeguards to prevent out-of-memory (OOM) conditions.

### Rationale

1. **Performance**: Crane provides faster image loading compared to Docker, which is crucial for maintaining a good developer experience.
2. **Future Compatibility**: Crane's output format aligns better with our future plans for OCI imports, ensuring smoother transitions in upcoming releases.
3. **Stability**: To mitigate the OOM issue, we will implement the following:
   - **Memory Monitoring**: Introduce a mechanism to monitor the memory usage during the image loading process. If the memory usage exceeds a certain threshold, the process will be paused or terminated to prevent an OOM condition.
   - **Configuration Option**: Provide a configuration option that allows users to switch to a more stable but slower method (e.g., using the Docker CLI directly) if they encounter issues with Crane.
   - **Incremental Loading**: Investigate and potentially implement incremental loading of images to reduce the memory footprint. This could involve breaking down large images into smaller chunks and loading them sequentially.

### Implementation Steps

1. **Update Crane Usage**: Ensure Crane is used with the appropriate options to avoid OOM conditions, even if it means a slight performance hit.
2. **Add Memory Monitoring**: Develop and integrate a memory monitoring system to detect and handle high memory usage scenarios.
3. **Configuration Flexibility**: Add a configuration flag to allow users to choose between performance-optimized and stability-optimized methods.
4. **Documentation**: Update the documentation to inform users about the new configuration options and best practices for handling large images.
5. **Testing**: Conduct thorough testing to ensure the new implementation works as expected and does not introduce new issues.

### Expected Outcomes

- Improved stability and reliability of Zarf when loading large images.
- Maintained or slightly reduced performance, with the option for users to prioritize stability over speed.
- Better alignment with future package schema changes, ensuring long-term compatibility.

This decision balances the need for performance and stability, providing a robust solution for local development with Zarf.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision

To address the complexities and challenges associated with deploying Big Bang using Zarf, we will implement a multi-step approach that simplifies the process while optimizing storage and ensuring only necessary components are included. The decision is broken down into the following key points:

1. **Flux Integration**:
   - **Decision**: Integrate Flux as a dependency within the Zarf package for Big Bang.
   - **Rationale**: Since Big Bang requires Flux to deploy correctly, including Flux in the Zarf package ensures that the necessary CRDs and controllers are available. This simplifies the deployment process and reduces the risk of missing dependencies.

2. **Image Management**:
   - **Decision**: Implement a dynamic image selection mechanism.
   - **Rationale**: Instead of including all possible images in the Zarf package, which would be highly inefficient, we will create a script or tool that dynamically selects and includes only the images required based on the `values.yaml` file provided by the user. This approach significantly reduces the storage footprint and ensures that only the necessary images are deployed.

3. **Git Repository Management**:
   - **Decision**: Use a configuration-driven approach to manage git repositories.
   - **Rationale**: Similar to the image management, we will create a mechanism to include only the necessary git repositories based on the `values.yaml` file. This can be achieved by generating a custom `kustomization.yaml` file that references only the required repositories, reducing the overall complexity and size of the deployment.

4. **Simplified `values.yaml` Injection**:
   - **Decision**: Develop a tool or script to simplify the injection of `values.yaml` into the Big Bang deployment structure.
   - **Rationale**: The default deployment structure of Big Bang is complex, and injecting a `values.yaml` file can be challenging. By providing a tool or script that automates this process, we can ensure that the `values.yaml` file is correctly applied, and the necessary images and git repositories are discovered and included in the deployment.

5. **Documentation and User Guidance**:
   - **Decision**: Provide comprehensive documentation and user guidance.
   - **Rationale**: To support users in deploying Big Bang with Zarf, we will create detailed documentation that explains the new processes, tools, and scripts. This will include step-by-step instructions, examples, and best practices to help users understand and effectively use the Zarf package for Big Bang.

By implementing these decisions, we aim to streamline the deployment of Big Bang with Zarf, making it more efficient, manageable, and user-friendly.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision

We will implement support for publishing Zarf packages to an OCI (Open Container Initiative) compliant registry. This decision aligns with our primary goal of enabling users to distribute and consume Zarf packages in a scalable, secure, and trustable manner. By leveraging OCI registries, we can take advantage of their widespread adoption, robust security features, and the ability to integrate with existing CI/CD pipelines.

### Key Components of the Decision:

1. **OCI Registry Support:**
   - **Publishing:** Develop functionality within Zarf to allow users to publish Zarf packages to any OCI-compliant registry. This includes generating the necessary metadata and ensuring that the package is stored in a format that can be easily consumed by other Zarf instances.
   - **Consumption:** Enable Zarf to pull and install packages from OCI registries, ensuring that the process is seamless and integrates well with existing workflows.

2. **Package Signing:**
   - **Signing Mechanism:** Implement a mechanism for package creators to sign Zarf packages using cryptographic signatures. This will involve generating and managing public/private key pairs, and embedding the signature in the package metadata.
   - **Verification:** Develop a verification process within Zarf to check the integrity and authenticity of signed packages. This will ensure that only trusted packages are installed, enhancing supply chain security.

3. **Security and Trust:**
   - **Integrity Checks:** Ensure that all packages published to and pulled from OCI registries undergo integrity checks, such as checksum validation, to prevent tampering.
   - **Trust Model:** Establish a trust model where package deployers can verify the identity of the package creator and the integrity of the package. This may include maintaining a list of trusted public keys or integrating with existing trust frameworks.

4. **User Experience:**
   - **Simplicity:** Design the publishing and consumption processes to be as simple and intuitive as possible, minimizing the learning curve for new users.
   - **Documentation and Support:** Provide comprehensive documentation and support to help users understand how to publish and consume Zarf packages via OCI registries, including best practices for signing and verifying packages.

By implementing these changes, we will achieve our primary goal of allowing users to natively publish Zarf packages to OCI-compliant registries, while also addressing the secondary goal of enabling package creators to sign and verify the integrity of their packages. This will significantly enhance the distribution, security, and trustworthiness of Zarf packages.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision

To address the limitations in supporting airgap development and to enhance the flexibility of using `git` within the Zarf environment, we will implement the following changes:

1. **Expand Artifact and Library Support**: Introduce a new feature that allows Zarf to manage and host a broader range of artifacts and libraries, including those needed for various coding languages. This will be achieved by:
   - Adding support for package managers (e.g., npm, pip, Maven) to facilitate the download and caching of necessary dependencies.
   - Implementing a local repository or mirror for these packages, ensuring that developers can access them even in an airgapped environment.

2. **Enhance Git Integration**:
   - Develop a more generic and flexible `git` integration that supports not only Flux `GitRepository` objects but also other common `git` use cases.
   - Provide a mechanism for clients to interact with upstream `git` hosts in a native way, such as through SSH or HTTPS, while still maintaining the security and isolation required in an airgapped environment.
   - Consider integrating a `git` proxy or gateway service that can handle authentication, caching, and mirroring of `git` repositories, making it easier for developers to work with `git` in an airgapped setup.

3. **Documentation and Best Practices**:
   - Update the documentation to include detailed instructions on how to set up and use the new features, including best practices for managing dependencies and `git` repositories in an airgapped environment.
   - Provide examples and templates for common use cases, such as setting up a local npm registry or configuring a `git` proxy.

By implementing these changes, Zarf will better support both production and development workflows in airgapped environments, enhancing the overall user experience and expanding the tool's utility.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision

To transition Zarf into a ""Generally Available"" v1.x.x product suitable for mission-critical operations, we will implement the following key changes:

1. **Stabilize the API and CLI Interface**: We will freeze the current API and CLI interface to prevent breaking changes. Any new features or modifications that require changes to the existing interfaces will be introduced in a backwards-compatible manner or through a clearly defined deprecation process.

2. **Enhance Testing and Quality Assurance**:
   - **Expand Unit and Integration Tests**: Increase the coverage of unit and integration tests to ensure that all critical paths and edge cases are thoroughly tested.
   - **Implement End-to-End (E2E) Testing**: Develop and maintain a suite of E2E tests that simulate real-world usage scenarios, ensuring that the entire system works as expected in various environments.
   - **Automate Testing Pipelines**: Integrate automated testing into the CI/CD pipeline to catch issues early and ensure that every commit is tested before it is merged.

3. **Documentation and User Guides**:
   - **Comprehensive Documentation**: Provide detailed and up-to-date documentation covering installation, configuration, and usage of Zarf. This should include best practices, troubleshooting guides, and examples.
   - **User Training and Support**: Offer training sessions and support channels (e.g., forums, Slack, email) to assist users and gather feedback.

4. **Release Management**:
   - **Semantic Versioning**: Adopt semantic versioning (SemVer) to manage releases, ensuring that minor and patch versions are backwards-compatible, and major versions introduce breaking changes with proper deprecation notices.
   - **Release Notes and Changelogs**: Provide clear and detailed release notes and changelogs for each version, highlighting new features, bug fixes, and any breaking changes.

5. **Community and Feedback Loop**:
   - **Engage with the Community**: Actively engage with the community through regular updates, blog posts, and participation in relevant forums and events.
   - **Feedback Mechanism**: Establish a robust feedback mechanism to collect and address user feedback, ensuring that the product evolves based on real user needs.

By implementing these changes, we will ensure that Zarf is reliable, well-tested, and easy to use, making it a trusted tool for mission-critical operations.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision

To address the challenges of using ECR as a remote registry and to ensure that Zarf packages remain system-agnostic, we will introduce a new feature called `hooks` specifically designed to handle the lifecycle events related to ECR. These hooks will be part of the core Zarf functionality, ensuring that they are integrated seamlessly into the deployment process without requiring package creators or deployers to manage ECR-specific configurations.

### Key Points of the Decision:

1. **Introduction of ECR-Specific Hooks:**
   - **Token Refresh Hook:** A hook that automatically refreshes ECR authentication tokens before they expire. This hook will run periodically (e.g., every 10 hours) to ensure that the cluster always has a valid token.
   - **Repository Creation Hook:** A hook that automatically creates the necessary ECR repositories for images being pushed during the package deployment. This hook will run before the image push operation to ensure that the repository exists.

2. **Integration with Core Zarf Functionality:**
   - The hooks will be integrated into the core Zarf codebase, making them available to all Zarf users without requiring any additional configuration or actions from the package creators or deployers.
   - The hooks will be triggered based on the lifecycle events (e.g., `pre-deploy`, `post-deploy`, `token-expiration-check`) and will be executed by Zarf itself, ensuring a consistent and reliable experience.

3. **System-Agnostic Packages:**
   - Package creators will not need to include ECR-specific actions in their packages. Instead, they can rely on the core Zarf functionality to handle ECR-related tasks.
   - This approach ensures that Zarf packages remain portable and can be deployed on clusters initialized with different types of remote registries, including ECR, without any modifications.

4. **User Experience:**
   - Users deploying Zarf packages on a cluster initialized with ECR will not need to perform any additional steps or configurations. The hooks will handle the necessary ECR-specific tasks in the background.
   - This simplifies the deployment process and reduces the likelihood of errors or issues related to ECR token expiration or missing repositories.

### Implementation Plan:

1. **Develop ECR Token Refresh Hook:**
   - Implement a periodic task that checks the validity of the ECR token and refreshes it if it is close to expiration.
   - Integrate this task into the Zarf deployment lifecycle, ensuring it runs at appropriate intervals.

2. **Develop ECR Repository Creation Hook:**
   - Implement a pre-deployment check that verifies the existence of the required ECR repositories and creates them if they do not exist.
   - Integrate this check into the Zarf deployment process, ensuring it runs before any image push operations.

3. **Testing and Validation:**
   - Thoroughly test the hooks to ensure they work correctly in various scenarios, including initial deployments, updates, and edge cases.
   - Validate that the hooks do not interfere with other Zarf functionalities and that they provide a seamless experience for both package creators and deployers.

4. **Documentation and Communication:**
   - Update the Zarf documentation to include information about the new ECR-specific hooks and how they simplify the deployment process.
   - Communicate the changes to the Zarf community, highlighting the benefits of the new hooks and how they improve the overall user experience.

By introducing these ECR-specific hooks, we aim to provide a more robust and user-friendly solution for managing ECR as a remote registry, while maintaining the portability and simplicity of Zarf packages.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision

After careful consideration of the options, we have decided to use **gRPC** for decoupling transformations from Zarf. 

### Rationale

- **WASM (WebAssembly)**: While WASM is a powerful technology that allows for running code across different environments, it would require significant changes to our existing Go and TypeScript codebases. Additionally, the overhead of managing WASM modules and ensuring compatibility with both languages could be complex and time-consuming.

- **gRPC**: gRPC is a high-performance, open-source RPC (Remote Procedure Call) framework that supports multiple languages, including Go and TypeScript. It provides a robust and efficient way to communicate between services. By using gRPC, we can define a clear, versioned API for our transformations, which will help in maintaining consistency and reducing the risk of divergence between the Go and TypeScript implementations. gRPC also supports streaming, which can be beneficial if we need to handle large data sets or real-time updates.

- **REST**: REST is a widely used and well-understood protocol, but it is generally less performant than gRPC, especially for high-frequency, low-latency operations. REST also lacks the strong typing and contract enforcement that gRPC provides through Protocol Buffers, which can lead to more errors and maintenance overhead.

- **Rewrite the code in TypeScript**: Rewriting the entire transformation library in TypeScript would be a significant undertaking and would introduce a substantial amount of risk. It would also mean losing the battle-tested, hardened, and reliable nature of the existing Go code. This approach would likely result in a longer development cycle and increased maintenance costs.

### Implementation Plan

1. **Define the gRPC Service**: We will start by defining the gRPC service and its methods using Protocol Buffers. This will include the necessary input and output types for the transformations.

2. **Implement the gRPC Server in Go**: The existing Go code will be adapted to serve as the gRPC server, handling the transformation requests and responses.

3. **Implement the gRPC Client in TypeScript**: We will create a gRPC client in TypeScript that can call the gRPC server to perform the transformations. This will ensure that the TypeScript code remains synchronized with the Go code.

4. **Testing and Validation**: We will thoroughly test the new gRPC-based system to ensure that it meets the performance and reliability requirements. This will include unit tests, integration tests, and end-to-end tests.

5. **Deployment and Monitoring**: Once the system is validated, we will deploy it and set up monitoring to ensure that it operates correctly in production.

By choosing gRPC, we aim to achieve a balance between performance, maintainability, and ease of integration, while ensuring that the transformation logic remains consistent and reliable.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision

To address the inconsistencies in Zarf's handling of different package sources, we will standardize and enhance the functionality across all supported actions. Specifically, we will ensure that all package sources (Local Tarball, Split Tarball, OCI package, and In-cluster) can be:

1. **Created**: Ensure that all package types can be created using appropriate commands.
2 - **Deployed**: Ensure that all package types can be deployed to a cluster.
3. **Inspected**: Implement the ability to inspect all package types, providing details such as contents, metadata, and status.
4. **Removed**: Ensure that all package types can be removed, either by name or by reference.

### Implementation Steps

1. **Split Tarball Packages**:
   - Add support for inspecting split tarball packages.
   - Add support for removing split tarball packages.

2. **In-cluster Packages**:
   - Add support for inspecting in-cluster packages, including detailed metadata and status.
   - Ensure that in-cluster packages can be removed by name or reference.

3. **HTTPs URLs**:
   - Add support for inspecting HTTPs URL packages, including fetching and displaying metadata.
   - Add support for removing HTTPs URL packages, possibly by maintaining a local cache or registry.

4. **General Enhancements**:
   - Refactor the command-line interface (CLI) to provide a consistent user experience for all package types.
   - Update the documentation to reflect the new capabilities and ensure it is clear and comprehensive.

### Rationale

By standardizing the functionality across all package types, we improve the usability and reliability of Zarf. This decision ensures that users can manage their packages consistently, regardless of the source type, leading to a more robust and user-friendly tool.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision

To address the issues with Zarf's package composition, we will implement the following changes:

1. **Refactor Import Chain Handling:**
   - **Centralize and Optimize Import Chain Logic:** We will refactor the import chain logic to be more centralized and optimized. This will involve creating a dedicated service or module responsible for managing import chains, ensuring that all layers are handled uniformly and efficiently.
   - **Enhance Test Coverage:** We will expand the test coverage to include a broader range of import scenarios, including deep import chains. This will help in identifying and fixing any potential bugs and ensure that the system can handle larger numbers of layers.

2. **Integrate OCI Skeletons Seamlessly:**
   - **Rethink Composability with OCI Skeletons:** We will re-evaluate how OCI skeletons integrate with the existing composability features. This may involve redesigning the way OCI skeletons are created and managed to ensure they are seamlessly integrated into the package creation process.
   - **Ensure Consistency and Usability:** The integration will be designed to maintain consistency and usability, ensuring that users can easily create and manage packages with OCI skeletons without encountering unexpected issues.

3. **Centralize Component Filtering:**
   - **Implement Centralized Filtering Logic:** We will centralize the `only` filter logic to a single, well-defined location within the codebase. This will ensure that the filtering is applied consistently across all stages of the package creation process.
   - **Improve Create-Time Filters:** We will enhance the create-time filters to ensure they work correctly and reliably. This will involve thorough testing and validation to prevent any bugs related to component filtering.

By implementing these changes, we aim to improve the robustness, scalability, and usability of Zarf's package composition features, ensuring that users can create complex and reusable packages with confidence.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision

To improve the package development lifecycle, especially for connected (non-air-gapped) environments, we will introduce a new `zarf package dev` command. This command will streamline the process of creating and deploying packages by:

1. **Combining Package Creation and Deployment**: The `zarf package dev` command will both create and deploy the package in a single step, reducing the need to manually run `zarf package create` and `zarf package deploy`.

2. **Enabling YOLO Mode by Default**: In the `zarf package dev` command, YOLO mode will be enabled by default. This will allow developers to skip the heavy dependency fetching (like Docker images) and `zarf init` steps, making the development process faster and more efficient.

3. **Automatic Cluster Reset**: The `zarf package dev` command will include an option to automatically reset the cluster before each deployment. This can be toggled on or off via a CLI flag (e.g., `--reset-cluster`), allowing developers to choose whether to reset the cluster or not based on their needs.

4. **Enhanced Error Handling and Debugging**: The command will provide enhanced error messages and debugging information, making it easier to identify and resolve issues in the `zarf.yaml` file.

5. **CLI Flag for YOLO Mode**: To make YOLO mode more accessible, we will add a CLI flag (e.g., `--yolo`) to the existing `zarf package create` and `zarf package deploy` commands. This will allow users to explicitly enable YOLO mode when needed, even if they are not using the new `zarf package dev` command.

By introducing the `zarf package dev` command and enhancing the existing YOLO mode, we aim to significantly reduce the time and effort required for package development and debugging, while maintaining the flexibility and robustness of the Zarf package management system.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision

We have decided to migrate our documentation site from Docusaurus 2 to MkDocs with the Material for MkDocs theme. This decision is based on several factors:

1. **Performance**: MkDocs, especially with the Material theme, is known for its fast build times and efficient rendering, which should address the performance issues we've experienced with Docusaurus.

2. **Simplicity and Ease of Use**: MkDocs has a simpler configuration and setup process, which can reduce the learning curve for new contributors and make it easier to manage the documentation.

3. **Dependency Management**: MkDocs relies on a smaller set of dependencies, which can simplify our dependency management and reduce the risk of conflicts or vulnerabilities.

4. **Customization and Themes**: The Material for MkDocs theme offers a modern, responsive design that is highly customizable, allowing us to maintain a professional and visually appealing documentation site.

5. **Community and Support**: Both MkDocs and the Material theme have active communities and comprehensive documentation, ensuring that we can find support and resources as needed.

By making this transition, we aim to improve the overall user experience, streamline our development and maintenance processes, and ensure that our documentation remains a high-quality resource for our users.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision

To address the issues with the current in-house logging solution and improve the overall quality, testability, and maintainability of the Zarf CLI, we will adopt a more robust and flexible logging library. Specifically, we will transition to using **Zap** from Uber, which is a high-performance, structured logging library for Go. This decision is based on the following considerations:

1. **Structured Logging**: Zap supports structured logging, which allows for more detailed and consistent log messages. This can be particularly useful for debugging and monitoring purposes.

2. **Performance**: Zap is designed to be highly performant, making it suitable for high-throughput applications. This is important for maintaining the responsiveness of the Zarf CLI.

3. **Customizability**: Zap provides a wide range of options for customizing log output, including different log levels, formats, and destinations. This flexibility will allow us to better control the user experience and meet the needs of different environments (e.g., development, testing, production).

4. **Thread Safety**: Zap is thread-safe, which will resolve the issues with the current global state and multi-threading. This will simplify the code and make it more reliable.

5. **Error Handling**: We will replace the use of `message.Fatal` with a more controlled error handling approach. Instead of exiting the program immediately, we will propagate errors up the call stack, allowing for proper context handling and clean-up before exiting. This will also make unit testing easier and more effective.

6. **Progress Indicators**: For progress bars and spinners, we will use a library like **go-echarts** or **progressbar** that is designed to be thread-safe and can be easily integrated with Zap. This will ensure that these components are decoupled from the global state and can be used safely in a multi-threaded environment.

7. **Logging Levels and Destinations**: We will define clear logging levels (e.g., DEBUG, INFO, WARN, ERROR) and configure Zap to direct logs to appropriate destinations (e.g., STDERR for errors, a file for debug logs). This will help in filtering and managing log output based on the severity and context.

By adopting Zap and implementing these changes, we aim to improve the reliability, performance, and maintainability of the Zarf CLI, while also enhancing the user experience and simplifying the development and testing processes.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision

We will introduce a formal schema versioning system starting with the release of Zarf v1. This decision is driven by the need to provide a more stable and predictable experience for our users, while also allowing for necessary changes and improvements in the future. The following steps will be taken:

1. **Define and Document v1 Schema**: Clearly define and document the v1 schema, including all supported features and their specifications. This documentation will serve as a reference for both users and developers.

2. **Deprecation Policy**: Establish a deprecation policy that ensures any deprecated feature will remain functional throughout the v1 lifecycle but will be marked as deprecated and will trigger warnings during use. This will give users ample time to migrate to new features.

3. **Automatic Migration**: For deprecated keys that can be automatically migrated (e.g., `setVariable` to `setVariables`), implement automatic migration scripts. These scripts will update the zarf.yaml file during the package creation process, but the original deprecated key will not be removed from the packaged zarf.yaml to maintain compatibility with older Zarf binaries.

4. **Removal of Deprecated Features in Future Versions**: Clearly communicate that deprecated features will be removed in the next major version (v2). This will allow users to plan and migrate their configurations accordingly.

5. **Versioned Schemas**: Introduce a version number in the zarf.yaml file to indicate which schema version it adheres to. This will help in managing backward compatibility and enforcing schema validation.

6. **Validation and Warnings**: Enhance the validation process to check the schema version and provide detailed warnings or errors if deprecated features are used. This will help users identify and update their configurations.

By implementing these steps, we aim to provide a more robust and user-friendly experience, ensuring that Zarf remains a reliable and forward-compatible tool for our users.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
