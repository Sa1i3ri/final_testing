context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision 

We will implement native lazy loading for images and thumbnails in the Storefront. Given that current browsers support the `loading=""lazy""` attribute, we can leverage this feature to improve the performance of our web pages by deferring the loading of offscreen images until they are needed.

### Implementation Steps:
1. **Update HTML Templates**: Modify the HTML templates where images and thumbnails are rendered to include the `loading=""lazy""` attribute on `<img>` tags.
2. **Browser Compatibility Check**: Implement a fallback mechanism for older browsers that do not support the `loading` attribute. This can be done using JavaScript to dynamically add the `loading=""lazy""` attribute if the browser supports it, or to use a polyfill like ""lazysizes"" for unsupported browsers.
3. **Testing**: Conduct thorough testing across different browsers and devices to ensure that the lazy loading works as expected and does not introduce any new issues.
4. **Documentation**: Update the developer documentation to include information about the new lazy loading implementation and how to handle browser compatibility.

### Benefits:
- **Improved Page Load Times**: By only loading images when they are about to enter the viewport, we can significantly reduce the initial load time of the page.
- **Reduced Bandwidth Usage**: Users with limited bandwidth will benefit from not downloading images that are not visible.
- **Simplified Implementation**: Using native browser features simplifies the codebase and reduces the need for third-party libraries.

### Considerations:
- **Polyfills for Older Browsers**: Ensure that the fallback mechanism is robust and does not degrade the user experience on older browsers.
- **Performance Monitoring**: Continuously monitor the performance impact and user feedback to make further adjustments if necessary.

By adopting this approach, we align with modern web standards and provide a better user experience without the overhead of additional dependencies.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision 

We will deprecate and subsequently remove the `autoload` parameter from `OneToOneAssociationField` and `ManyToOneAssociationField` associations. Instead, we will encourage the use of explicit eager loading through query methods or specific API calls when the associated data is required.

### Implementation Steps:
1. **Deprecation Phase:**
   - Mark the `autoload` parameter as deprecated in the next minor release.
   - Update documentation to reflect the deprecation and provide guidance on how to use explicit eager loading.
   - Add warnings in the codebase to notify developers when the `autoload` parameter is used, suggesting the use of explicit eager loading.

2. **Removal Phase:**
   - In a subsequent major release, remove the `autoload` parameter entirely.
   - Ensure that all internal uses of `autoload` are refactored to use explicit eager loading.
   - Update any affected tests and examples in the documentation.

### Benefits:
- **Performance Improvement:** Reduces unnecessary data transfer and SQL query complexity.
- **Flexibility:** Developers can control when and how associated data is loaded, leading to more efficient and targeted queries.
- **Maintainability:** Simplifies the field configuration and reduces the risk of unintended performance bottlenecks.

### Considerations:
- **Backward Compatibility:** Provide a clear migration path and ample time for developers to update their code before the removal.
- **Documentation and Support:** Ensure that the transition is well-documented and that support is available to assist with the migration.

By making this change, we aim to improve the overall performance and maintainability of applications using these association fields.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision 
We will implement a hybrid approach to organizing ADRs that caters to different search and navigation preferences. This will involve:

1. **Maintaining the current folder structure by areas**: This will continue to support users who are looking for ADRs in a specific area, as it provides a clear and organized way to find related ADRs.

2. **Creating a centralized index file**: We will create a single `index.md` file at the root of the ADR directory. This file will list all ADRs in reverse chronological order, with links to each ADR. This will help users who want to read the latest ADRs or need a quick overview of all ADRs.

3. **Adding a unique identifier to each ADR**: Each ADR will have a unique identifier (e.g., `ADR-001`, `ADR-002`, etc.) in addition to its descriptive title. This will make it easier to reference and search for specific ADRs, reducing the likelihood of duplicate matches.

4. **Implementing a search-friendly naming convention**: The filenames of ADRs will follow a consistent and search-friendly naming convention, such as `adr-001-descriptive-title.md`. This will help users who know the headline or part of the title to quickly find the ADR using the directory's search functionality.

5. **Providing a README with instructions**: We will include a `README.md` file in the ADR directory that explains the organization, how to use the index, and how to search for ADRs. This will help new contributors and users understand the structure and how to navigate it effectively.

By combining these strategies, we can provide a more flexible and user-friendly system for accessing and navigating ADRs, catering to various use cases and preferences.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision 
Given the challenges identified with using native Vue components and the Composition API for extending and overwriting components, we will continue to use our current Component Factory approach but with targeted improvements to address the main drawbacks. Specifically, we will:

1. **Optimize the Component Factory:**
   - Enhance the performance of the Component Factory by pre-compiling as much as possible.
   - Implement caching mechanisms to reduce the overhead of generating components at runtime.

2. **Improve Tooling and Static Analysis:**
   - Develop custom tooling and scripts to support linting and static analysis of the generated Vue components.
   - Integrate these tools into our development workflow to ensure better code quality and maintainability.

3. **Vue Upgrade Strategy:**
   - Create a more robust upgrade path for Vue by documenting and testing the impact of Vue updates on our Component Factory.
   - Establish a process for regular updates and maintenance of the Component Factory to stay compatible with the latest Vue versions.

4. **Enhance Extensibility:**
   - Provide more granular and well-documented hooks for plugin developers to extend and overwrite components.
   - Ensure that the extensibility points are designed in a way that minimizes the need for deep integration with internal Vue logic, thus reducing the risk of breaking changes.

5. **Documentation and Developer Experience:**
   - Improve documentation for plugin developers to clearly explain how to extend and overwrite components using the Component Factory.
   - Provide examples and best practices to help developers understand and leverage the extensibility features effectively.

By focusing on these areas, we can retain the flexibility and extensibility of our current system while addressing the performance and tooling issues. This approach will allow us to continue to provide a powerful and flexible plugin extension system without the risks associated with deeply integrating with Vue's internal logic.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision 
We will adopt a per-branch changelog approach, where each feature or bug fix branch contains its own CHANGELOG.md file. This file will be used to document all changes made in that specific branch. Once the branch is merged into the main development trunk, the contents of the CHANGELOG.md from the branch will be automatically integrated into the main project's CHANGELOG.md using a script or an automated tool.

### Workflow:
1. **Branch Creation**: When a new branch is created for a feature or a bug fix, a new `CHANGELOG.md` file is also created in that branch. Developers are responsible for documenting their changes in this file as they work on the branch.
   
2. **Branch Merging**:
   - Before merging a branch into the main development trunk, the `CHANGELOG.md` in the branch must be reviewed and approved along with the code changes.
   - Upon merging, a pre-merge hook or a CI/CD pipeline step will automatically integrate the changes from the branch's `CHANGELOG.md` into the main project's `CHANGELOG.md`. This integration will include the version number of the upcoming release, which can be determined by the CI/CD pipeline based on the current version and the type of changes (e.g., major, minor, patch).

3. **Versioning**:
   - The version number in the branch's `CHANGELOG.md` will be a placeholder (e.g., `NEXT_VERSION`). The actual version number will be determined and updated during the merge process.
   - The main project's `CHANGELOG.md` will maintain a clear and chronological record of all changes, including the version numbers and release dates.

4. **Automation**:
   - Use a script or a tool to handle the automatic integration of the branch's `CHANGELOG.md` into the main `CHANGELOG.md`. This can be part of the CI/CD pipeline.
   - Ensure that the script or tool handles formatting, sorting, and deduplication of entries in the main `CHANGELOG.md`.

### Benefits:
- **Reduced Merge Conflicts**: By maintaining separate `CHANGELOG.md` files per branch, we minimize the risk of merge conflicts in the main `CHANGELOG.md`.
- **Clear Versioning**: The version number is determined and updated at the time of merging, ensuring that the correct version is associated with the changes.
- **Consistency**: The automated integration ensures that the main `CHANGELOG.md` is always up-to-date and consistent with the code changes.
- **Ease of Maintenance**: Developers only need to focus on documenting their changes in the branch's `CHANGELOG.md`, making the process simple and straightforward.

### Implementation:
- Develop or adopt a tool/script to handle the automatic integration of `CHANGELOG.md` files.
- Update the CI/CD pipeline to include the pre-merge hook or step for integrating the changelog.
- Provide clear documentation and guidelines for developers on how to use the new changelog workflow.

This approach will streamline the changelog management process, reduce conflicts, and ensure that the changelog is always accurate and up-to-date.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision 

To address the challenges of implementing the DelayAction in Flow Builder, we will adopt a strategy that ensures data and rule consistency even after a delay. Here’s how we plan to tackle the issues:

1. **Data Versioning and Snapshotting:**
   - When an event is triggered and a delay is set, we will create a snapshot of the relevant data at that moment. This snapshot will be stored alongside the delayed action.
   - The snapshot will include all necessary data required for the action to execute correctly, ensuring that the data used is consistent with the state at the time the delay was initiated.

2. **Rule Evaluation and Re-evaluation:**
   - We will store the rules associated with the delayed action at the time of the initial trigger. This ensures that the rules are preserved and can be re-evaluated when the delay period ends.
   - Upon the expiration of the delay, the system will first check if the stored rules still exist. If they do, the system will re-evaluate the rules using the snapshot data.
   - If the rules have been updated or removed, the system will log this and either use the latest version of the rules (if available) or fail the action with an appropriate error message, depending on the specific requirements and fallbacks defined by the user.

3. **Error Handling and Notifications:**
   - Implement robust error handling to manage cases where the rules no longer exist or the data is inconsistent.
   - Provide notifications or alerts to the user if the delayed action fails due to outdated rules or data, allowing them to take corrective actions.

4. **Testing and Validation:**
   - Develop comprehensive tests to ensure that the DelayAction works as expected, including scenarios where data and rules change during the delay period.
   - Validate the solution with real-world use cases to ensure it meets the needs of the users and handles edge cases effectively.

By implementing these strategies, we aim to ensure that the DelayAction in Flow Builder is reliable and consistent, even when dealing with delays and potential changes in data and rules.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision 

We will integrate the app system directly into the Shopware 6 platform as a core feature, rather than maintaining it as a separate plugin. This integration will be carried out in a way that ensures compatibility and stability across both cloud and on-prem environments.

### Implementation Steps
1. **Code Integration**: The codebase of the app system will be merged into the main Shopware 6 repository. This will involve refactoring and ensuring that the app system adheres to the coding standards and practices of the Shopware 6 platform.
2. **Testing and Validation**: Comprehensive testing will be conducted to ensure that the app system functions correctly within the platform. This includes unit tests, integration tests, and end-to-end tests.
3. **Documentation Update**: The documentation for Shopware 6 will be updated to include detailed information about the app system, including how to develop, install, and manage apps. This will help partners and app manufacturers understand and utilize the new feature.
4. **Release Cycle Alignment**: The app system will follow the same release cycle as the rest of the Shopware 6 platform. This means that updates and bug fixes will be released in line with the platform’s versioning and release schedule.
5. **Community and Partner Communication**: Clear communication will be provided to the community and partners about the integration of the app system. This will include timelines, benefits, and any changes in the development and deployment processes.

### Benefits
- **Stability and Reliability**: By integrating the app system into the core platform, we signal to partners and app manufacturers that the app system is stable and reliable.
- **Simplified Deployment**: Users will no longer need to install an extra plugin to use apps, simplifying the setup and maintenance process.
- **Consistent Upgrade Process**: Partners and app manufacturers can rely on the established upgrade process of the Shopware 6 platform, ensuring a consistent and predictable experience.
- **Enhanced Support and Maintenance**: The app system will benefit from the same level of support and maintenance as the rest of the platform, leading to a more robust and well-supported feature.

### Considerations
- **Backward Compatibility**: Ensure that the integration does not break existing functionality or plugins that depend on the current state of the platform.
- **Performance Impact**: Monitor and optimize the performance impact of the app system on the overall platform to maintain a high level of performance.
- **Security**: Conduct thorough security reviews to ensure that the app system does not introduce any new vulnerabilities into the platform.

By following these steps and considerations, we aim to seamlessly integrate the app system into the Shopware 6 platform, providing a robust and reliable extension mechanism for all users.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision 
To reduce the boilerplate and simplify the process of adding new `FlowStorer` implementations, we will introduce a more generic `FlowStorer` interface that can handle scalar values and potentially other types of data. This new interface will include a method to determine if an event should be stored, thereby unifying the current disparate interfaces.

### New `FlowStorer` Interface
```java
public interface FlowStorer<T> {
    boolean shouldStore(Event event);
    void store(Event event, T data);
}
```

### Implementation Strategy
1. **Unified Interface**: All `FlowStorer` implementations will now implement this single interface. The `shouldStore` method will replace the need for individual interfaces.
2. **Default Implementations**: Provide default implementations for common storage logic (e.g., storing scalar values) to further reduce boilerplate.
3. **Plugin Support**: Plugins can easily bypass or extend specific `FlowStorer` implementations by checking the `shouldStore` method and providing their own logic if needed.
4. **Configuration**: Allow configuration of `FlowStorer` instances through a central configuration mechanism, making it easier to manage and extend storers without modifying core code.

### Example Usage
```java
public class ScalarValueStorer implements FlowStorer<Object> {
    @Override
    public boolean shouldStore(Event event) {
        // Common logic to determine if the event should be stored
        return event.getType() == EventType.SCALAR;
    }

    @Override
    public void store(Event event, Object data) {
        // Store the scalar value
        System.out.println(""Storing: "" + data);
    }
}
```

### Benefits
- **Reduced Boilerplate**: Simplifies the addition of new storer implementations.
- **Enhanced Flexibility**: Easier for plugins to customize or bypass specific storers.
- **Maintainability**: Centralized and consistent approach to handling event storage.

This decision aligns with our goals of modularity, transparency, and configurability, ensuring that Arachne remains flexible and easy to extend.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision 
We will replace the current sw-text-editor with a modern, well-maintained, and highly customizable WYSIWYG editor that meets our requirements for stability, ease of maintenance, and extensibility. After evaluating several options, we have decided to adopt **TinyMCE** as the new text editor for the administration.

### Rationale:
1. **Stability and Reliability**: TinyMCE is a mature and widely-used WYSIWYG editor with a strong track record of stability and reliability. It has a large community and active development, which ensures that bugs are quickly addressed and new features are regularly added.

2. **Ease of Maintenance**: TinyMCE is well-documented and has a clean, modular architecture. This makes it easier for developers to understand, maintain, and extend the editor. The editor also provides a robust set of APIs and plugins, which can be used to customize and extend its functionality.

3. **Feature Set**: TinyMCE comes with a comprehensive set of built-in features, including rich text formatting, image and media embedding, and advanced editing tools. These features can be easily enabled or disabled based on our specific needs, ensuring that the editor is both powerful and flexible.

4. **Extensibility**: TinyMCE is highly extensible, allowing us to add custom plugins and integrations. This flexibility ensures that the editor can evolve with our project's needs and can be adapted to support new features and requirements in the future.

5. **Community and Support**: TinyMCE has a large and active community, which means there is a wealth of resources, tutorials, and third-party plugins available. Additionally, commercial support is available if needed, providing an extra layer of assurance.

### Implementation Plan:
1. **Evaluation and Customization**: We will conduct a detailed evaluation of TinyMCE to ensure it meets all our requirements. This will include setting up a test environment to experiment with different configurations and plugins.

2. **Integration**: We will integrate TinyMCE into the existing administration system, replacing the current sw-text-editor. This will involve updating the necessary code and configurations to ensure a smooth transition.

3. **Testing**: We will perform thorough testing to ensure that the new editor works correctly and does not introduce any new issues. This will include unit tests, integration tests, and user acceptance testing.

4. **Documentation and Training**: We will update the documentation to reflect the changes and provide training for the development team to ensure they are familiar with the new editor and its capabilities.

5. **Deployment**: Once testing is complete and the editor is stable, we will deploy the updated system to production. We will monitor the deployment closely to address any issues that may arise.

By adopting TinyMCE, we aim to provide a more stable, maintainable, and feature-rich text editing experience for the administration, ultimately improving the overall quality and usability of the system.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision
To ensure that the platform is inclusive and can gracefully handle cases where a salutation is not specified, we will update the default salutation setting in Shopware 6 to ""not_specified"". This change will be applied as follows:

1. **Update Default Salutation**: The default salutation in the system will be set to ""not_specified"" for any new or existing records where the salutation is currently null or empty. This will provide a clear and consistent fallback option.

2. **Database Schema Update**: Modify the database schema to include ""not_specified"" as a valid option in the salutation field. This ensures that the database can store and recognize this new value.

3. **Frontend and Backend Adjustments**: Update the frontend and backend code to handle and display ""not_specified"" appropriately. This includes updating form fields, validation rules, and any other relevant UI elements to reflect the new default.

4. **Documentation and Communication**: Update the developer and user documentation to reflect this change. Clearly communicate the purpose and usage of the ""not_specified"" salutation to all stakeholders, including developers, administrators, and end-users.

5. **Testing and Validation**: Conduct thorough testing to ensure that the new default salutation works as expected across all parts of the platform. This includes unit tests, integration tests, and user acceptance testing (UAT) to validate the changes.

By implementing these steps, we aim to create a more inclusive and robust system that can handle unspecified salutations gracefully, while also providing a clear and consistent user experience.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision 

We will discontinue the use of `padaliyajay/php-autoprefixer` for prefixing CSS in the storefront. Given that the current browser support, as defined in `.browserslist`, no longer necessitates the majority of the vendor prefixes being generated, and considering the performance impact on the `theme:compile` process, it is more efficient and sustainable to remove this step.

Instead, we will adopt a more modern and efficient approach to handle any necessary vendor prefixing. This can be achieved by integrating a tool like `PostCSS` with `autoprefixer`, which dynamically applies only the necessary prefixes based on the `.browserslist` configuration. This will ensure that:
- Only the required vendor prefixes are added, reducing the size of the compiled CSS.
- The `theme:compile` process becomes more efficient, improving the overall build time.
- The solution remains dynamic and adaptable to future changes in browser support, as it will respect the `.browserslist` file.

### Implementation Steps
1. **Remove `padaliyajay/php-autoprefixer`**:
   - Remove the package from the project dependencies.
   - Update the `theme:compile` script to exclude the auto-prefixing step.

2. **Integrate `PostCSS` and `autoprefixer`**:
   - Add `postcss` and `autoprefixer` to the project's development dependencies.
   - Configure `PostCSS` to use `autoprefixer` and set up the integration with the existing SCSS compilation process.
   - Ensure that the `.browserslist` file is correctly configured and referenced in the `PostCSS` setup.

3. **Update Documentation**:
   - Update the project documentation to reflect the new setup and the rationale behind the change.
   - Provide instructions for developers on how to set up and use the new tools.

4. **Testing and Validation**:
   - Conduct thorough testing to ensure that the CSS is correctly prefixed and that the `theme:compile` process is performing as expected.
   - Validate the output in all supported browsers to confirm that there are no issues with the new approach.

By making these changes, we will improve the performance and maintainability of the storefront CSS, while ensuring that the application remains compatible with the specified browser support.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision
To address the issue of pre-release versions being interpreted as less than `0.0.0` by npm, and to ensure that security vulnerabilities are properly reported, we will adopt the following practices:

1. **Avoid Using Pre-Release Versions for Critical Paths**: For any critical or production paths, we will avoid using pre-release versions of packages. This ensures that we are always on a stable, fully released version that can be correctly evaluated by npm for security vulnerabilities.

2. **Use Specific Version Ranges in package.json**: When specifying dependencies in `package.json`, we will use specific version ranges that exclude pre-release versions unless explicitly needed. For example, instead of `""^1.9.0""`, which could include `1.9.0-alpha1`, we will specify `""~1.9.0""` or `""1.9.0""` to pin down to a specific patch or minor version.

3. **Implement a Post-Install Script for Security Checks**: We will add a post-install script in `package.json` that runs a security audit (e.g., `npm audit`) after the installation process. This script will check for known vulnerabilities and report them, even if pre-release versions are used. This ensures that any potential security issues are not overlooked.

4. **Regularly Update Dependencies**: We will set up a regular schedule to update our dependencies to the latest stable versions. This helps in keeping the project secure and ensures that we are not relying on pre-release versions for an extended period.

5. **Documentation and Communication**: We will document these practices in our development guidelines and communicate them clearly to all team members. This ensures that everyone is aware of the importance of using stable versions and the potential risks associated with pre-release versions.

By implementing these practices, we can mitigate the risk of missing security reports due to the way npm handles pre-release versions, while still allowing for the use of pre-release versions in non-critical or development environments.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision 

To improve the performance of the SQL query for deleting outdated cart entries, we will add a composite index on the `cart` table. The index will be created on the columns `updated_at` and `created_at`. This should allow the database to more efficiently find and delete the relevant records.

The new index will be defined as follows:
- If `updated_at` is not null, the index will first use `updated_at` to filter out old entries.
- If `updated_at` is null, the index will then use `created_at` to filter out old entries.

The SQL statement to create the index will be:
```sql
CREATE INDEX idx_cart_updated_created ON cart (updated_at, created_at);
```

After adding this index, the `DELETE` query can be optimized to leverage the new index. The optimized query will look like this:
```sql
DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01')
LIMIT 1000;
```

This approach should significantly reduce the query execution time by allowing the database to use the index to quickly locate and delete the outdated entries. Additionally, the `LIMIT 1000` clause ensures that the operation is performed in manageable batches, which can help prevent locking issues and reduce the load on the database during the deletion process.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision 

We will introduce an ""Experimental Features"" framework to allow for the early release of features that are not yet finalized, in order to gather user feedback and iterate on the design. This approach will help us to:

- Release features earlier, allowing for real-world testing and feedback.
- Gather user insights and requirements, which can be used to refine and improve the feature.
- Lower the risk of developing features that do not meet user needs.
- Provide a clear communication channel with our users about the nature of these features and their potential changes.

### Implementation Details

1. **Experimental Feature Flagging:**
   - Each experimental feature will be marked with a special flag (e.g., `@experimental`).
   - The flag will indicate that the feature is not covered by the standard backwards compatibility promise and may undergo significant changes or even be removed in future releases.

2. **Documentation and Communication:**
   - Experimental features will be clearly documented in the release notes and documentation, highlighting their experimental status and the potential for changes.
   - Users will be informed that using these features involves a higher risk and that they should not rely on them for critical functionality until they are officially released as stable.

3. **Feedback Mechanism:**
   - A dedicated feedback mechanism (e.g., a forum, issue tracker, or survey) will be set up for each experimental feature to collect user input and suggestions.
   - Regular updates and summaries of the feedback will be provided to the development team and the broader community.

4. **Review and Graduation:**
   - After a sufficient period of testing and feedback, the development team will review the experimental feature.
   - If the feature is deemed stable and useful, it will be graduated to a stable release, at which point it will be covered by the backwards compatibility promise.
   - If the feature is not successful or requires significant changes, it may be revised, deprecated, or removed based on the feedback and internal review.

5. **Versioning and Deprecation:**
   - Experimental features will be versioned separately from the main product to allow for more frequent updates and changes.
   - When a feature is ready to graduate, it will be integrated into the main product, and the experimental version will be deprecated.

By implementing this ""Experimental Features"" framework, we aim to balance the need for innovation and user feedback with the stability and reliability that our users expect from our product.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision 

To address the limitations in stock handling within Shopware 6, we will implement a more flexible and extensible architecture for managing stock. The new approach will include the following key changes:

1. **Plugin System for Custom Stock Management:**
   - Introduce a plugin system that allows for custom implementations of stock loading and management. This will enable integration with external systems like ERPs.
   - Define clear interfaces and hooks for developers to plug in their own stock management logic. This will include methods for loading stock, increasing stock, and decreasing stock.

2. **Event-Driven Stock Updates:**
   - Implement an event-driven system for stock updates. This will allow for easy modification of how stock is increased or decreased at various points in the order lifecycle (e.g., when an order is placed, confirmed, or canceled).
   - Define specific events such as `StockDecrease`, `StockIncrease`, and `StockSync` that can be listened to and acted upon by custom plugins.

3. **Optimized Stock Calculation:**
   - Optimize the available stock calculation to improve performance, especially for large catalogs. This can be achieved through better indexing, caching, and possibly using more efficient data structures.
   - Consider implementing a background job or scheduled task to periodically recalculate and cache the available stock, reducing the load during real-time operations.

4. **Simplify Stock Representation:**
   - Simplify the stock representation by moving to a single value for stock. The available stock will be calculated on-the-fly based on the current stock and open orders.
   - This change will reduce complexity and make the stock management system easier to understand and maintain.

5. **Documentation and Best Practices:**
   - Provide comprehensive documentation and best practices for developers who want to create custom stock management plugins.
   - Include examples and guidelines to help developers integrate their solutions seamlessly with the core Shopware 6 platform.

By implementing these changes, we aim to make the stock handling in Shopware 6 more flexible, efficient, and capable of supporting a wider range of use cases.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision
We will implement a configurable logging level for specific exceptions to reduce noise in the logs and improve the ability to identify genuine errors. Specifically, we will:

1. **Maintain a Configuration List**: Create and maintain a configuration list that maps specific exception classes (e.g., `ShopwareHttpException` and its subclasses) to their desired logging levels. This list will be easily accessible and modifiable by developers and system administrators.

2. **Adjust Logging Levels Based on Configuration**: Modify the logging mechanism to check this configuration list when an exception is thrown. If the exception class is listed, it will log the exception at the specified level (e.g., `notice` for `ShopwareHttpException` with 40x status codes). If the exception class is not listed, it will continue to log the exception at the default `error` level.

3. **Update Documentation**: Update the project documentation to include instructions on how to modify the configuration list and the rationale behind this decision. This will help new contributors and users understand the importance of this approach and how to use it effectively.

4. **Monitor and Review**: Regularly review the configuration list and the impact of the logging changes. This will ensure that the logging levels are appropriate and that no critical information is being overlooked.

By implementing this decision, we aim to make the logs more meaningful and easier to analyze, while still providing the necessary information about client errors.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision 
We will transition from using UUIDv4 to UUIDv7 as the primary key generation strategy for our database entities. This decision is based on the following considerations:

1. **Performance and Efficiency**: UUIDv7, being time-ordered, will help in maintaining a more compact B-tree index. This leads to fewer page allocations in the index, which can improve read and write performance, especially in large-scale and high-transaction environments.

2. **Integration and Compatibility**: While UUIDv7 is time-ordered, it still provides a globally unique identifier, which is crucial for integrating data from multiple sources. The change from UUIDv4 to UUIDv7 should not affect the uniqueness and reliability of the identifiers.

3. **Index Size and Maintenance**: A more compact index due to the sequential nature of UUIDv7 can reduce the overall size of the index, leading to lower storage costs and potentially faster query times.

4. **Future Scalability**: As the system scales, the efficiency gains from using UUIDv7 could become more significant, providing a scalable solution for future growth.

### Implementation Plan
- **Phase 1: Testing and Validation**
  - Conduct thorough testing in a staging environment to ensure that the transition does not introduce any unforeseen issues.
  - Validate that the new UUIDv7 keys are compatible with all existing systems and integrations.

- **Phase 2: Gradual Rollout**
  - Begin the transition in a controlled manner, starting with non-critical or less frequently accessed tables.
  - Monitor the performance and stability of the system during this phase.

- **Phase 3: Full Transition**
  - Once the initial rollout is successful, proceed with updating all remaining tables and systems to use UUIDv7.
  - Ensure that all documentation and developer guidelines are updated to reflect the new primary key generation strategy.

- **Phase 4: Review and Optimization**
  - After the full transition, review the performance metrics and make any necessary optimizations.
  - Continuously monitor the system to ensure that the benefits of using UUIDv7 are realized and sustained.

This decision aligns with our goals of maintaining a performant and scalable system while ensuring that our data integration processes remain robust and reliable.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision 
To address the limitation where applications cannot extend the list of available events in the flow builder, we will implement a plugin system that allows developers to define and register custom events. This approach aligns with our goal of modularity and extensibility.

### Implementation Details:
1. **Event Definition Interface**: We will introduce an `EventDefinition` interface or protocol (depending on the language) that any custom event must implement. This interface will specify the necessary methods or properties for an event, such as `event_name`, `trigger_conditions`, and `payload_schema`.

2. **Plugin Registration Mechanism**: Developers will be able to register their custom events through a plugin registration mechanism. This can be done via configuration files, a dedicated API, or by placing the event definitions in a specific directory that the system scans at startup.

3. **Flow Builder Integration**: The flow builder will be updated to dynamically load and display the list of available events, including both built-in and custom events. This will ensure that all registered events are visible and usable within the flow builder.

4. **Validation and Documentation**: To maintain transparency and ensure that custom events are well-documented, we will provide tools and guidelines for validating and documenting custom events. This will include automatic validation against the `EventDefinition` interface and a template for documentation.

5. **Testing and Reusability**: We will encourage the creation of unit tests for custom events and provide a framework for testing these events. Additionally, we will facilitate the sharing and reusability of custom events across different projects and teams.

By implementing this plugin system, we will enhance the flexibility and extensibility of the flow builder, allowing developers to tailor the event system to their specific needs while maintaining the integrity and transparency of the architecture.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision 

To enable secure communication between an app and the App Server, we will implement a token-based authentication mechanism. This approach will allow the App Server to verify the authenticity of requests coming from the app, ensuring that only authorized apps can access protected resources.

The solution will involve the following steps:

1. **Token Generation:**
   - The Shopware 6 backend will generate a unique, time-limited token for each app that needs to communicate with the App Server.
   - This token will be signed using the shop-to-app server secret, ensuring that it cannot be tampered with.

2. **Token Distribution:**
   - The generated token will be securely transmitted to the app, possibly through an encrypted channel or via a secure API endpoint.
   - The app will store this token securely (e.g., in a secure storage mechanism provided by the platform).

3. **Token Usage:**
   - When the app makes a request to the App Server, it will include the token in the request headers.
   - The App Server will validate the token by checking its signature and expiration time.
   - If the token is valid, the App Server will process the request; otherwise, it will reject the request.

4. **Token Revocation:**
   - The Shopware 6 backend will provide a mechanism to revoke tokens if necessary, such as when an app is uninstalled or when security is compromised.
   - Revoked tokens will be stored in a list, and the App Server will check against this list before processing any request.

5. **Logging and Monitoring:**
   - Both the Shopware 6 backend and the App Server will log token generation, usage, and revocation events.
   - These logs will be used for monitoring and auditing purposes to ensure the security of the system.

By implementing this token-based authentication mechanism, we can ensure that the App Server can verify the origin of requests from the app, providing a secure and reliable way for the app to access protected resources.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision 
We will implement a Flow Builder Preview feature to enhance the user experience and provide better clarity on how flows are executed. This preview will be a visual representation of the flow, showing all the steps and decision points without actually executing the flow. Here’s how we will proceed:

1. **Visual Representation**: The preview will display the flow in a graphical format, with nodes representing actions and decisions, and edges showing the flow of execution. This will help users understand the sequence and structure of their flow.

2. **Interactive Elements**: Users will be able to interact with the preview by selecting nodes to see more detailed information about each step, such as the conditions for decision nodes and the actions for action nodes.

3. **Step-by-Step Walkthrough**: The preview will include a step-by-step walkthrough feature, allowing users to manually traverse the flow and see which paths would be taken under different conditions. This will help in understanding the logic and potential outcomes of the flow.

4. **Validation and Feedback**: The preview will also include basic validation to highlight potential issues or errors in the flow, such as missing conditions or unconnected nodes. This will help users catch and correct problems before the flow is executed.

5. **Documentation and Tips**: Each node in the preview will have associated documentation and tips, providing guidance on best practices and common pitfalls. This will help users build more reliable and effective flows.

6. **Integration with Flow Builder**: The preview feature will be seamlessly integrated into the Flow Builder interface, making it easily accessible and intuitive to use.

By implementing this Flow Builder Preview, we aim to reduce the time and effort required for debugging and ensure that merchants can build and maintain reliable flows with confidence.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃÂÃÂ¡ÃÂÃÂ°realÃÂÃÂ¡ÃÂÃÂ± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision
To maintain a clear separation of concerns and to enhance the modularity and flexibility of the media system, we will introduce a dedicated service for URL generation. This service will encapsulate the logic for generating both absolute and relative URLs for media and thumbnails, using the `UrlGeneratorInterface` implementations.

### Changes:
1. **Create a `MediaUrlService`**:
   - This service will be responsible for generating URLs for media and thumbnails.
   - It will use an instance of `UrlGeneratorInterface` to generate the URLs.
   - The `MediaUrlService` will be injected with the `UrlGeneratorInterface` and `PathnameStrategyInterface` dependencies.

2. **Refactor the Event Subscriber**:
   - The current event subscriber that listens to the `media.loaded` event will be refactored to use the `MediaUrlService`.
   - Instead of directly calling the `UrlGeneratorInterface` methods, the event subscriber will delegate the URL generation to the `MediaUrlService`.

3. **Update the `UrlGeneratorInterface`**:
   - The `UrlGeneratorInterface` will remain unchanged, but it will be used by the `MediaUrlService` to generate URLs.
   - The `MediaUrlService` will handle the orchestration of URL generation, ensuring that the correct strategy is applied based on the configured `PathnameStrategyInterface`.

4. **Configuration and Dependency Injection**:
   - The `MediaUrlService` and the `UrlGeneratorInterface` implementation will be registered as services in the dependency injection container.
   - The `PathnameStrategyInterface` will also be registered and configured to allow for different strategies to be used.

### Example Implementation:

```php
// MediaUrlService.php
class MediaUrlService
{
    private $urlGenerator;
    private $pathnameStrategy;

    public function __construct(UrlGeneratorInterface $urlGenerator, PathnameStrategyInterface $pathnameStrategy)
    {
        $this->urlGenerator = $urlGenerator;
        $this->pathnameStrategy = $pathnameStrategy;
    }

    public function getAbsoluteMediaUrl(MediaEntity $media): string
    {
        return $this->urlGenerator->getAbsoluteMediaUrl($media);
    }

    public function getRelativeMediaUrl(MediaEntity $media): string
    {
        return $this->urlGenerator->getRelativeMediaUrl($media);
    }

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string
    {
        return $this->urlGenerator->getAbsoluteThumbnailUrl($media, $thumbnail);
    }

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string
    {
        return $this->urlGenerator->getRelativeThumbnailUrl($media, $thumbnail);
    }
}

// MediaLoadedEventSubscriber.php
class MediaLoadedEventSubscriber implements EventSubscriberInterface
{
    private $mediaUrlService;

    public function __construct(MediaUrlService $mediaUrlService)
    {
        $this->mediaUrlService = $mediaUrlService;
    }

    public static function getSubscribedEvents()
    {
        return [
            'media.loaded' => 'onMediaLoaded',
        ];
    }

    public function onMediaLoaded(MediaLoadedEvent $event)
    {
        $media = $event->getMedia();
        $media->setAbsoluteUrl($this->mediaUrlService->getAbsoluteMediaUrl($media));
        $media->setRelativeUrl($this->mediaUrlService->getRelativeMediaUrl($media));

        foreach ($media->getThumbnails() as $thumbnail) {
            $thumbnail->setAbsoluteUrl($this->mediaUrlService->getAbsoluteThumbnailUrl($media, $thumbnail));
            $thumbnail->setRelativeUrl($this->mediaUrlService->getRelativeThumbnailUrl($media, $thumbnail));
        }
    }
}
```

### Benefits:
- **Modularity**: The `MediaUrlService` can be easily extended or replaced without affecting the rest of the system.
- **Separation of Concerns**: The URL generation logic is encapsulated in a single service, making the codebase cleaner and more maintainable.
- **Flexibility**: Different URL generation strategies can be easily swapped out by changing the configuration of the `UrlGeneratorInterface` and `PathnameStrategyInterface`.

This approach aligns with the goals of modularity, transparency, and configurability, ensuring that the media system remains flexible and easy to understand.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision 
We will implement a dedicated one-time data migration service to handle specific, non-repetitive data updates between different Shopware versions. This approach ensures that critical data migrations are executed only once and not repeatedly with each indexer run, thereby avoiding potential system errors.

### Implementation Details:
1. **Creation of a One-Time Migration Service:**
   - Develop a new service within the Shopware architecture specifically for handling one-time data migrations.
   - This service will be responsible for executing predefined migration scripts or logic that are designed to run only once.

2. **Migration Script Management:**
   - Create a directory structure to store migration scripts.
   - Each script will include a unique identifier and a version number to ensure it is executed only once per version update.
   - Use a database table to track which migrations have been executed, ensuring they do not run multiple times.

3. **Triggering Migrations:**
   - The one-time migration service will be triggered as part of the Shopware update process.
   - Ensure that the service checks the migration status before running any script to prevent re-execution.

4. **Integration with Indexers:**
   - Modify existing indexers to check if a one-time migration has been executed before performing their regular tasks.
   - If a required one-time migration is detected, the indexer can log a warning and skip its operation, or gracefully handle the situation without causing system errors.

5. **Testing and Validation:**
   - Implement comprehensive testing for the one-time migration service to ensure that migrations are executed correctly and only once.
   - Include validation steps to verify the integrity of the migrated data and the overall system state after the migration.

6. **Documentation and Communication:**
   - Document the new one-time migration service and provide clear instructions for developers on how to create and manage migration scripts.
   - Communicate the changes to the development team and provide training if necessary to ensure smooth adoption.

By implementing this dedicated one-time data migration service, we can ensure that critical data updates are handled efficiently and safely, reducing the risk of system errors and improving the overall stability of the platform.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision 
To address the challenges of managing and tracking experimental features more effectively, we will introduce a structured approach to organizing and annotating experimental code. This will help in maintaining a clear and manageable codebase, even as experiments are extended or deprecated.

### Key Changes:
1. **Feature-Specific Directories:**
   - Create a dedicated directory structure for each experimental feature within the codebase. For example, all code related to an experimental feature `X` should be placed in a directory named `experimental/X`.
   - This will help in isolating the code related to each feature, making it easier to manage and track.

2. **Enhanced Annotation:**
   - Enhance the `@experimental` annotation to include additional metadata such as the `featureName`, `stableVersion`, and `deprecationStatus`.
   - Example:
     ```java
     @Experimental(featureName = ""X"", stableVersion = ""1.2.0"", deprecationStatus = ""active"")
     ```
   - This will provide a more detailed and consistent way to track the status of each experimental feature.

3. **Automated Tooling:**
   - Develop or integrate a tool that can scan the codebase and generate a report of all experimental features, their locations, and their current status (e.g., active, deprecated, prolonged).
   - This tool can also automatically update the `stableVersion` property when an experiment is extended and mark components as deprecated when a feature is killed.
   - The tool can be integrated into the CI/CD pipeline to ensure that the codebase is always up-to-date and compliant with the experimental feature guidelines.

4. **Documentation and Guidelines:**
   - Update the development documentation to include best practices for using the new directory structure and enhanced annotations.
   - Provide clear guidelines on how to extend, deprecate, and remove experimental features, including the use of the automated tooling.

### Workflow:
1. **Adding a New Experimental Feature:**
   - When adding a new experimental feature, create a new directory under `experimental/` and place all related code there.
   - Use the enhanced `@experimental` annotation with the appropriate metadata.

2. **Extending an Experimental Feature:**
   - When extending an experimental feature, use the automated tool to update the `stableVersion` property across all relevant code sections.
   - Ensure that the `deprecationStatus` is set to ""active"" if the feature is still in development.

3. **Deprecating a Feature:**
   - When a feature is decided to be deprecated, use the automated tool to mark all related components as deprecated.
   - Set the `deprecationStatus` to ""deprecated"" in the `@experimental` annotation.
   - Ensure that the deprecated feature remains in the codebase until the next major version, as per the existing guidelines.

### Benefits:
- **Improved Traceability:** The feature-specific directories and enhanced annotations will make it easier to trace and manage experimental code.
- **Consistency and Compliance:** Automated tooling will help maintain consistency and ensure compliance with the experimental feature guidelines.
- **Reduced Manual Effort:** The automated tool will reduce the manual effort required to update and deprecate features, allowing developers to focus on other tasks.

By implementing these changes, we aim to streamline the management of experimental features, improve the overall quality of the codebase, and enhance the development experience.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision 
To address the challenge of uniquely identifying payment and shipping methods, we will introduce a more robust and flexible identification system. Specifically, we will implement a unique identifier (UUID) for each payment and shipping method. This UUID will be generated at the time of creation and will remain constant throughout the lifecycle of the method, even if other attributes such as the name or description are changed.

### Implementation Details:
1. **UUID Generation**: Each payment and shipping method will be assigned a UUID upon creation. This UUID will be stored in the database alongside the existing ID and other attributes.
   
2. **API Enhancements**:
   - The Shopware Admin API will be updated to include the UUID in the response when fetching payment and shipping methods.
   - New endpoints will be added to allow querying payment and shipping methods by UUID.
   - Existing endpoints will be updated to ensure that the UUID is included in the response data.

3. **Documentation and Developer Guidance**:
   - Documentation will be updated to guide developers on how to use the UUID for identifying payment and shipping methods.
   - Best practices and examples will be provided to help developers transition to using UUIDs.

4. **Backward Compatibility**:
   - To maintain backward compatibility, the current ID-based system will remain in place, but developers will be encouraged to migrate to the new UUID-based system.
   - A deprecation notice will be added to the documentation for the ID-based system, with a timeline for its eventual removal.

5. **Testing and Validation**:
   - Comprehensive testing will be conducted to ensure that the new UUID-based system works seamlessly with existing and new extensions.
   - Validation will be added to the API to ensure that UUIDs are correctly formatted and valid.

### Benefits:
- **Uniqueness and Consistency**: UUIDs provide a globally unique identifier, reducing the risk of conflicts and ensuring consistency across different environments.
- **Improved Performance**: By using UUIDs, app servers can avoid the need for additional API calls to identify payment and shipping methods, leading to improved performance.
- **Flexibility and Scalability**: The UUID system is more flexible and scalable, making it easier to manage and extend the system in the future.

This decision aligns with the goals of improving the modularity, transparency, and configurability of the system, while also addressing the specific needs of extension developers.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision 

We will adopt a strategy that prioritizes the use of Bootstrap utility classes for simple styling needs, such as spacing, borders, and other common CSS properties. This approach will help reduce the amount of custom SCSS code in `app/storefront/src/scss` and make the HTML templates more maintainable and readable.

### Guidelines for Implementing UI with Bootstrap Utility Classes

1. **Use Bootstrap Utilities for Simple Styling:**
   - For simple styling needs like margins, padding, borders, and colors, prefer using Bootstrap utility classes.
   - Example:
     ```html
     <div class=""register-login-collapse-toogle mb-3""></div>
     ```

2. **Add Semantic Classes for Extensibility:**
   - Always add semantic classes to elements that may need individual styling in the future.
   - These classes should not have any default CSS styles but should be used to provide better semantics and extensibility.
   - Example:
     ```html
     <div class=""border p-3 mb-3 shipping-modal-actions"">
         <button class=""btn btn-light shipping-abort"">Abort</button>
         <button class=""btn btn-primary shipping-submit"">Submit</button>
     </div>
     ```

3. **Custom SCSS for Complex Layouts:**
   - For more complex layouts or when using utilities would result in overly cluttered or hard-to-maintain templates, it is acceptable to use custom SCSS.
   - Ensure that the custom SCSS is well-documented and follows our coding standards.
   - Example:
     ```scss
     .custom-layout {
         display: flex;
         flex-direction: column;
         align-items: center;
         margin-bottom: $spacer;
     }
     ```

4. **Review and Refactor Existing SCSS:**
   - Conduct a review of the existing custom SCSS in `app/storefront/src/scss` to identify and refactor any unnecessary custom styles that can be replaced with Bootstrap utilities.
   - Update the HTML templates to use the appropriate Bootstrap utility classes where applicable.

5. **Documentation and Best Practices:**
   - Document these guidelines in the project's documentation to ensure all developers are aware of the best practices.
   - Include examples and explanations to help developers understand when to use Bootstrap utilities and when to use custom SCSS.

By following these guidelines, we aim to achieve a balance between leveraging the power and simplicity of Bootstrap utilities and maintaining the flexibility and extensibility of our custom SCSS. This will help keep our codebase clean, maintainable, and easy to understand.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision 
We will adopt a new end-to-end (E2E) testing strategy and framework that addresses the current limitations of our Cypress-based test suite. The new approach will be based on the following decisions:

1. **Framework Selection**: We will migrate to Playwright as our E2E testing framework. Playwright is known for its robust cross-browser support, better performance, and more deterministic tests. It also provides a clear and readable syntax, making it easier for both technical and non-technical team members to understand and contribute to the test suite.

2. **Test Strategy**:
   - **Page Object Model (POM)**: Implement the Page Object Model to separate the test logic from the page-specific details. This will make the tests more maintainable and less prone to changes in the UI.
   - **Behavior-Driven Development (BDD)**: Adopt BDD practices by using a tool like Cucumber with Playwright. This will allow us to write tests in a natural language format, ensuring that the tests are derived from real product requirements and validate behavior. This also makes it easier for non-technical stakeholders to review and understand the tests.
   - **Isolation and Mocking**: Use Playwright's capabilities to mock network requests and isolate the tests from the environment state. This will make the tests more deterministic and faster, as they will not depend on external systems or specific data states.

3. **Environment Agnosticism**:
   - **Configuration Management**: Use environment variables and configuration files to make the tests environment-agnostic. This will allow the same test suite to run against different environments, including our cloud environment, without any code changes.
   - **Containerization**: Consider running the tests in Docker containers to ensure consistency across different development and testing environments.

4. **Test Suite Maintenance**:
   - **Regular Refactoring and Cleanup**: Schedule regular refactoring sessions to clean up and improve the test suite. This includes removing redundant tests, updating POMs, and ensuring that the tests align with the latest product requirements.
   - **Documentation and Training**: Provide comprehensive documentation and training sessions for the team to ensure everyone is familiar with the new framework and best practices. This will help in maintaining a high standard of test quality and readability.

5. **Integration and CI/CD**:
   - **Continuous Integration (CI)**: Integrate the new test suite into our CI pipeline to ensure that tests are run automatically on every commit. This will help catch issues early and maintain a high level of test coverage.
   - **Parallel Execution**: Configure the CI pipeline to run tests in parallel to reduce the overall test execution time, making the feedback loop faster and more efficient.

By implementing these decisions, we aim to create a more reliable, maintainable, and user-friendly E2E test suite that can be easily understood and maintained by all team members.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision 
To address the issues with transaction management and error handling in flow actions, we will implement a more robust and transparent transactional mechanism. The following steps outline the decision:

1. **Centralized Transaction Management**: Introduce a centralized transaction manager that handles the creation, commitment, and rollback of transactions. This ensures that all database interactions within a flow action are managed consistently and reduces the risk of nested transaction issues.

2. **Savepoint Configuration**: Configure the database connections to use savepoints by default. Savepoints allow for partial rollbacks within a transaction, which can help manage nested transactions more effectively. This will prevent the connection from being marked as ""rollback only"" if an inner transaction fails.

3. **Error Handling and Logging**: Enhance the error handling and logging mechanism to provide more detailed and actionable error messages. When a transaction commit fails, the error should be logged with sufficient context (e.g., the specific flow action, the transaction details, and the error message). This will help in diagnosing and resolving issues more quickly.

4. **Flow Dispatcher Improvements**: Modify the flow dispatcher to handle transaction errors more gracefully. If a transaction fails, the dispatcher should:
   - Log the error with a clear and detailed message.
   - Roll back the transaction to ensure data consistency.
   - Optionally, retry the flow action if the error is transient and the system is configured to do so.
   - Notify the calling code or external monitoring systems about the failure, so that appropriate action can be taken.

5. **Documentation and Best Practices**: Provide clear documentation and best practices for developers on how to handle transactions within flow actions. This should include guidelines on when and how to start, commit, and rollback transactions, as well as how to handle errors and exceptions.

By implementing these changes, we aim to improve the reliability and maintainability of transactional operations within flow actions, while also providing better visibility and control over the transaction lifecycle.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision

We will disable the Vue 3 compatibility mode in the administration as part of our next major release. This decision is based on several key factors:

1. **Long-term Maintenance and Stability**: Keeping the compatibility mode enabled indefinitely could lead to maintenance issues and potential bugs that are harder to track down, as it allows for the use of deprecated features and patterns. Disabling the mode will ensure that the codebase adheres to the latest and most stable practices.

2. **Performance and Optimization**: Vue 3 offers significant performance improvements and optimizations over Vue 2. By fully adopting Vue 3, we can take advantage of these enhancements, leading to a more efficient and responsive application.

3. **Future-Proofing**: As Vue 3 evolves, new features and improvements will be added. By disabling the compatibility mode, we ensure that the administration and its plugins are aligned with the latest developments in the Vue ecosystem, making it easier to adopt future updates and features.

4. **Community and Ecosystem Alignment**: The broader Vue community and ecosystem are moving towards Vue 3. By aligning with this trend, we can leverage the latest tools, libraries, and best practices, and ensure that our developers and users are working with the most up-to-date technology.

5. **Plugin Migration Strategy**: To facilitate the transition, we will provide clear guidelines and support for plugin developers to migrate their code to Vue 3. This includes documentation, migration guides, and possibly a phased approach where we gradually deprecate and remove compatibility mode features.

**Action Plan**:
- **Documentation and Communication**: Prepare comprehensive documentation and communicate the deprecation timeline to all stakeholders, including plugin developers.
- **Migration Support**: Offer support and resources to help plugin developers migrate their code to Vue 3.
- **Testing and Validation**: Ensure thorough testing of the administration and plugins to identify and address any issues that arise from disabling the compatibility mode.
- **Phased Rollout**: Consider a phased rollout if necessary, where we gradually disable specific parts of the compatibility mode to minimize disruption.

By taking these steps, we aim to ensure a smooth and successful transition to a fully Vue 3-compliant administration, while minimizing the impact on our plugin ecosystem.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision 
Given the issues with both `4.1.0` and `4.0.2` versions of Vuex, we will take the following approach:

1. **Evaluate Alternative Solutions**: We will explore if there are any patches or workarounds available for the specific bug in `4.1.0` that affects getter reactivity. This could involve community-provided patches, custom code modifications, or using a fork of Vuex that addresses the issue.

2. **Consider Upgrading to a Fixed Version**: If a newer version of Vuex (e.g., `4.1.1` or higher) is released that fixes the bug in `4.1.0` without introducing new issues, we will upgrade to this version. This ensures we have the latest features and bug fixes while maintaining stability.

3. **Implement Temporary Workaround**: If no immediate fix is available, we will implement a temporary workaround to mitigate the impact of the bug. This might involve refactoring the affected parts of the application to avoid the use of problematic getters or using alternative methods to achieve the same functionality.

4. **Monitor and Test Thoroughly**: Regardless of the chosen solution, we will conduct thorough testing to ensure that the issue is resolved and that no new bugs are introduced. This includes unit tests, integration tests, and end-to-end tests to cover all critical paths in the application.

5. **Document the Decision and Rationale**: We will document the decision, the rationale behind it, and the steps taken to resolve the issue. This documentation will be shared with the development team and included in our project's architecture decision records (ADRs) for future reference.

By taking these steps, we aim to maintain the stability and reliability of our application while addressing the specific issues with Vuex.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision

To address the challenges posed by the current use of PHP decoration, Adapter, and Factory patterns, we will adopt a more modular and flexible approach using a combination of Dependency Injection (DI) and a Plugin System. This new approach will help us achieve better maintainability, extensibility, and reduce the complexity of our codebase.

### Key Components of the New Approach:

1. **Dependency Injection (DI):**
    - We will leverage a robust DI container to manage the instantiation and wiring of dependencies. This will allow for more straightforward and flexible management of dependencies, making it easier to swap out or extend components without modifying core code.
    - The DI container will be responsible for resolving dependencies, which will help in maintaining backward and forward compatibility. Changes in the core system can be managed through the DI configuration, reducing the risk of breaking existing extensions.

2. **Plugin System:**
    - We will introduce a plugin system that allows third-party developers to extend and customize the application's functionality. Plugins will be able to register hooks, filters, and custom services, providing a clear and structured way to extend the application.
    - The plugin system will provide a well-defined API for extending and customizing the application, reducing the need for complex decoration and adapter patterns. This will also simplify the process of adding new features and functionalities without the need for extensive rework.

3. **Event-Driven Architecture:**
    - We will incorporate an event-driven architecture to allow for the extension of subprocesses. By defining and triggering events at key points in the application's workflow, third-party developers can hook into these events and extend or modify the behavior of the application.
    - This approach will eliminate the need to extract subprocesses into separate classes and interfaces, reducing the proliferation of code and making the system more maintainable.

4. **Documentation and Best Practices:**
    - We will develop comprehensive documentation and best practices for using the DI container and the plugin system. This will help third-party developers understand how to extend and customize the application effectively, reducing the cognitive load and ensuring that the system remains maintainable.

### Implementation Steps:

1. **Evaluate and Select a DI Container:**
    - Choose a reliable and widely-used DI container (e.g., Symfony DI, Laravel DI) that fits our needs and integrates well with our existing codebase.

2. **Refactor Existing Code:**
    - Refactor the existing codebase to use the DI container for managing dependencies. This will involve identifying and refactoring classes and services to be injectable and configurable via the DI container.

3. **Design and Implement the Plugin System:**
    - Design the plugin system, including the API for registering hooks, filters, and custom services. Implement the necessary infrastructure to support the plugin system.

4. **Introduce Event-Driven Architecture:**
    - Identify key points in the application where events should be triggered and define the event types. Implement the event dispatcher and listeners to handle these events.

5. **Update Documentation:**
    - Update the documentation to include detailed information on the new DI container, plugin system, and event-driven architecture. Provide examples and best practices for third-party developers.

By adopting this new approach, we aim to create a more maintainable, extensible, and developer-friendly architecture that addresses the challenges of backward and forward compatibility, process extension, and code proliferation.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision 
We will introduce a new Jest configuration specifically for running component tests without the compat mode. This approach allows us to maintain the existing test suite while gradually transitioning components to the new, non-compat mode setup.

### Steps to Implement:
1. **Create a New Jest Configuration:**
   - Add a new Jest configuration file (e.g., `jest.config.no-compat.js`) that disables the compat mode.
   - Ensure this configuration is set up to run only the component tests that are ready to be tested without compat mode.

2. **Update Test Scripts:**
   - Modify the `package.json` or build scripts to include a new script that runs Jest with the new configuration.
   - For example, add a script like `test:no-compat` that runs `jest --config=jest.config.no-compat.js`.

3. **Gradual Transition:**
   - Begin by moving a small, manageable set of component tests to the new configuration.
   - Verify that these tests pass and that the components function as expected without the compat mode.
   - Gradually move more components to the new configuration as they are updated and tested.

4. **Documentation:**
   - Update the project documentation to reflect the new testing setup.
   - Provide guidelines for developers on how to transition their component tests to the new configuration.

5. **Continuous Integration:**
   - Integrate the new test script into the CI/CD pipeline to ensure that both the compat and non-compat tests are run and validated.

By following these steps, we can systematically and safely remove the compat mode from our component tests, ensuring that the transition is smooth and well-documented.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision 
To improve the reliability and maintainability of the system, we will implement a comprehensive health check mechanism. This mechanism will be designed to monitor the status of all critical components within the system, allowing for early detection of issues that could affect the overall functionality and performance.

### Key Components of the Health Check Mechanism:
1. **Component-Specific Health Checks:**
   - Each component in the system will have its own health check function. This function will return a status indicating whether the component is operational or if there are any issues.
   - The health check function should be lightweight and non-intrusive, ensuring that it does not significantly impact the performance of the component itself.

2. **Aggregated Health Status:**
   - A central service will aggregate the health statuses from all components. This service will provide an overall health status of the system, which can be used to determine if the system is fully operational, partially degraded, or experiencing critical failures.
   - The aggregated health status will be accessible via a well-defined API, allowing for easy integration with monitoring and alerting tools.

3. **Thresholds and Alerts:**
   - Define thresholds for what constitutes a healthy, warning, or critical state for each component. For example, a database might be considered in a warning state if its response time exceeds a certain threshold.
   - Set up alerts to notify the operations team when a component or the entire system falls below the defined health thresholds. These alerts can be sent via email, SMS, or through an incident management tool.

4. **Automated Recovery Actions:**
   - For some components, automated recovery actions can be implemented. For example, if a specific service is detected as unhealthy, the system can automatically attempt to restart the service or switch to a backup instance.
   - Automated recovery actions should be carefully designed to avoid causing further issues, and they should be tested thoroughly in a staging environment before being deployed to production.

5. **Documentation and Testing:**
   - Document the health check mechanisms, including how to interpret the health statuses and the expected behavior of the system under different conditions.
   - Regularly test the health check mechanisms to ensure they are functioning correctly. This includes both unit tests for individual components and integration tests for the system as a whole.

### Implementation Steps:
1. **Identify Critical Components:**
   - List all components that are critical to the system's operation and prioritize them based on their importance and impact on the system.

2. **Develop Health Check Functions:**
   - Develop and integrate health check functions for each identified component. Ensure these functions are efficient and do not introduce additional overhead.

3. **Create Aggregation Service:**
   - Develop a central service to aggregate the health statuses from all components. This service should be robust and highly available.

4. **Set Up Monitoring and Alerting:**
   - Integrate the health check mechanism with existing monitoring and alerting tools. Configure thresholds and set up alerts for different health states.

5. **Test and Validate:**
   - Thoroughly test the health check mechanism in a staging environment to ensure it works as expected. Validate that the alerts and automated recovery actions are functioning correctly.

By implementing this health check mechanism, we aim to improve the overall reliability and maintainability of the system, reducing the likelihood of undetected errors and improving the ability to respond to and recover from issues quickly.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision

To ensure that the `TestPreparationStartedSubscriber` can enable major features for a broader range of test namespaces, we will modify its configuration to allow for more flexible namespace inclusion. This modification will support better testing practices by enabling developers to write and run tests for major features in different parts of the application, not just within the `Shopware\Tests\Unit` namespace.

### Implementation Steps
1. **Update Configuration**: Modify the `TestPreparationStartedSubscriber` to accept a list of allowed namespaces as a configurable parameter. This could be done through an environment variable or a configuration file.
2. **Namespace Validation**: Ensure that the subscriber validates the incoming namespaces against the allowed list before enabling the major feature.
3. **Documentation**: Update the documentation to reflect the new configuration options and provide examples on how to include additional namespaces.
4. **Testing**: Add unit tests to verify that the subscriber correctly enables the major feature for the specified namespaces and does not enable it for namespaces not in the allowed list.

### Example Configuration
```php
// config/test.php
return [
    'allowed_test_namespaces' => [
        'Shopware\Tests\Unit',
        'Shopware\Tests\Integration',
        'Shopware\Tests\Functional',
    ],
];
```

### Code Changes
```php
// Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber
public function prepareTest(\PHPUnit\Framework\Test $test): void
{
    $testName = $test->getName();
    $reflection = new \ReflectionClass($test);
    $namespace = $reflection->getNamespaceName();

    // Load allowed namespaces from configuration
    $allowedNamespaces = $this->config->get('allowed_test_namespaces');

    if (in_array($namespace, $allowedNamespaces)) {
        // Enable major feature
        $this->featureFlagService->enableMajorFeature();
    }
}
```

This approach ensures that the `TestPreparationStartedSubscriber` remains flexible and can be adapted to the needs of different testing scenarios, while still maintaining the necessary controls to prevent unintended feature enabling.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision

We will adopt the `vue-codemod` tool to automate the migration of our Vue.js application from the Options API to the Composition API. This decision is based on the following considerations:

1. **Efficiency and Accuracy**: `vue-codemod` is a powerful tool designed specifically for transforming Vue.js code. It can handle the bulk of the conversion process, reducing the time and effort required for manual updates. Additionally, it minimizes the risk of human error during the migration.

2. **Community Support and Maintenance**: `vue-codemod` is an actively maintained project with strong community support. This ensures that the tool is up-to-date with the latest changes in Vue 3 and the Composition API, and any issues or bugs are likely to be addressed quickly.

3. **Customization and Flexibility**: The tool allows for customization through codemods, which can be tailored to fit the specific needs of our codebase. This flexibility is crucial for handling any unique or complex cases that may not be covered by the default transformations.

4. **Documentation and Examples**: `vue-codemod` comes with comprehensive documentation and examples, making it easier for developers to understand how to use the tool effectively. This will help in ensuring a smooth transition and reduce the learning curve for the team.

5. **Integration with Existing Workflow**: The tool can be easily integrated into our existing development workflow, including CI/CD pipelines, allowing us to automate the migration process and ensure consistency across the codebase.

### Implementation Plan

1. **Initial Setup**:
   - Install `vue-codemod` and set up the necessary configurations.
   - Review the available codemods and select the ones that best fit our requirements.

2. **Pilot Phase**:
   - Select a small, representative subset of components to migrate using `vue-codemod`.
   - Review the results, make any necessary adjustments, and document the process.

3. **Full Migration**:
   - Apply the codemods to the entire codebase.
   - Conduct thorough testing to ensure that the converted components function as expected.

4. **Review and Refinement**:
   - Perform a code review to check for any issues or improvements.
   - Make any additional manual adjustments as needed.

5. **Training and Documentation**:
   - Provide training sessions for the development team on the new Composition API and the use of `vue-codemod`.
   - Update the project documentation to reflect the new coding standards and practices.

By using `vue-codemod`, we aim to streamline the migration process, ensure the quality and consistency of the code, and take full advantage of the benefits offered by the Composition API in Vue 3.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃÂÃÂ¡ÃÂÃÂ¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃÂÃÂ¡ÃÂÃÂ¯s templating and reactivity model.","## Decision
We will transition from using TwigJs for the block system to a native Vue.js approach by introducing two new components: `sw-block` and `sw-block-parent`. This change will leverage Vue.js's more dynamic and reactive component-based architecture, which is better suited for modern frontend development.

### Implementation Details:
1. **`sw-block` Component**:
   - This component will serve as the base for defining reusable and extendable blocks.
   - It will accept a `name` prop to identify the block.
   - It will support slots for content, allowing developers to define default content that can be overridden or extended.
   - The `sw-block` component will also handle the registration of blocks, ensuring that they can be easily referenced and extended by other parts of the application.

2. **`sw-block-parent` Component**:
   - This component will act as a container for `sw-block` components.
   - It will manage the rendering of child blocks and handle the logic for overriding or extending content.
   - The `sw-block-parent` component will use Vue.js's slot mechanism to dynamically render the content of child blocks.
   - It will provide a way to specify which blocks should be rendered and in what order, ensuring that the content is displayed correctly.

### Benefits:
- **Reactivity**: Vue.js's reactivity system will ensure that any changes to the block content are immediately reflected in the UI, providing a more dynamic and responsive user experience.
- **Flexibility**: The new components will offer enhanced flexibility, allowing developers to override or extend block content more easily and with more control.
- **Simplicity**: By using Vue.js's built-in features like slots and props, the new block system will be more intuitive and easier to understand for developers familiar with Vue.js.
- **Maintainability**: The component-based approach will make the codebase more modular and easier to maintain, as each block can be developed, tested, and updated independently.

### Migration Plan:
- **Phase 1**: Develop and test the `sw-block` and `sw-block-parent` components.
- **Phase 2**: Gradually replace existing TwigJs blocks with the new Vue.js components, starting with less critical parts of the application.
- **Phase 3**: Ensure all blocks are migrated and fully functional, and deprecate the TwigJs block system.
- **Phase 4**: Remove the TwigJs block system and related dependencies from the codebase.

This decision aligns with our goal of adopting a more modern and efficient frontend development approach, while maintaining the flexibility and extensibility of the current block system.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃÂÃÂ¡ÃÂÃÂ¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃÂÃÂ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision 
To address the downsides of using an asterisk (*) next to every price in the Storefront, we will adopt a more user-friendly and clear approach. The new strategy will involve:

1. **Removing the Asterisk from Prices**: The asterisk (*) will be removed from all product prices displayed in the Storefront. This change will make the prices cleaner and more immediately understandable for users.

2. **Adding a Clear Disclaimer Section**: We will introduce a clearly visible section, either at the top or bottom of the page, that provides a comprehensive disclaimer about tax, shipping costs, and any other potential charges. This section will be designed to be easily noticeable and accessible, ensuring that customers are well-informed without cluttering the price display.

3. **Enhanced Tooltips or Pop-ups**: For specific products where additional information (such as special delivery charges) is necessary, we will use tooltips or pop-ups. These will appear when the user hovers over or clicks on a designated icon or link near the price. This method ensures that detailed information is available but does not overwhelm the main price display.

4. **User Education through FAQs and Help Sections**: We will enhance our FAQ and help sections to include detailed explanations about how prices, taxes, and shipping costs are calculated. This will provide a resource for users who need more information and help to build trust and transparency.

5. **Testing and Feedback Loop**: After implementing these changes, we will conduct usability testing and gather feedback from users to ensure that the new design is effective and meets their needs. Based on the feedback, we will make any necessary adjustments to improve clarity and user satisfaction.

By removing the asterisk and providing clear, accessible information, we aim to create a more transparent and user-friendly shopping experience.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision

We will adopt a multi-step approach to ensure that the PDFs generated by Shopware 6 are A11y-compliant, adhering to the Web Content Accessibility Guidelines (WCAG). This decision is driven by the need to meet legal requirements and enhance usability for customers with disabilities.

### Steps to Achieve A11y Compliance in PDF Generation

1. **Evaluate Alternative PDF Libraries:**
   - Conduct a thorough evaluation of alternative PDF generation libraries that support accessibility features such as tagging, alt text for images, and proper structure.
   - Consider libraries like TCPDF, FPDF, or commercial solutions like iText, which have better support for accessibility features.

2. **Integrate the Selected Library:**
   - Once an appropriate library is selected, integrate it into the Shopware 6 codebase.
   - Ensure that the new library can be seamlessly integrated without disrupting existing functionalities.

3. **Implement Accessibility Features:**
   - Enable and configure the necessary accessibility features in the new PDF generation library.
   - Ensure that all generated PDFs include:
     - Proper document structure (headings, paragraphs, lists, etc.)
     - Alt text for images
     - Table summaries and captions
     - Logical reading order
     - Tagging for screen readers

4. **Testing and Validation:**
   - Develop a comprehensive testing strategy to validate the accessibility of the generated PDFs.
   - Use tools like the Adobe Acrobat Pro DC Accessibility Checker, PAC (PDF Accessibility Checker), and screen readers to test the PDFs.
   - Conduct user testing with individuals who have disabilities to gather feedback and make necessary adjustments.

5. **Documentation and Training:**
   - Update the documentation to include guidelines on creating accessible PDFs within Shopware 6.
   - Provide training sessions for developers and content creators to ensure they understand the importance of accessibility and how to use the new features effectively.

6. **Continuous Improvement:**
   - Regularly review and update the PDF generation process to incorporate new accessibility standards and best practices.
   - Establish a feedback loop with users to continuously improve the accessibility of the documents.

By following these steps, we aim to ensure that the PDFs generated by Shopware 6 are fully compliant with WCAG, making them accessible to all users, including those with disabilities.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision
To address the complexity and maintenance issues of the existing rule system, we will implement a more structured and controlled approach to extending rule classes. This decision is based on the following considerations:

1. **Simplify the Rule System**: By limiting the ways in which rule classes can be extended, we aim to reduce the overall complexity of the system. This will make it easier for developers to understand and work with the rules, leading to fewer bugs and faster development cycles.

2. **Controlled Extensions**: We will introduce a set of predefined extension points within the rule classes. These extension points will be well-documented and designed to cover common use cases. Developers will be able to extend the rule classes only through these predefined points, ensuring that extensions are consistent and predictable.

3. **Approval Process for New Extensions**: To further control the evolution of the rule system, any new extension point or significant change to an existing one will require approval from a designated team (e.g., the core development team). This will ensure that all changes are thoroughly reviewed and aligned with the overall architecture and design principles of the system.

4. **Documentation and Training**: We will provide comprehensive documentation and training materials to help developers understand how to use the new extension points effectively. This will include examples, best practices, and guidelines for maintaining the integrity of the rule system.

5. **Backward Compatibility**: While introducing these changes, we will ensure that the new system remains backward compatible with existing rules. This will allow for a smooth transition and minimize disruptions to current users.

6. **Continuous Review and Refinement**: The rule system will be subject to regular reviews and refinements to ensure it remains effective and efficient. Feedback from the developer community will be actively solicited and incorporated into future updates.

By implementing these changes, we aim to create a more maintainable and scalable rule system that can evolve in a controlled and predictable manner.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision

We will deprecate and eventually remove the `iterator.helper.js` in favor of using native JavaScript iteration methods. This decision aligns with our goals to reduce unnecessary complexity, improve code readability, and adhere to web standards.

### Steps to Implement:
1. **Deprecation Phase:**
   - Mark the `iterator.helper.js` as deprecated in the documentation.
   - Add a deprecation warning in the helper's code to alert developers that it will be removed in future versions.
   - Update the project's coding guidelines to recommend using native iteration methods.

2. **Codebase Refactoring:**
   - Gradually replace instances of `Iterator.iterate()` with appropriate native iteration methods (e.g., `forEach`, `for...of`, `map`, etc.).
   - Ensure that all new code uses native iteration methods.

3. **Documentation and Training:**
   - Update the project's documentation to include examples and best practices for using native iteration methods.
   - Provide training or resources for developers to understand and use native iteration methods effectively.

4. **Removal:**
   - After a suitable deprecation period (e.g., 1-2 major releases), remove the `iterator.helper.js` file from the codebase.
   - Ensure that all references to the helper are updated or removed.

### Benefits:
- **Simplicity:** Reduces the need for an additional abstraction layer, making the code more straightforward.
- **Consistency:** Aligns the codebase with standard JavaScript practices, improving consistency and reducing confusion.
- **Performance:** Native iteration methods are optimized and can offer better performance.
- **Maintainability:** Easier to maintain and understand, as developers are already familiar with these methods.

This decision will help in maintaining a cleaner and more maintainable codebase, while also ensuring that the code remains consistent and adheres to web standards.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision

We will implement a new approach for executing flows that separates their execution from the main business process. This change will be controlled by a feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` to allow for gradual adoption and testing.

### Key Points of the New Approach:

1. **Asynchronous Execution:**
   - Flows will be executed asynchronously, after the completion of the primary business process (e.g., checkout or product update).
   - This will be achieved using a message queue (such as RabbitMQ, Kafka, or a similar system) to decouple flow execution from the business process.
   - A dedicated worker or service will listen to the queue and execute the flows, ensuring that any errors in flow execution do not disrupt the business process.

2. **Error Handling:**
   - The asynchronous execution model will provide better error handling and retry mechanisms. If a flow fails, it can be retried without affecting the business process.
   - Errors in flow execution will be logged and monitored separately, allowing for more granular and effective error management.

3. **Performance Improvement:**
   - By moving expensive operations like sending emails to an asynchronous task, the performance of the business process will be improved.
   - The user experience will be enhanced as the main business process will complete faster, and any additional actions (like sending emails) will be handled in the background.

4. **Debugging and Maintenance:**
   - Flows will be executed by a dedicated event listener, which will simplify debugging and reduce the complexity of stack traces.
   - The separation of concerns will make the codebase cleaner and more maintainable, as the responsibilities of the business process and flow execution are clearly delineated.

5. **Feature Flag Control:**
   - The new approach will be enabled via the `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` feature flag. This allows for a phased rollout and easy rollback if issues are encountered.
   - The feature flag will be configurable, allowing for selective enabling on different environments (e.g., staging, production).

### Implementation Steps:

1. **Message Queue Setup:**
   - Set up a message queue system (e.g., RabbitMQ, Kafka) to handle the asynchronous execution of flows.
   - Configure the queue and create necessary topics/queues for different types of flows.

2. **Flow Execution Service:**
   - Develop a dedicated service that listens to the message queue and executes the flows.
   - Implement error handling and retry logic within this service.

3. **Business Process Integration:**
   - Modify the existing business process to enqueue flow execution messages instead of executing them directly.
   - Ensure that the business process completes successfully before enqueuing the flow execution messages.

4. **Testing and Validation:**
   - Thoroughly test the new implementation in a staging environment to ensure that it works as expected.
   - Validate that the business process performance is improved and that flow execution is reliable and error-tolerant.

5. **Feature Flag Management:**
   - Enable the `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` feature flag in the configuration.
   - Monitor the system closely after enabling the feature flag to catch and address any issues.

By implementing this new approach, we aim to improve the robustness, performance, and maintainability of our system, while also providing a better user experience.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision 
We will adopt the Architecture Decision Records (ADR) format for documenting architectural decisions in this project. The ADR format is a lightweight and structured way to capture and communicate architectural decisions, providing a clear and consistent record of the rationale, context, and outcomes of each decision.

### Structure of an ADR

Each ADR should follow a standardized structure to ensure consistency and clarity. The recommended structure for an ADR includes the following sections:

1. **Title**: A short, descriptive title that summarizes the decision.
2. **Status**: The current status of the decision (e.g., proposed, accepted, rejected, deprecated).
3. **Date**: The date when the ADR was created or last updated.
4. **Context**: A detailed description of the problem or situation that led to the need for a decision. This section should provide enough background information to understand the decision's importance and relevance.
5. **Decision**: A clear and concise statement of the decision made. This should be specific and actionable.
6. **Rationale**: An explanation of why this particular decision was made. This section should include the reasoning, trade-offs, and any alternatives considered.
7. **Consequences**: A discussion of the implications of the decision, including both positive and negative outcomes, and any potential future considerations.
8. **References**: Any relevant links, documents, or other resources that support the decision or provide additional context.

### Example ADR Template

```markdown
# [Title]

- **Status**: [proposed | accepted | rejected | deprecated]
- **Date**: [YYYY-MM-DD]
- **Context**:
  - [Detailed description of the context and problem]
- **Decision**:
  - [Clear and concise statement of the decision]
- **Rationale**:
  - [Explanation of the reasoning behind the decision, including trade-offs and alternatives]
- **Consequences**:
  - [Discussion of the implications of the decision]
- **References**:
  - [List of relevant links, documents, or other resources]
```

### Process for Creating and Maintaining ADRs

1. **Creation**: When a significant architectural decision needs to be made, a new ADR should be created using the template above. The ADR should be stored in a designated directory within the project repository (e.g., `docs/adr/`).
2. **Review**: The ADR should be reviewed by the relevant stakeholders, including the development team and any other interested parties. Feedback and discussions should be incorporated into the ADR.
3. **Approval**: Once the ADR has been reviewed and any necessary changes have been made, it should be approved by the appropriate authority (e.g., the project lead or steering committee). The status of the ADR should be updated to reflect its approval.
4. **Implementation**: After the ADR is approved, the decision should be implemented in the project. The ADR should be updated with any relevant implementation details.
5. **Maintenance**: ADRs should be maintained over time. If a decision is revisited or changed, a new ADR should be created to document the new decision, and the original ADR should be updated to reference the new one.

By following this format and process, we can ensure that our architectural decisions are well-documented, transparent, and easily accessible to all team members.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision 

We will develop a new, more modular and maintainable Repository Management API that addresses the shortcomings of the legacy python-tuf implementation. The new API will be designed with the following principles in mind:

1. **Modularity and Separation of Concerns**: The new repository management tools will be structured to separate the concerns of metadata handling, repository operations, and utility functions. This will make the codebase easier to understand, maintain, and extend.

2. **Simplified and Coherent API Design**: We will design the API to have a clear, intuitive interface with well-defined methods. Each method will have a limited number of arguments, and global dependencies will be minimized or eliminated. This will improve the usability and testability of the library.

3. **Flexibility and Configurability**: The new API will be designed to support a wide range of use cases, from small repositories to large-scale server operations. It will provide configurable options for loading and processing metadata, allowing users to tailor the behavior to their specific needs.

4. **Modern Python Practices**: The new API will leverage modern Python features and best practices, such as type annotations, to enhance readability and reduce the likelihood of errors.

5. **Documentation and Examples**: Comprehensive documentation and examples will be provided to help users understand how to use the new API effectively. This will include detailed explanations of the API's components, usage patterns, and common scenarios.

### Implementation Plan

- **Phase 1: Design and Specification**
  - Define the high-level architecture and design of the new API.
  - Create a detailed specification document outlining the API's structure, methods, and expected behavior.
  - Gather feedback from the community and stakeholders to refine the design.

- **Phase 2: Development**
  - Implement the core components of the new API, starting with the most critical and frequently used parts.
  - Write unit tests and integration tests to ensure the reliability and correctness of the implementation.
  - Refactor and optimize the code to meet the design goals of modularity and simplicity.

- **Phase 3: Documentation and Testing**
  - Develop comprehensive documentation, including tutorials, examples, and API reference.
  - Conduct thorough testing, including performance and scalability testing, to ensure the API meets the requirements for both small and large repositories.
  - Gather feedback from early adopters and make necessary adjustments.

- **Phase 4: Release and Maintenance**
  - Release the new API and provide support for users migrating from the legacy implementation.
  - Continuously monitor and address any issues or feature requests that arise.
  - Regularly update and maintain the API to keep it aligned with the evolving needs of the TUF community.

By following this plan, we aim to create a robust, flexible, and user-friendly Repository Management API that overcomes the limitations of the current implementation and supports a wide range of use cases.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision 

We will migrate Primer React to use TypeScript for its source code, which will allow us to generate and maintain type definitions alongside the implementation. This approach will ensure that the type definitions are always up-to-date and accurate, as they will be directly tied to the source code.

### Implementation Steps
1. **TypeScript Integration**:
   - Gradually convert existing JavaScript files to TypeScript.
   - Add TypeScript configuration (e.g., `tsconfig.json`) to the project.
   - Ensure that the build process can handle TypeScript files, either by using a TypeScript compiler or a tool like Babel with the appropriate plugins.

2. **Type Definitions**:
   - Remove the ambient declaration file (`index.d.ts`).
   - Use TypeScript's built-in support to generate `.d.ts` files from the TypeScript source code.
   - Ensure that the generated type definitions are included in the published package.

3. **Testing and Validation**:
   - Update the testing framework to work with TypeScript.
   - Add type-checking as part of the continuous integration (CI) pipeline to catch type-related issues early.

4. **Documentation and Communication**:
   - Update the documentation to reflect the new TypeScript-based development process.
   - Communicate the changes to the community and provide guidance on how to consume the updated library in TypeScript projects.

5. **Community and Contribution**:
   - Encourage contributions in TypeScript.
   - Provide guidelines and best practices for contributing TypeScript code to the project.

### Benefits
- **Improved Accuracy**: Type definitions will be automatically generated and kept in sync with the source code, reducing the likelihood of type-related bugs.
- **Better Developer Experience**: Developers using TypeScript will have a more seamless experience, with better type information and autocompletion.
- **Sustainability**: The effort required to maintain type definitions will be significantly reduced, as they will be part of the regular development process.

### Trade-offs
- **Initial Migration Effort**: There will be an initial investment of time and effort to convert the existing codebase to TypeScript.
- **Learning Curve**: Contributors who are not familiar with TypeScript may need to learn the language, which could slow down contributions initially.

By making this change, we aim to improve the reliability and maintainability of Primer React, while also enhancing the developer experience for those using the library in TypeScript projects.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision

We will standardize on using the Box component for creating new React components in Primer React and consuming applications. This decision is based on several factors:

1. **Simplicity and Consistency**: The Box component provides a more straightforward and consistent way to create components. It leverages the existing system of props and styles, which can be easily understood and extended by developers. This reduces the learning curve and makes the codebase more maintainable.

2. **Flexibility and Reusability**: The Box component is highly flexible and can be used to create a wide range of UI elements. It allows for easy customization and extension, making it a versatile tool for building components. The ability to merge styles with `sx` props also ensures that components can be customized without losing their base styles.

3. **Avoiding Redundancy**: By standardizing on the Box component, we avoid the redundancy of having multiple ways to create similar components. This simplifies the codebase and reduces the potential for inconsistencies and bugs.

4. **Better Integration with Theme and Styles**: The Box component integrates seamlessly with the theme and style system, making it easier to apply and override styles. This ensures that components are consistent with the overall design language and can adapt to changes in the theme.

5. **Future-Proofing**: As the Box component is a core part of the Primer React library, it is likely to receive ongoing support and improvements. Standardizing on this component aligns with the long-term vision of the library and ensures that our components remain up-to-date and compatible with future updates.

### Implementation Guidelines

- **New Components**: All new components should be created using the Box component.
- **Existing Components**: Existing components that use styled-components should be refactored to use the Box component when they are next updated or when there is a significant change in their functionality.
- **Documentation**: Update the documentation to reflect the new standard and provide examples and best practices for using the Box component.

By adopting this standard, we aim to create a more cohesive and maintainable codebase, while also ensuring that our components are flexible, reusable, and consistent with the overall design language.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision 

We will continue to use the `Drafts` directory for developing and testing new or experimental versions of components in parallel with their stable counterparts, as well as for components that are still under active development and not yet ready for production use. This approach provides a clear and consistent way to manage multiple versions of components and ensures that developers can collaborate on and test new features without impacting the stability of the main package.

### Guidelines for Using `Drafts`

1. **Component Versions in Drafts:**
   - When creating a new version of an existing component, place the new version in the `@primer/react/drafts` directory.
   - The new version should be clearly named and documented to reflect its status (e.g., `UnderlineNavV2`).
   - Ensure that the new version is fully tested and reviewed before considering it for promotion to the root bundle.

2. **Experimental Components:**
   - For components that are still in the early stages of development and not yet ready for production, place them in the `@primer/react/drafts` directory.
   - These components should be clearly marked as ""experimental"" or ""work in progress"" in the documentation.
   - Developers should be encouraged to collaborate with the maintainers before using these components in production.

3. **Documentation:**
   - Maintain clear and up-to-date documentation for all components in the `Drafts` directory, including their status and any known issues.
   - Provide guidance on how to contribute to and test components in the `Drafts` directory.

4. **Promotion to Root Bundle:**
   - Establish a clear process for promoting components from the `Drafts` directory to the root bundle.
   - This process should include thorough testing, review, and approval from the steering group.
   - Once a component is promoted, ensure that the old version (if any) is deprecated and eventually removed, with appropriate deprecation notices and migration guides provided.

By following these guidelines, we can ensure that the `Drafts` directory remains a valuable tool for managing the evolution of our components while maintaining the stability and reliability of the `@primer/react` package.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision

Given the challenges associated with snapshot testing, especially in large and complex projects, we will adopt a more balanced approach to using snapshot tests. This approach will help us leverage the benefits of snapshot testing while mitigating its drawbacks.

1. **Selective Use of Snapshot Testing**: We will use snapshot testing for components that have a stable and predictable output, such as presentational components. For components that are highly dynamic or frequently change, we will prefer other types of tests, such as unit tests or integration tests, to ensure that the behavior is correct.

2. **Snapshot Test Documentation**: Each snapshot test will be accompanied by a clear comment explaining what the test is intended to verify. This will help future developers understand the purpose of the test and make informed decisions when a snapshot fails.

3. **Regular Review and Pruning**: As part of our regular code review process, we will periodically review and prune snapshot tests. This includes removing outdated snapshots, simplifying overly complex snapshots, and ensuring that each snapshot test is still relevant and useful.

4. **Granular Snapshots**: To address the issue of large and unwieldy snapshots, we will aim to create more granular snapshots. For example, instead of snapshotting an entire component tree, we can snapshot smaller, more focused parts of the component. This will make it easier to identify and debug changes.

5. **Interactive Snapshot Updates**: When a snapshot test fails, we will use the interactive mode provided by testing frameworks like Jest to review and update the snapshots. This will help us quickly determine if the change is intentional and whether the new snapshot should be accepted.

6. **Integration with CI/CD**: We will integrate snapshot testing into our continuous integration (CI) pipeline. This will ensure that any changes to snapshots are caught early and reviewed before they are merged into the main branch.

By following these guidelines, we can maintain a robust and effective suite of snapshot tests that provide value without becoming a maintenance burden.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision 

We will utilize environment checks and build-time tools to ensure that `console.warn()` calls and other development-related code are excluded from the production build. Specifically, we will:

1. **Use Environment Variables**: Implement a check against an environment variable (e.g., `process.env.NODE_ENV`) to conditionally include or exclude warning messages. For example, warnings should only be logged if `process.env.NODE_ENV` is set to 'development'.

   ```javascript
   if (process.env.NODE_ENV === 'development') {
     console.warn('This feature is deprecated and will be removed in the next major version.');
   }
   ```

2. **Leverage Build Tools**: Configure our build tools (e.g., Webpack, Babel) to strip out development-only code during the production build process. This can be done using plugins like `babel-plugin-transform-remove-console` which removes all `console.*` calls from the production build.

3. **Create a Utility Function**: To make it easier and more consistent, we will create a utility function that encapsulates the environment check and the `console.warn()` call. This function will be used throughout the codebase for any development warnings.

   ```javascript
   // utils/warning.js
   export function warn(message) {
     if (process.env.NODE_ENV === 'development') {
       console.warn(message);
     }
   }
   ```

   Usage in the codebase:
   ```javascript
   import { warn } from './utils/warning';

   warn('This feature is deprecated and will be removed in the next major version.');
   ```

4. **Documentation and Guidelines**: Update our developer documentation to include guidelines on how to use the new utility function and the importance of ensuring that development-only code does not end up in production builds.

By following these steps, we can ensure that `@primer/react` provides useful warnings and messages to developers during development while keeping the production code clean and free from unnecessary logging and checks.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision

To address the inconsistencies and potential issues with ARIA live regions in Primer React, we will adopt a centralized pattern for creating and managing live region announcements. This pattern will be designed to ensure that live region announcements are reliable and consistent across different screen readers and user scenarios.

### Centralized Live Region Pattern

1. **Create a Custom Live Region Component:**
   - Develop a reusable `LiveRegion` component that encapsulates the logic for creating and managing live regions.
   - This component will handle the insertion of live regions into the DOM and manage their lifecycle (e.g., when to announce, when to remove).

2. **Define Standard Roles and Attributes:**
   - The `LiveRegion` component will support standard ARIA roles such as `alert`, `log`, and `status`.
   - It will also support the `aria-live` attribute with values `assertive` and `polite`, and the `aria-atomic` attribute.

3. **Provide Clear APIs for Different Scenarios:**
   - **Initial Announcement:** For scenarios where content needs to be announced on initial render or page load, provide a method or prop to trigger the announcement.
   - **Content Change Announcement:** For scenarios where the content of an element changes, provide a method or prop to update the live region and trigger a new announcement.
   - **Programmatic Announcement:** For scenarios where a message needs to be announced programmatically, provide a method to add and announce the message.

4. **Ensure Consistent Behavior:**
   - Implement the `LiveRegion` component to handle dynamic content injection and visibility changes, ensuring that announcements are made consistently across different screen readers.
   - Use techniques such as setting `aria-relevant` and `aria-atomic` appropriately to ensure that only relevant changes are announced.

5. **Documentation and Best Practices:**
   - Provide comprehensive documentation for the `LiveRegion` component, including examples and best practices for different use cases.
   - Include guidance on when to use each type of live region and how to integrate it into existing components.

### Implementation Example

```jsx
import React, { useState } from 'react';
import { LiveRegion } from 'primer-react';

const MyComponent = () => {
  const [count, setCount] = useState(0);

  const incrementCount = () => {
    setCount(count + 1);
    // Announce the new count
    LiveRegion.announce(`Count is now ${count + 1}`);
  };

  return (
    <div>
      <button onClick={incrementCount}>
        Increment Count
      </button>
      <LiveRegion role=""status"" aria-live=""polite"" aria-atomic=""true"">
        Count: {count}
      </LiveRegion>
    </div>
  );
};

export default MyComponent;
```

### Benefits

- **Consistency:** Ensures that live region announcements are consistent and reliable across different screen readers and user scenarios.
- **Reusability:** Provides a reusable component that can be easily integrated into various parts of the application.
- **Maintainability:** Simplifies the management of live regions by centralizing the logic in a single component.
- **Accessibility:** Enhances the accessibility of the application by providing a robust and well-documented pattern for live region announcements.

By adopting this centralized pattern, we aim to improve the overall accessibility and user experience of Primer React and GitHub applications.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,It seems like the context is missing in your request. Could you please provide the necessary details or background information so I can help you with an appropriate decision?,"## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision

To address the issues with our current testing infrastructure, we will adopt a more integrated and efficient approach to testing by leveraging Storybook for both interactive development and as a source for generating Jest tests. This decision is based on the following key points:

1. **Integration of Storybook and Jest**: We will use a tool like `@storybook/test-runner` or `@testing-library/react` in conjunction with Storybook to automate the creation of Jest tests from our stories. This reduces the need for duplicating test scenarios and ensures that our UI components are tested in the same way they are developed and visually inspected.

2. **Visual Testing with Storybook and Chromatic**: Continue using Chromatic for visual regression testing, but integrate it more closely with our Storybook setup. This will allow us to automatically generate and update snapshots for visual changes, ensuring that any visual regressions are caught early in the development process.

3. **Enhanced Debugging Experience**:
   - **Storybook Addons for Debugging**: Utilize Storybook addons such as `addon-actions` and `addon-knobs` to provide more interactive and visual debugging capabilities. These tools can help developers understand the state and behavior of components in a more intuitive way.
   - **Jest Integration with Storybook**: Use `@storybook/addon-jest` to display Jest test results directly within Storybook. This provides a visual interface for viewing test results, making it easier to debug and understand the state of our tests.

4. **Documentation and Training**:
   - **Guidelines for Writing Stories and Tests**: Develop and document best practices for writing stories and tests. This will include guidelines on how to structure stories so that they can be easily converted into Jest tests, and how to write tests that are both effective and maintainable.
   - **Training Sessions**: Conduct training sessions for the team to ensure everyone is familiar with the new workflow and tools. This will help in reducing the learning curve and ensuring that the new approach is adopted smoothly.

By implementing these changes, we aim to streamline our testing process, reduce duplication, and improve the overall visibility and maintainability of our tests. This will lead to a more robust and reliable testing infrastructure, ultimately enhancing the quality of our software.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision

We will adopt a monorepo management tool, specifically `lerna`, to streamline the management of workspaces and dependencies within the `primer/react` repository. This decision addresses the need for a more organized and efficient way to handle interdependencies between workspaces and to manage scripts that are growing in complexity.

### Rationale

1. **Unified Dependency Management**: Lerna allows us to manage all npm packages in the monorepo with a single command, such as `lerna bootstrap`, which will install all dependencies across all workspaces. This eliminates the need for contributors to manually run `npm install` in each workspace or rely on the `npm run setup` script.

2. **Inter-Workspace Dependencies**: Lerna provides a feature called ""hoisting"" which can automatically resolve and install shared dependencies at the root level, reducing duplication and ensuring consistency. Additionally, it simplifies the process of linking local packages, making it easier for one workspace to depend on another (e.g., `docs` depending on `@primer/react`).

3. **Script Management**: By using Lerna, we can centralize the management of scripts. Complex scripts can be organized into their own packages if they grow large enough, or they can remain in the `scripts` directory but be managed more effectively with Lerna's lifecycle scripts. This ensures that scripts can have their own dependencies and can be versioned and updated independently.

4. **Versioning and Publishing**: Lerna supports versioning and publishing of multiple packages in the monorepo. This is particularly useful when we need to release new versions of `@primer/react` and any dependent packages, ensuring that all changes are synchronized and published together.

5. **Simplified Contribution Process**: With Lerna, the contribution process becomes more straightforward. New contributors can simply run `lerna bootstrap` to set up the entire project, including all workspaces and dependencies, without needing to understand the intricacies of each individual package.

### Implementation Steps

1. **Install Lerna**: Add Lerna as a development dependency to the top-level `package.json` and initialize Lerna in the project.
   ```sh
   npm install --save-dev lerna
   npx lerna init
   ```

2. **Configure Lerna**: Set up Lerna to use hoisting and to manage the versioning strategy (independent or fixed) based on our project's needs. This configuration will be done in the `lerna.json` file.

3. **Update Workspace Configuration**: Ensure that each workspace is correctly defined in the `packages` field of `lerna.json`. Update the `package.json` files in each workspace to use the correct relative paths or file dependencies.

4. **Refactor Scripts**: Organize and refactor scripts under the `scripts` directory. If necessary, create new packages for complex scripts and update the `lerna.json` to include these new packages.

5. **Documentation and Training**: Update the project documentation to reflect the new setup and provide training or guidelines for contributors on how to use Lerna effectively.

By adopting Lerna, we will achieve a more organized, maintainable, and scalable monorepo structure, which aligns with the goals of improving dependency management and script organization in the `primer/react` repository.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision

To clearly delineate between the public and internal APIs, we will reorganize the project structure to separate the code intended for public use from the internal implementation details. This involves moving the internal files to a new `internal` directory under `src`, while keeping the public API in the `src` directory.

### New Project Structure
- **src/**: Contains all the public API components and utilities.
  - **components/**: Public React components.
  - **utils/**: Public utility functions.
  - **styles/**: Public styles and CSS.
- **src/internal/**: Contains all the internal implementation details.
  - **components/**: Internal React components.
  - **utils/**: Internal utility functions.
  - **styles/**: Internal styles and CSS.

### Updated `npm` Package Exports
We will update the `exports` field in the `package.json` to exclude the `internal` directory. This ensures that only the files and folders under the `src` directory (excluding `src/internal`) are part of the public API.

```json5
{
  ""exports"": {
    ""./lib-esm/*"": {
      ""import"": [
        ""./dist/esm/src/*.js""
      ],
      ""require"": [
        ""./dist/cjs/src/*.js""
      ]
    }
  }
}
```

### Rollup Configuration
We will also update the Rollup configuration to ensure that it only bundles and exports the files in the `src` directory, excluding the `src/internal` directory. This can be achieved by updating the input and output paths in the Rollup configuration file.

### Benefits
- **Clarity**: The separation of public and internal APIs makes it clear which parts of the codebase are intended for external use.
- **Maintainability**: It is easier to maintain and evolve the public API without affecting internal implementation details.
- **Security**: Reduces the risk of exposing internal implementation details to consumers, which could lead to misuse or security vulnerabilities.

By implementing this decision, we ensure that the public API of `@primer/react` is well-defined and that internal implementation details remain hidden from consumers.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision

We will transition from a runtime CSS injection approach to a static CSS file inclusion for our styling architecture. This decision is driven by the need to improve performance, especially on initial page loads and server-side rendering, as well as to leverage CSS variables for theming and enhance the guardrails around using Primer/Primitives.

### Key Points of the Decision:

1. **Static CSS Files**:
   - **Performance Improvement**: Static CSS files can be loaded in parallel with other resources, reducing the time it takes for the page to become visually complete.
   - **Initial Page Load**: Using static CSS files will significantly reduce the initial page load times, as demonstrated by the benchmarks (static .css files: 96ms vs. runtime injection: 242ms).
   - **Server-Side Rendering**: By eliminating the need to collect styles on the server, we can reduce the rendering time, as seen in the issues-app experiment where collecting styles added 450ms to the total rendering time.

2. **CSS Variables for Theming**:
   - **Dynamic Theming**: CSS variables allow for dynamic theming without the need for JavaScript, which can further improve performance and maintainability.
   - **Consistency and Flexibility**: CSS variables provide a consistent way to manage and update themes across the application, making it easier to apply and maintain design changes.

3. **Improved Guardrails**:
   - **Simpler Consumption**: By moving to static CSS files, we can simplify the consumption of Primer/Primitives, providing clearer and more robust guardrails for developers.
   - **Reduced Complexity**: The current approach of updating styles based on props or state is computationally expensive. Static CSS files will reduce this complexity and improve the overall performance of the application.

### Implementation Plan:

1. **Audit and Refactor**:
   - Conduct a thorough audit of the current styling implementation to identify all components and styles that are currently being injected at runtime.
   - Refactor these components to use static CSS files, ensuring that the new styles are consistent with the existing design.

2. **CSS Variables**:
   - Define and implement CSS variables for theming, ensuring that they are used consistently across the application.
   - Update the documentation to reflect the new theming approach and provide guidelines for developers.

3. **Testing and Validation**:
   - Perform extensive testing to ensure that the transition to static CSS files does not introduce any visual or functional regressions.
   - Validate the performance improvements through both lab and application benchmarks.

4. **Documentation and Training**:
   - Update the developer documentation to include the new styling approach and best practices.
   - Provide training sessions and resources for developers to understand and adopt the new architecture.

By making this change, we aim to significantly improve the performance and maintainability of our applications, while also enhancing the developer experience and ensuring a consistent and flexible theming system.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision

We will adopt a more flexible approach to authoring interaction tests, moving away from the exclusive use of Storybook's `play` functions as proposed in `ADR 009`. Instead, we will leverage a combination of Jest and Playwright for both unit, integration, and end-to-end (E2E) tests, including those that require user interactions. This change is motivated by the following considerations:

1. **Enhanced Test Coverage**: Playwright provides a robust set of features for simulating user interactions, which can be more comprehensive and reliable than the `play` functions in Storybook. This allows us to write more detailed and realistic interaction tests.

2. **Consistency Across Test Types**: By using Jest and Playwright for all types of tests, including interaction tests, we can maintain a consistent testing strategy across the project. This consistency can simplify the test writing process and make it easier for developers to understand and contribute to the test suite.

3. **Better Integration with CI/CD Pipelines**: Playwright integrates well with continuous integration and continuous deployment (CI/CD) pipelines, allowing us to run E2E tests in a headless browser environment. This can help catch issues earlier in the development cycle and ensure that our application behaves as expected in different environments.

4. **Scalability and Maintenance**: As the project grows, maintaining a single, unified testing framework can reduce the overhead of managing multiple testing tools. This can lead to more efficient and scalable testing practices.

5. **Community and Ecosystem Support**: Playwright has a strong community and a rich ecosystem of plugins and tools, which can provide additional support and resources for our testing needs.

### Implementation Plan

1. **Update Existing Tests**: Review and update existing interaction tests written using Storybook's `play` functions to use Jest and Playwright. This will involve refactoring the tests to use Playwright's APIs for simulating user interactions.

2. **Documentation and Training**: Update the project documentation to reflect the new testing strategy. Provide training and resources for the team to familiarize themselves with Jest and Playwright, especially for writing and maintaining interaction tests.

3. **Integration with CI/CD**: Ensure that the CI/CD pipeline is configured to run Playwright tests, including interaction tests, in a headless browser environment. This may involve setting up the necessary infrastructure and configurations.

4. **Continuous Improvement**: Regularly review and refine the testing strategy based on feedback and the evolving needs of the project. Encourage the team to share best practices and lessons learned.

By adopting this new approach, we aim to improve the quality and reliability of our tests, while also making the testing process more efficient and maintainable.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision

To address the issue of layout shifts during server-side rendering (SSR) when using responsive props, we will implement a strategy that ensures consistent rendering between the server and the client. This involves providing a mechanism to pre-determine the viewport size on the server and use it to render the correct responsive value. Here’s how we will achieve this:

1. **Server-Side Viewport Detection:**
   - On the server, we will detect the viewport size by analyzing the `User-Agent` string or other request headers. This can be done using a library like `ua-parser-js` or a similar tool.
   - We will define a set of default viewport sizes that map to our named breakpoints (narrow, regular, wide). For example:
     ```javascript
     const defaultBreakpoints = {
       narrow: { width: 480 },
       regular: { width: 768 },
       wide: { width: 1280 }
     };
     ```
   - The server will then use these default breakpoints to determine the appropriate responsive value for the prop.

2. **Client-Side Hydration:**
   - On the client, we will use the `useResponsiveValue` hook to re-evaluate the responsive value based on the actual viewport size.
   - To prevent a layout shift, we will ensure that the initial hydration uses the same responsive value that was rendered on the server. This can be achieved by passing the server-detected breakpoint as an initial state to the `useResponsiveValue` hook.
   - If the client detects a different viewport size, it will update the responsive value, but this should not cause a layout shift because the initial render matches the server-rendered output.

3. **Component Implementation:**
   - Components that support responsive props will have an optional `initialBreakpoint` prop that can be used to pass the server-detected breakpoint to the `useResponsiveValue` hook.
   - The `useResponsiveValue` hook will be updated to accept an `initialBreakpoint` parameter and use it to initialize the responsive value.

4. **Example Usage:**
   - Server-side rendering:
     ```javascript
     // Server-side code
     const userAgent = req.headers['user-agent'];
     const detectedBreakpoint = getBreakpointFromUserAgent(userAgent); // Custom function to determine the breakpoint
     <Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }} initialBreakpoint={detectedBreakpoint} />
     ```

   - Client-side hydration:
     ```tsx
     // Component code
     import { useResponsiveValue } from 'path/to/hooks';

     const Stack = ({ gap, initialBreakpoint }) => {
       const resolvedGap = useResponsiveValue(gap, initialBreakpoint);
       return <div style={{ gap: resolvedGap }}>...</div>;
     };
     ```

By implementing this strategy, we ensure that the responsive values are consistent between the server and the client, thereby preventing layout shifts during SSR. This approach provides a paved path for authors to use responsive props without worrying about the potential issues of server-side rendering.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision 

To facilitate a smooth transition from the deprecated prop to the new API, we will implement a deprecation strategy that includes the following steps:

1. **Deprecation Warning**: When the deprecated prop is used, a console warning will be issued, clearly indicating that the prop is deprecated and providing guidance on how to migrate to the new API. This warning will include a link to the relevant documentation or migration guide.

   ```javascript
   if (props.title) {
     console.warn(
       'The ""title"" prop in <ActionList.Group> is deprecated. Please use <ActionList.GroupHeading> instead. See: [Migration Guide URL]'
     );
   }
   ```

2. **Documentation Update**: The documentation for the component will be updated to reflect the deprecation. The deprecated prop will be marked as such, and the new API will be prominently featured with examples and explanations of its benefits.

3. **Temporary Compatibility Layer**: To ensure that existing codebases can continue to function without immediate breaking changes, we will provide a temporary compatibility layer. This layer will automatically convert the deprecated prop usage into the new API, issuing a deprecation warning in the process.

   ```tsx
   const Group = ({ title, children, ...rest }) => {
     if (title) {
       console.warn(
         'The ""title"" prop in <ActionList.Group> is deprecated. Please use <ActionList.GroupHeading> instead. See: [Migration Guide URL]'
       );
       return (
         <div {...rest}>
           <GroupHeading as=""h2"">{title}</GroupHeading>
           {children}
         </div>
       );
     }
     return <div {...rest}>{children}</div>;
   };
   ```

4. **Grace Period**: A grace period will be provided to allow users to update their code. During this period, the deprecated prop will still work but will issue a warning. After the grace period, the deprecated prop will be removed, and any usage will result in a runtime error.

5. **Migration Guide**: A detailed migration guide will be created and linked in the deprecation warning. This guide will provide step-by-step instructions on how to migrate from the old API to the new one, including examples and best practices.

By following these steps, we aim to provide a clear and supportive path for users to migrate to the new API, ensuring that the transition is as smooth and painless as possible.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision

To support proxy configurations for self-hosted runners, we will adopt the use of environment variables `http_proxy`, `https_proxy`, and `no_proxy` for specifying HTTP, HTTPS, and no-proxy settings, respectively. This approach aligns with the conventions used by a wide range of applications and tools, ensuring familiarity and ease of setup for users in enterprise and organizational settings.

### Implementation Details
- **Environment Variables:**
  - `http_proxy`: For specifying the proxy server to use for HTTP connections.
  - `https_proxy`: For specifying the proxy server to use for HTTPS connections.
  - `no_proxy`: For listing hosts that should be reached directly, bypassing the proxy.

- **Case Sensitivity:**
  - We will primarily support the lowercase variants (`http_proxy`, `https_proxy`, `no_proxy`). However, for compatibility with some applications that may use uppercase variants (e.g., `HTTPS_PROXY`), we will also check for these and treat them as equivalent.

- **Fallback Mechanism:**
  - If both the lowercase and uppercase variants are set, the lowercase variant will take precedence. This ensures a consistent and predictable behavior across different environments.

- **Documentation:**
  - Clear documentation will be provided, explaining how to set up and use these environment variables. Examples will be included to demonstrate their usage in common scenarios.

- **Testing:**
  - Comprehensive testing will be conducted to ensure that the self-hosted runners correctly handle proxy configurations specified via these environment variables. This includes testing with both lowercase and uppercase variants to ensure robustness.

By adopting this decision, we aim to provide a flexible and user-friendly solution for configuring proxies, making it easier for enterprises and organizations to integrate and use self-hosted runners.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision 

We will implement a new system for controlling command echoing in the user logs, where the default behavior is to not echo commands unless explicitly requested. This change aims to reduce clutter in the logs and improve readability, while still providing necessary information in case of errors.

### Implementation Details:
1. **Default Behavior**: 
   - Command echoing will be disabled by default.
   - If the `ACTIONS_STEP_DEBUG` secret is enabled, command echoing will be turned on by default. This allows for detailed debugging when needed.

2. **Explicit Control**:
   - Users can explicitly enable or disable command echoing using the new commands `::echo::on` and `::echo::off`.
   - These commands will override the default behavior set by the `ACTIONS_STEP_DEBUG` secret.

3. **Error Handling**:
   - In the event of an error during command execution, the command and its output will always be echoed in the logs. This provides additional context for troubleshooting.

### Example Usage:
- To enable command echoing:
  ```plaintext
  ::echo::on
  ```
- To disable command echoing:
  ```plaintext
  ::echo::off
  ```

### Summary:
- **Default State**: Echoing off (unless `ACTIONS_STEP_DEBUG` is enabled).
- **User Control**: Explicitly control with `::echo::on` and `::echo::off`.
- **Error Logging**: Always echo commands and outputs in case of errors.

This approach ensures that the logs remain clean and focused, while still providing the necessary details for debugging and troubleshooting.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision 
We will implement a feature in the runner that allows for the inclusion of extra information in the `Set up job` step's log. This feature will be designed to dynamically gather and log system and software information, particularly useful when running on hosted images where the environment details are crucial for debugging and ensuring compatibility.

The approach will involve:
1. **Extending the Runner Configuration**: We will extend the runner's configuration to support an optional parameter or flag (e.g., `--include-env-info`) that, when set, triggers the collection of additional environment information.
2. **Environment Information Collection**: The runner will include a utility function to gather relevant OS and software information. This could include:
   - Operating System (OS) name and version
   - Kernel version
   - Installed software versions (e.g., Java, Python, Node.js)
   - Environment variables
   - Other relevant system details
3. **Logging the Information**: The collected information will be formatted and logged as part of the `Set up job` step. This ensures that the setup logs provide a comprehensive view of the environment, which is especially useful for troubleshooting and maintaining consistent builds across different environments.
4. **Documentation and Usage Examples**: We will update the documentation to include this new feature, providing clear instructions and examples on how to use it. This will help users understand the benefits and ensure they can easily enable this feature in their CI/CD pipelines.

This decision aligns with the goal of increasing transparency and introspectability in Arachne applications, making it easier for developers to understand and debug the environment in which their jobs are running.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision 

We will add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context. This enhancement will allow for more granular control over the workflow, enabling subsequent steps to conditionally execute based on the success or failure of previous steps.

### Implementation Details:
- **`steps.<id>.outcome`**: This field will store the outcome of the step, which can be either ""success"" or ""failure"". It will provide a clear, binary indication of whether the step completed its intended operation without errors.
- **`steps.<id>.conclusion`**: This field will store a detailed conclusion or message about the step's execution. For example, if the step failed, it might contain an error message or a reason for the failure. If the step succeeded, it might contain a summary or a confirmation message.

### Benefits:
- **Enhanced Control**: Downstream steps can now make decisions based on the outcome of previous steps, leading to more dynamic and responsive workflows.
- **Improved Transparency**: The inclusion of `conclusion` provides more context about why a step succeeded or failed, improving the overall transparency and introspectability of the application.
- **Modularity and Flexibility**: This change supports the modular nature of Arachne, as different steps can be designed to handle various outcomes and conclusions, making them more swappable and adaptable.

### Workflow Adjustments:
- **Step Execution Logic**: Developers will need to update their step logic to check the `outcome` and `conclusion` fields of previous steps before proceeding. This can be done using conditional statements within the step definitions.
- **Documentation and Training**: Clear documentation and examples will be provided to help developers understand how to use these new fields effectively. Training sessions may also be conducted to ensure that all contributors are familiar with the new capabilities.

### Backward Compatibility:
- **Existing Steps**: Existing steps will not be affected by this change, as the new fields are optional. However, to fully leverage the new features, steps should be updated to include and utilize `outcome` and `conclusion`.
- **Configuration**: No changes to the existing configuration schema are required, as the new fields are simply additional properties in the `steps` context.

### Conclusion:
By adding `steps.<id>.outcome` and `steps.<id>.conclusion`, we enhance the flexibility and control of our workflows, aligning with Arachne's goals of modularity, transparency, and configurability. This decision will be implemented in the next development cycle, and we will provide the necessary support and documentation to ensure a smooth transition.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision

To address the need for adding custom labels to self-hosted runners during the configuration process, we will extend the runner registration command to accept additional labels via command line arguments. This approach ensures that users can fully automate the setup of their runners, including the application of both built-in and custom labels.

### Implementation Details
1. **Command Line Argument**: Add a new command line argument, e.g., `--labels`, to the runner registration command. This argument will accept a comma-separated list of key-value pairs, where each pair represents a label in the format `key=value`.

2. **Validation**: Implement validation to ensure that the provided labels are in the correct format and do not conflict with reserved or built-in labels (e.g., `os`, `arch`).

3. **Registration Process**: During the registration process, the runner will:
   - Register the built-in labels (e.g., `os`, `arch`) as it currently does.
   - Parse and validate the custom labels provided via the `--labels` argument.
   - Combine the built-in and custom labels into a single set and register them with the GitHub Actions service.

4. **Documentation**: Update the documentation to include the new `--labels` option, providing examples and best practices for using custom labels.

### Example Usage
```sh
./config.sh --url https://github.com/your-org/your-repo --token YOUR_TOKEN --labels env=production,region=us-west-1
```

### Benefits
- **Automation**: Users can fully automate the runner setup, including the assignment of custom labels, without manual intervention.
- **Flexibility**: Custom labels can be easily added, modified, or removed by updating the script, making it easier to manage different environments and configurations.
- **Consistency**: Ensures that all runners are consistently configured, reducing the risk of human error.

### Considerations
- **Backward Compatibility**: Ensure that the addition of the `--labels` argument does not break existing scripts. The argument should be optional, and the runner should continue to function as expected if no custom labels are provided.
- **Security**: Validate and sanitize the input to prevent injection attacks or other security risks.

By implementing this decision, we will enhance the flexibility and automation capabilities of self-hosted runners, making it easier for users to manage and configure their CI/CD pipelines.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision 
To handle transient faults during communication with the authentication provider, SS?? will implement a robust retry mechanism and circuit breaker pattern. This approach will help in managing temporary outages or rate limiting issues effectively, ensuring that the system remains resilient and responsive.

### Retry Mechanism
- **Exponential Backoff**: The system will use an exponential backoff strategy for retries. After each failed attempt, the wait time before the next retry will increase exponentially. This helps to reduce the load on the provider and increases the likelihood of success as the issue is resolved.
- **Jitter**: To avoid synchronous retries from multiple clients, a random jitter will be added to the backoff time. This will distribute the retry attempts more evenly over time, reducing the chance of overwhelming the provider when it recovers.
- **Maximum Retries and Timeout**: A maximum number of retries and a total timeout for the entire operation will be set to prevent infinite loops and ensure that the system does not get stuck in a retry loop indefinitely.

### Circuit Breaker Pattern
- **State Management**: The circuit breaker will have three states: Closed, Open, and Half-Open.
  - **Closed**: In this state, the system operates normally, but if the number of failures exceeds a threshold within a certain time window, the circuit breaker transitions to the Open state.
  - **Open**: In this state, all requests to the provider are blocked, and a fallback response (e.g., a cached response or a default error message) is returned. After a predefined cooldown period, the circuit breaker transitions to the Half-Open state.
  - **Half-Open**: In this state, a limited number of requests are allowed to pass through to the provider. If these requests succeed, the circuit breaker resets to the Closed state. If they fail, the circuit breaker reopens and the cycle repeats.

### Implementation
- **Libraries and Tools**: We will use established libraries such as Resilience4j for Java or Polly for .NET to implement the retry mechanism and circuit breaker pattern. These libraries provide well-tested and configurable implementations of these patterns.
- **Monitoring and Logging**: The system will log and monitor the state of the circuit breaker and the outcomes of retries. This will help in diagnosing issues and tuning the parameters of the retry and circuit breaker mechanisms.

### Benefits
- **Resilience**: The retry mechanism and circuit breaker pattern will make the system more resilient to transient faults, ensuring that it can continue to operate even when the authentication provider experiences issues.
- **Performance**: By using exponential backoff and jitter, the system will avoid overwhelming the provider and improve the chances of successful retries.
- **Maintainability**: Using established libraries and patterns will make the system easier to maintain and extend in the future.

This decision aligns with our goal of building a robust and fault-tolerant system that can handle transient issues with third-party services effectively.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision 
We will adopt the use of Architecture Decision Records (ADRs) to document significant decisions made during the project. ADRs provide a structured and lightweight way to capture, discuss, and record architectural and other important project decisions. This approach supports our goal of transparency and inclusivity by making the decision-making process accessible to all team members and outside contributors.

### Workflow for ADRs:
1. **Proposal**: When a significant decision is needed, a team member will draft an ADR. The ADR should include:
   - A clear title and date.
   - A brief description of the problem or context.
   - Proposed solutions or options.
   - Rationale for the preferred solution.
   - Any known pros and cons.
   - Status: Initially set to ""proposed.""

2. **Discussion**: The ADR will be shared with the team and, if applicable, the broader community. This can be done via a pull request in the repository, a dedicated forum, or a meeting. Feedback and discussion will be encouraged to ensure that all perspectives are considered.

3. **Review and Update**: Based on the feedback, the ADR may be updated to reflect additional information, concerns, and proposed changes. This iterative process ensures that the decision is well-considered and takes into account multiple viewpoints.

4. **Decision**: Once the discussion is complete and a consensus is reached, the ADR will be finalized. The status will be updated to either ""accepted"" or ""rejected,"" and the ADR will be merged into the project's documentation.

5. **Implementation and Follow-Up**: If the ADR is accepted, the necessary changes can be implemented. The ADR will remain as a historical record of the decision and its context. If new information or circumstances arise, the ADR can be revisited, and a new ADR can be created to update or reverse the decision.

### Benefits of ADRs:
- **Transparency**: All stakeholders, including new team members and external contributors, can understand the reasoning behind past decisions.
- **Inclusivity**: By opening up the decision-making process, we encourage broader participation and ownership in the project.
- **Documentation**: ADRs serve as a form of documentation that does not get outdated, as they record the decision at a specific point in time.
- **Accountability**: ADRs help ensure that decisions are well-considered and based on thorough analysis and discussion.

By adopting ADRs, we aim to create a more transparent, inclusive, and well-documented project, which aligns with our goals of developing the product in the open and fostering a collaborative environment.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂÃÂ¡ÃÂÃÂª with all relevant discussion summarized ÃÂÃÂ¡ÃÂÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision 
We will adopt a standardized approach for defining and accessing extensions using an EDN (Extensible Data Notation) file format, which will be stored in decentralized storage. This decision is based on the need for a lightweight, human-readable, and Clojure-friendly data format that can easily represent complex structures. The use of decentralized storage ensures that these extension definitions are accessible, resilient to failures, and not dependent on a single point of control or failure.

The URI schema will be designed to uniquely identify each EDN file, allowing for easy retrieval and referencing within the Arachne framework. This URI should include enough information to both locate the file in the decentralized storage and potentially indicate the version of the extension, ensuring that we can manage updates and compatibility effectively. 

This method supports the modular, transparent, and configurable goals of Arachne by providing a clear, structured, and easily accessible way to define and integrate extensions. Additionally, it aligns with the ""everything is data"" philosophy, as the extension definitions are treated as data, making them more introspectable and manageable.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision 

Given the constraints and priorities for the beta launch in Q2, we will de-prioritize the implementation and bug fixing of group chat features. Our focus will remain on 1-1 chat and public chat functionalities, which are more critical for an MVP and provide a better user experience at this stage. This decision is made to ensure that we can meet our launch timeline without compromising the quality and security of the core chat functionalities.

We will:
- Remove group chat from the scope of the initial beta release.
- Document all outstanding issues related to group chat and prioritize them for future sprints after the beta launch.
- Communicate this decision clearly to all stakeholders, including the development team, to avoid any further confusion or wasted effort.
- Ensure that the infrastructure and architecture remain flexible enough to incorporate group chat features in future releases, once the initial feedback and stabilization phases are complete.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision 

We will implement the Discovery v5 protocol to dynamically discover and connect to new peers, replacing the current hardcoded peer setup. This approach will significantly enhance our system's scalability, availability, and security by allowing us to:

- **Scale Dynamically**: As the number of active users increases, we can automatically add more server peers to the network without manual intervention.
- **Improve High Availability and Reliability**: By not relying on a fixed set of peers, we reduce the risk of service disruption due to any single point of failure. The dynamic discovery of peers ensures that the system remains robust even if some nodes go offline.
- **Enhance Security**: The Discovery v5 protocol incorporates mechanisms for secure peer discovery and communication, which helps in mitigating potential security risks associated with static peer lists.

The implementation of this protocol will involve:
1. Researching and understanding the Discovery v5 specifications and best practices.
2. Developing and integrating the necessary code changes to support the Discovery v5 protocol.
3. Testing the new peer discovery mechanism thoroughly to ensure it meets our performance, reliability, and security requirements.
4. Gradually rolling out the changes to monitor the impact and make adjustments as needed.

This decision aligns with our goals of building a highly scalable, reliable, and secure system.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision 

We will implement a new key generation mechanism that is compatible with the BIP-39 standard for generating keys from 12 mnemonic words. This change will allow users to create a wallet in other applications and import the same address into Status using the 12 mnemonic words, thereby improving wallet interoperability.

### Implementation Details:
- **Adoption of BIP-39 Standard**: We will adopt the BIP-39 standard for generating keys from mnemonic phrases. This standard is widely used and supported by many cryptocurrency wallets, ensuring compatibility.
- **Mnemonic Phrase Handling**: The 12-word mnemonic phrase will be used as the seed for generating the wallet's private and public keys. This ensures that the same mnemonic phrase can be used across different applications to generate the same wallet.
- **Key Derivation Path**: We will use a specific key derivation path (e.g., `m/44'/60'/0'/0/0` for Ethereum) to ensure that the derived keys are consistent with the BIP-44 standard, which defines a hierarchical deterministic wallet structure.
- **User Experience**: We will update the user interface to clearly guide users through the process of importing or creating a wallet using the 12-word mnemonic. This includes providing clear instructions and error messages to help users understand the process.
- **Security Considerations**: We will ensure that the implementation follows best security practices, including secure storage of the mnemonic phrase and derived keys, and protection against common attacks such as brute force and dictionary attacks.

### Benefits:
- **Interoperability**: Users can easily move their wallet between Status and other applications, enhancing the overall user experience and flexibility.
- **Standardization**: By adopting a widely accepted standard, we align with the broader cryptocurrency community, making it easier for developers and users to work with Status.
- **User Convenience**: Users no longer need to manage multiple sets of mnemonic phrases for different wallets, simplifying the management of their digital assets.

### Next Steps:
- **Development**: Implement the BIP-39 and BIP-44 standards in the key generation and wallet import processes.
- **Testing**: Conduct thorough testing to ensure that the new key generation mechanism works correctly and securely.
- **Documentation**: Update the documentation to reflect the new key generation process and provide clear instructions for users.
- **User Communication**: Inform users about the changes and the benefits of the new key generation mechanism through our communication channels.

By implementing this decision, we aim to enhance the usability and interoperability of the Status wallet, making it more accessible and convenient for our users.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision 

Given the current context and the outlined performance issues, we will prioritize the development and integration of the Universal Light Client (ULC) as our primary solution for improving the performance, decentralization, and security of the Status app. This decision is based on the following considerations:

1. **Performance Improvement**: ULC promises to significantly enhance the performance of the Status app by reducing the reliance on resource-intensive protocols like LES, which have been identified as a major bottleneck.

2. **Decentralization and Security**: By allowing the Status app to connect to multiple nodes, ULC inherently improves the decentralization and security of the application. This reduces the risk associated with relying on a single point of failure or a centralized service like Infura.

3. **User Control and Flexibility**: ULC provides users with more control over their node connections, including the ability to run an upstream node. This aligns with the principles of decentralization and user sovereignty, which are core to the Status project.

4. **Future-Proofing**: Investing in ULC now will not only address immediate performance and security concerns but also set a strong foundation for future enhancements and scalability.

### Implementation Plan

1. **Short-term (Immediate)**:
   - Continue to use Infura as a temporary solution to ensure the app remains functional while ULC is being developed.
   - Provide clear documentation and support for users who wish to run their own upstream nodes to reduce reliance on Infura.

2. **Mid-term (Next 3-6 months)**:
   - Accelerate the development and testing of ULC to ensure it meets the performance, security, and usability requirements.
   - Conduct thorough testing and benchmarking to compare the performance of ULC against the current LES-based implementation.
   - Develop a migration plan to transition from Infura to ULC, ensuring a smooth and seamless experience for users.

3. **Long-term (Beyond 6 months)**:
   - Fully integrate ULC into the Status app, making it the default and preferred method for connecting to the Ethereum network.
   - Continuously monitor and optimize the performance and security of ULC, incorporating feedback and improvements from the community.
   - Explore additional features and functionalities that can be enabled by ULC, such as enhanced privacy and more robust peer-to-peer communication.

By prioritizing ULC, we aim to address the performance, decentralization, and security challenges while also laying the groundwork for a more robust and user-centric application.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision

To prevent sensitive data, such as user passwords, from accidentally being logged, we will introduce a new type or wrapper specifically for sensitive strings. This approach will ensure that these values are not directly converted to strings and thus cannot be inadvertently included in log entries.

### Implementation Steps:
1. **Create a SensitiveString Wrapper:**
   - Define a `SensitiveString` class or type that encapsulates the actual string value.
   - Override the `toString` method (or equivalent in the language) to return a placeholder value, such as `[REDACTED]`, instead of the actual string.

2. **Update Data Handling:**
   - Replace all instances where sensitive data is currently passed as a plain string with the `SensitiveString` wrapper.
   - Ensure that any function or method that processes or logs strings checks if the input is a `SensitiveString` and handles it appropriately.

3. **Logging and Debugging:**
   - Modify logging and debugging utilities to recognize `SensitiveString` and log the placeholder value `[REDACTED]` instead of the actual content.
   - Provide a way to safely inspect the actual value of a `SensitiveString` in a secure environment, such as a debugger, but never in a log file.

4. **Testing:**
   - Add unit tests to verify that sensitive data is properly wrapped and that the `toString` method returns the correct placeholder.
   - Test the logging system to ensure that sensitive data is not logged and that the placeholder is used instead.

5. **Documentation:**
   - Update the project documentation to include guidelines on handling sensitive data using the `SensitiveString` wrapper.
   - Provide examples and best practices for developers to follow when working with sensitive information.

By implementing this decision, we will significantly reduce the risk of sensitive data being exposed in log files, thereby enhancing the security and privacy of our application.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision 

We will deprecate and remove the current 3rd party status extensions that use the JavaScript jailed execution environment (Jail) and the associated API. Instead, we will focus on developing and adopting the new concept of extensions under project Pluto, which offers a more declarative and better-integrated approach for extending Status.

The transition to the new extension model will involve:
1. **Documentation and Communication**: Clearly document the deprecation timeline and the benefits of the new extension model. Communicate this change to all stakeholders, including developers who might have been considering or are currently using the Jail-based extensions.
2. **Migration Support**: Provide tools and guidelines to help migrate existing extensions to the new model. This includes creating a migration guide and possibly a tool to automate parts of the migration process where feasible.
3. **Development of New Extensions**: Encourage and support the development of new extensions using the Pluto framework. This includes providing comprehensive documentation, examples, and possibly a template or boilerplate to get started.
4. **Phased Removal**: Gradually phase out the Jail-based system, starting with no longer accepting new extensions using the old model, and eventually removing the Jail and its API from the codebase entirely.
5. **Performance and Testing Improvements**: Leverage the new extension model to improve performance and testing capabilities. The new model should be designed to avoid the performance overhead and testing challenges associated with the Jail-based system.

By making this change, we aim to improve the quality, maintainability, and performance of the Status application, while also providing a more robust and flexible platform for future extensions.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision

We will implement the Tribute to Talk (TtT) mechanism as described, with a few key clarifications and decisions to ensure it aligns with our goals of creating a fair, user-friendly, and economically sound system:

1. **Stakeholder Setting Minimum SNT Stake**: Each user (stakeholder) can set a minimum amount of SNT that they require for incoming messages from other users. This amount is configurable and can be adjusted by the user at any time.

2. **Deposit Requirement for Incoming Messages**: When a user (the sender) wants to send a message or contact request to another user (the recipient), they must deposit the minimum SNT amount set by the recipient. This deposit will be held in an escrow-like smart contract until the interaction is resolved.

3. **Forfeiture and Release of Deposit**:
   - If the recipient replies to the message, the deposited SNT will be released to the recipient.
   - If the recipient does not reply within a predefined period (e.g., 7 days), the deposited SNT will be returned to the sender.
   - The recipient has the option to manually release the deposit back to the sender if they choose not to reply but do not want the sender to lose their SNT.

4. **Smart Contract Implementation**: The TtT mechanism will be implemented using a smart contract on the Ethereum blockchain. This ensures transparency, security, and automation of the deposit and release process.

5. **User Interface and Experience**:
   - The user interface will clearly display the minimum SNT stake required for each user before sending a message.
   - Users will be prompted to confirm the SNT deposit before sending a message.
   - Both the sender and the recipient will receive notifications about the status of the deposit and any actions taken (e.g., deposit made, deposit released, deposit refunded).

6. **Economic Incentives**:
   - The TtT mechanism aims to create an economic barrier to spam and unwanted messages, encouraging senders to only contact recipients with genuine and valuable messages.
   - Recipients are incentivized to respond to messages that they find valuable, as they will receive the SNT deposit.

7. **Community and Governance**:
   - The community will have the ability to propose and vote on changes to the TtT mechanism, ensuring it remains fair and effective.
   - A governance model will be established to handle disputes and edge cases, such as fraudulent behavior or technical issues.

By implementing the TtT mechanism with these considerations, we aim to create a robust and fair system that reduces spam and encourages meaningful interactions within the Status community.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision 

To enhance the polish and oversight of the mobile app, we will introduce a new role: Section Lead. Each Section Lead will be responsible for a specific section of the mobile application (communities, syncing & onboarding, activity center & shell, wallet, messaging). The responsibilities of the Section Lead include:

1. **Oversight and Quality Assurance**: Ensuring that the section they are responsible for meets high standards of quality and user experience. This includes reviewing code, design, and functionality to ensure they align with the overall vision and goals of the project.

2. **Coordination and Communication**: Acting as the primary point of contact for their section, coordinating between developers, designers, and other stakeholders. They will facilitate communication and ensure that all team members are aligned and informed about the progress and any changes in the section.

3. **Planning and Prioritization**: Working with the team lead to plan and prioritize tasks and features for their section. This involves setting and tracking milestones, and ensuring that the development roadmap is clear and achievable.

4. **Mentorship and Support**: Providing guidance and support to team members working on their section. This includes helping to resolve technical challenges, offering feedback, and fostering a collaborative and productive work environment.

5. **Documentation and Knowledge Sharing**: Maintaining up-to-date documentation for their section, including design decisions, technical specifications, and best practices. This will help in onboarding new team members and maintaining consistency across the team.

6. **User Feedback and Iteration**: Gathering and analyzing user feedback related to their section, and using this information to drive continuous improvement. This includes collaborating with the design team to refine the user experience and address any usability issues.

By introducing these Section Leads, we aim to distribute leadership and responsibility more evenly across the team, leading to a more polished and cohesive mobile application.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision

To handle the Product Option's values collection in a way that is both efficient and easy to use within the new Sylius API, we will adopt the following strategies:

1. **Use Embedded Resources for Values Collection**: We will represent the values of a Product Option as embedded resources within the main Product Option resource. This allows for a more straightforward and intuitive API design, where the values are directly accessible and modifiable through the same endpoint as the Product Option itself.

2. **Leverage API Platform Features**:
   - **Serialization Groups**: Use serialization groups to control which fields (including the values collection) are included in the response. This provides flexibility in handling different levels of detail based on the client's needs.
   - **Filters and Pagination**: Implement filters and pagination for the values collection to manage large sets of data efficiently. This ensures that the API remains performant even with a high volume of option values.
   - **Validation Groups**: Utilize validation groups to apply specific validation rules to the values collection, ensuring data integrity and consistency.

3. **Optimize Data Storage and Retrieval**:
   - **Database Indexing**: Ensure that the database tables for Product Options and their values are properly indexed to speed up queries, especially when filtering or sorting the values.
   - **Eager Loading**: Use eager loading where appropriate to reduce the number of database queries, improving performance when fetching a Product Option along with its values.

4. **Documentation and Examples**:
   - Provide clear and comprehensive documentation for the API endpoints related to Product Options and their values. Include examples of how to create, update, and retrieve Product Options with their values.
   - Highlight best practices for using the API, such as how to effectively use filters and pagination to manage large collections of values.

By implementing these strategies, we aim to create an API that is not only efficient and easy to use but also scalable and maintainable, aligning with the overall goals of the Sylius project.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision

We will introduce a standardized and consistent approach for handling locale-aware resources in Sylius, ensuring that the API responses are tailored to the customer's locale. This involves:

1. **Locale Parameter in API Requests**: All API requests will support an optional `locale` query parameter. When this parameter is provided, the API will return only the translation corresponding to the specified locale. If no `locale` is specified, the API will default to returning all available translations.

2. **Consistent Embedding of Translations**: For resources that have locale-aware fields, the API will embed the appropriate translation based on the `locale` parameter. If the `locale` parameter is not provided, the API will return all translations as it currently does, but in a more structured and consistent manner.

3. **Standardized Response Format**: The response format will be standardized to include a `translations` field, which will contain an array of objects, each representing a different translation. Each object will have a `locale` key and the translated content. If a specific `locale` is requested, the API will return only the matching translation, or the first available translation if the exact match is not found.

4. **Documentation and Best Practices**: We will update the API documentation to clearly explain how to use the `locale` parameter and how to handle the `translations` field in the response. Additionally, we will provide best practices and examples for frontend developers to easily integrate and display the correct translations.

5. **Backward Compatibility**: To ensure backward compatibility, the existing endpoints that already embed some part of translatables will be updated to follow the new standard. This will involve a phased approach where we first introduce the new behavior as an optional feature, and then gradually deprecate the old behavior.

6. **Testing and Validation**: We will implement comprehensive testing to ensure that the new locale handling works correctly across all relevant endpoints. This includes unit tests, integration tests, and end-to-end tests to validate the behavior with and without the `locale` parameter.

By implementing these changes, we will provide a clear, easy-to-use, and consistent way of handling customer locales, improving the user experience and reducing the complexity for frontend developers.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision

We will adopt the use of Architecture Decision Records (ADRs) to document significant decisions made during the development of the Terraform Controller. ADRs are a lightweight and effective way to capture and communicate architectural decisions, providing a clear and structured format for documenting the context, decision, and consequences of each choice.

### Implementation
1. **Template and Structure**: We will standardize on a template for ADRs that includes sections for Title, Status, Date, Context, Decision, Consequences, and any additional notes or references.
2. **Storage and Access**: ADRs will be stored in a dedicated directory within the project's repository (e.g., `docs/adr/`). This ensures that they are version-controlled and easily accessible to all contributors.
3. **Workflow**:
   - **Proposal**: When a significant decision needs to be made, a contributor will create an ADR with a ""proposed"" status.
   - **Discussion**: The ADR will be reviewed and discussed by the team, possibly through pull requests or issues.
   - **Approval**: Once a decision is reached, the ADR will be updated with the final decision and marked as ""approved.""
   - **Revisiting Decisions**: If a decision needs to be revisited, a new ADR will be created, referencing the original ADR, and the process will repeat.
4. **Maintenance**: ADRs will be maintained and kept up-to-date. If a decision is no longer relevant or has been superseded, the ADR will be updated to reflect this, but not deleted to preserve the historical context.

### Benefits
- **Transparency**: ADRs provide a clear and transparent record of decisions, making it easier for new and existing contributors to understand the rationale behind specific choices.
- **Consistency**: By using a standardized format, ADRs ensure that decisions are documented consistently, making them easier to read and reference.
- **Collaboration**: The review and discussion process around ADRs encourages collaboration and ensures that multiple perspectives are considered before a decision is finalized.

By adopting ADRs, we aim to improve the maintainability and clarity of the Terraform Controller project, ensuring that all significant decisions are well-documented and easily accessible.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision 

We will use webhooks to detect pull request changes. Webhooks provide a more efficient and timely way to receive updates about repository events, including pull requests, compared to polling. By setting up a webhook in GitHub, we can configure it to send a payload to a specified URL whenever a relevant event (such as a pull request being opened, updated, or merged) occurs. This allows our system to react immediately to the event without the need for continuously querying the GitHub API, which can be resource-intensive and may hit rate limits if not carefully managed. Additionally, webhooks reduce the latency between when an event happens and when our system becomes aware of it, improving the responsiveness of our application.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision

We will maintain the ability to use cross-namespace references in the tf-controller API, but with additional safeguards and restrictions to mitigate the security concerns. The following measures will be implemented:

1. **Namespace Whitelisting**: Introduce a mechanism for namespace whitelisting, where only specific namespaces can be referenced across namespaces. This will help to limit the scope of cross-namespace interactions and reduce the risk of unintended or malicious access.

2. **Role-Based Access Control (RBAC) Enhancements**: Strengthen RBAC policies to ensure that only authorized users and service accounts can create, modify, or view objects that reference other namespaces. This will involve creating more granular roles and permissions specifically for managing cross-namespace references.

3. **Documentation and Best Practices**: Provide clear documentation and best practices for using cross-namespace references. This will include guidelines on when and how to use them, as well as recommendations for minimizing their use to maintain better security and isolation.

4. **Audit Logging**: Implement enhanced audit logging for any operations involving cross-namespace references. This will help in monitoring and detecting any unauthorized or suspicious activities related to these references.

5. **Configuration Validation**: Add validation checks to the tf-controller to ensure that cross-namespace references are used only when necessary and that they comply with the defined security policies. This will help to prevent accidental or inappropriate use of cross-namespace references.

By implementing these measures, we aim to balance the need for flexibility and functionality with the importance of maintaining strong security and namespace isolation in Kubernetes environments.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision

To address the challenges related to the deletion of Terraform resources, particularly focusing on single object deletion, we will implement a caching mechanism for the Workspace BLOBs. This mechanism will ensure that the TF-Controller can efficiently manage and delete Terraform objects without leaving them in an inconsistent state. The decision is structured as follows:

1. **Caching Mechanism Implementation**:
   - **BLOB Cache**: Implement a cache within the TF-Controller to store the Workspace BLOBs. This cache will be used to quickly retrieve the necessary information for deleting a single Terraform object.
   - **Cache Invalidation**: Define clear rules for cache invalidation. For example, the cache should be invalidated when a new Workspace BLOB is generated or when a Terraform object is updated.

2. **Enhanced Finalization Procedures**:
   - **Finalization Logic**: Update the finalization procedures in the TF-Controller to use the cached Workspace BLOBs. This will allow the controller to access all necessary information (Source, Secrets, ConfigMaps) required for the deletion process.
   - **Dependency Handling**: Ensure that the finalization logic correctly handles dependencies. If a Terraform object is tied to other resources, the controller should first attempt to delete or detach these dependencies before proceeding with the deletion of the main object.

3. **Error Handling and Logging**:
   - **Robust Error Handling**: Implement robust error handling to manage cases where the deletion process fails. This includes retry mechanisms and fallback strategies to ensure that the Terraform object is not left in an inconsistent state.
   - **Detailed Logging**: Enhance logging to provide detailed information about the deletion process. This will help in diagnosing issues and ensuring that the deletion process is transparent and traceable.

4. **Testing and Validation**:
   - **Unit and Integration Tests**: Develop comprehensive unit and integration tests to validate the new caching mechanism and finalization procedures. These tests should cover various scenarios, including successful deletions, failures, and edge cases.
   - **User Feedback Loop**: Engage with users to gather feedback on the new deletion process. Use this feedback to make further improvements and ensure that the solution meets the needs of the user community.

By implementing these changes, the TF-Controller will be better equipped to handle the deletion of single Terraform objects, setting the stage for addressing more complex deletion scenarios involving resource dependencies and namespace deletions.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision

Given the current state of Zarf and the removal of the need for repository secrets to create the initialization package, we will make the following changes to our E2E testing strategy:

1. **Enable Automatic E2E Testing on PRs**: Since the risk associated with exposing repository secrets during the E2E tests has been mitigated, we will enable automatic E2E testing for all pull requests (PRs). This change will help in catching issues early in the development cycle and reduce the burden on maintainers to manually trigger these tests.

2. **Expand Test Coverage**:
   - **Kubernetes Distributions**: We will enhance our E2E test suite to include different Kubernetes distributions. This will ensure that Zarf remains compatible and robust across a variety of Kubernetes environments.
   - **Linux Distributions**: We will also extend our E2E tests to cover multiple Linux distributions. This will help in verifying that Zarf functions correctly on different operating systems, thereby increasing its reliability and usability.

3. **Local E2E Testing**: To facilitate faster feedback loops and ease of development, we will provide a mechanism for developers to run at least some of the E2E tests locally. This could involve setting up a local Kubernetes cluster (e.g., using Minikube or Kind) and providing clear documentation and scripts to set up and run the tests. This will allow developers to quickly validate their changes without relying on an EC2 instance, thus speeding up the development process.

### Implementation Steps

1. **Update GitHub Workflows**:
   - Modify the existing GitHub workflows to automatically trigger E2E tests on PRs.
   - Add steps to the workflow to test against different Kubernetes and Linux distributions.

2. **Local Testing Setup**:
   - Create a guide and scripts for setting up a local Kubernetes cluster.
   - Provide instructions and scripts for running the E2E tests locally.
   - Ensure that the local testing environment can be easily set up and torn down.

3. **Documentation**:
   - Update the project documentation to reflect the new E2E testing procedures.
   - Include detailed instructions on how to run the E2E tests both in the CI/CD pipeline and locally.

By implementing these changes, we aim to improve the robustness and reliability of Zarf, while also making the development process more efficient and accessible for contributors.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision

Zarf will implement a mechanism to create and manage a temporary, in-cluster registry for the purpose of pushing images into K8s clusters that lack a CLI option for image injection. This approach will ensure that Zarf can operate in unknown environments without external dependencies, such as an existing registry.

### Key Components of the Solution:

1. **Temporary In-Cluster Registry:**
   - Zarf will deploy a lightweight, in-cluster registry (e.g., using a containerized version of a minimal OCI Distribution server) that can be used to push and pull images.
   - This registry will be deployed as a Kubernetes service and will be accessible to all nodes in the cluster.
   - The registry will be set up with a short-lived lifecycle, intended only for the duration of the deployment or until the images are no longer needed.

2. **Image Pushing and Pulling:**
   - Zarf will provide a command to push images to the temporary in-cluster registry.
   - The images will then be pulled by the CRI on each node as needed, using the standard K8s image pulling mechanisms.
   - Once the images are successfully pulled, the temporary registry can be removed to clean up resources.

3. **TLS Trust Management:**
   - To handle TLS trust, Zarf will generate a self-signed certificate for the temporary registry.
   - Zarf will also provide a method to inject this self-signed certificate into the CRI's trusted root CA on each node.
   - This can be done by modifying the CRI's configuration or by adding the certificate to the node's root CA store, depending on the CRI and environment.

4. **Node Affinity and Taints:**
   - To ensure that the initial bootstrap process is reliable, Zarf will use node affinity and taints to control where the initial pods are scheduled.
   - This will allow the bootstrap process to run on a single, designated node, ensuring that the necessary images are available before scaling out to other nodes.

5. **Cleanup and Resource Management:**
   - Zarf will include a cleanup mechanism to remove the temporary registry and any associated resources once the images have been successfully pulled and the deployment is complete.
   - This ensures that the cluster is left in a clean state and that resources are not wasted.

### Implementation Steps:

1. **Deploy Temporary Registry:**
   - Deploy a containerized OCI Distribution server as a Kubernetes service.
   - Expose the registry service internally within the cluster.

2. **Push Images to Registry:**
   - Use Zarf commands to push the required images to the temporary registry.

3. **Modify TLS Trust:**
   - Generate a self-signed certificate for the registry.
   - Inject the certificate into the CRI's trusted root CA on each node.

4. **Bootstrap Process:**
   - Use node affinity and taints to schedule the initial bootstrap pod on a specific node.
   - Ensure the images are pulled and available on that node before proceeding.

5. **Cleanup:**
   - After the images are successfully pulled and the deployment is complete, remove the temporary registry and any related resources.

By implementing this solution, Zarf will be able to push images into K8s clusters in a way that is both flexible and independent of external dependencies, ensuring that it can operate in a wide range of environments.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision 
To comply with Executive Order 14028, which mandates the use of Software Bills of Materials (SBOMs) for software running on government hardware, Arachne will implement a process to generate and maintain SBOMs for all its software components. 

The following steps will be taken:

1. **Tool Selection**: Choose and integrate an appropriate tool or service that can automatically generate SBOMs. This could be a tool like CycloneDX, SPDX, or another solution that is widely recognized and compatible with the standards set by the National Telecommunications and Information Administration (NTIA).

2. **Integration into Build Process**: Integrate the SBOM generation into the continuous integration/continuous deployment (CI/CD) pipeline. This ensures that every build and release of Arachne software includes an up-to-date SBOM.

3. **Documentation and Training**: Provide documentation and training for developers and other stakeholders on the importance of SBOMs, how they are generated, and how to interpret them. This will ensure that everyone understands the requirements and can contribute to maintaining accurate SBOMs.

4. **Regular Audits and Updates**: Conduct regular audits of the SBOMs to ensure they remain accurate and up-to-date. This includes reviewing dependencies, updating the SBOMs as new versions of dependencies are used, and ensuring that any changes in the software composition are reflected in the SBOMs.

5. **Compliance Verification**: Implement a process to verify compliance with EO14028. This may include periodic reviews, automated checks, and reporting mechanisms to ensure that all software deployed on government hardware has a valid and up-to-date SBOM.

By implementing these steps, Arachne will ensure that it meets the SBOM requirements set by EO14028, providing transparency and security for its software running on government hardware.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision

To address the limitations of Helm Post Rendering and to support more complex GitOps workflows with tools like Flux and ArgoCD, Zarf will introduce a new `zarf mutate` command. This command will be designed to mutate Kubernetes manifests and other resources in a way that is compatible with GitOps practices and can be easily integrated into CI/CD pipelines.

### Key Features of `zarf mutate`:
1. **Image Path Mutation**: The `zarf mutate` command will automatically update image paths in Kubernetes manifests to use the internal Zarf Registry. This will ensure that all images are correctly referenced, even when using GitOps tools.
2. **Secret Injection**: The command will also handle the injection of secrets into the manifests, ensuring that sensitive information is securely managed and available to the Kubernetes resources.
3. **CRD Mutation**: For Custom Resource Definitions (CRDs) that reference the git server, the `zarf mutate` command will update the necessary fields to point to the correct internal git server.
4. **Compatibility with GitOps Tools**: The mutated manifests will be compatible with GitOps tools like Flux and ArgoCD, allowing for seamless integration into existing GitOps workflows.
5. **Automated and Prescriptive**: The `zarf mutate` command will be designed to be as automated and prescriptive as possible, reducing the burden on package authors and ensuring consistent and reliable mutations.

### Workflow:
1. **Prepare Manifests**: Package authors will prepare their Kubernetes manifests as they normally would, without needing to manually perform mutations.
2. **Run `zarf mutate`**: Before applying the manifests, the `zarf mutate` command will be run to automatically update the image paths, inject secrets, and modify CRDs as needed.
3. **Apply Mutated Manifests**: The mutated manifests can then be applied using GitOps tools or directly via `kubectl`.

### Implementation:
- **Command Line Interface (CLI)**: The `zarf mutate` command will be added to the Zarf CLI, making it easy for users to integrate into their workflows.
- **Configuration Files**: Users will be able to provide configuration files to specify which resources need to be mutated and how.
- **Documentation and Examples**: Comprehensive documentation and examples will be provided to guide users on how to use the `zarf mutate` command effectively.

By introducing the `zarf mutate` command, Zarf will provide a robust and flexible solution for mutating Kubernetes resources, ensuring compatibility with GitOps practices and reducing the burden on package authors.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision

Given the context and the identified overhead, we will simplify the current process by removing the `stage2` go registry binary. Instead, we will leverage a more lightweight and efficient approach for the operations currently handled by `stage2`. The revised workflow will be as follows:

1. **Stage 1**: The existing `stage1` rust binary will continue to re-assemble the `registry:2` image and the necessary configuration from the configmaps.
2. **Direct Docker Registry Operations**: We will use a simple, lightweight script or a small set of shell commands to:
   - Start the docker registry in `rw` (read-write) mode.
   - Push the `registry:2` crane tarball into the registry.
   - Switch the docker registry to `r` (read-only) mode.

This approach will eliminate the need for the `stage2` binary, thereby reducing the number of binaries to manage, decreasing the number of configmaps, and simplifying the build and CI processes. Additionally, using a lightweight script or shell commands will reduce the overhead and resource usage, making the process more efficient and less wasteful.

### Implementation Steps
1. **Remove `stage2` Binary**:
   - Remove the `stage2` go registry binary from the project.
   - Update the build and CI/CD pipelines to no longer compile and include this binary.

2. **Develop Lightweight Script**:
   - Create a simple script (e.g., in Bash) that performs the required operations:
     - Start the docker registry in `rw` mode.
     - Push the `registry:2` crane tarball.
     - Switch the docker registry to `r` mode.
   - Ensure the script is well-documented and easy to maintain.

3. **Update Documentation**:
   - Update the project documentation to reflect the new, simplified process.
   - Provide clear instructions on how the new script integrates with the existing `stage1` binary.

4. **Testing and Validation**:
   - Thoroughly test the new script to ensure it performs the required operations correctly.
   - Validate that the overall system still meets the functional and performance requirements.

By adopting this approach, we will achieve a more streamlined and efficient solution, reducing complexity and overhead while maintaining the necessary functionality.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision

We will introduce Unit Testing in a defined and limited capacity within Zarf. This approach strikes a balance between ensuring that critical, hard-to-test areas of the codebase are covered, while also managing the maintenance burden of these tests.

### Rationale
- **Targeted Coverage**: By focusing unit tests on specific, difficult-to-test areas, we can ensure that these parts of the codebase are well-validated. This is particularly important as Zarf evolves into a library for other products, where robustness and reliability of interfaces are crucial.
- **Maintenance Burden**: Limiting the scope of unit tests to key areas helps manage the maintenance effort. We can be selective about what we test, ensuring that the tests remain relevant and effective over time.
- **Interface Focus**: We will prioritize writing unit tests that focus on the interfaces rather than the internal implementation details. This approach will help maintain the efficacy of the tests even as the codebase evolves.
- **Review Process**: To ensure the quality and correctness of the unit tests, we will incorporate a review process for PRs that include new or modified unit tests. This will help catch any issues early and ensure that the tests are correctly applied.

### Implementation Plan
1. **Identify Key Areas**: Conduct a thorough review of the codebase to identify the most critical and hard-to-test sections. These areas should be prioritized for unit testing.
2 . **Define Testing Guidelines**: Establish clear guidelines for writing unit tests, emphasizing the importance of testing interfaces and avoiding overly tight coupling with implementation details.
3. **Integrate with CI/CD**: Ensure that the new unit tests are integrated into the continuous integration and delivery (CI/CD) pipeline to run automatically with each build.
4. **Review and Refine**: Regularly review the unit tests during the PR review process to ensure they meet the defined guidelines and are correctly implemented.
5. **Monitor and Adjust**: Continuously monitor the effectiveness of the unit tests and make adjustments as needed to ensure they remain valuable and maintainable.

By following this approach, we aim to enhance the reliability and maintainability of Zarf, while also addressing the challenges of testing complex and evolving code.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision 

We will adopt a two-pronged approach to SBOM generation for Zarf packages, combining the benefits of both automatic and user-controlled processes. This decision aims to balance the need for comprehensive dependency tracking with the practicality and clarity of the information provided.

1. **Automatic SBOM Generation for Files and Data Injections:**
   - We will run Syft against files and data injections included in the Zarf package.
   - **Rationale:** These are the most common ways to include additional artifacts in packages, and we know these files are actually inside the package, reducing noise and ensuring that the SBOM reflects only the actual contents.
   - **Implementation:** This will be an automatic process integrated into the Zarf build pipeline. The SBOMs generated will be included in the package metadata, providing users with a clear and accurate view of the dependencies.

2. **User-Configurable SBOM Generation:**
   - We will provide a configuration option allowing users to specify additional directories or files for SBOM generation.
   - **Rationale:** This gives users the flexibility to include SBOMs for other artifacts that may be part of their Zarf package but not covered by the automatic process (e.g., git repos or other custom artifacts).
   - **Implementation:** Users can define these additional targets in the Zarf package manifest. If specified, the build process will run Syft on these additional targets and include the resulting SBOMs in the package metadata.

### Summary
- **Automatic SBOM for files and data injections** ensures that the most common and relevant artifacts are included in the SBOM, providing a clear and accurate dependency picture.
- **User-configurable SBOM** allows for flexibility and completeness, enabling users to include additional artifacts as needed.

This approach strikes a balance between providing a comprehensive SBOM and maintaining the clarity and usability of the information. It also ensures that the SBOM generation process is not overly complex, making it more likely to be used and valued by our users.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision 

We will introduce a new feature in Zarf that allows for the deployment of Zarf packages in connected environments without the need for a Zarf init package. This feature will be designed to support declarative deployments and upgrades, maintaining the core principles of Zarf while adapting to the needs of connected environments.

The workflow and implementation details for this feature will include:

1. **Declarative Configuration**: Users will define their Zarf package deployments through a YAML or JSON configuration file. This file will specify the Zarf packages to be deployed, along with any necessary parameters and dependencies.

2. **Simplified Deployment Command**: A new command, `zarf deploy`, will be introduced. This command will take the path to the configuration file as an argument and handle the deployment of the specified Zarf packages. The command will ensure that all dependencies are resolved and that the deployment is performed in the correct order.

3. **Automatic Dependency Resolution**: The `zarf deploy` command will automatically resolve and fetch any required dependencies (e.g., other Zarf packages, Helm charts, etc.) from the connected environment's available sources, such as public repositories or internal registries.

4. **Upgrade Support**: The `zarf deploy` command will also support upgrading existing deployments by comparing the current state with the desired state defined in the configuration file. It will apply the necessary changes to bring the environment to the desired state, ensuring that upgrades are handled declaratively and idempotently.

5. **Validation and Error Handling**: The system will include validation checks to ensure that the configuration file is correctly formatted and that all specified dependencies are available. If any issues are detected, the deployment process will halt, and clear error messages will be provided to the user.

6. **Documentation and Examples**: Comprehensive documentation and examples will be provided to guide users on how to create and use the configuration files for deploying Zarf packages in connected environments. This will include best practices and common use cases.

By introducing this feature, Zarf will become more versatile, allowing it to be used effectively in both disconnected and connected environments, while maintaining its core principle of declarative deployments.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision

To address the limitations and to provide more flexibility, we will enhance the `scripts` section within components. The enhancement will include:

1. **Extending Script Phases**: In addition to the existing `before`, `after`, and `prepare` phases, we will introduce new phases such as `install`, `uninstall`, and `upgrade`. This will allow for more granular control over the lifecycle of a package.

2. **Support for Conditional Execution**: Scripts will be able to conditionally execute based on certain criteria, such as environment variables, file existence, or other conditions. This can be achieved by adding a `when` clause to each script entry, which specifies the condition under which the script should run.

3. **Script Configuration**: We will add a configuration section to each script entry, allowing users to pass in parameters or environment variables. This will make scripts more reusable and adaptable to different environments.

4. **Error Handling and Retry Mechanism**: Scripts will have an option to specify error handling behavior, including retry attempts and timeout settings. This will help in making the scripts more robust and reliable.

5. **Logging and Output Capture**: Enhance logging capabilities to capture and store the output of scripts. This will aid in debugging and provide better visibility into what the scripts are doing.

6. **Script Versioning**: Introduce a versioning system for scripts, allowing teams to manage and track changes to scripts over time. This will help in maintaining backward compatibility and managing updates.

7. **Documentation and Examples**: Provide comprehensive documentation and examples for the new script features. This will help users understand how to use the enhanced scripting capabilities effectively.

By implementing these enhancements, we aim to provide a more powerful and flexible scripting system that can better meet the diverse needs of our users.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision

Given the context and the trade-offs between performance and stability, we will implement a configurable approach to loading images into the local Docker daemon. This solution aims to provide users with the flexibility to choose based on their specific needs and constraints.

### Key Points of the Decision:
1. **Introduce a Configuration Option:**
   - Add a new configuration option in Zarf that allows users to select between using `crane` or an alternative method (e.g., `skopeo` or the Docker CLI) for loading images.
   - The default setting will be `crane` to maintain the current performance benefits for most users.
   - Users who experience OOM issues or have large images can switch to a more stable but slower method.

2. **Implement a Fallback Mechanism:**
   - If `crane` fails due to an OOM condition, Zarf will automatically fall back to a more stable method (e.g., `skopeo`).
   - This fallback mechanism will ensure that the process does not crash and can complete, albeit at a slower pace.

3. **Documentation and User Guidance:**
   - Update the documentation to clearly explain the new configuration option and the trade-offs between performance and stability.
   - Provide guidance on how to diagnose and handle OOM conditions, including recommended system configurations and settings.

4. **Performance and Stability Monitoring:**
   - Continuously monitor the performance and stability of the image loading process.
   - Gather user feedback and usage data to inform future improvements and optimizations.

### Implementation Steps:
1. **Add Configuration Option:**
   - Introduce a new configuration flag in Zarf, e.g., `--image-loader` with options `crane` (default) and `skopeo`.
   - Ensure the flag is well-documented in the help and man pages.

2. **Develop Fallback Logic:**
   - Implement logic to detect OOM errors when using `crane`.
   - Automatically switch to `skopeo` if `crane` fails, and log the fallback action.

3. **Update Documentation:**
   - Revise the Zarf documentation to include the new configuration option and its implications.
   - Provide examples and best practices for different use cases.

4. **Testing and Validation:**
   - Conduct thorough testing to ensure the new configuration and fallback mechanisms work as expected.
   - Validate the performance and stability of both methods under various conditions.

By providing this flexible and robust solution, we aim to address the usability gap while maintaining the performance benefits for the majority of users.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision

To address the complexity and resource-intensive nature of deploying Big Bang via Zarf, we will implement a more modular and flexible approach to packaging and deploying Big Bang components. This will involve several key strategies:

1. **Modular Packaging**: Instead of including all possible images and git repositories in a single Zarf package, we will create smaller, modular packages that can be selectively included based on user needs. Each module will correspond to a specific component or set of components within Big Bang, allowing users to only include what they need.

2. **Dynamic Image and Git Repository Inclusion**: We will develop a mechanism to dynamically determine which images and git repositories are required for a given deployment. This will be based on the `values.yaml` file provided by the user. The Zarf package will include a script or tool that analyzes the `values.yaml` and generates a list of necessary images and git repositories, which can then be fetched and included in the package.

3. **Flux Integration**: Since Flux is a requirement for Big Bang, we will ensure that Flux is either pre-installed or included as a dependency in the Zarf package. This will simplify the installation process and ensure that the necessary Flux CRDs are available.

4. **Simplified `values.yaml` Injection**: We will provide a streamlined method for injecting the `values.yaml` file into the Big Bang deployment structure. This could involve creating a custom Kustomize patch or a script that modifies the `kustomization.yaml` to include the user-provided `values.yaml`.

5. **Documentation and User Guidance**: We will provide detailed documentation and examples to guide users through the process of creating and deploying modular Big Bang packages with Zarf. This will include best practices for managing dependencies, optimizing storage, and troubleshooting common issues.

By implementing these strategies, we aim to make the deployment of Big Bang more efficient, flexible, and user-friendly, while also reducing the storage and computational overhead associated with large, monolithic packages.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision 

We will adopt the use of Open Container Initiative (OCI) compliant registries for the distribution and publication of Zarf packages. This decision aligns with our goals of making Zarf packages easily consumable, while also ensuring security and trust in the package supply chain.

### Implementation Details:

1. **OCI Compliance:**
   - We will ensure that Zarf packages can be published to any OCI-compliant registry. This includes popular container registries such as Docker Hub, Amazon ECR, Google Container Registry, and others.
   - Zarf will support the `zarf package publish` command, which will allow users to push a Zarf package to an OCI-compliant registry. The command will handle the necessary steps to convert the Zarf package into a format that can be stored in the registry.

2. **Package Signing:**
   - To ensure the integrity and authenticity of Zarf packages, we will implement a signing mechanism. Package creators will be able to sign their Zarf packages using a private key.
   - The `zarf package sign` command will be introduced, allowing package creators to sign their packages. This command will generate a signature file that can be verified by the package deployer.
   - The `zarf package verify` command will be provided to allow package deployers to verify the signature of a Zarf package before deployment. This ensures that the package has not been tampered with and comes from a trusted source.

3. **Security and Trust:**
   - We will provide documentation and best practices for managing and securing private keys used for signing.
   - Zarf will include built-in checks to ensure that only signed packages are deployed, unless explicitly overridden by the user. This helps to maintain the trust in the package supply chain.

4. **User Experience:**
   - The `zarf package publish` and `zarf package sign` commands will be designed to be simple and intuitive, minimizing the barrier to entry for both package creators and deployers.
   - We will provide clear and concise documentation on how to use these commands, including examples and common use cases.

By adopting this approach, we aim to make Zarf packages more accessible and secure, thereby enhancing the overall user experience and trust in the Zarf ecosystem.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision 

To better support airgap development and improve the flexibility of Zarf in handling various coding language artifacts and libraries, as well as to enhance the support for `git` beyond just flux `GitRepository` objects, we will implement the following changes:

1. **Expand Git Support**: Enhance Zarf's `git` capabilities to support more generic use cases, allowing clients to interact with upstream `git` hosts in a native way. This includes supporting standard `git` operations like clone, pull, push, and fetch directly from within the airgapped environment. This will be achieved by integrating a robust `git` proxy or server that can handle these operations efficiently and securely.

2. **Add Artifact and Library Management**: Introduce a new feature in Zarf to manage and host a variety of software development artifacts and libraries. This will include:
   - A dedicated artifact repository that can store and serve binaries, packages, and other files necessary for development.
   - Support for multiple package managers (e.g., npm, Maven, pip) to facilitate the management of dependencies across different programming languages.
   - Integration with existing container registries and git servers to ensure seamless access and versioning of all required resources.

3. **Enhanced Configuration and Flexibility**: Update Zarf’s configuration options to allow users to specify which repositories and registries are available within their airgapped environment. This includes defining custom rules and permissions for accessing and managing these resources, ensuring that developers have the tools they need while maintaining security and compliance.

4. **Developer-Friendly Documentation and Tools**: Provide comprehensive documentation and tooling to help developers set up and use the new features effectively. This includes:
   - Detailed guides on setting up and configuring the new artifact and library management system.
   - Examples and best practices for using the enhanced `git` support.
   - Tools and scripts to automate common tasks, such as setting up a new development environment or updating dependencies.

By implementing these changes, Zarf will become a more versatile and powerful tool for both production deployments and airgap development, providing a more seamless and efficient experience for developers working in restricted environments.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision 

To transition Zarf into a ""Generally Available"" v1.x.x product suitable for mission-critical operations, we will implement the following strategies:

1. **Stabilize the API and CLI:**
   - Freeze the current API and CLI to prevent further breaking changes. Any new features or modifications must be backward-compatible.
   - Clearly document the public and stable parts of the API and CLI, ensuring that users can rely on these interfaces.

2. **Enhance Testing:**
   - Expand the test suite to cover all critical paths and edge cases. This includes unit tests, integration tests, and end-to-end (E2E) tests.
   - Implement continuous integration (CI) pipelines to automatically run tests on every commit and pull request.
   - Introduce automated testing in various environments (e.g., different Kubernetes versions, cloud providers) to ensure broad compatibility and reliability.

3. **Improve Documentation:**
   - Develop comprehensive and clear documentation for installation, configuration, and usage.
   - Include examples and best practices to help users understand how to use Zarf effectively.
   - Provide detailed release notes with each version, highlighting any changes, improvements, and known issues.

4. **Establish a Release Process:**
   - Define a clear and consistent release process, including versioning, changelogs, and release schedules.
   - Use semantic versioning (SemVer) to manage version numbers, making it easier for users to understand the impact of updates.
   - Create a beta and RC (Release Candidate) phase for major releases, allowing the community to test and provide feedback before a final release.

5. **Community and Support:**
   - Build a strong community around Zarf by engaging with users through forums, Slack, GitHub, and other channels.
   - Provide support mechanisms, such as a dedicated support team or a community-driven support model, to assist users with issues and questions.
   - Encourage contributions from the community and maintain an open and transparent development process.

6. **Monitoring and Feedback:**
   - Implement monitoring and logging capabilities within Zarf to help users track its performance and troubleshoot issues.
   - Collect user feedback regularly and use it to inform future development and improvements.
   - Establish a formal process for handling bug reports, feature requests, and other user feedback.

By implementing these strategies, Zarf will become a more reliable and user-friendly tool, ready for mission-critical operations and broader adoption.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision

To address the issues with ECR and to provide a more flexible, maintainable, and user-friendly solution, we will introduce a new feature called `hooks` in Zarf. These `hooks` will allow cluster maintainers to register functionality that runs during specific deployment lifecycle events, such as pre-deployment, post-deployment, and on-demand.

### Key Features of `hooks`:
1. **System Agnostic**: `hooks` will be designed to be system-agnostic, allowing package creators to write packages that can work seamlessly across different environments, including ECR.
2. **Lifecycle Events**: `hooks` will be triggered at specific points in the deployment lifecycle, such as before or after a package is deployed, or on-demand when certain conditions are met.
3. **Modular and Extensible**: `hooks` will be modular, allowing for easy addition and removal of functionality. This will enable Zarf to adapt to new requirements and environments without requiring changes to the core package code.
4. **Configuration-Driven**: The behavior of `hooks` will be driven by configuration, allowing cluster maintainers to define how `hooks` should behave in their specific environment.

### Implementation Details:
- **Hook Types**:
  - **Pre-Deployment Hooks**: Run before the package is deployed, useful for setting up necessary resources like ECR repositories.
  - **Post-Deployment Hooks**: Run after the package is deployed, useful for cleanup or additional setup.
  - **On-Demand Hooks**: Run based on specific triggers or conditions, such as token expiration.

- **ECR-Specific Considerations**:
  - **Token Refresh**: A `hook` will be created to handle the automatic refresh of ECR authentication tokens. This `hook` will run on a scheduled basis (e.g., every 10 hours) to ensure that the tokens do not expire.
  - **Repository Creation**: Another `hook` will be created to automatically create ECR image repositories if they do not already exist. This `hook` will run as part of the pre-deployment process to ensure that all necessary repositories are in place before the package is deployed.

### Workflow:
1. **Cluster Initialization**:
   - When a cluster is initialized with ECR as the remote registry, Zarf will automatically configure the necessary `hooks` to handle token refresh and repository creation.
   
2. **Package Deployment**:
   - During the deployment of a package, the pre-deployment `hooks` will run to ensure that all necessary ECR repositories are created and that the authentication tokens are valid.
   - The package will then be deployed.
   - Post-deployment `hooks` will run to perform any necessary cleanup or additional setup.

3. **Ongoing Maintenance**:
   - On-demand `hooks` will run on a scheduled basis to refresh ECR authentication tokens, ensuring that the cluster remains operational without manual intervention.

### Benefits:
- **Simplified Package Management**: Package creators do not need to write ECR-specific code, making packages more general and easier to maintain.
- **Automated Maintenance**: Cluster maintainers do not need to manually manage ECR tokens or repositories, reducing the risk of human error and downtime.
- **Flexibility and Extensibility**: The `hooks` system can be easily extended to support other cloud providers or custom requirements, future-proofing Zarf for new challenges.

By introducing `hooks`, Zarf will provide a robust, flexible, and maintainable solution for managing ECR and other cloud provider-specific requirements, while keeping the core package code simple and system-agnostic.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision 

We will use WebAssembly (WASM) to decouple the transformations from Zarf and synchronize the TypeScript with the Go code. This approach offers several advantages that align well with our goals:

1. **Performance**: WASM is designed to run at near-native speed, making it a suitable choice for performance-critical operations like transformations.
2. **Portability**: WASM is a binary format that can be executed in any environment that supports it, including web browsers and server-side environments. This ensures that the transformation logic can be seamlessly integrated into both the TypeScript and Go codebases.
3. **Language Agnostic**: WASM allows us to compile the Go code into a binary format that can be executed in a TypeScript environment. This means we can maintain the battle-tested and reliable Go code while still integrating it with TypeScript.
4. **Synchronization**: By using WASM, we avoid the need to maintain two separate libraries. The Go code can be compiled into a WASM module, which can then be imported and used directly in the TypeScript code. This ensures that the transformation logic remains consistent and synchronized between the two languages.
5. **Security**: WASM runs in a sandboxed environment, which provides an additional layer of security. This is particularly important when dealing with potentially sensitive data transformations.

### Implementation Plan

1. **Compile Go Code to WASM**:
   - Use a tool like `tinygo` or `wasm-gc` to compile the Go transformation library into a WASM module.
   - Ensure that the Go code is compatible with the WASM environment by avoiding any platform-specific dependencies.

2. **Integrate WASM Module in TypeScript**:
   - Import the compiled WASM module in the TypeScript code.
   - Use the WebAssembly API to load and execute the WASM module.
   - Provide a TypeScript interface to interact with the WASM module, ensuring that the transformation functions are easily accessible and usable.

3. **Testing and Validation**:
   - Write comprehensive tests to ensure that the transformations work correctly in both the Go and TypeScript environments.
   - Validate the performance and reliability of the WASM-based solution to ensure it meets the required standards.

By adopting this approach, we can leverage the strengths of both Go and TypeScript, while maintaining a single, reliable transformation library. This will simplify maintenance and ensure that the transformation logic remains consistent and robust.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision

To address the inconsistencies in Zarf's handling of different package sources, we will standardize and enhance the capabilities for each supported package type. This will ensure that all package types can be created, deployed, inspected, and removed, providing a consistent and robust user experience.

### Standardized Capabilities
1. **Local Tarball (`.tar` and `.tar.zst`)**:
   - **Create**: `zarf package create <dir> -o <dir>`
   - **Deploy**: `zarf package deploy <source>`
   - **Inspect**: `zarf package inspect <source>`
   - **Remove**: `zarf package remove <source>`

2. **Split Tarball (`.part...`)**:
   - **Create**: `zarf package create <dir> --max-package-size <size> -o <dir>`
   - **Deploy**: `zarf package deploy <source>`
   - **Inspect**: `zarf package inspect <source>`
   - **Remove**: `zarf package remove <source>`

3. **OCI Package (`oci://`)**:
   - **Create**: `zarf package create <dir> -o oci://...`
   - **Publish**: `zarf package publish <source> oci://`
   - **Deploy**: `zarf package deploy <source>`
   - **Inspect**: `zarf package inspect <source>`
   - **Remove**: `zarf package remove <source>`

4. **In-Cluster (Deployed) Package**:
   - **Deploy**: `zarf package deploy <source>`
   - **List**: `zarf package list`
   - **Inspect**: `zarf package inspect <name>`
   - **Remove**: `zarf package remove <name>`

5. **HTTPs URL**:
   - **Deploy**: `zarf package deploy <url>`
   - **Inspect**: `zarf package inspect <url>`
   - **Remove**: `zarf package remove <url>`

### Implementation Steps
1. **Enhance Inspect Command**:
   - Add support for inspecting split tarball, in-cluster, and HTTPs URL packages.
   - Ensure that the `inspect` command can handle both local and remote package sources.

2. **Enhance Remove Command**:
   - Add support for removing split tarball, in-cluster, and HTTPs URL packages.
   - Ensure that the `remove` command can handle both local and remote package sources.

3. **Update Documentation**:
   - Update the documentation to reflect the new and enhanced capabilities.
   - Provide clear examples and use cases for each package type.

4. **Testing and Validation**:
   - Develop and run comprehensive tests to ensure that all package types can be created, deployed, inspected, and removed without issues.
   - Validate the consistency and reliability of the commands across different environments and scenarios.

By standardizing and enhancing the capabilities for each package type, we will provide a more consistent and reliable user experience, making Zarf a more powerful and flexible tool for managing and deploying packages.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision

To address the issues with Zarf's composability and to improve the robustness, scalability, and maintainability of the package creation process, we will implement the following changes:

1. **Refactor Import Chains:**
   - We will refactor the import chain logic to ensure that it can handle larger numbers of layers more effectively. This includes improving the internal handling of imports to manage a deeper hierarchy without performance degradation.
   - Enhance test coverage to include multiple levels of imports, ensuring that the entire import chain is tested, not just the first level. This will help catch and resolve issues that arise from complex import structures.

2. **Integrate OCI Skeletons:**
   - Revisit the integration of OCI skeletons to ensure they are seamlessly integrated into the composability framework. This involves rethinking how OCI skeletons interact with the existing package creation and import mechanisms.
   - Update the documentation and developer guidelines to clearly explain how OCI skeletons fit into the package creation process and how they can be used in conjunction with other components.

3. **Centralize Component Filtering:**
   - Implement a centralized mechanism for component filtering, specifically for the `only` filter. This will ensure that the filtering logic is consistent and applied uniformly across all parts of the package creation process.
   - Refactor the current implementation to use this centralized filtering mechanism, which will help eliminate bugs related to create-time filters and make the codebase more maintainable.

4. **Enhance Documentation and Developer Experience:**
   - Provide detailed documentation on the new and improved composability features, including best practices for creating and managing complex packages.
   - Include examples and tutorials that demonstrate how to use the new features effectively, especially for those who are new to Zarf or are transitioning from older versions.

5. **Community and Feedback Loop:**
   - Engage with the community to gather feedback on the new composability features. This will help identify any additional pain points or areas for improvement.
   - Establish a feedback loop to continuously refine and enhance the composability features based on user experiences and needs.

By implementing these changes, we aim to make Zarf's composability feature more robust, scalable, and user-friendly, thereby enhancing the overall experience for package creators and users.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision

To improve the package development lifecycle and reduce the time and effort required for debugging and redeployment, we will introduce a new `zarf package dev` command. This command will streamline the process of creating, deploying, and debugging packages by:

1. **Enabling an enhanced `YOLO` mode via CLI flags**: The `zarf package dev` command will allow users to enable `YOLO` mode with specific flags, such as `--yolo`, which will skip the heavy dependency fetching and initialization steps. This will make it easier to quickly test and debug changes in a connected environment.

2. **Automating the reset and redeploy process**: The `zarf package dev` command will include options to automatically reset the cluster and redeploy the package, reducing the manual steps required. For example, a `--reset` flag can be used to automatically delete and recreate the cluster, followed by reinitializing and redeploying the package.

3. **Simplifying the workflow**: The `zarf package dev` command will handle the creation, deployment, and debugging steps in a single, streamlined process. This will eliminate the need for users to manually run multiple commands and manage the state of the cluster.

4. **Providing real-time feedback**: The `zarf package dev` command will provide real-time feedback on the status of the package creation and deployment, making it easier to identify and resolve issues quickly.

### Example Workflow

1. **Create and deploy the package**:
   ```sh
   zarf package dev <dir> --yolo
   ```

2. **Automatically reset the cluster and redeploy**:
   ```sh
   zarf package dev <dir> --yolo --reset
   ```

By introducing the `zarf package dev` command, we aim to significantly enhance the developer experience, making it faster and more efficient to iterate on and debug Zarf packages.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision

We will migrate the current documentation site from Docusaurus 2 to a more performant and dependency-friendly static site generator, such as Gatsby or Next.js. This decision is based on the following considerations:

1. **Performance**: Both Gatsby and Next.js are known for their performance optimizations, including server-side rendering (SSR), lazy loading, and automatic code splitting. These features can significantly improve the load time and user experience of our documentation site.

2. **Dependency Management**: Gatsby and Next.js have robust ecosystems and better support for modern JavaScript and TypeScript. They also provide more flexibility in managing dependencies, which can help us avoid some of the issues we've encountered with Docusaurus.

3. **Community and Ecosystem**: Both Gatsby and Next.js have large and active communities, which means better support, more plugins, and a wealth of resources and tutorials available. This can be beneficial for both current and future contributors.

4. **Customization and Flexibility**: Gatsby and Next.js offer more customization options and flexibility in terms of layout, styling, and functionality. This will allow us to tailor the documentation site more closely to our specific needs and branding.

5. **Future-Proofing**: By moving to a more modern and flexible framework, we can ensure that our documentation site remains up-to-date and can easily adapt to future changes in web technologies and best practices.

### Implementation Plan
1. **Evaluate Gatsby and Next.js**: Conduct a detailed evaluation of both Gatsby and Next.js to determine which one better fits our needs in terms of performance, ease of use, and community support.
2. **Prototype**: Create a prototype of the new documentation site using the chosen framework to ensure it meets our requirements.
3. **Migration**: Migrate the existing content and structure from Docusaurus to the new framework, ensuring all links and references are updated.
4. **Testing**: Thoroughly test the new site to ensure it performs well and all features work as expected.
5. **Launch**: Deploy the new documentation site and monitor its performance and user feedback.

### Maintenance
- Regularly update the site and its dependencies to keep it secure and performant.
- Continuously gather feedback from users and contributors to make further improvements.

This decision will help us address the current limitations and provide a better, more maintainable documentation experience for our users.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision

To address the issues with the current in-house logging solution and improve the overall logging and error handling in Zarf, we will adopt a more robust and flexible logging library. Specifically, we will migrate to a well-established logging library such as `logrus` or `zap`, which are designed to handle structured logging, multiple log levels, and thread safety.

### Key Points of the Decision:
1. **Logging Library Selection**:
   - We will evaluate and select a logging library that supports structured logging, multiple log levels, and is thread-safe.
   - `logrus` and `zap` are strong candidates due to their popularity, flexibility, and performance.

2. **Error Handling**:
   - Replace the use of `message.Fatal` with proper error handling mechanisms.
   - Use `log.Fatal` or equivalent from the chosen logging library, but ensure that it is only used in truly unrecoverable error scenarios.
   - For recoverable errors, return the error and handle it appropriately in the calling function, allowing for proper context handling and clean-up.

3. **Thread Safety**:
   - Ensure that the new logging solution is thread-safe, especially for components like progress bars and spinners.
   - Avoid using global state for these components. Instead, pass the necessary context and state through function parameters or use a context object.

4. **Testing**:
   - Refactor unit tests to handle the new logging and error handling patterns.
   - Use mocking and testing utilities provided by the logging library to facilitate unit testing.
   - Ensure that tests can capture and verify log outputs and error messages.

5. **Configuration and Customization**:
   - Provide a configuration mechanism to easily enable or disable features like progress bars and spinners.
   - Allow for customization of log formats and destinations (e.g., console, file, network).

6. **Documentation and Training**:
   - Update the documentation to reflect the new logging practices and error handling.
   - Provide training and support for developers to transition to the new logging solution.

### Implementation Steps:
1. **Evaluate and Select Logging Library**:
   - Conduct a detailed evaluation of `logrus` and `zap` to determine the best fit for Zarf.
   - Consider factors such as performance, ease of use, and community support.

2. **Refactor Code**:
   - Gradually replace the current logging calls with the new logging library.
   - Refactor error handling to use the new logging library's `Fatal` and other error logging methods appropriately.
   - Remove or refactor the use of global state for progress bars and spinners.

3. **Update Tests**:
   - Update unit tests to work with the new logging and error handling patterns.
   - Ensure that all tests pass and that the new logging behavior is correctly captured.

4. **Documentation and Training**:
   - Update the developer documentation to include the new logging and error handling practices.
   - Organize training sessions for the development team to familiarize them with the new logging solution.

By adopting a more robust and flexible logging library, we aim to improve the reliability, maintainability, and testability of the Zarf CLI, while also enhancing the user experience and providing better support for multi-threaded operations.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision

Zarf will introduce an explicit schema versioning system starting with the v1 release. This system will allow for a clear and managed transition of features, ensuring that deprecated features are supported throughout the v1 lifecycle while also providing a path for future updates.

### Key Points:
1. **Schema Versioning**:
   - Zarf will adopt a versioned schema approach, starting with `v1`. The schema version will be explicitly defined in the `zarf.yaml` file.
   - The schema version will be used to determine which features and fields are valid and how they should be processed.

2. **Deprecation Policy**:
   - Deprecated keys will be supported for the entire v1 lifecycle. This ensures that users have a stable and predictable experience.
   - When a key is deprecated, it will be clearly documented, and a warning will be issued during the `create` process if the deprecated key is used.
   - Automatic migration scripts will be provided for deprecated keys that can be directly mapped to new keys. For example, `setVariable` will be automatically migrated to `setVariables` as a single-item list.

3. **Migration and Backward Compatibility**:
   - During the `create` process, Zarf will perform automatic migrations for deprecated keys that can be safely converted to their new equivalents. The original deprecated key will remain in the packaged `zarf.yaml` to ensure compatibility with older Zarf binaries.
   - For deprecated keys that cannot be automatically migrated (e.g., `group` to `flavor`), users will be required to manually update their `zarf.yaml` files. These cases will be clearly documented and communicated to users.

4. **Future Schema Changes**:
   - Future major versions (e.g., v2) will introduce new schema versions, allowing for the removal of deprecated keys from the previous version.
   - A clear upgrade path will be provided, including tools and documentation to help users migrate their `zarf.yaml` files to the new schema.

5. **Communication and Documentation**:
   - All deprecations and changes will be clearly documented in the release notes and in the official Zarf documentation.
   - Users will be informed well in advance of any planned deprecations, and guidance will be provided on how to update their configurations.

By introducing this structured approach to schema versioning and deprecation, Zarf will provide a more robust and predictable experience for its users, ensuring long-term support and a smooth transition to new features.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
