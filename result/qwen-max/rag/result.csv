context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision 
## Decision

* Implement native lazy loading for images and thumbnails in the Storefront.
* Use the `loading=""lazy""` attribute on `<img>` tags to enable native lazy loading where supported.
* For browsers that do not support the `loading` attribute, consider using a polyfill or a fallback mechanism like ""lazysizes"" to ensure consistent lazy loading behavior across all browsers.
* Update the image rendering logic in the Storefront to automatically add the `loading=""lazy""` attribute to all images and thumbnails, except for those that are critical for above-the-fold content.
* Provide a configuration option to disable lazy loading if needed, allowing for flexibility in different use cases or environments.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision 
## Decision

To address the performance issues caused by the `autoload` parameter in `OneToOneAssociationField` and `ManyToOneAssociationField`, we will deprecate and eventually remove the `autoload` parameter. Instead, we will introduce a more controlled and explicit way to load associations on demand.

### Steps to Implement the Decision:

1. **Deprecate the `autoload` Parameter:**
   - Mark the `autoload` parameter as deprecated in the `OneToOneAssociationField` and `ManyToOneAssociationField` constructors.
   - Add deprecation notices and warnings in the documentation and code to inform developers about the upcoming change.

2. **Introduce Explicit Loading Mechanisms:**
   - Encourage the use of explicit loading mechanisms such as `with` or `load` methods in the repository or query builder.
   - For example, when querying for an entity, developers can explicitly specify which associations to load:
     ```php
     $entities = $repository->search(new Criteria(), $context)
         ->with(['association1', 'association2']);
     ```

3. **Update Documentation:**
   - Update the DAL documentation to provide clear guidance on how to load associations explicitly.
   - Include examples and best practices for efficient data loading.

4. **Provide Migration Path:**
   - Provide a migration path for existing code that uses the `autoload` parameter.
   - Offer tools or scripts to help developers identify and update their code to use the new explicit loading mechanisms.

5. **Remove the `autoload` Parameter:**
   - After a sufficient deprecation period (e.g., 1-2 major releases), remove the `autoload` parameter from the `OneToOneAssociationField` and `ManyToOneAssociationField` constructors.
   - Ensure that all internal and external code has been updated to use the new explicit loading mechanisms.

### Example of Explicit Loading:

```php
// Before (with autoload)
$oneToOneField = new OneToOneAssociationField('association', AssociationDefinition::class, 'foreign_key', 'local_key', true);
$manyToOneField = new ManyToOneAssociationField('association', AssociationDefinition::class, 'foreign_key', 'local_key', true);

// After (without autoload, using explicit loading)
$oneToOneField = new OneToOneAssociationField('association', AssociationDefinition::class, 'foreign_key', 'local_key');
$manyToOneField = new ManyToOneAssociationField('association', AssociationDefinition::class, 'foreign_key', 'local_key');

// Query with explicit loading
$criteria = new Criteria();
$criteria->addAssociation('association');
$entities = $repository->search($criteria, $context);
```

By following these steps, we can improve the performance of the DAL and ensure that associations are only loaded when necessary, reducing unnecessary data transfer, SQL joins, and application overhead.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision
## Decision

To address the different search scenarios and improve the overall navigation experience, we will implement a hybrid approach that combines the current folder structure with additional metadata and tooling support. Here's how we will handle each of the search cases:

1. **I am looking for an ADR and I know the headline:**
   - We will maintain a centralized index file (e.g., `adr-index.md`) that lists all ADRs with their titles, slugs, and links to the files. This index will be updated automatically whenever a new ADR is created or an existing one is modified.
   - The tool will provide a `search` subcommand that allows users to search for ADRs by title, slug, or content. This subcommand will use the index file to quickly find relevant ADRs.

2. **I would like to read the latest ADRs:**
   - The `adr` tool will include a `latest` subcommand that lists the most recently created or modified ADRs. This command will use Git history to determine the order of ADRs.
   - We will also add a `recent` section to the `adr-index.md` file, which will be updated to show the latest ADRs. This section will be useful for users who prefer to browse the index file directly.

3. **I would like to see ADRs for a specific area:**
   - The current folder structure will be retained, allowing users to navigate to the specific area and view the ADRs within that folder.
   - The `adr` tool will include an `area` subcommand that allows users to list ADRs for a specific area. This subcommand will use the folder structure to filter the ADRs.

4. **Other search scenarios:**
   - The `adr` tool will provide a `list` subcommand that lists all ADRs in a specified format (e.g., by date, by area, by title). This will give users flexibility in how they want to view the ADRs.
   - The `adr` tool will also include a `help` subcommand that provides detailed information on how to use the various subcommands and options.

### Implementation Details:
- **Index File (`adr-index.md`):** This file will be automatically generated and updated by the `adr` tool. It will contain a table of contents with links to each ADR, sorted by date and area.
- **Git History Integration:** The `latest` subcommand will use Git commands to fetch the most recent ADRs based on commit history.
- **Subcommands:**
  - `adr search <keyword>`: Search for ADRs by title, slug, or content.
  - `adr latest [n]`: List the `n` most recent ADRs.
  - `adr area <area>`: List ADRs for a specific area.
  - `adr list [format]`: List all ADRs in a specified format (e.g., `date`, `area`, `title`).
  - `adr help`: Provide help and usage information for the `adr` tool.

By implementing these changes, we will provide a more flexible and user-friendly way to navigate and search through ADRs, catering to various use cases and preferences.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision 
## Decision
We will not proceed with the proposed approach of using native Vue components and the Composition API for extending and overwriting components in the administration. While this approach offers several benefits, such as better performance, improved static analysis, and the ability to use the full power of Vue, the challenges and risks associated with it outweigh these advantages.

### Key Challenges:
1. **Data Merging**: The Vue compiler optimizes many parts of the component, making it difficult to merge data from the original component and the extended or overwritten component. This would require using internal Vue logic, which is not update-safe and could break with every Vue update.
2. **Complexity**: The proposed solution introduces a significant amount of complexity, especially for plugin developers who need to understand and work with the new system.
3. **Update Safety**: Relying on internal Vue logic for data merging and other functionalities makes the system vulnerable to breaking changes in future Vue updates.

### Alternative Approach:
Instead, we will continue to use the existing Component Factory approach while exploring ways to improve its performance and maintainability. We will also investigate other potential solutions that can address the current limitations without introducing the same level of risk and complexity.

### Next Steps:
1. **Performance Improvements**: Look into ways to optimize the current Component Factory to improve performance, such as caching and precompilation where possible.
2. **Tooling Enhancements**: Explore tools and techniques to enhance static analysis and linting capabilities for the existing system.
3. **Future Research**: Continue to research and monitor developments in the Vue ecosystem for potential future improvements that could address our needs more effectively.

By taking this approach, we aim to maintain the flexibility and extensibility of the current system while addressing its limitations in a more controlled and sustainable manner.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision 
## Decision
We will adopt the use of a `changelog.d` directory to store individual changelog entries as separate files, each with a unique and descriptive filename. These files will be versioned alongside the codebase, and a script or tool will aggregate these files into a single, comprehensive changelog when a release is prepared. This approach addresses the issues with the current workflow:

*  **Reduced Merge Conflicts**: Since each change is recorded in a separate file, merge conflicts in the changelog are minimized.
*  **Clear Versioning**: Each changelog entry can be tagged with the appropriate version, ensuring that the correct release version is associated with each change.
*  **Consistency with Jira**: The changelog entries can be linked to Jira issues, ensuring that the issue state in Jira aligns with the code state.
*  **Ease of Maintenance**: Developers can easily add new changelog entries without worrying about the structure of a single, large changelog file.

### Implementation Details

1. **Directory Structure**:
   - Create a `changelog.d` directory at the root of the repository.
   - Each changelog entry will be a separate file within this directory, named with a unique identifier (e.g., `YYYYMMDD-change-description.md`).

2. **Changelog Entry Format**:
   - Each file will follow a standard format, including the version number, type of change (e.g., `feat`, `fix`, `docs`), and a brief description.
   - Example:
     ```markdown
     ## [Unreleased]
     ### Added
     - New feature: Description of the new feature.
     
     ## [1.2.0] - 2023-10-01
     ### Fixed
     - Bug fix: Description of the bug fix.
     ```

3. **Aggregation Script**:
   - Develop a script (e.g., in Python or a shell script) that reads all the files in the `changelog.d` directory and aggregates them into a single, formatted changelog file.
   - The script should sort the entries by version and type, and handle unreleased changes appropriately.

4. **Release Process**:
   - As part of the release process, the aggregation script will be run to generate the final changelog.
   - The generated changelog will be included in the release notes and committed to the repository.

### Benefits

- **Reduced Merge Conflicts**: Separate files for each change minimize the likelihood of merge conflicts.
- **Clear Versioning**: Each entry is tagged with the correct version, ensuring accurate release information.
- **Consistency with Jira**: Changelog entries can be linked to Jira issues, maintaining consistency between the issue tracker and the codebase.
- **Ease of Maintenance**: Developers can easily add and manage changelog entries without the complexity of a single, large file.

This approach provides a more flexible and maintainable way to manage technical changelogs, aligning well with trunk-based development and branching processes.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision 
## Decision

To address the challenges of implementing a `DelayAction` in Flow Builder, we will adopt the following strategies:

1. **Data Snapshotting:**
   - When an event is first received and the `DelayAction` is triggered, we will create a snapshot of the relevant data.
   - This snapshot will be stored along with the delayed action. The snapshot should include all necessary data for the action to be executed correctly after the delay.
   - When the delayed action is finally executed, it will use this snapshot instead of the current state of the data. This ensures that the action is performed based on the data that was valid at the time the delay was initiated.

2. **Rule Re-evaluation:**
   - After the delay, the rules will need to be re-evaluated. To ensure that the rules are up-to-date, we will reload the rules from the database or rule store.
   - If the rules have been updated or deleted during the delay, the reloaded rules will reflect these changes.
   - We will also implement a mechanism to check if the rules still exist before re-evaluating them. If a rule no longer exists, the action will be skipped or handled according to a predefined fallback strategy.

3. **Event Handling and Error Management:**
   - If the data snapshot or rule re-evaluation fails, we will log the error and handle it appropriately. This could involve retrying the action, sending a notification, or taking another predefined action.
   - We will also implement logging and monitoring to track the execution of delayed actions and any issues that arise.

4. **Implementation Details:**
   - **Snapshot Storage:** The snapshot can be stored in a reliable storage system, such as a database or a message queue with persistence.
   - **Rule Loading:** Use a service or repository to load the latest rules from the rule store.
   - **Fallback Strategy:** Define a fallback strategy for cases where the rules no longer exist or the data is invalid. This could be a default action, a notification, or a graceful failure.

By implementing these strategies, we can ensure that the `DelayAction` in Flow Builder is robust and handles the challenges of outdated data and changing rules effectively.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision 
## Decision

- Integrate the app system into the core of the Shopware 6 platform.
- The app system will be a first-class citizen within the platform, following the same release and upgrade cycles as the rest of the platform.
- Remove the app system from being a separate plugin and incorporate its functionality directly into the platform codebase.
- Ensure that the integration is seamless and does not require additional installation steps for users.
- Provide clear documentation and migration guides for existing users of the app system plugin to transition to the integrated version.
- Continue to gather feedback and iterate on the app system, but now within the context of the platform's development and release processes.
- Communicate the change to partners and app manufacturers, emphasizing the stability and reliability of the app system within the platform.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision 
## Decision

To reduce boilerplate and improve the maintainability and extensibility of the `FlowStorer` implementations, we will introduce a more generic and unified approach. This will involve the following steps:

1. **Define a Generic Storer Interface:**
   Create a single, generic `Storer` interface that all storer implementations will adhere to. This interface will include methods for storing and retrieving data, as well as a method to check if the storer should handle a given event.

   ```java
   public interface Storer<T> {
       boolean shouldStore(Event event);
       void store(Event event, T data);
       T retrieve(Event event);
   }
   ```

2. **Implement Scalar Value Storers:**
   For scalar values, create a base implementation of the `Storer` interface that can be used for most scalar types. This base implementation will handle the common logic for storing and retrieving scalar values.

   ```java
   public abstract class ScalarValueStorer<T> implements Storer<T> {
       @Override
       public boolean shouldStore(Event event) {
           // Default implementation or template method
           return true; // or some condition
       }

       @Override
       public void store(Event event, T data) {
           // Common logic for storing scalar values
       }

       @Override
       public T retrieve(Event event) {
           // Common logic for retrieving scalar values
       }
   }
   ```

3. **Create Specific Scalar Storers:**
   Extend the `ScalarValueStorer` for specific scalar types (e.g., `IntStorer`, `StringStorer`). These specific storers will override the `shouldStore` method if necessary and provide any additional type-specific logic.

   ```java
   public class IntStorer extends ScalarValueStorer<Integer> {
       @Override
       public boolean shouldStore(Event event) {
           // Specific condition for integer storage
           return event.getType().equals(""INT_EVENT"");
       }
   }

   public class StringStorer extends ScalarValueStorer<String> {
       @Override
       public boolean shouldStore(Event event) {
           // Specific condition for string storage
           return event.getType().equals(""STRING_EVENT"");
       }
   }
   ```

4. **Use a Storer Factory:**
   Introduce a `StorerFactory` that can create instances of the appropriate `Storer` based on the event type. This will allow plugins to easily bypass or add new storer implementations.

   ```java
   public class StorerFactory {
       private final Map<String, Storer<?>> storers;

       public StorerFactory() {
           this.storers = new HashMap<>();
           this.storers.put(""INT_EVENT"", new IntStorer());
           this.storers.put(""STRING_EVENT"", new StringStorer());
           // Add more storer mappings as needed
       }

       public <T> Storer<T> getStorer(Event event) {
           @SuppressWarnings(""unchecked"")
           Storer<T> storer = (Storer<T>) storers.get(event.getType());
           return storer;
       }
   }
   ```

5. **Update Event Handling Logic:**
   Update the event handling logic to use the `StorerFactory` to determine which `Storer` to use for a given event. This will centralize the decision-making process and reduce the need for individual checks in each event handler.

   ```java
   public void handleEvent(Event event) {
       StorerFactory factory = new StorerFactory();
       Storer<?> storer = factory.getStorer(event);

       if (storer != null && storer.shouldStore(event)) {
           Object data = event.getData(); // Assuming event has a method to get the data
           storer.store(event, (T) data);
       }
   }
   ```

By following these steps, we can reduce the amount of boilerplate code, make it easier to add new storer implementations, and allow plugins to bypass or extend the existing storer logic.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision

We will replace the current sw-text-editor with a more modern and maintainable WYSIWYG editor. After evaluating several options, we have decided to use [TinyMCE](https://www.tiny.cloud/) as the new text editor for the administration. TinyMCE is well-documented, widely used, and has a robust feature set that can be easily extended. It also provides better stability and a more user-friendly interface, which will improve the overall experience for our users. Additionally, its active community and frequent updates ensure that it remains a reliable choice for the long term.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision 
## Decision

We will update the default salutation in Shopware 6 to ""not_specified"" for cases where the salutation is not specified by the customer or administrator. This change will be implemented in the following ways:

1. **Database Schema Update**: Add a new salutation option ""not_specified"" to the database schema if it does not already exist.
2. **Default Value in Code**: Set ""not_specified"" as the default value for the salutation field in the relevant entities and forms.
3. **UI/UX Adjustments**:
   - Update the user interface to include ""not_specified"" as an option in the salutation dropdown.
   - Ensure that the ""not_specified"" option is clearly labeled and easily selectable.
4. **Validation and Data Migration**:
   - Implement validation to ensure that the salutation field is always set, either by the user or by defaulting to ""not_specified"".
   - Perform a data migration to update existing records with null or empty salutations to ""not_specified"".
5. **Documentation and Communication**:
   - Update the documentation to reflect the new default salutation and provide guidance on how to handle unspecified salutations.
   - Communicate this change to the community and stakeholders to ensure a smooth transition.

This approach ensures that the platform remains inclusive and handles unspecified salutations gracefully, providing a consistent and user-friendly experience.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision 
## Decision

* Discontinue the use of `padaliyajay/php-autoprefixer` for automatic vendor prefixing in the storefront CSS.
* Rely on modern browser support as defined in `.browserslist`, which is now more aligned with Bootstrap v5.
* Remove the autoprefixing step from the `theme:compile` process to improve performance, especially in the SaaS solution.
* Ensure that any necessary vendor prefixes are manually added in the SCSS files where they are required. This will be done by maintaining a list of known properties that might still need prefixes based on the supported browsers.
* Use a more dynamic and up-to-date tool like `postcss` with `autoprefixer` in a separate build step if manual management becomes too cumbersome or if new browser support requirements arise.

### Implementation Steps:
1. **Remove Autoprefixer Dependency:**
   - Remove the `padaliyajay/php-autoprefixer` package from the project dependencies.
   - Update the `theme:compile` script to no longer include the autoprefixing step.

2. **Update Build Process:**
   - If needed, integrate `postcss` and `autoprefixer` into the build process. This can be done using a Node.js-based build step.
   - Configure `autoprefixer` to read the `.browserslist` file and apply the necessary vendor prefixes dynamically.

3. **Manual Prefix Management:**
   - Maintain a list of CSS properties that may still require vendor prefixes based on the supported browsers.
   - Add these prefixes manually in the SCSS files where they are needed.

4. **Testing and Validation:**
   - Thoroughly test the compiled CSS in all supported browsers to ensure compatibility and correct rendering.
   - Validate the performance improvement in the `theme:compile` process.

### Benefits:
- **Performance Improvement:** The `theme:compile` process will be faster, reducing the overall build time.
- **Modern Browser Support:** Leverage the updated browser support and reduce unnecessary prefixes.
- **Sustainability:** Move away from a hard-coded solution to a more dynamic and maintainable approach.

### Example Configuration (if using `postcss` and `autoprefixer`):
```json
// package.json
{
  ""devDependencies"": {
    ""autoprefixer"": ""^10.4.0"",
    ""postcss"": ""^8.4.5"",
    ""postcss-cli"": ""^9.1.0""
  },
  ""scripts"": {
    ""build:css"": ""postcss src/scss/main.scss -o dist/css/main.css""
  }
}
```

```js
// postcss.config.js
module.exports = {
  plugins: [
    require('autoprefixer')
  ]
};
```

By following this decision, we will streamline the build process, improve performance, and ensure that our CSS remains compatible with the supported browsers.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision 
## Decision

To address the issue of pre-release package versions being interpreted as `<0.0.0` by npm, we will adopt the following approach:

1. **Avoid Using Pre-Release Versions for Security Fixes:**
   - We will ensure that any version containing security fixes is released as a stable version (e.g., `1.9.0` instead of `1.9.0-alpha1`). This ensures that npm correctly interprets the version and can report any security issues.

2. **Update Dependency Management Practices:**
   - We will update our dependency management practices to avoid using pre-release versions in production environments. Pre-release versions should be used only for testing and development purposes.

3. **Automate Version Bumping:**
   - We will automate the process of bumping the version number to a stable release when a security fix is merged. This can be done using CI/CD pipelines to ensure that the version is correctly updated and published.

4. **Monitor and Alert:**
   - We will set up monitoring and alerting for any pre-release versions in our dependencies. This will help us identify and address any instances where pre-release versions are being used in production.

By following these steps, we can ensure that security issues are properly reported and managed, and that our dependencies are always up-to-date with the latest security patches.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision 
## Decision

To improve the performance of the SQL query that deletes outdated cart entries, we will create a composite index on the `cart` table. This index will include the `created_at` and `updated_at` columns, which are used in the deletion criteria. 

The new composite index will be defined as follows:
- Index Name: `idx_cart_created_at_updated_at`
- Columns: `created_at`, `updated_at`

This composite index will allow the database to efficiently filter and delete the outdated cart entries by using the index to quickly locate the relevant rows.

The updated SQL statement to create the index will be:
```sql
CREATE INDEX idx_cart_created_at_updated_at ON cart (created_at, updated_at);
```

After creating the index, the deletion query will be optimized to use this index, significantly reducing the query execution time. The optimized deletion query will remain the same, but it will now benefit from the new index:
```sql
DELETE FROM cart
WHERE (updated_at IS NULL AND created At <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```

By adding this composite index, we ensure that the database can efficiently handle the deletion of outdated cart entries, even in high-traffic shops, without causing long query times.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision 
## Decision

To address the challenges of releasing features in an early state and gathering valuable user feedback, we will introduce an ""Experimental Features"" framework. This framework will allow us to release features in an experimental state, gather feedback, and make necessary changes without being constrained by our backwards compatibility promise.

### Key Components of the Experimental Features Framework:

1. **Experimental Flagging:**
   - Features will be marked as ""experimental"" in their initial release.
   - These features will be clearly documented and communicated to users as being in an experimental state, meaning they are subject to change and may not be fully stable.

2. **Opt-In Mechanism:**
   - Users will need to explicitly opt-in to use experimental features.
   - This can be done through a configuration setting or a feature toggle in the application.

3. **Feedback Loop:**
   - A structured process for collecting and analyzing user feedback on experimental features.
   - Regular check-ins with users who have opted into experimental features to gather insights and suggestions.

4. **Iterative Development:**
   - Features will be developed and released in smaller, incremental updates.
   - Each update will be based on the feedback received from the previous version.

5. **Backwards Compatibility Exemption:**
   - Experimental features will be exempt from the backwards compatibility promise.
   - This allows for more flexibility in making changes and improvements based on user feedback.

6. **Promotion to Stable:**
   - Once a feature has been sufficiently tested and refined, it can be promoted to a stable release.
   - At this point, the feature will be subject to the backwards compatibility promise.

### Example Implementation:

- **B2B Multi-Account Feature:**
  - Initial release: Simple employee management system.
  - Feedback collection: Gather user feedback on the basic functionality.
  - Iterative updates: Add budget management and access control based on user feedback.
  - Promotion: Once the feature is mature and well-received, promote it to a stable release.

- **Advanced Search Feature:**
  - Initial release: Basic advanced search functionality.
  - Feedback collection: Gather user feedback on the initial implementation.
  - Iterative updates: Add additional configuration options and customizability based on user feedback.
  - Promotion: Promote the feature to a stable release once it meets user needs and expectations.

### Benefits:
- **Faster Time-to-Market:** Release features sooner, delivering business value to customers earlier.
- **Reduced Risk:** Lower the risk of building the wrong thing by incorporating real user feedback.
- **Flexibility:** Allow for more flexible and iterative development processes.
- **User Engagement:** Increase user engagement and participation in the development of new features.

By implementing this ""Experimental Features"" framework, we can balance the need for innovation and user feedback with our commitment to backwards compatibility, ultimately leading to better and more user-centric features.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision 
## Decision

To address the limitations and improve the flexibility of stock handling in Shopware 6, we will implement the following changes:

1. **Introduce a Stock Provider Interface:**
   - Create a `StockProviderInterface` that defines methods for loading and updating stock.
   - The default implementation will use the current database-based approach, but this interface will allow for custom implementations, such as integrating with an ERP system.

2. **Refactor Stock Calculation:**
   - Simplify the stock model by using a single `stock` value. The available stock will be calculated on the fly based on open orders.
   - Implement a more efficient algorithm for calculating available stock, possibly using caching or batch processing to handle large catalogs.

3. **Hook into Order Lifecycle:**
   - Introduce hooks or events in the order lifecycle (e.g., `order.created`, `order.completed`, `order.cancelled`) to allow for custom stock adjustment logic.
   - Provide a default implementation that updates stock based on these events, but allow plugins to override or extend this behavior.

4. **Performance Improvements:**
   - Optimize the database queries used for stock calculations. This could include indexing, query optimization, and possibly denormalizing data for faster access.
   - Consider implementing a background job to periodically update and cache stock values, especially for large catalogs.

5. **Documentation and Examples:**
   - Provide detailed documentation on how to implement a custom `StockProvider` and how to hook into the order lifecycle for stock adjustments.
   - Include example implementations, such as a simple ERP integration, to help developers get started.

### Implementation Details

#### StockProviderInterface
```php
interface StockProviderInterface
{
    public function loadStock(string $productId): int;

    public function updateStock(string $productId, int $quantity): void;
}
```

#### Default Stock Provider
```php
class DatabaseStockProvider implements StockProviderInterface
{
    private $entityManager;

    public function __construct(EntityManagerInterface $entityManager)
    {
        $this->entityManager = $entityManager;
    }

    public function loadStock(string $productId): int
    {
        // Load stock from the database
        $product = $this->entityManager->find(Product::class, $productId);
        return $product->getStock();
    }

    public function updateStock(string $productId, int $quantity): void
    {
        // Update stock in the database
        $product = $this->entityManager->find(Product::class, $productId);
        $product->setStock($quantity);
        $this->entityManager->flush();
    }
}
```

#### Order Lifecycle Hooks
```php
class OrderService
{
    private $stockProvider;

    public function __construct(StockProviderInterface $stockProvider)
    {
        $this->stockProvider = $stockProvider;
    }

    public function createOrder(Order $order): void
    {
        // Custom logic for creating an order
        // ...

        // Dispatch event
        $event = new OrderCreatedEvent($order);
        $this->dispatcher->dispatch($event);

        // Adjust stock
        foreach ($order->getLineItems() as $lineItem) {
            $this->stockProvider->updateStock($lineItem->getProductId(), -$lineItem->getQuantity());
        }
    }

    public function completeOrder(Order $order): void
    {
        // Custom logic for completing an order
        // ...

        // Dispatch event
        $event = new OrderCompletedEvent($order);
        $this->dispatcher->dispatch($event);
    }

    public function cancelOrder(Order $order): void
    {
        // Custom logic for canceling an order
        // ...

        // Dispatch event
        $event = new OrderCancelledEvent($order);
        $this->dispatcher->dispatch($event);

        // Adjust stock
        foreach ($order->getLineItems() as $lineItem) {
            $this->stockProvider->updateStock($lineItem->getProductId(), $lineItem->getQuantity());
        }
    }
}
```

By implementing these changes, we will make the stock handling in Shopware 6 more flexible, efficient, and easier to customize, addressing the common use cases and performance issues.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision 
## Decision

To address the issue of logging noise caused by client-side errors, we will configure the `symfony/monolog-bridge` to log specific exceptions at a lower log level, such as `notice`, instead of `error`. This will help in reducing the noise in the logs and make it easier to identify actual system errors.

We will use the existing configuration list approach that has been successfully implemented in our cloud product. This configuration list will map specific exception classes, such as `ShopwareHttpException` and its subclasses, to a lower log level. 

Here is an example of how this can be configured:

```php
// config/packages/monolog.yaml
monolog:
    handlers:
        main:
            type: stream
            path: ""%kernel.logs_dir%/%kernel.environment%.log""
            level: debug
            channels: [""!event""]
        # Add a custom handler for specific exceptions
        custom_exception_handler:
            type: fingers_crossed
            action_level: notice
            handler: nested
            excluded_400s:
                - Symfony\Component\HttpKernel\Exception\BadRequestHttpException
                - Shopware\Core\System\CustomField\Exception\CustomFieldNotFoundException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InconsistentCriteriaIdsException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\EntityNotFoundException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\SearchRequestException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterQueryException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationQueryException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationQueryException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffsetException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageParameterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageNumberException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPageSizeException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortOrderException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidSortFieldException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidFilterException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAggregationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidAssociationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidPaginationException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidLimitException
                - Shopware\Core\Framework\DataAbstractionLayer\Exception\InvalidOffset","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision 
## Decision

We will switch from using UUIDv4 to UUIDv7 for primary keys in our database. The time-ordered nature of UUIDv7 will improve the efficiency and performance of B-tree indexes, as it will reduce the fragmentation caused by the random distribution of UUIDv4. This change will result in more compact and efficient indexes, leading to better overall database performance, especially in scenarios with high insert rates.

To implement this change, we will:
1. Update the codebase to generate UUIDv7 instead of UUIDv4.
2. Ensure that all new records use UUIDv7.
3. Consider a migration strategy for existing data, if necessary, to convert UUIDv4 to UUIDv7, though this may not be required if the existing UUIDs can remain as they are without causing significant performance issues.
4. Test the new implementation thoroughly to ensure that the transition does not introduce any regressions or issues.

This decision aligns with our goal of maintaining a performant and scalable system while also ensuring that the integration of different data sources remains straightforward.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision 
## Decision

To address the limitation of not being able to extend the list of available events in the flow builder, we will introduce a plugin system that allows developers to define and register custom events. This approach will provide flexibility and extensibility, enabling apps to add new event types as needed.

### Implementation Details

1. **Define a Custom Event Interface:**
   Create an interface for custom events that all custom event types must implement. This ensures a consistent structure and behavior for all events.

   ```typescript
   interface CustomEvent {
     type: string;
     payload: any;
     // Additional methods or properties can be added as needed
   }
   ```

2. **Create a Plugin System:**
   Develop a plugin system that allows developers to register their custom events. This system should include a registration mechanism and a way to integrate these events into the flow builder.

   ```typescript
   class EventPluginManager {
     private customEvents: Map<string, CustomEvent> = new Map();

     registerEvent(event: CustomEvent) {
       this.customEvents.set(event.type, event);
     }

     getEventTypes() {
       return Array.from(this.customEvents.keys());
     }

     getEventByType(type: string): CustomEvent | undefined {
       return this.customEvents.get(type);
     }
   }
   ```

3. **Integrate Custom Events into the Flow Builder:**
   Modify the flow builder to recognize and handle custom events. This involves updating the UI to display custom events and the backend logic to process them.

   ```typescript
   class FlowBuilder {
     private eventPluginManager: EventPluginManager;

     constructor(eventPluginManager: EventPluginManager) {
       this.eventPluginManager = eventPluginManager;
     }

     buildFlow() {
       const availableEvents = this.eventPluginManager.getEventTypes();
       // Use availableEvents to populate the flow builder UI and backend logic
     }
   }
   ```

4. **Documentation and Examples:**
   Provide clear documentation and examples on how to create and register custom events. This will help developers understand and use the new plugin system effectively.

   ```markdown
   ## Creating and Registering Custom Events

   ### Step 1: Define a Custom Event
   ```typescript
   class MyCustomEvent implements CustomEvent {
     type: 'my_custom_event';
     payload: { key: string, value: any };

     constructor(key: string, value: any) {
       this.payload = { key, value };
     }
   }
   ```

   ### Step 2: Register the Custom Event
   ```typescript
   const eventPluginManager = new EventPluginManager();
   const myCustomEvent = new MyCustomEvent('exampleKey', 'exampleValue');
   eventPluginManager.registerEvent(myCustomEvent);
   ```

   ### Step 3: Use the Custom Event in the Flow Builder
   ```typescript
   const flowBuilder = new FlowBuilder(eventPluginManager);
   flowBuilder.buildFlow();
   ```
   ```

### Benefits

- **Flexibility:** Developers can now extend the list of available events to meet their specific needs.
- **Extensibility:** The plugin system allows for easy addition of new event types without modifying the core flow builder code.
- **Consistency:** The custom event interface ensures that all custom events follow a consistent structure and behavior.

This decision will make the flow builder more versatile and adaptable, enhancing its utility for a wide range of applications.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision 
## Decision

To enable secure communication between apps and the App Server, we will introduce a new authentication mechanism that allows the App Server to verify the origin and authenticity of requests from apps. This will involve the following steps:

1. **App Registration and Secret Generation:**
   - When an app is registered with the Shopware 6 backend, a unique secret key will be generated and stored securely.
   - This secret key will be used to sign requests from the app to the App Server.

2. **Request Signing:**
   - The app will sign its requests using the secret key and include the signature in the request headers.
   - The signature will be generated using a secure hashing algorithm (e.g., HMAC-SHA256) and will include the request payload and a timestamp to prevent replay attacks.

3. **Signature Verification:**
   - The App Server will verify the signature of incoming requests using the stored secret key.
   - If the signature is valid, the App Server will process the request; otherwise, it will reject the request.

4. **Context Information:**
   - The app will include the necessary context information (e.g., customer ID, sales channel context) in the request payload.
   - The App Server will use this context information to perform the requested operations securely.

5. **Token-Based Authentication (Optional):**
   - For additional security, the app can also use token-based authentication (e.g., JWT) to authenticate the user and include the token in the request headers.
   - The App Server will validate the token and use the claims to determine the user's identity and permissions.

By implementing this mechanism, we ensure that only authorized and authenticated apps can communicate with the App Server, and the App Server can reliably verify the origin and context of the requests. This will enhance the security and reliability of the app ecosystem in Shopware 6.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision 
## Decision

We will implement a Flow Builder Preview feature that allows users to visualize and understand the execution flow of their custom-built flows without actually executing or simulating the real flow. This preview will provide a step-by-step breakdown of the flow, including the conditions and actions, to help users better understand how their flow is structured and how it will behave.

### Key Points of the Flow Builder Preview:
1. **Visualization**: The preview will display the flow in a visual format, showing the sequence of steps, conditions, and actions.
2. **Step-by-Step Breakdown**: Users will be able to see each step in the flow, including the conditions that trigger different paths and the actions that are executed.
3. **No Execution**: The preview will not execute or simulate the actual flow. It will only show the structure and logic of the flow.
4. **User-Friendly Interface**: The preview will be integrated into the Flow Builder interface, making it easy for users to access and use.
5. **Documentation and Tooltips**: The preview will include tooltips and documentation to explain each step and condition, helping users understand the purpose and behavior of each part of the flow.

### Benefits:
- **Improved Understanding**: Users will have a clearer understanding of how their flow is structured and how it will behave, reducing the likelihood of errors and unintended behavior.
- **Easier Debugging**: By visualizing the flow, users can more easily identify potential issues and make necessary adjustments before the flow is executed.
- **Time Savings**: Users will spend less time trying to figure out what went wrong and more time building and refining their flows.
- **Empowerment**: The preview feature will empower users to build more reliable and effective flows, enhancing their overall experience with the Flow Builder.

### Implementation:
- **Integration with Flow Builder**: The preview feature will be seamlessly integrated into the existing Flow Builder interface.
- **Backend Support**: The backend will need to support the generation of the preview data based on the flow configuration.
- **Frontend Visualization**: The frontend will render the preview in a user-friendly and visually appealing manner.

### Next Steps:
- **Design and Planning**: Detailed design and planning for the preview feature, including UI/UX design and backend requirements.
- **Development**: Implementation of the preview feature, including both frontend and backend components.
- **Testing**: Thorough testing to ensure the preview accurately reflects the flow and is easy to use.
- **Documentation**: Updating the documentation to include information on how to use the preview feature effectively.

By implementing the Flow Builder Preview, we aim to significantly improve the user experience and reliability of custom-built flows, making it easier for merchants to create and maintain their workflows.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much real code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision

The current approach of generating URLs in an event subscriber triggered by the `media.loaded` event is functional, but it tightly couples the URL generation logic to a specific event. To improve flexibility and maintainability, we will refactor the URL generation process as follows:

1. **Decouple URL Generation from Event Subscriber:**
   - Move the URL generation logic out of the event subscriber and into a dedicated service that implements the `UrlGeneratorInterface`.
   - This service will be responsible for generating both absolute and relative URLs for media and thumbnails.

2. **Introduce a URL Generator Service:**
   - Create a new service, e.g., `MediaUrlGenerator`, that implements `UrlGeneratorInterface`.
   - This service will use the `PathnameStrategyInterface` to generate the necessary parts of the URL (path hash, cache buster, and filename).

3. **Update the Event Subscriber:**
   - Modify the existing event subscriber to use the `MediaUrlGenerator` service.
   - The event subscriber will now simply call the appropriate methods on the `MediaUrlGenerator` to generate the URLs and set them on the `MediaEntity`.

4. **Ensure Backward Compatibility:**
   - Ensure that the new `MediaUrlGenerator` service can be used with the existing `PathnameStrategyInterface` implementations without breaking changes.
   - Provide a default implementation of `UrlGeneratorInterface` if no custom strategy is provided.

### Example Implementation

```php
// MediaUrlGenerator.php
class MediaUrlGenerator implements UrlGeneratorInterface
{
    private PathnameStrategyInterface $pathnameStrategy;

    public function __construct(PathnameStrategyInterface $pathnameStrategy)
    {
        $this->pathnameStrategy = $pathnameStrategy;
    }

    public function getAbsoluteMediaUrl(MediaEntity $media): string
    {
        // Generate the absolute URL using the pathname strategy
        $pathHash = $this->pathnameStrategy->generatePathHash($media);
        $cacheBuster = $this->pathnameStrategy->generatePathCacheBuster($media);
        $filename = $this->pathnameStrategy->generatePhysicalFilename($media);

        return $this->buildAbsoluteUrl($pathHash, $cacheBuster, $filename);
    }

    public function getRelativeMediaUrl(MediaEntity $media): string
    {
        // Generate the relative URL using the pathname strategy
        $pathHash = $this->pathnameStrategy->generatePathHash($media);
        $cacheBuster = $this->pathnameStrategy->generatePathCacheBuster($media);
        $filename = $this->pathnameStrategy->generatePhysicalFilename($media);

        return $this->buildRelativeUrl($pathHash, $cacheBuster, $filename);
    }

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string
    {
        // Generate the absolute thumbnail URL using the pathname strategy
        $pathHash = $this->pathnameStrategy->generatePathHash($media, $thumbnail);
        $cacheBuster = $this->pathnameStrategy->generatePathCacheBuster($media, $thumbnail);
        $filename = $this->pathnameStrategy->generatePhysicalFilename($media, $thumbnail);

        return $this->buildAbsoluteUrl($pathHash, $cacheBuster, $filename);
    }

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string
    {
        // Generate the relative thumbnail URL using the pathname strategy
        $pathHash = $this->pathnameStrategy->generatePathHash($media, $thumbnail);
        $cacheBuster = $this->pathnameStrategy->generatePathCacheBuster($media, $thumbnail);
        $filename = $this->pathnameStrategy->generatePhysicalFilename($media, $thumbnail);

        return $this->buildRelativeUrl($pathHash, $cacheBuster, $filename);
    }

    private function buildAbsoluteUrl(?string $pathHash, ?string $cacheBuster, string $filename): string
    {
        // Construct the absolute URL
        // Example: ""https://example.com/media/{$pathHash}/{$cacheBuster}/{$filename}""
    }

    private function buildRelativeUrl(?string $pathHash, ?string $cacheBuster, string $filename): string
    {
        // Construct the relative URL
        // Example: ""/media/{$pathHash}/{$cacheBuster}/{$filename}""
    }
}

// MediaLoadedEventSubscriber.php
class MediaLoadedEventSubscriber
{
    private MediaUrlGenerator $urlGenerator;

    public function __construct(MediaUrlGenerator $urlGenerator)
    {
        $this->urlGenerator = $urlGenerator;
    }

    public function onMediaLoaded(MediaLoadedEvent $event): void
    {
        $media = $event->getMedia();
        $media->setAbsoluteUrl($this->urlGenerator->getAbsoluteMediaUrl($media));
        $media->setRelativeUrl($this->urlGenerator->getRelativeMediaUrl($media));

        foreach ($media->getThumbnails() as $thumbnail) {
            $thumbnail->setAbsoluteUrl($this->urlGenerator->getAbsoluteThumbnailUrl($media, $thumbnail));
            $thumbnail->setRelativeUrl($this->urlGenerator->getRelativeThumbnailUrl($media, $thumbnail));
        }
    }
}
```

### Advantages

- **Separation of Concerns:** The URL generation logic is now separated from the event handling, making the code more modular and easier to test.
- **Flexibility:** The `MediaUrlGenerator` service can be easily extended or replaced with different strategies without affecting the event subscriber.
- **Maintainability:** The code is cleaner and more maintainable, as the responsibilities are clearly defined.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision 
## Decision

We will introduce a new dedicated mechanism for one-time data migrations that ensures the migration logic is only executed once, rather than being tied to the indexer process. This will help prevent system errors and ensure that data migrations are handled more efficiently and safely.

### Implementation Details:

1. **Create a New Migration Class:**
   - We will create a new class specifically for one-time data migrations. This class will extend the existing migration base class but will include additional logic to ensure it is only run once.
   - The class will have a method to check if the migration has already been executed (e.g., by checking a flag in the database or a specific table).

2. **Database Flag:**
   - Add a new table or use an existing one to store a flag indicating whether the one-time migration has been completed.
   - For example, a `migrations` table with a `migration_name` and `executed_at` timestamp.

3. **Migration Logic:**
   - The one-time migration logic will be encapsulated in a method within the new migration class.
   - Before executing the migration logic, the class will check the database flag to determine if the migration has already been run.
   - If the migration has not been run, the logic will execute, and the flag will be set to indicate completion.

4. **Triggering the Migration:**
   - The one-time migration will be triggered during the normal update process, similar to how other migrations are triggered.
   - This can be done by adding a new entry to the migration list or by modifying the existing migration runner to include the one-time migration.

5. **Example Use Case:**
   - For the media path information migration, the one-time migration class will handle the logic to update the paths and set the flag to indicate completion.
   - Future executions of the indexer will not re-run this migration, ensuring that the data remains consistent and preventing potential system errors.

### Example Code Snippet:

```php
class OneTimeDataMigration extends AbstractMigration
{
    public function up(Schema $schema): void
    {
        // Check if the migration has already been executed
        if ($this->hasBeenExecuted()) {
            return;
        }

        // Execute the one-time migration logic
        $this->migrateMediaPaths();

        // Set the flag to indicate the migration has been executed
        $this->setExecutedFlag();
    }

    private function hasBeenExecuted(): bool
    {
        // Check the database for the migration flag
        $query = $this->connection->createQueryBuilder()
            ->select('id')
            ->from('migrations')
            ->where('migration_name = :name')
            ->setParameter('name', 'media_path_migration');

        return (bool) $query->execute()->fetchOne();
    }

    private function migrateMediaPaths(): void
    {
        // Logic to migrate media paths
        // ...
    }

    private function setExecutedFlag(): void
    {
        // Insert a record into the migrations table
        $query = $this->connection->createQueryBuilder()
            ->insert('migrations')
            ->values([
                'migration_name' => ':name',
                'executed_at' => ':time',
            ])
            ->setParameter('name', 'media_path_migration')
            ->setParameter('time', new \DateTime());

        $query->execute();
    }
}
```

### Benefits:
- **Efficiency:** Ensures that the one-time migration logic is only executed once, improving performance and reducing the risk of system errors.
- **Maintainability:** Simplifies the codebase by separating one-time migrations from the indexer logic.
- **Reliability:** Prevents the accidental re-execution of critical one-time data migrations, ensuring data consistency and integrity.

This approach will provide a robust and maintainable solution for handling one-time data migrations in Shopware.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision 
## Decision

To address the challenges related to managing and tracking experimental features, we will introduce a more structured approach to organizing and tagging experimental code. This will help in maintaining, updating, and deprecating experimental features more efficiently.

### 1. **Feature-Specific Directories and Namespaces**
- **Create Feature-Specific Directories:** For each experimental feature, create a dedicated directory within the codebase. This directory will contain all the code related to that specific feature.
- **Use Feature-Specific Namespaces:** Use namespaces that clearly indicate the feature they belong to. For example, if the feature is named `FeatureX`, the namespace could be `App\Experimental\FeatureX`.

### 2. **Consistent Annotation Usage**
- **Standardize `@experimental` Annotations:** Ensure that the `@experimental` annotation is consistently used across all relevant classes, methods, and properties. The annotation should include the `stableVersion` property and any other relevant metadata.
- **Centralized Configuration for `stableVersion`:** Create a centralized configuration file (e.g., `experimental_features.php`) where the `stableVersion` for each experimental feature can be defined. This file will be referenced by the `@experimental` annotations, making it easier to update the `stableVersion` for prolonged experiments.

### 3. **Tagging and Labeling**
- **Use Tags in Code Comments:** Add tags or labels in the comments of the code to indicate which experimental feature the code belongs to. For example, `// [FeatureX] - This code is part of the FeatureX experiment`.
- **Automated Tag Detection:** Implement a script or tool that can automatically detect and list all the code related to a specific experimental feature based on the tags and namespaces. This will help in quickly identifying and managing the codebase for a given feature.

### 4. **Deprecation Management**
- **Automated Deprecation Script:** Develop a script that can automatically mark the components associated with a deprecated experimental feature as deprecated. This script should scan the codebase for the `@experimental` annotations and update them accordingly.
- **Deprecation Documentation:** Maintain a documentation file (e.g., `deprecated_features.md`) that lists all deprecated experimental features and their corresponding deprecation dates. This will serve as a reference for developers and stakeholders.

### 5. **Code Review and Maintenance**
- **Code Review Process:** Include a step in the code review process to ensure that the `@experimental` annotations and tags are correctly applied and updated.
- **Regular Audits:** Conduct regular audits of the experimental code to ensure that it is properly organized, tagged, and maintained. This will help in identifying and addressing any issues early on.

### 6. **Tooling and Automation**
- **IDE Integration:** Integrate the new tagging and annotation system with the IDEs used by the development team. This will provide better support for detecting and managing experimental code.
- **CI/CD Pipeline Checks:** Add checks in the CI/CD pipeline to validate that the `@experimental` annotations and tags are correctly applied and that the `stableVersion` is up-to-date.

By implementing these changes, we can improve the management and tracking of experimental features, making it easier to update, deprecate, and maintain the codebase.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision 
## Decision

To address the challenge of uniquely identifying payment and shipping methods, we will introduce a standardized and unique identifier for each method. This identifier will be:

1. **Unique and Immutable**: Each payment and shipping method will have a unique, immutable identifier (e.g., a UUID or a specific string) that is assigned during the creation of the method and remains constant throughout its lifecycle.

2. **Accessible via API**: The unique identifier will be included in the API responses for payment and shipping methods, making it easily accessible to extension developers and app servers. This will eliminate the need for additional calls to the Shopware Admin API to identify methods based on their IDs.

3. **Documentation and Best Practices**: We will provide clear documentation and best practices for using these unique identifiers, ensuring that extension developers and app servers can seamlessly integrate and use them.

4. **Backward Compatibility**: To maintain backward compatibility, the existing ID-based identification will still be supported, but the new unique identifier will be the recommended and preferred method for future development.

By implementing this solution, we aim to simplify the process of identifying payment and shipping methods, reduce the number of API calls, and improve the overall developer experience.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision 
## Decision

* Adopt the principle of using Bootstrap utility classes for simpler styling needs to reduce custom SCSS.
* For elements that require default spacing, margins, padding, or other simple styles, use Bootstrap utility classes in the HTML instead of defining custom SCSS rules.
  * This approach leverages the existing Bootstrap utilities and reduces the amount of custom SCSS, making the codebase cleaner and more maintainable.
* Continue to add semantic CSS classes to HTML elements for better semantics and to provide hooks for theme developers to add individual styling.
  * These classes will not have any default CSS applied, but they will be available for theme developers to use as needed.
* For more complex layouts or styles that cannot be achieved with Bootstrap utilities alone, continue to use custom SCSS.
  * Ensure that custom SCSS is well-organized and follows best practices to avoid bloating the stylesheet.

Example:

```diff
- /* app/storefront/src/scss/custom.scss */
- .register-login-collapse-toogle {
-     margin-bottom: $spacer;
- }

+ <!-- app/storefront/src/templates/register-login.html.twig -->
+ <div class=""register-login-collapse-toogle mb-3""></div>
```

For more complex scenarios, continue to use custom SCSS, but ensure it is necessary and well-documented.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision

We will refactor and enhance the current E2E test suite to meet the new requirements. The following steps will be taken:

1. **Decoupling from Test Environment State:**
   - Implement a setup and teardown mechanism to ensure that each test starts with a clean, known state.
   - Use environment variables to configure the test environment dynamically, allowing tests to run against any environment (local, staging, cloud).

2. **Test Strategy and Consistency:**
   - Define a clear test strategy and guidelines for writing tests. This includes:
     - Page Object Model (POM) to encapsulate UI interactions and improve maintainability.
     - Data-driven testing to handle different input scenarios.
     - Use of fixtures and factories to manage test data.
   - Enforce these guidelines through code reviews and automated checks.

3. **Deterministic and Reliable Tests:**
   - Use explicit waits and assertions to make tests more deterministic.
   - Implement retry mechanisms for flaky tests to reduce false negatives.
   - Regularly review and update tests to address any flakiness or performance issues.

4. **Behavior-Driven Development (BDD):**
   - Adopt BDD practices to derive tests from real product requirements.
   - Use a BDD framework like Cucumber with Cypress to write tests in a readable, human-friendly syntax.
   - Ensure that tests are written in a way that non-technical stakeholders can understand and validate.

5. **Testing Against Cloud Environment:**
   - Configure the test suite to support running tests against the cloud environment.
   - Implement necessary authentication and authorization mechanisms to access the cloud environment.
   - Use environment-specific configurations to handle differences between local, staging, and cloud environments.

6. **Performance and Speed:**
   - Optimize test execution by parallelizing tests where possible.
   - Use headless browsers for faster test runs.
   - Implement caching and other performance improvements to reduce test execution time.

7. **Training and Documentation:**
   - Provide training sessions and documentation to help the team adopt the new test framework and strategies.
   - Create a comprehensive guide on how to write, maintain, and run tests.
   - Establish a community of practice to share best practices and continuously improve the test suite.

By implementing these changes, we aim to create a robust, reliable, and maintainable E2E test suite that supports our goals of reducing manual testing and improving the overall quality of our product.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision 
## Decision

To address the issues with transaction management and error handling in flow actions, we will implement the following changes:

1. **Transaction Management in Flow Actions:**
   - **Centralized Transaction Management:** Introduce a centralized mechanism for managing transactions within flow actions. This can be achieved by using a `TransactionManager` service that handles the start, commit, and rollback of transactions.
   - **Transaction Scopes:** Use transaction scopes to ensure that each flow action runs within its own transaction. If a flow action needs to interact with the database, it should use the `TransactionManager` to start a new transaction or join an existing one.
   - **Savepoints:** Configure the database connection to use savepoints. This will allow nested transactions to be managed more effectively, ensuring that if an inner transaction fails, the outer transaction can still be committed or rolled back as needed.

2. **Error Handling and Logging:**
   - **Detailed Error Logging:** Improve the error logging mechanism to provide more detailed and actionable error messages. When a transaction commit fails, log the specific error message and stack trace to help diagnose the issue.
   - **Propagate Errors:** Instead of catching and ignoring errors, propagate them to the flow dispatcher. The dispatcher should then handle the error appropriately, such as by rolling back the transaction and stopping further execution of the flow.
   - **Rollback on Failure:** Ensure that if a transaction commit fails, the transaction is rolled back. This can be done by setting the connection to rollback-only mode and propagating the error to the calling code.

3. **Flow Dispatcher Enhancements:**
   - **Transaction Awareness:** Make the flow dispatcher aware of the transactional state. If a flow action fails to commit a transaction, the dispatcher should roll back the entire flow and log a detailed error message.
   - **Retry Mechanism:** Consider implementing a retry mechanism for transient errors. If a transaction commit fails due to a transient issue (e.g., a temporary network problem), the flow dispatcher can retry the operation after a short delay.

By implementing these changes, we can ensure that flow actions interact with the database in a transactional manner, handle errors more robustly, and provide better visibility into the root causes of failures.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision 
## Decision

We will keep the Vue 3 compatibility mode enabled in the administration for the current major release. This decision is based on the following reasons:

1. **Ease of Migration for Plugins**: Enabling the compatibility mode allows plugin developers to continue using Vue 2 features without immediate changes, reducing the burden on them during this transition period. This ensures a smoother upgrade process for the entire ecosystem.

2. **Reduced Breaking Changes**: By keeping the compatibility mode, we minimize the number of breaking changes that plugins and the administration need to handle at once. This helps in maintaining stability and reduces the risk of introducing bugs or issues during the migration.

3. **Phased Migration**: The compatibility mode allows us to split the migration work into two separate major releases:
   - **First Major Release**: Focus on updating the core administration to Vue 3 while keeping the compatibility mode enabled.
   - **Second Major Release**: Focus on deprecating and removing the compatibility mode, allowing plugins and the administration to fully adopt Vue 3 features and best practices.

4. **Community and Ecosystem Readiness**: Keeping the compatibility mode gives the community and ecosystem more time to prepare for the full migration to Vue 3. This includes updating third-party libraries, tools, and documentation.

### Next Steps
- **Documentation**: Update the documentation to clearly state that the compatibility mode is enabled and provide guidance on how to start preparing for the full Vue 3 migration.
- **Deprecation Plan**: Develop a clear deprecation plan for the compatibility mode, including timelines and steps for plugin developers to follow.
- **Testing and Validation**: Ensure thorough testing and validation of the administration and plugins in the compatibility mode to identify and address any potential issues early.

By following this approach, we can ensure a more manageable and less disruptive migration process for both the administration and the broader plugin ecosystem.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision 
## Decision
We will upgrade to Vuex `4.1.1` or the next stable version that includes a fix for the reactivity issue. This decision is based on the following considerations:

- **Reactivity Bug Fix**: The reactivity bug in `4.1.0` is a critical issue that can lead to unexpected behavior and bugs in our application. Upgrading to a version that fixes this issue is essential to maintain the reliability and correctness of our state management.
- **Avoiding Known Bugs**: Downgrading to `4.0.2` is not an option because it introduces other known bugs that affect module functionality. These bugs could cause more significant issues and require additional effort to work around.
- **Stability and Maintenance**: By upgrading to the latest stable version, we ensure that we are using a well-maintained and supported version of Vuex. This reduces the risk of encountering other known issues and ensures that we have access to the latest features and improvements.
- **Community and Support**: Using the latest stable version also means that we can benefit from community support and any future patches or updates that address other potential issues.

In summary, upgrading to the next stable version of Vuex that includes the reactivity fix is the most prudent and sustainable approach to resolve the current issues while maintaining the overall stability and quality of our application.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision 
## Decision

Given the challenges with the current use of PHP decoration, Adapter, and Factory patterns, we need to adopt a more flexible and maintainable approach. We will transition to using a **Dependency Injection (DI) container** combined with **Event-Driven Architecture (EDA)**. This approach will help address the issues of backward and forward compatibility, process extension limitations, and code proliferation.

### Key Changes:

1. **Dependency Injection (DI) Container:**
    - Introduce a DI container to manage the creation and lifecycle of objects. This will allow for more flexible and dynamic object creation, reducing the need for complex factory patterns.
    - Use the DI container to inject dependencies into classes, making it easier to swap out or extend functionality without modifying the core codebase.

2. **Event-Driven Architecture (EDA):**
    - Implement an event system where key actions in the application trigger events. Third-party developers can then listen to these events and add their own custom logic.
    - This approach allows for extending and customizing the behavior of the application without modifying the core code or creating a large number of interfaces and abstract classes.

3. **Simplification of Interfaces and Abstract Classes:**
    - Reduce the number of interfaces and abstract classes by leveraging the DI container and event system. Only create interfaces and abstract classes when they are truly necessary for abstraction and polymorphism.
    - Focus on creating well-defined, cohesive, and minimal interfaces that are easy to understand and implement.

4. **Documentation and Best Practices:**
    - Provide clear documentation and best practices for using the DI container and event system. This will help third-party developers understand how to extend and customize the application effectively.
    - Include examples and guidelines for common use cases to ensure consistency and ease of use.

### Benefits:

- **Improved Maintainability:**
    - The DI container and event system will make the codebase more modular and easier to maintain. Changes and updates will be less likely to break existing extensions.
- **Enhanced Extensibility:**
    - Third-party developers can easily extend and customize the application by listening to events and injecting their own services, without the need for complex inheritance and interface implementations.
- **Reduced Code Proliferation:**
    - By minimizing the number of interfaces and abstract classes, the codebase will be more streamlined and easier to navigate. This will reduce the cognitive load on developers and make the codebase more approachable.

### Implementation Plan:

1. **Phase 1: Introduction of DI Container:**
    - Introduce and configure a DI container (e.g., Symfony's DependencyInjection component).
    - Refactor existing classes to use the DI container for dependency injection.

2. **Phase 2: Event System Implementation:**
    - Design and implement the event system, including event dispatchers and listeners.
    - Refactor key parts of the application to trigger and handle events.

3. **Phase 3: Simplification of Interfaces and Abstract Classes:**
    - Review and refactor existing interfaces and abstract classes to minimize their number.
    - Ensure that the remaining interfaces and abstract classes are well-defined and necessary.

4. **Phase 4: Documentation and Best Practices:**
    - Create comprehensive documentation for the new architecture.
    - Provide examples and guidelines for third-party developers.

By adopting this new architecture, we aim to create a more flexible, maintainable, and extensible codebase that better supports the needs of both the core development team and third-party developers.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision 
## Decision

To ensure that our component tests run without the compat mode, we will introduce a new Jest configuration with the compat mode disabled. This new configuration will be used specifically for running tests that should not rely on the compat mode.

### Steps to Implement:
1. **Create a New Jest Configuration:**
   - Create a new Jest configuration file (e.g., `jest.config.no-compat.js`) that disables the compat mode.
   - Ensure this configuration extends the main Jest configuration but overrides the settings related to the compat mode.

2. **Update Test Scripts:**
   - Update the test scripts in `package.json` to include a new script that runs the tests using the new configuration.
   - For example, add a script like `test:no-compat` that runs `jest --config=jest.config.no-compat.js`.

3. **Migrate Tests Gradually:**
   - Start migrating individual component tests to use the new configuration.
   - Ensure that each test is thoroughly validated to confirm it works correctly without the compat mode.

4. **Documentation and Communication:**
   - Document the new configuration and the process for migrating tests.
   - Communicate the changes to the development team to ensure everyone is aware of the new setup and the steps required to migrate their tests.

By following these steps, we can gradually transition our tests to run without the compat mode, ensuring that our components are tested in a more modern and consistent environment.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision 
## Decision

To address the issue of undetected errors in system functionalities after a rollout, we will implement a comprehensive health check mechanism. This will include the following steps:

1. **Component-Level Health Checks:**
   - Each component of the system will have its own health check endpoint or method.
   - These health checks will be designed to verify the critical functionalities and dependencies of the component.
   - The health check should return a status (e.g., `OK`, `WARNING`, `CRITICAL`) and, if applicable, additional details about the health state.

2. **Aggregated System Health Check:**
   - A central health check service will aggregate the health statuses from all components.
   - The aggregated health check will provide an overall system health status.
   - The aggregation logic will consider the criticality of each component and determine the overall system health based on the worst-case status among the components.

3. **Health Check Monitoring and Alerts:**
   - The health check results will be monitored continuously.
   - Alerts will be set up to notify the operations team if the health status of any component or the overall system degrades to a `WARNING` or `CRITICAL` state.
   - The alerts will include detailed information about the affected components and the nature of the issue.

4. **Automated Rollback Mechanism:**
   - In case of a `CRITICAL` health status, an automated rollback mechanism will be triggered to revert the system to the last known healthy state.
   - This rollback will be configured to occur only if the health check failure is persistent and not a transient issue.

5. **Documentation and Testing:**
   - Comprehensive documentation will be provided for the health check mechanisms, including how to interpret the health statuses and what actions to take in different scenarios.
   - The health check mechanisms will be thoroughly tested in a staging environment before being deployed to production.

6. **Continuous Improvement:**
   - Regular reviews and updates of the health check mechanisms will be conducted to ensure they remain effective and cover new components and functionalities as the system evolves.
   - Feedback from the operations team and post-mortem analyses of incidents will be used to improve the health check mechanisms.

By implementing these measures, we aim to detect and address issues early, minimize downtime, and ensure that the system remains in a healthy state even if some components experience problems.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision
## Decision

To ensure that the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` is more flexible and can be used across different test suites, we will extend its functionality to allow configuration of additional namespaces. This will enable developers to specify which namespaces should be considered when enabling the major feature in their tests.

### Implementation Steps:
1. **Configuration Option**: Add a configuration option in the `phpunit.xml.dist` file (or another relevant configuration file) to specify additional namespaces that should be considered by the `TestPreparationStartedSubscriber`.

2. **Update Subscriber Logic**: Modify the `TestPreparationStartedSubscriber` to read this configuration and include the specified namespaces in addition to the `Shopware\Tests\Unit` namespace.

3. **Documentation**: Update the documentation to inform developers about the new configuration option and how to use it.

4. **Tests**: Add or update tests to ensure that the `TestPreparationStartedSubscriber` correctly handles the new configuration and includes the specified namespaces.

### Example Configuration:
```xml
<phpunit>
    <extensions>
        <extension class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Extension"">
            <namespaces>
                <namespace>Shopware\Tests\Unit</namespace>
                <namespace>MyNamespace\Tests</namespace>
                <!-- Additional namespaces can be added here -->
            </namespaces>
        </extension>
    </extensions>
</phpunit>
```

### Code Changes:
1. **Configuration Reader**:
   - Add a method to read the configuration from the `phpunit.xml.dist` file.
   - Store the list of namespaces in a property within the `TestPreparationStartedSubscriber`.

2. **Namespace Check**:
   - Modify the logic in the `TestPreparationStartedSubscriber` to check if the current test class belongs to any of the configured namespaces.

### Benefits:
- **Flexibility**: Developers can now use the `TestPreparationStartedSubscriber` with their own test namespaces, making it more versatile.
- **Maintainability**: The configuration is centralized and easy to manage, reducing the need for code changes when adding new namespaces.
- **Consistency**: Ensures consistent behavior across different test suites and namespaces.

By implementing these changes, we will make the `TestPreparationStartedSubscriber` more robust and adaptable to the needs of different test scenarios.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision 
## Decision

We will use the `vue-codemod` tool to automate the migration of our Vue 2 Options API components to the Vue 3 Composition API. This tool provides a set of codemods that can help us transform our existing codebase with minimal manual intervention.

### Reasons for Using `vue-codemod`

1. **Efficiency**: Manually converting a large number of components would be time-consuming and prone to errors. `vue-codemod` can handle the bulk of the transformation, allowing us to focus on more complex and specific cases.
   
2. **Consistency**: The tool ensures that the transformation is consistent across all components, reducing the risk of human error and maintaining a uniform coding style.

3. **Community Support**: `vue-codemod` is a well-maintained and widely used tool in the Vue community, which means it is regularly updated and has a robust set of features.

4. **Customization**: While `vue-codemod` provides a good starting point, we can also extend or modify the codemods to fit our specific needs, ensuring that the migration aligns with our project's requirements.

### Steps for Migration

1. **Setup `vue-codemod`**:
   - Install `vue-codemod` globally or as a dev dependency in our project.
   - Ensure that all team members have the tool installed and are using the same version.

2. **Run the Codemod**:
   - Execute the `vue-codemod` command to transform the components. For example:
     ```sh
     npx vue-codemod composition-api <path-to-components>
     ```
   - Review the changes made by the tool to ensure they meet our standards and do not introduce any issues.

3. **Manual Adjustments**:
   - After the initial transformation, manually review and adjust the converted components to ensure they are fully functional and adhere to our coding guidelines.
   - Pay special attention to complex logic, custom hooks, and any edge cases that the tool might not handle perfectly.

4. **Testing**:
   - Thoroughly test the migrated components to ensure they work as expected and do not introduce any new bugs.
   - Use unit tests and integration tests to validate the functionality of the components.

5. **Documentation**:
   - Update our documentation to reflect the changes and provide guidance for developers on how to write and maintain components using the Composition API.

### Trade-offs

- **Initial Setup Time**: There will be some initial setup time to install and configure `vue-codemod`.
- **Potential for Incomplete Conversions**: While `vue-codemod` is powerful, it may not handle every edge case. Some manual adjustments will still be necessary.
- **Learning Curve**: Developers will need to familiarize themselves with the Composition API if they are not already comfortable with it.

### Conclusion

By using `vue-codemod`, we can efficiently and consistently migrate our Vue 2 Options API components to the Vue 3 Composition API. This approach will save significant development time and reduce the risk of errors, while also modernizing our codebase and improving its maintainability.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJss inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in Vues templating and reactivity model.","## Decision 
## Decision
We will introduce two new Vue.js components, `sw-block` and `sw-block-parent`, to replace the current TwigJs block system. This change will leverage Vue.js's dynamic and reactive component-based architecture, providing a more modern and flexible approach to content overriding and extension.

### Implementation Details

1. **`sw-block-parent` Component:**
   - This component will serve as the parent container for blocks.
   - It will manage the registration and rendering of child blocks.
   - It will provide a context for child blocks to access and override content.

2. **`sw-block` Component:**
   - This component will represent individual blocks that can be overridden or extended.
   - It will use Vue.js's slot mechanism to allow for dynamic content insertion.
   - It will support named slots to handle different sections within a block.

### Usage Example

#### Parent Component (e.g., `sw-page-layout.vue`)

```vue
<template>
  <div>
    <sw-block-parent name=""main-content"">
      <template #default>
        <p>This is the default content.</p>
      </template>
    </sw-block-parent>
  </div>
</template>

<script>
import SwBlockParent from 'path/to/sw-block-parent';

export default {
  components: {
    SwBlockParent,
  },
};
</script>
```

#### Child Component (e.g., `sw-custom-block.vue`)

```vue
<template>
  <sw-block name=""main-content"">
    <template #default>
      <p>This is the custom content that overrides the default content.</p>
    </template>
  </sw-block>
</template>

<script>
import SwBlock from 'path/to/sw-block';

export default {
  components: {
    SwBlock,
  },
};
</script>
```

### Benefits

- **Reactivity:** Vue.js's reactivity system ensures that any changes in the block content are automatically reflected in the UI.
- **Flexibility:** The use of slots allows for dynamic and flexible content overriding and extension.
- **Maintainability:** The component-based approach makes the code more modular and easier to maintain.
- **Performance:** Vue.js's optimized rendering and reactivity model can improve performance compared to the TwigJs approach.

### Migration Plan

- **Documentation:** Provide detailed documentation on how to migrate from the existing TwigJs block system to the new Vue.js components.
- **Deprecation:** Gradually deprecate the TwigJs block system while supporting both systems for a transition period.
- **Testing:** Ensure thorough testing of the new components to maintain the same level of functionality and stability as the current system.

By introducing these new components, we aim to provide a more robust and modern solution for content overriding and extension in the administration, leveraging the full power of Vue.js.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the components internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: ` 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision 
## Decision

We will remove the asterisk `*` from all product prices in the default Storefront and instead, we will clearly display the tax and shipping information in a more prominent and consistent manner. Here are the steps we will take:

1. **Remove Asterisk from Prices**: Remove the asterisk `*` from all product prices throughout the storefront.
2. **Add Clear Information in Footer**: Ensure that the tax and shipping costs information is clearly displayed in the footer of every page. The text will be: ""All prices include VAT. Shipping costs and possible delivery charges are additional, unless stated otherwise.""
3. **Display Tax Information on Product Pages**: On each product page, display a clear and concise message about the inclusion of VAT and any additional costs. For example: ""Price includes VAT. Additional shipping and delivery charges may apply.""
4. **Consistent Messaging**: Ensure that the messaging is consistent across all pages where prices are displayed, including category pages, cart, and checkout.
5. **User-Friendly Design**: Use a user-friendly design to highlight the tax and shipping information, such as a small, clearly labeled section or tooltip next to the price.

By making these changes, we aim to improve the clarity and user experience of the storefront, while still providing the necessary legal and financial information to customers.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision 
## Decision

To ensure that Shopware 6's document solution is A11y-compliant and meets the Web Content Accessibility Guidelines (WCAG), we will transition to a PDF generation library that supports accessibility features. Specifically, we will adopt [PDF/UA (Universal Accessibility) compliant libraries] such as [FPDF](https://www.fpdf.org/) or [TCPDF](https://tcpdf.org/), which are known for their support of accessibility standards.

### Implementation Steps:
1. **Evaluate and Select a New Library:**
   - Conduct a thorough evaluation of FPDF, TCPDF, and other potential libraries to determine which one best meets our requirements for generating accessible PDFs.
   - Consider factors such as community support, documentation, and ease of integration with Shopware 6.

2. **Integrate the New Library:**
   - Replace the existing DomPDF library with the selected PDF/UA compliant library in the document generation process.
   - Ensure that the new library is properly integrated into the Shopware 6 codebase and that all necessary dependencies are managed.

3. **Update Document Templates:**
   - Modify the document templates to include accessibility features such as proper tagging, alt text for images, and logical reading order.
   - Ensure that all generated PDFs are fully navigable using assistive technologies like screen readers.

4. **Testing and Validation:**
   - Perform extensive testing to ensure that the generated PDFs meet WCAG and PDF/UA standards.
   - Use tools like [PDF Accessibility Checker (PAC)](https://access-for-all.ch/en/pdf-lab/pdf-accessibility-checker-pac/) to validate the accessibility of the generated documents.
   - Conduct user testing with individuals who use assistive technologies to gather feedback and make any necessary adjustments.

5. **Documentation and Training:**
   - Update the developer and user documentation to reflect the changes and provide guidance on creating and maintaining accessible documents.
   - Provide training for developers and content creators on best practices for generating accessible PDFs.

### Benefits:
- **Compliance:** Ensures that Shopware 6's document solution complies with WCAG and legal requirements.
- **Inclusivity:** Makes documents accessible to customers with disabilities, enhancing the overall user experience.
- **Future-Proofing:** Adopting a modern, accessible PDF generation library prepares the system for future accessibility standards and requirements.

By making this change, we will not only meet legal and compliance requirements but also improve the overall accessibility and usability of our document solution.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision 
## Decision

To address the complexity and maintenance issues of the existing rule system, we will introduce a more structured and controlled approach to extending rule classes. This will involve the following steps:

1. **Define a Clear Extension Mechanism:**
   - Introduce a new interface or abstract class that all custom rule conditions must implement. This interface will define the necessary methods and properties required for a rule condition.
   - Ensure that this interface or abstract class enforces a consistent structure and behavior for all rule conditions.

2. **Use Dependency Injection:**
   - Utilize dependency injection to manage and inject rule conditions into the rule system. This will allow for better testability and modularity.
   - Tag custom rule conditions with a specific service tag (e.g., `shopware.rule.condition`) to make them discoverable by the rule system.

3. **Configuration and Metadata:**
   - Require custom rule conditions to provide metadata and configuration details. This metadata will include information such as the name, description, and parameters of the condition.
   - Use this metadata to dynamically generate the necessary forms and fields in the administration interface, reducing the need for hard-coded Vue components.

4. **Validation and Constraints:**
   - Implement a validation mechanism to ensure that custom rule conditions adhere to the defined interface and constraints.
   - Use Symfony's validation system to validate the metadata and configuration provided by custom rule conditions.

5. **Documentation and Best Practices:**
   - Provide clear documentation and best practices for creating and integrating custom rule conditions.
   - Include examples and guidelines to help developers understand how to extend the rule system effectively.

6. **Versioning and Backward Compatibility:**
   - Ensure that any changes to the rule system are versioned and backward-compatible to avoid breaking existing custom rule conditions.
   - Communicate any breaking changes well in advance and provide migration paths for existing custom rule conditions.

By implementing these changes, we aim to create a more maintainable and extensible rule system that can evolve without compromising the flexibility and power of the existing rules. This will also make it easier for developers to contribute and extend the rule system in a controlled and consistent manner.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision 
## Decision

We will deprecate and eventually remove the `src/helper/iterator.helper.js` in favor of using native JavaScript iteration methods. This decision is based on the following points:

* **Simplicity and Clarity**: Using native JavaScript iteration methods (e.g., `forEach`, `for...of`, `map`, `reduce`) will make the code more straightforward and easier to understand. Developers are already familiar with these methods, and they are well-documented in web standards.
* **Performance**: Native iteration methods are optimized by the browser and can be more performant than a custom abstraction.
* **Consistency**: By standardizing on native iteration methods, we can ensure that the codebase is consistent and avoid confusion between different iteration styles.
* **Reduced Dependencies**: Removing the iterator helper will eliminate the need for an additional import in every file, leading to cleaner and more maintainable code.
* **Future-Proofing**: Relying on web standards ensures that the code remains up-to-date and compatible with future JavaScript features and best practices.

### Implementation Steps
1. **Deprecation**: Mark the `Iterator.iterate()` method as deprecated and add a warning message in the console when it is used.
2. **Refactoring**: Gradually refactor the existing codebase to use native iteration methods.
3. **Documentation**: Update the documentation to reflect the new approach and provide examples of how to use native iteration methods.
4. **Removal**: After a transition period, remove the `iterator.helper.js` file and any references to it in the codebase.

By making this change, we aim to improve the readability, performance, and maintainability of the Storefront JavaScript code.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision 
## Decision

To address the issues with flow execution during the business process, we will introduce a new mechanism to execute flows after the business process has completed. This approach will help mitigate the risk of fatal errors disrupting the business process and improve overall performance by decoupling expensive operations from the critical path.

### Key Changes:

1. **Asynchronous Flow Execution:**
   - Flows will be executed asynchronously after the business process has successfully completed.
   - We will use a message queue (e.g., RabbitMQ, Kafka) to handle the asynchronous execution of flows. This will ensure that the business process is not blocked by flow execution and that flows can be retried in case of failures.

2. **Dedicated Event Listener:**
   - A dedicated event listener will be created to handle the scheduling of flow execution. This listener will be responsible for enqueuing flow execution tasks into the message queue.
   - The event listener will be triggered at the end of the business process, ensuring that all necessary data is available for the flows.

3. **Feature Flag:**
   - The new mechanism will be controlled by a feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS`. This will allow us to gradually roll out the changes and monitor their impact before fully enabling them.

4. **Error Handling and Retries:**
   - The message queue will handle retries and error handling for flow execution. If a flow fails, it can be retried a configurable number of times before being marked as failed.
   - Errors in flow execution will be logged and monitored, but they will not disrupt the business process.

5. **Performance Improvement:**
   - By executing flows asynchronously, we will reduce the latency of the business process, especially for expensive operations like sending emails.
   - The business process will complete faster, providing a better user experience.

6. **Simplified Debugging:**
   - With flows executed by a dedicated event listener and managed by a message queue, debugging will be simplified. Stack traces will be cleaner, and it will be easier to trace the flow of execution.
   - Logs and monitoring will be more focused, making it easier to identify and resolve issues.

### Implementation Steps:

1. **Introduce a Message Queue:**
   - Set up a message queue system (e.g., RabbitMQ, Kafka) to handle the asynchronous execution of flows.
   - Configure the message queue with appropriate settings for retries, dead-letter queues, and error handling.

2. **Create a Dedicated Event Listener:**
   - Develop an event listener that listens for the completion of the business process.
   - When the business process completes, the listener will enqueue the flow execution tasks into the message queue.

3. **Modify Flow Execution Logic:**
   - Refactor the flow execution logic to be triggered by messages from the message queue.
   - Ensure that flows are idempotent and can be safely retried.

4. **Enable Feature Flag:**
   - Add the feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` to control the new mechanism.
   - Gradually enable the feature flag and monitor the system to ensure that the new mechanism works as expected.

5. **Testing and Monitoring:**
   - Thoroughly test the new mechanism to ensure that it does not introduce any new issues.
   - Set up monitoring and logging to track the performance and reliability of the new flow execution mechanism.

By implementing these changes, we will improve the robustness, performance, and maintainability of the system, while also providing a better user experience.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision

Chosen option: ""MADR 3.0.0"", because

- **Explicit Documentation**: Implicit assumptions should be made explicit. Design documentation is important to enable people to understand the decisions later on. See also [""A rational design process: How and why to fake it""](https://doi.org/10.1109/TSE.1986.6312940).
- **Structured Format**: MADR provides a structured format for capturing architectural decisions, which ensures consistency and completeness.
- **Lean and Comprehensible**: The MADR format is lean and fits well with our development style. It is also comprehensible, making it easy to use and maintain.
- **Community Support**: The MADR project is active and has a supportive community, which means it is likely to continue evolving and improving.
- **Versioning**: Version 3.0.0 is the latest stable version at the time of this decision, ensuring we have the most up-to-date features and improvements.

### Structure of an ADR

An ADR in MADR 3.0.0 typically follows this structure:

1. **Title**: A short, descriptive title for the decision.
2. **Status**: The current status of the decision (e.g., proposed, accepted, rejected, deprecated).
3. **Context**: A description of the problem or situation that led to the need for a decision.
4. **Decision**: The chosen solution or course of action.
5. **Consequences**: The expected positive and negative outcomes of the decision.
6. **Tags**: Optional tags to categorize the ADR (e.g., `architecture`, `code`, `performance`).

### Example ADR

```markdown
# 001-record-architectural-decisions

## Status
Accepted

## Context
We want to record architectural decisions made in this project to ensure that all team members and future contributors can understand the reasoning behind the choices made.

## Decision
We will use the MADR 3.0.0 format for recording architectural decisions.

## Consequences
- **Positive**:
  - Consistent and structured documentation.
  - Easier to understand and maintain.
  - Active community support.
- **Negative**:
  - Initial learning curve for the team.
  - Potential for over-documentation if not managed properly.

## Tags
`architecture`, `documentation`
```

This structure ensures that all necessary information is captured and easily accessible, making it a robust choice for our project.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision 
## Decision

To address the issues with the legacy python-tuf implementation and to provide a more coherent, maintainable, and scalable solution, we will develop a new Python library for TUF repository management. This new library will be designed with the following principles in mind:

1. **Modularity and Separation of Concerns**:
   - The library will be modular, with clear separation between different components such as metadata handling, repository management, and I/O operations.
   - Each module will have a well-defined and limited scope, making it easier to maintain and extend.

2. **Simplified and Coherent API**:
   - The API will be designed to be intuitive and easy to use, with methods that have a reasonable number of arguments.
   - We will avoid the use of global variables and instead rely on object-oriented design principles to encapsulate state and behavior.
   - The library will provide high-level abstractions for common tasks, such as creating, updating, and validating a TUF repository.

3. **Performance and Scalability**:
   - The library will be optimized for performance, especially for large-scale repositories.
   - It will support lazy loading and incremental updates to metadata, avoiding the need to load all metadata files at once.
   - The library will be designed to handle both small and large repositories efficiently.

4. **Extensibility and Flexibility**:
   - The library will be extensible, allowing users to plug in custom behaviors or integrate with other systems.
   - It will support different storage backends (e.g., local file system, S3, etc.) and allow for easy configuration.

5. **Documentation and Examples**:
   - Comprehensive documentation will be provided, including examples and best practices for using the library.
   - The documentation will cover both the API and the overall architecture, helping users understand how to effectively use the library.

6. **Testing and Reliability**:
   - The library will be thoroughly tested, with a focus on unit tests, integration tests, and end-to-end tests.
   - We will ensure that the library is reliable and robust, with a strong emphasis on error handling and validation.

7. **Community and Ecosystem Integration**:
   - The library will be designed to integrate well with the broader TUF ecosystem and other related tools.
   - We will engage with the community to gather feedback and contributions, ensuring that the library meets the needs of its users.

By following these principles, the new library will provide a modern, maintainable, and efficient solution for managing TUF repositories, addressing the shortcomings of the legacy implementation and providing a better experience for developers and users.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision 
## Decision

* We will migrate the Primer React components to TypeScript by gradually converting the existing JavaScript codebase to TypeScript.
* The migration will be done incrementally, starting with the most critical and frequently used components.
* During the migration, we will remove the ambient declaration file (`index.d.ts`) and replace it with inline type definitions in the TypeScript files.
* To ensure a smooth transition, we will:
  * Add TypeScript to the project and configure it to work alongside the existing JavaScript code.
  * Use a linter and type checker to catch and fix type issues as we convert each component.
  * Update the build process to support both TypeScript and JavaScript files.
  * Ensure that the public API of the components remains consistent during the migration to avoid breaking changes for consumers.
* We will also create a detailed migration plan and timeline, and communicate this plan to the community to manage expectations and gather feedback.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision

We will standardize on using the `Box` component for creating new React components in Primer React and consuming applications. This decision is based on the following considerations:

1. **Consistency**: Using a single pattern for creating components will make the codebase more consistent and easier to understand for new and existing developers.
2. **Flexibility**: The `Box` component provides a high degree of flexibility, allowing for easy customization of styles and behavior through props. This aligns well with the design system's goal of providing reusable and composable components.
3. **Maintainability**: The `Box` component approach is more maintainable because it centralizes the styling logic in a single place, making it easier to update and manage over time.
4. **Performance**: The `Box` component can be optimized for performance by leveraging the `sx` prop and other built-in utilities, which can help reduce the number of re-renders and improve overall performance.
5. **Backward Compatibility**: Existing components that use `styled-components` can be gradually refactored to use the `Box` component, ensuring backward compatibility and a smooth transition.

### Implementation

1. **Refactor Existing Components**: Gradually refactor existing components that use `styled-components` to use the `Box` component. This can be done incrementally to avoid breaking changes.
2. **Update Documentation**: Update the documentation to reflect the new standard and provide examples and best practices for using the `Box` component.
3. **Create a Migration Guide**: Provide a migration guide for developers who need to convert their `styled-components` to `Box` components, including common patterns and potential pitfalls.

### Example

Here is an example of how a component might look after refactoring to use the `Box` component:

```tsx
import { Box } from '@primer/react';

interface AvatarProps {
  size?: number;
  alt?: string;
  square?: boolean;
  sx?: BetterSystemStyleObject;
}

const Avatar: React.FC<AvatarProps> = ({ size = 20, alt = '', square = false, sx = {}, ...props }) => {
  const styles: BetterSystemStyleObject = {
    display: 'inline-block',
    overflow: 'hidden',
    lineHeight: 'condensedUltra',
    borderRadius: getBorderRadius({ size, square }),
  };

  return (
    <Box
      as=""img""
      alt={alt}
      sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
      {...props}
    />
  );
};
```

This approach will help us maintain a more consistent and maintainable codebase while providing the flexibility and performance benefits of the `Box` component.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision 
## Decision

We will continue to use the `Drafts` directory for developing and testing new or updated components before they are officially released in the root bundle of `@primer/react`. This approach allows us to:

- Create, test, and maintain multiple versions of the same component in parallel.
- Clearly indicate that a component is still under development and not yet ready for production use.
- Encourage collaboration with developers who might want to use these components, ensuring that they are aware of the draft status and can provide feedback.

This method will help us manage the lifecycle of components more effectively and ensure that only stable and tested components are included in the main package.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision

We will adopt a more selective and intentional approach to snapshot testing to address the challenges of managing a growing number of snapshots.

### Key Points:
1. **Selective Snapshot Testing:**
   - Only use snapshot testing for components that have a high likelihood of unintended changes, such as utility components or complex UI elements.
   - Avoid snapshot testing for simple, stateless, or purely presentational components where the output is straightforward and unlikely to change unintentionally.

2. **Snapshot Naming and Documentation:**
   - Use descriptive names for snapshot tests to clearly indicate what the snapshot is testing. For example, `it('renders correctly with default props')` instead of `it('renders correctly')`.
   - Add comments in the test files to explain the purpose of each snapshot test, especially for complex components.

3. **Limit Snapshot Size:**
   - For components with wide or deep trees, consider using partial snapshots. This can be done by isolating specific parts of the component's output for snapshot testing.
   - Use the `jest-snapshot-serializer` to customize the snapshot format and reduce noise, making it easier to identify meaningful changes.

4. **Regular Review and Pruning:**
   - Regularly review and prune outdated or unnecessary snapshots. This can be part of a periodic code review process.
   - Consider automating the process of identifying and suggesting removal of unused or redundant snapshots.

5. **Interactive Snapshot Updates:**
   - Use the `--updateSnapshot` flag judiciously. When a snapshot fails, carefully review the changes and update the snapshot only if the changes are intentional and expected.
   - Encourage developers to run tests with `--updateSnapshot` in an interactive mode to manually review and accept changes.

6. **Integration with Visual Regression Testing:**
   - For critical visual components, consider integrating visual regression testing tools (e.g., Percy, Chromatic) alongside snapshot testing. This provides a more robust way to ensure visual consistency.

### Example Implementation:

```jsx
import renderer from 'react-test-renderer';
import Link from '../Link';

// Descriptive test name
it('renders correctly with default props', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON();
  expect(tree).toMatchSnapshot();
});

// Partial snapshot for a specific part of the component
it('renders the correct link text', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON();
  const linkText = tree.children[0];
  expect(linkText).toMatchSnapshot();
});
```

By following these guidelines, we can maintain a more manageable and effective set of snapshot tests, ensuring that they provide meaningful and actionable feedback during development.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision 
## Decision

To address the issues of `console.warn()` calls and other development-related code being included in production, we will use a combination of environment variables and build-time conditionals. Specifically, we will:

1. **Use Environment Variables**: We will leverage environment variables to distinguish between development and production environments. For example, we can use `process.env.NODE_ENV` to check if the application is running in a development or production environment.

2. **Conditional Warnings**: We will wrap our `console.warn()` calls and other development-specific code in conditional statements that only execute when the environment is set to development. For example:
   ```javascript
   if (process.env.NODE_ENV !== 'production') {
     console.warn('This feature is deprecated and will be removed in the next major version.');
   }
   ```

3. **Babel Plugin for Stripping Development Code**: To ensure that the development-specific code is completely removed from the production build, we will use a Babel plugin like `babel-plugin-transform-remove-console` or `babel-plugin-dev-expression`. These plugins can strip out `console` calls and other development-specific code during the build process.

4. **Documentation and Best Practices**: We will document these practices and include them in our coding guidelines to ensure that all developers are aware of and follow these conventions.

By implementing these steps, we can ensure that development warnings and other non-essential code do not make it into the production build, improving performance and reducing the risk of exposing internal messages to end-users.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision 
## Decision

To address the inconsistencies and ensure reliable live region announcements across different screen readers, we will adopt a centralized pattern for live region announcements in Primer React. This pattern will provide a consistent and predictable way to manage live regions, ensuring that all components can make accessible announcements without relying on potentially unreliable techniques.

### Centralized Live Region Pattern

1. **Create a Central Live Region Component:**
   - Develop a reusable `LiveRegion` component that encapsulates the logic for managing live region announcements.
   - This component will handle the creation and management of live regions, ensuring that they are properly announced by screen readers.

2. **Define Live Region Types:**
   - The `LiveRegion` component will support different types of live regions, such as `alert`, `log`, and `status`.
   - Each type will have its own specific behavior and attributes (e.g., `aria-live=""assertive""` for alerts, `aria-live=""polite""` for status updates).

3. **API for Live Region Announcements:**
   - Provide a simple API for components to use the `LiveRegion` component.
   - The API will allow components to programmatically announce messages, update content, and control the live region's behavior.

4. **Consistent Behavior Across Screen Readers:**
   - Ensure that the `LiveRegion` component is tested across multiple screen readers (e.g., JAWS, NVDA, VoiceOver) to ensure consistent behavior.
   - Implement fallbacks and workarounds for known issues with dynamic content and visibility changes.

5. **Documentation and Best Practices:**
   - Document the usage of the `LiveRegion` component and provide best practices for integrating it into Primer React components.
   - Include examples and guidelines for common scenarios, such as announcing form submissions, spinner displays, and dynamic content updates.

### Implementation Details

1. **LiveRegion Component:**

   ```jsx
   import React, { useState } from 'react';

   const LiveRegion = ({ type, children, atomic = true }) => {
     const liveRegionId = `live-region-${Math.random().toString(36).substr(2, 9)}`;
     const [content, setContent] = useState(children);

     return (
       <div
         id={liveRegionId}
         role={type}
         aria-live={type === 'alert' ? 'assertive' : 'polite'}
         aria-atomic={atomic}
         style={{ position: 'absolute', clip: 'rect(0 0 0 0)', width: 1, height: 1, margin: -1, padding: 0, border: 0, overflow: 'hidden' }}
       >
         {content}
       </div>
     );
   };

   export default LiveRegion;
   ```

2. **Using the LiveRegion Component:**

   ```jsx
   import React, { useState } from 'react';
   import LiveRegion from './LiveRegion';

   const MyComponent = () => {
     const [count, setCount] = useState(0);
     const [loading, setLoading] = useState(false);

     const incrementCount = () => {
       setCount(count + 1);
       LiveRegion.announce(`Count incremented to ${count + 1}`, 'status');
     };

     const startLoading = () => {
       setLoading(true);
       LiveRegion.announce('Loading started', 'alert');
     };

     return (
       <div>
         <button onClick={incrementCount}>Increment Count</button>
         <button onClick={startLoading}>Start Loading</button>
         <LiveRegion type=""status"" children={`Current count: ${count}`} />
         <LiveRegion type=""alert"" children={loading ? 'Loading...' : ''} />
       </div>
     );
   };

   export default MyComponent;
   ```

### Benefits

- **Consistency:** Ensures that live region announcements are consistent and reliable across different screen readers.
- **Reusability:** Provides a reusable component that can be easily integrated into any Primer React component.
- **Simplicity:** Simplifies the process of making live region announcements, reducing the likelihood of errors and inconsistencies.
- **Accessibility:** Improves the overall accessibility of Primer React components by providing a robust and tested solution for live region announcements.

By adopting this centralized pattern, we can ensure that live region announcements in Primer React are both effective and accessible, providing a better user experience for all users, including those who rely on screen readers.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"It seems like the context is missing in your request. Could you please provide more details about the situation or problem that needs a decision? For example, you might want to describe the current state, any issues with it, and what you're trying to achieve.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision 
## Decision

To address the issues with repeated markup and the lack of visual debugging in Jest tests, we will implement the following changes:

1. **Consolidate Test Scenarios Using Storybook:**
   - Use Storybook as the primary source for creating and maintaining test scenarios, especially for complex interactive components.
   - Integrate Storybook with Jest to generate snapshots and run interaction tests. This can be achieved using tools like `@storybook/test-runner` or `@testing-library/react`.

2. **Automate Interaction Testing in Storybook:**
   - Utilize Storybook's built-in testing capabilities, such as `@storybook/addon-interactions`, to automate interaction testing directly within Storybook.
   - This will reduce the need to manually transfer test cases from Storybook to Jest, thereby minimizing duplicated effort and ensuring consistency.

3. **Visual Debugging and Visibility:**
   - Integrate a visual regression testing tool like Chromatic with Storybook to provide a visual representation of the test results.
   - Use tools like `jest-image-snapshot` to capture and compare visual snapshots in Jest tests, providing a visual reference for debugging.
   - Enhance the visibility of Jest tests by adding more descriptive and detailed logs, and consider using a tool like `jest-extended` to improve the readability of test output.

4. **Documentation and Best Practices:**
   - Create and maintain documentation on best practices for writing and maintaining tests in both Storybook and Jest.
   - Encourage the use of consistent naming conventions and structures for test cases to make them easier to understand and maintain.

By implementing these changes, we aim to streamline our testing process, reduce duplication, and improve the overall quality and maintainability of our tests.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision 
## Decision

To address the issues with dependency management and workspace connections, we will adopt `npm workspaces` for the `primer/react` repo. This will provide a more structured and efficient way to manage dependencies across multiple workspaces and simplify the development process.

### Key Points:
1. **Unified Dependency Management**:
   - Use `npm workspaces` to manage all dependencies from the top-level `package.json`.
   - This will allow us to install all dependencies in one go using `npm install` at the root level.
   - Dependencies can be shared and managed centrally, reducing redundancy and ensuring consistency.

2. **Workspace Connections**:
   - Define workspaces in the top-level `package.json` under the `workspaces` field.
   - Use `workspace:*` in `package.json` files of individual workspaces to reference other workspaces within the project.
   - This will replace the need for relative paths, file dependencies, or `npm link`.

3. **Script Management**:
   - Create a dedicated `scripts` workspace for complex and reusable scripts.
   - Scripts can have their own `package.json` to manage their dependencies independently.
   - Use `npm` scripts in the top-level `package.json` to invoke these scripts, providing a clear and consistent interface for contributors.

4. **Convenience Script**:
   - Update the `npm run setup` script to use `npm workspaces` for installing dependencies.
   - Ensure that the setup script is robust and handles all necessary configurations, including linking workspaces and installing dependencies.

### Implementation Steps:
1. **Update `package.json`**:
   - Add the `workspaces` field to the top-level `package.json` to define the workspaces.
   - Example:
     ```json
     {
       ""workspaces"": [
         ""packages/*"",
         ""docs"",
         ""examples/*"",
         ""scripts""
       ]
     }
     ```

2. **Refactor Workspace Dependencies**:
   - Update `package.json` files in each workspace to use `workspace:*` for inter-workspace dependencies.
   - Example:
     ```json
     {
       ""dependencies"": {
         ""@primer/react"": ""workspace:*""
       }
     }
     ```

3. **Create `scripts` Workspace**:
   - Move existing scripts into a new `scripts` directory.
   - Create a `package.json` in the `scripts` directory to manage script-specific dependencies.
   - Example:
     ```json
     {
       ""name"": ""@primer/scripts"",
       ""version"": ""1.0.0"",
       ""dependencies"": {
         ""some-script-dependency"": ""^1.0.0""
       }
     }
     ```

4. **Update Top-Level Scripts**:
   - Update the `npm run setup` script to use `npm install` at the root level.
   - Example:
     ```json
     {
       ""scripts"": {
         ""setup"": ""npm install""
       }
     }
     ```

5. **Documentation and Communication**:
   - Update the project documentation to reflect the new structure and workflow.
   - Communicate the changes to all contributors and provide guidance on how to use the new setup.

By adopting `npm workspaces`, we will streamline dependency management, improve the development experience, and ensure that our project remains maintainable and scalable.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision 
## Decision

To clearly separate the public API from the internal implementation, we will restructure the project to have a dedicated `public` directory for all files that are intended to be part of the public API. This will ensure that only the files and folders under the `public` directory are exposed to consumers of the `@primer/react` package.

### Steps to Implement:
1. **Create a `public` Directory:**
   - Move all files and folders that are intended to be part of the public API from the `src` directory to the `public` directory.
   - Ensure that the `public` directory contains only the necessary files and folders that should be exposed to consumers.

2. **Update the `exports` Field in `package.json`:**
   - Modify the `exports` field in `package.json` to point to the `public` directory instead of the `src` directory.
   - Update the export pattern to reflect the new structure:

     ```json
     {
       ""exports"": {
         ""./lib-esm/*"": {
           ""import"": [
             ""./public/lib-esm/*.js""
           ],
           ""require"": [
             ""./public/lib-esm/*.js""
           ]
         }
       }
     }
     ```

3. **Update Rollup Configuration:**
   - Adjust the Rollup configuration to build the `public` directory and output the compiled files to the appropriate destination.
   - Ensure that the Rollup configuration does not include any internal files or directories that are not part of the `public` directory.

4. **Internal Code Organization:**
   - Keep the internal implementation and private files in the `src` directory.
   - Ensure that the internal code is not directly accessible from the `public` directory.

### Benefits:
- **Clear Separation:** The `public` directory will clearly indicate which parts of the codebase are intended for external use.
- **Reduced Risk of Breaking Changes:** By limiting the public API to the `public` directory, we reduce the risk of accidentally exposing internal changes as breaking changes.
- **Improved Maintainability:** The separation will make it easier to maintain and update the public API without affecting the internal implementation.

### Example Directory Structure:
```
project-root/
 public/
    lib-esm/
       ...
    ...
 src/
    internal/
       ...
    ...
 package.json
 rollup.config.js
```

By following this approach, we can ensure that the public API of `@primer/react` is well-defined and easily maintainable.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision 
## Decision

- **Move to Static CSS Files:**
  - Transition from runtime injection of CSS to using static `.css` files. This will allow the browser to parallelize the download and processing of styles, leading to faster initial page loads.
  - **Benefits:**
    - **Faster Initial Page Loads:** Static CSS files can be downloaded and processed in parallel with other resources, reducing the time the page is unstyled.
    - **Improved Performance on Slower Devices:** The processing time for static CSS files is significantly lower, especially on devices with limited processing power.
    - **Server-Side Rendering (SSR) Optimization:** Eliminate the need to render the app twice on the server to collect styles, reducing the overall rendering time.

- **Utilize CSS Variables for Theming:**
  - Adopt CSS variables (custom properties) for theming. This allows for more flexible and dynamic theming without the performance overhead of JavaScript-based theming.
  - **Benefits:**
    - **Easier Theming:** CSS variables make it easier to manage and update themes across the application.
    - **Performance:** CSS variables are natively supported by the browser and do not require additional JavaScript processing, leading to better performance.
    - **Consistency:** Ensure consistent theming across the application by defining variables in a single place.

- **Improve Guardrails for Consuming Primer/Primitives:**
  - Enhance the guardrails and documentation for consuming `primer/primitives` to ensure that developers follow best practices and avoid common pitfalls.
  - **Benefits:**
    - **Better Developer Experience:** Clearer guidelines and better tooling will help developers use `primer/primitives` more effectively.
    - **Reduced Bugs:** By providing better guardrails, we can reduce the likelihood of bugs and inconsistencies in the application.
    - **Maintainability:** Improved documentation and tooling will make the codebase easier to maintain and extend over time.

- **Optimize Style Updates:**
  - Optimize the way styles are updated based on props or state. Use static styles where possible and minimize the use of dynamic styles.
  - **Benefits:**
    - **Faster Re-renders:** Static styles are faster to process during re-renders, leading to a smoother user experience.
    - **Reduced Performance Overhead:** Minimizing the use of dynamic styles will reduce the performance overhead, especially on slower devices.

By implementing these changes, we aim to significantly improve the performance and maintainability of our applications while also leveraging modern CSS features for better theming and styling.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision 
## Decision

We have decided to shift our approach and use Playwright for interaction tests instead of Storybook's `play` functions. This change is motivated by the following reasons:

1. **Comprehensive Testing Capabilities**: Playwright offers a more robust and comprehensive set of features for end-to-end and interaction testing, including support for multiple browsers, headless and headed modes, and advanced navigation and interaction capabilities.

2. **Better Integration with Jest**: Playwright can be easily integrated with Jest, allowing us to leverage Jest's powerful test runner and assertion libraries. This integration provides a more unified testing experience and allows us to maintain a consistent testing framework across our project.

3. **Improved Test Reliability**: Playwright's auto-waiting and retry mechanisms make it more reliable for handling asynchronous operations and dynamic content, reducing flakiness in our tests.

4. **Enhanced Debugging and Reporting**: Playwright provides detailed trace files and video recordings, which are invaluable for debugging and understanding test failures. This feature is particularly useful for complex interaction tests.

5. **Scalability and Performance**: Playwright is designed to handle large and complex test suites efficiently, making it a better choice for our growing test suite.

### Implementation Plan

- **Migration**: We will gradually migrate existing interaction tests from Storybook's `play` functions to Playwright.
- **Documentation**: Update our testing documentation to reflect the new approach and provide examples and best practices for using Playwright.
- **Training**: Conduct training sessions or provide resources to help team members get up to speed with Playwright.

By adopting Playwright for interaction tests, we aim to improve the reliability, maintainability, and scalability of our test suite.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision

To address the issue of layout shifts during server-side rendering (SSR) when using responsive values, we will implement a solution that ensures the initial value on the server matches the value on the client. This can be achieved by using a combination of `matchMedia` and a consistent fallback mechanism.

### Implementation Steps:

1. **Enhance `useResponsiveValue` Hook:**
   - Modify the `useResponsiveValue` hook to accept an additional `fallback` prop. This `fallback` prop will be used as the default value when the component is server-side rendered.
   - The hook will use `matchMedia` to determine the current viewport size and return the appropriate value based on the responsive configuration.
   - If the component is being server-side rendered, the hook will return the `fallback` value.

2. **Consistent Fallback Mechanism:**
   - Define a consistent fallback strategy for each responsive prop. For example, if the prop is `gap`, the fallback could be the `regular` breakpoint value.
   - Ensure that the fallback value is chosen in a way that minimizes the visual impact if the viewport size changes after hydration.

3. **Usage Example:**
   ```tsx
   // Value
   <Stack gap=""condensed"" />

   // Responsive value with fallback
   <Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }} fallback=""normal"" />
   ```

4. **Server-Side Rendering:**
   - On the server, the `useResponsiveValue` hook will always return the `fallback` value.
   - On the client, the hook will re-evaluate the responsive value and update the component's state if necessary.

5. **Hydration Strategy:**
   - During hydration, the component will check the current viewport size and update the prop value if it differs from the fallback value.
   - Use CSS transitions or animations to smooth out any visual changes, ensuring a more seamless user experience.

### Benefits:
- **Consistency:** The initial render on the server and the hydrated render on the client will be consistent, reducing layout shifts.
- **Flexibility:** Authors can still use responsive values while having control over the fallback behavior.
- **Performance:** Minimizes the number of re-renders and layout recalculations, improving the overall performance of the application.

### Example Implementation:

```tsx
import { useState, useEffect } from 'react';

const useResponsiveValue = (responsiveValue, fallback) => {
  const [value, setValue] = useState(fallback);

  useEffect(() => {
    const handleResize = () => {
      const mediaQueryList = window.matchMedia('(min-width: 768px)');
      const newValue = mediaQueryList.matches ? responsiveValue.regular : responsiveValue.narrow;
      setValue(newValue);
    };

    handleResize();
    window.addEventListener('resize', handleResize);
    return () => {
      window.removeEventListener('resize', handleResize);
    };
  }, [responsiveValue]);

  return value;
};

// Usage in a component
const Stack = ({ gap, children }) => {
  const resolvedGap = useResponsiveValue(gap, gap.regular || 'normal');

  return (
    <div style={{ gap: resolvedGap }}>
      {children}
    </div>
  );
};
```

By following this approach, we can provide a robust and consistent solution for handling responsive props in a way that avoids layout shifts during server-side rendering.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision 
## Decision

To handle the deprecation of props and provide a clear migration path, we will follow these steps:

1. **Introduce the New Prop/Component:**
   - Introduce the new prop or component that provides a more accessible or flexible API.
   - Document the new prop or component in the documentation, including its benefits and how it differs from the deprecated one.

2. **Deprecate the Old Prop:**
   - Mark the old prop as deprecated in the codebase. This can be done by adding a `@deprecated` tag in the JSDoc comments.
   - Add a warning message in the console when the deprecated prop is used. The warning should clearly state that the prop is deprecated and provide guidance on how to migrate to the new API.

3. **Provide a Migration Path:**
   - Update the documentation to include a migration guide. This guide should explain the changes, provide examples of the old and new APIs, and offer step-by-step instructions for migrating.
   - Include a section in the release notes that highlights the deprecation and the new API, with links to the migration guide.

4. **Example Code:**
   - Provide example code snippets in the documentation that show both the old and new APIs side by side. This will help authors understand the changes and make the necessary updates.

5. **Grace Period:**
   - Allow a grace period (e.g., 1-2 major releases) before removing the deprecated prop. During this period, continue to support the old prop but encourage users to migrate to the new API.

6. **Remove the Deprecated Prop:**
   - After the grace period, remove the deprecated prop from the codebase. Ensure that all references to the old prop are updated to use the new API.

### Example Implementation

**Old API:**
```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

**New API:**
```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

**Deprecation Warning:**
```jsx
if (props.title) {
  console.warn(
    'The ""title"" prop in <ActionList.Group> is deprecated. Please use <ActionList.GroupHeading> instead. See the migration guide for more details.'
  );
}
```

**Documentation:**
- **Migration Guide:**
  - Explain the changes and why they were made.
  - Provide examples of the old and new APIs.
  - Offer step-by-step instructions for migrating.
- **Release Notes:**
  - Highlight the deprecation and the new API.
  - Link to the migration guide for more details.

By following these steps, we can ensure a smooth transition for users and maintain a high-quality, accessible, and flexible API.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision 
## Decision

To support proxy configurations for self-hosted runners, we will adopt the use of standard environment variables that are widely recognized and used by many applications. Specifically, we will support the following environment variables:

- `http_proxy`
- `https_proxy`
- `no_proxy`

These environment variables will be used to configure the proxy settings for the self-hosted runners. This approach aligns with the conventions used by popular tools and libraries such as curl, wget, Perl, Python, Docker, Git, and R, ensuring a consistent and familiar experience for users.

### Implementation Details

1. **Environment Variable Support**:
   - The self-hosted runner will check for the presence of `http_proxy` and `https_proxy` environment variables.
   - If these variables are set, the runner will use the specified proxy for HTTP and HTTPS requests, respectively.
   - The `no_proxy` environment variable will be used to specify a list of hosts that should not be proxied.

2. **Case Sensitivity**:
   - While some applications also support `HTTPS_PROXY` (uppercase), we will primarily support the lowercase variants (`http_proxy`, `https_proxy`, `no_proxy`) to maintain consistency with the majority of applications.
   - We will provide documentation that clearly states our support for the lowercase variants and recommend their use.

3. **Fallback Mechanism**:
   - If the lowercase variants are not set, the runner will check for the uppercase variants (`HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`) as a fallback.
   - This ensures compatibility with environments where the uppercase variants are already in use.

4. **Documentation**:
   - Update the documentation to include clear instructions on how to configure proxy settings using the supported environment variables.
   - Provide examples and best practices for setting up proxies in various environments.

By adopting this decision, we ensure that our self-hosted runners can be easily configured to work within enterprise and organizational networks that require proxy support, while maintaining consistency with widely accepted standards.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision 
## Decision

To address the issue of command echoing cluttering user logs, we will implement a new system where command echoing is opt-in. This change will provide users with more control over what information is logged, thereby reducing log clutter and improving readability.

### Key Points:
1. **Default Behavior**:
   - Command echoing will be disabled by default.
   - If the `ACTIONS_STEP_DEBUG` secret is enabled, command echoing will be turned on by default. Otherwise, it will be off.

2. **Explicit Control**:
   - Users can explicitly enable or disable command echoing using the new commands `::echo::on` and `::echo::off`.

3. **Error Handling**:
   - In the case of errors, command outputs will still be echoed to provide additional context for troubleshooting.

### Implementation Details:
- **New Commands**:
  - `::echo::on`: Enables command echoing.
  - `::echo::off`: Disables command echoing.

- **Environment Variable**:
  - The `ACTIONS_STEP_DEBUG` environment variable will be checked at the start of the process. If it is set, command echoing will be enabled by default.

- **Logging**:
  - When command echoing is disabled, only error messages and relevant context will be logged.
  - When command echoing is enabled, all command outputs will be included in the logs.

### Example Usage:
```plaintext
# Enable command echoing
::echo::on

# Run a command
some-command

# Disable command echoing
::echo::off

# Run another command (this will not be echoed)
another-command
```

### Benefits:
- **Reduced Clutter**: By default, logs will be less cluttered, making them easier to read and understand.
- **Enhanced Troubleshooting**: Error messages and relevant context will always be logged, providing useful information for debugging.
- **User Control**: Users have the flexibility to enable or disable command echoing based on their needs.

This decision balances the need for clean logs with the requirement for detailed information when necessary, giving users the best of both worlds.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision 
## Decision

We will implement a feature in the runner that allows for the inclusion of extra information, such as OS and software details from the hosted image, in the `Set up job` step's log. This can be achieved by:

1. **Environment Variable Injection**: Allow users to define environment variables in the CI/CD configuration that the runner will automatically include in the `Set up job` step's log. For example, users can set `CI_JOB_EXTRA_INFO` with the necessary details.

2. **Runner Configuration**: Add a configuration option in the runner to enable or disable this feature. This ensures that the feature is opt-in and does not affect existing workflows unless explicitly enabled.

3. **Script Execution**: Provide a built-in script or command that the runner can execute at the beginning of the `Set up job` step to gather and log the required information. This script can be customizable to allow for flexibility in what information is collected.

4. **Documentation and Examples**: Update the documentation to include examples and best practices for using this feature. This will help users understand how to configure and use the feature effectively.

By implementing these steps, we ensure that the `Set up job` step's log contains the necessary extra information, providing better visibility and context for the CI/CD pipeline.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision
## Decision

We will add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

### Rationale

- **Enhanced Control Flow**: By adding `steps.<id>.outcome` and `steps.<id>.conclusion`, we provide a way for downstream steps to conditionally execute based on the success or failure of previous steps. This allows for more dynamic and flexible workflows.
- **Clarity and Readability**: The new fields will make it clearer what the result of each step was, improving the readability and maintainability of the workflow.
- **Consistency with Existing Fields**: The addition of these fields is consistent with the existing `steps.<id>.outputs` field, which already provides a way to pass data between steps.

### Implementation Details

- **`steps.<id>.outcome`**: This field will contain a string indicating the outcome of the step, such as ""success"" or ""failure"".
- **`steps.<id>.conclusion`**: This field will contain a string providing a brief conclusion or reason for the outcome, such as ""Build succeeded"" or ""Test failed due to timeout"".

### Example

```yaml
steps:
  - id: build
    run: make build
  - id: test
    if: steps.build.outcome == 'success'
    run: make test
  - id: deploy
    if: steps.test.outcome == 'success'
    run: make deploy
```

### Considerations

- **Backward Compatibility**: Ensure that the addition of these fields does not break existing workflows. The new fields should be optional and only used when explicitly defined.
- **Documentation**: Update the documentation to include the new fields and provide examples of how to use them effectively.

This decision will enhance the flexibility and control of our workflows, making it easier to manage complex build, test, and deployment processes.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision

Chosen option: ""Allow self-hosted runners to accept labels via command line arguments during registration.""

### Rationale

- **Automation and Flexibility**: Allowing labels to be set via command line arguments during registration enables better automation. This is particularly useful for environments where self-hosted runners are provisioned and configured programmatically, such as in CI/CD pipelines or infrastructure-as-code setups.
- **Consistency with Existing Behavior**: The runner already registers built-in labels (e.g., `os`, `arch`) during registration. Extending this capability to include custom labels via command line arguments maintains consistency and simplifies the configuration process.
- **User Request and Feedback**: There is a clear user need for this feature, as evidenced by the GitHub issue [actions/runner#262](https://github.com/actions/runner/issues/262). Addressing this request will improve the usability of self-hosted runners.
- **Backward Compatibility**: This change can be implemented in a way that does not break existing configurations. Runners that do not specify additional labels via command line arguments will continue to function as they do now.

### Implementation Details

- **Command Line Argument**: Add a new command line argument, e.g., `--labels` or `--add-labels`, to the runner registration command. This argument will accept a comma-separated list of labels.
- **Validation and Sanitization**: Ensure that the labels provided via the command line are validated and sanitized to prevent injection of malicious data.
- **Documentation**: Update the documentation to include the new command line argument and provide examples of how to use it.

### Example Usage

```sh
./config.sh --url https://github.com/your-repo --token your-token --labels ""custom-label1,custom-label2""
```

### Backward Compatibility

- **Default Behavior**: If the `--labels` argument is not provided, the runner will register only the built-in labels as it does currently.
- **No Breaking Changes**: Existing scripts and configurations that do not use the new `--labels` argument will continue to work without any changes.

### Future Considerations

- **Configuration File Support**: In the future, consider adding support for specifying labels in a configuration file, which can be read during registration. This would provide an alternative method for setting labels, especially for more complex configurations.
- **Dynamic Label Management**: Explore the possibility of allowing dynamic label management after the runner has been registered, such as through a REST API or a dedicated CLI command.

This decision aims to enhance the flexibility and automation capabilities of self-hosted runners while maintaining backward compatibility and addressing user feedback.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision 
## Decision

To handle transient faults when communicating with the authentication provider, we will implement a robust retry mechanism and circuit breaker pattern in our authentication service. This approach will help us to:

1. **Retry Mechanism**: Implement a configurable retry policy that allows for multiple attempts to authenticate a request if the initial attempt fails due to a transient fault. The retry policy should include:
   - **Number of Retries**: A configurable number of retries.
   - **Backoff Strategy**: An exponential backoff strategy to avoid overwhelming the provider during temporary outages or rate limiting.
   - **Jitter**: Add a random delay (jitter) to the backoff to prevent synchronized retries from multiple clients.

2. **Circuit Breaker Pattern**: Introduce a circuit breaker to detect and mitigate failures by temporarily stopping further requests to the provider when a certain threshold of failures is reached. The circuit breaker should:
   - **Open State**: If the number of consecutive failures exceeds a threshold, the circuit breaker will open, and subsequent requests will be immediately rejected or redirected to a fallback mechanism.
   - **Half-Open State**: After a predefined cooldown period, the circuit breaker will transition to a half-open state, allowing a limited number of requests to pass through. If these requests succeed, the circuit breaker will reset to the closed state; otherwise, it will return to the open state.
   - **Closed State**: In this state, the circuit breaker allows all requests to pass through to the provider.

3. **Fallback Mechanism**: Define a fallback strategy to handle cases where the circuit breaker is open. This could include:
   - **Cache Authentication Tokens**: If possible, cache valid authentication tokens to reduce the need for frequent calls to the provider.
   - **Local Authentication**: Use a local authentication mechanism as a fallback, if feasible, to ensure that the system remains operational even if the provider is down.
   - **User Notifications**: Inform users about the issue and provide guidance on how to proceed, such as trying again later or using an alternative method.

4. **Monitoring and Logging**: Implement comprehensive monitoring and logging to track the state of the circuit breaker, the number of retries, and the success/failure rates of authentication requests. This will help in identifying and addressing the root causes of transient faults and improving the overall reliability of the system.

By implementing these strategies, we can ensure that SS?? is resilient to transient faults in the authentication provider, providing a more reliable and user-friendly experience.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision 
## Decision

Chosen option: ""Use Architecture Decision Records (ADRs) with a structured format"", because

- **Transparency and Clarity**: ADRs provide a clear and transparent way to document decisions, making it easier for new team members and outside contributors to understand the reasoning behind specific choices.
- **Constructive Criticism**: By documenting decisions, we open up the process to constructive criticism, which can lead to better decision-making and continuous improvement.
- **Documentation that Stays Relevant**: ADRs document the decision at a specific point in time, ensuring that the documentation remains relevant and does not become outdated.
- **Structured Format**: Using a structured format like the one proposed by Michael Nygard or the MADR (Markdown Architectural Decision Records) format ensures consistency and ease of use. The MADR format, in particular, is widely adopted and provides a clear structure for each decision record.

### Recommended Structure for ADRs
Each ADR should follow a consistent structure, such as:

1. **Title**: A short, descriptive title for the decision.
2. **Status**: The current status of the decision (e.g., proposed, accepted, rejected, deprecated).
3. **Date**: The date the decision was made.
4. **Context**: The problem or situation that led to the decision.
5. **Decision**: The chosen solution or action.
6. **Consequences**: The expected positive and negative outcomes of the decision.
7. **Tags**: Optional tags for categorization (e.g., architecture, code, design, etc.).
8. **References**: Links to related documents, discussions, or other relevant information.

### Example ADR Template

```markdown
# Title: [Short, Descriptive Title]

## Status
[proposed | accepted | rejected | deprecated]

## Date
[YYYY-MM-DD]

## Context
[The problem or situation that led to the decision.]

## Decision
[The chosen solution or action.]

## Consequences
- **Positive**: [Expected positive outcomes.]
- **Negative**: [Expected negative outcomes.]

## Tags
[tag1, tag2, ...]

## References
- [Link to related documents, discussions, or other relevant information.]
```

By following this structured format, we ensure that all decisions are documented consistently and comprehensively, providing a valuable resource for the project's ongoing development and maintenance.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up  with all relevant discussion summarized  in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision 
## Decision

To address the need for a flexible and decentralized way to define and access extensions, we will use an EDN (Extensible Data Notation) file format. This decision is based on the following considerations:

1. **Decentralized Storage**: Extensions will be stored in a decentralized manner, allowing for greater resilience and accessibility. This can be achieved using technologies like IPFS, Swarm, or other decentralized storage solutions.

2. **URI Schema for Identification**: We will define a custom URI schema to uniquely identify and locate these EDN files. The URI schema will follow a standard format, such as `extension://<hash-or-name>`, where `<hash-or-name>` can be a unique identifier (e.g., a hash of the file content or a human-readable name).

3. **EDN File Format**: EDN is a subset of Clojure's data literal syntax, which is both human-readable and machine-readable. It is well-suited for defining structured data and can easily represent complex extension configurations.

4. **Structure of the EDN File**:
   - **Metadata**: The EDN file will include metadata such as the name, version, author, and description of the extension.
   - **Dependencies**: A list of dependencies required by the extension, including other extensions or libraries.
   - **Configuration**: Any configuration parameters or settings needed for the extension.
   - **Code or Scripts**: References to any code or scripts that the extension needs to execute. These can be included inline or referenced via additional URIs.

5. **Resolution and Fetching**:
   - **URI Resolution**: When an extension is referenced by its URI, the system will resolve the URI to the actual location of the EDN file in the decentralized storage.
   - **Fetching**: The system will fetch the EDN file from the decentralized storage and parse it to extract the necessary information.
   - **Validation**: The system will validate the EDN file to ensure it conforms to the expected structure and contains all required fields.

6. **Caching and Versioning**:
   - **Caching**: To improve performance, the system will cache frequently accessed EDN files locally.
   - **Versioning**: The URI schema will support versioning, allowing for multiple versions of the same extension to coexist. For example, `extension://<hash-or-name>/v1.0.0`.

7. **Security and Integrity**:
   - **Hash Verification**: The URI can include a hash of the EDN file to ensure integrity. The system will verify the hash after fetching the file.
   - **Signature**: Optionally, the EDN file can be signed to ensure its authenticity. The system will verify the signature before processing the file.

By using this approach, we can create a flexible, secure, and decentralized system for managing and accessing extensions, while also ensuring that the system remains robust and easy to maintain.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision 
## Decision

Group chat will be out of scope for the beta launch in Q2. We will focus our efforts on 1-1 chat and public chat, as these are more critical for the MVP and provide a better initial user experience. This decision will allow us to concentrate on resolving existing issues and ensuring the stability and security of the core chat functionalities. Once the beta is successfully launched, we can revisit the inclusion of group chat based on user feedback and prioritize it accordingly.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision 
## Decision

We will implement the Discovery v5 (Discv5) protocol to enable dynamic peer discovery and improve the scalability, availability, and security of our application. This decision is based on the following considerations:

1. **Scalability**: Discv5 allows us to dynamically discover new peers, which means we can scale our network more easily as the number of active users grows. This eliminates the need for hardcoding peers and reduces the risk of nodes becoming overloaded.

2. **High Availability and Reliability**: By using Discv5, we can ensure that our network remains highly available and reliable. If a node goes down, the protocol will help in finding and connecting to other available nodes, thus maintaining the overall health of the network.

3. **Security**: Discv5 includes features such as encryption and authentication, which enhance the security of the peer discovery process. This helps in preventing unauthorized access and ensures that only trusted nodes are part of the network.

4. **Flexibility**: The ability to dynamically discover and connect to new peers provides greater flexibility in managing our network. We can add or remove nodes as needed without disrupting the service.

### Implementation Plan

1. **Research and Documentation**:
   - Conduct a thorough review of the Discv5 protocol specifications and existing implementations.
   - Document the key features and requirements for our use case.

2. **Prototype Development**:
   - Develop a prototype implementation of Discv5 in our application.
   - Test the prototype to ensure it meets the required performance and security standards.

3. **Integration**:
   - Integrate the Discv5 implementation into the main application.
   - Ensure that the integration does not break existing functionality and that it is backward-compatible.

4. **Testing**:
   - Perform extensive testing, including unit tests, integration tests, and load testing.
   - Validate the scalability, reliability, and security of the new peer discovery mechanism.

5. **Deployment and Monitoring**:
   - Deploy the updated application to a staging environment for further testing.
   - Monitor the performance and stability of the network.
   - Gradually roll out the update to the production environment.

6. **Documentation and Training**:
   - Update the documentation to reflect the changes and provide guidance on how to use the new peer discovery feature.
   - Train the development and operations teams on the new implementation.

By implementing Discv5, we will significantly improve the scalability, reliability, and security of our messaging application, ensuring that it can handle a growing number of users and maintain high availability.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision 
## Decision

We will adopt the BIP-39 and BIP-44 standards for generating keys from the 12 mnemonic words. This will ensure compatibility with a wide range of wallets and applications that also adhere to these standards.

### Key Generation Process
1. **Mnemonic Phrase**: The user will provide a 12-word mnemonic phrase.
2. **Seed Generation**: The mnemonic phrase will be used to generate a seed using the PBKDF2 function with HMAC-SHA512, as specified in BIP-39.
3. **Master Key Derivation**: The seed will be used to derive the master key (m) using HMAC-SHA512, as specified in BIP-32.
4. **Derivation Path**: We will use the BIP-44 derivation path for Ethereum, which is `m/44'/60'/0'/0/0` for the first account. This path can be extended to support multiple accounts and addresses.
5. **Key Pair Generation**: The private key will be derived from the extended private key at the end of the derivation path. The corresponding public key will be generated from the private key.

### Implementation Steps
1. **Update Key Generation Library**: Integrate or update the existing key generation library to support BIP-39 and BIP-44 standards.
2. **User Interface**: Modify the user interface to allow users to input their 12-word mnemonic phrase and generate the corresponding wallet address.
3. **Testing**: Conduct thorough testing to ensure that the generated addresses match those generated by other compliant wallets.
4. **Documentation**: Update the documentation to reflect the new key generation process and provide clear instructions for users on how to import their mnemonic phrases into Status.

### Benefits
- **Interoperability**: Users can now import their existing wallets from other applications into Status, enhancing user experience and convenience.
- **Standardization**: Adhering to widely accepted standards ensures better security and reduces the risk of implementation errors.
- **Community Trust**: By following established standards, we build trust within the community and align with best practices in the industry.

### Considerations
- **Security**: Ensure that the implementation follows best security practices, such as securely storing the mnemonic phrase and derived keys.
- **Backward Compatibility**: Provide a migration path for existing users who may have generated keys using the old method.
- **User Education**: Educate users about the importance of securely storing their mnemonic phrases and the benefits of using a standardized key generation process.

By adopting BIP-39 and BIP-44, we will enhance the wallet compatibility and user experience, making it easier for users to manage their funds across different platforms.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision 
## Decision

We will prioritize the development and integration of ULC (Universal Light Client) to address the performance and decentralization concerns associated with using Infura and LES. 

In the short term, we will:
- Continue to use Infura as a temporary solution to ensure the usability of the beta release.
- Implement the option for users to connect to their own upstream nodes, providing an alternative to Infura and improving the trust model.

In the medium to long term, we will:
- Focus on the development and integration of ULC, which will allow us to connect to multiple nodes, enhancing both performance and decentralization.
- Conduct thorough testing and optimization of ULC to ensure it meets the performance and security requirements of the Status app.
- Gradually phase out the reliance on Infura and other third-party nodes as ULC becomes more stable and feature-complete.

This approach balances the immediate need for a usable beta with the long-term goal of a more decentralized and secure application.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision 
## Decision

To prevent sensitive data, such as user passwords, from being accidentally logged, we will introduce a new type called `SensitiveData` that encapsulates the sensitive information. This type will be used to handle and pass around sensitive data throughout the codebase.

### Implementation Details

1. **Define the `SensitiveData` Type:**
   - Create a new type `SensitiveData` that wraps the sensitive string.
   - Implement methods to safely handle and manipulate the data without exposing it directly.

2. **Usage in Code:**
   - Replace all instances where sensitive data is currently passed as a string with the `SensitiveData` type.
   - Ensure that any logging or debugging mechanisms do not directly log the contents of `SensitiveData`.

3. **Logging and Debugging:**
   - Override the `String` method for `SensitiveData` to return a masked or placeholder value (e.g., ""********"").
   - Use structured logging to ensure that sensitive data is not inadvertently included in log entries.

4. **Example Implementation:**

   ```go
   package main

   import (
       ""fmt""
       ""log""
   )

   // SensitiveData encapsulates sensitive information.
   type SensitiveData struct {
       data string
   }

   // NewSensitiveData creates a new SensitiveData instance.
   func NewSensitiveData(data string) *SensitiveData {
       return &SensitiveData{data: data}
   }

   // String returns a masked version of the sensitive data.
   func (s *SensitiveData) String() string {
       return ""********""
   }

   // GetData returns the actual sensitive data.
   func (s *SensitiveData) GetData() string {
       return s.data
   }

   func main() {
       password := NewSensitiveData(""mysecretpassword"")

       // This will log ""Password: ********"" instead of the actual password.
       log.Printf(""Password: %s"", password)

       // Access the actual data when needed.
       fmt.Println(""Actual Password:"", password.GetData())
   }
   ```

### Benefits

- **Security:** Ensures that sensitive data is not accidentally logged, reducing the risk of data breaches.
- **Consistency:** Provides a consistent way to handle sensitive data across the codebase.
- **Usability:** The `SensitiveData` type can be extended to include additional security features, such as encryption and secure storage.

### Considerations

- **Performance:** Ensure that the overhead introduced by the `SensitiveData` type is minimal and does not significantly impact performance.
- **Adoption:** Encourage and enforce the use of `SensitiveData` throughout the codebase to maximize its effectiveness.

By introducing the `SensitiveData` type, we can significantly reduce the risk of sensitive data being exposed in logs and improve the overall security of our application.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision 
## Decision

Given the numerous downsides and the lack of real-world usage of the current 3rd party status extensions via the JavaScript jailed execution environment, we have decided to deprecate and remove the existing jail-based extension system. Instead, we will focus on developing and adopting the new, more promising concept of extensions (Project Pluto).

### Key Points of the Decision:

1. **Deprecation and Removal of Jail-Based Extensions:**
   - We will discontinue support for the current jail-based 3rd party status extensions.
   - All existing commands and logic that were partially or fully implemented in the jail will be refactored and integrated directly into the main application codebase.

2. **Refactoring and Cleanup:**
   - The hard-coded ""magic"" and custom logic scattered throughout the codebase will be refactored to improve code quality and maintainability.
   - We will ensure that all command messages and related logic are implemented in a stateless, testable manner, leveraging the strengths of our host application (cljs/re-frame).

3. **Adoption of Project Pluto:**
   - We will shift our focus to Project Pluto, which provides a more declarative and extensible way to define and integrate extensions.
   - Project Pluto will allow for a broader range of extension points, including command messages, wallet assets/collectibles, chat contacts, and other application hooks.

4. **Performance Improvements:**
   - By removing the need for constant RN Bridge ""ping-pong"" and asynchronous call chains, we expect to see performance improvements.
   - The new extension system will be designed with performance in mind, ensuring that it does not introduce unnecessary overhead.

5. **Documentation and Community Engagement:**
   - We will provide comprehensive documentation for the new extension system, making it easier for developers to create and integrate extensions.
   - We will engage with the community to gather feedback and ensure that the new system meets the needs of both developers and users.

### Implementation Plan:

1. **Short Term:**
   - Deprecate and remove the jail-based extension system.
   - Refactor and clean up the existing codebase, integrating all command messages and related logic directly into the application.
   - Begin the initial development and testing of Project Pluto.

2. **Medium Term:**
   - Continue to develop and refine Project Pluto, adding support for various extension points.
   - Release an initial version of Project Pluto for community testing and feedback.

3. **Long Term:**
   - Fully transition to the new extension system, phasing out any remaining legacy code.
   - Continuously improve and expand the capabilities of Project Pluto based on community feedback and evolving needs.

By making this decision, we aim to improve the overall quality, performance, and extensibility of the Status application, while also providing a more robust and user-friendly extension system for the community.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision
## Decision

We will implement the Tribute to Talk (TtT) feature as an economics-based anti-spam mechanism for direct messages and ""cold"" contact requests. The core functionality will be as follows:

1. **Stakeholder Configuration**:
   - Each user (stakeholder) can set a minimum SNT deposit amount that other users must pay to send them a direct message or a ""cold"" contact request.
   - This minimum SNT deposit is configurable by the user and can be set to zero if the user does not wish to use TtT.

2. **Deposit Requirement**:
   - When a user (sender) attempts to send a direct message or a ""cold"" contact request to another user (recipient), the sender must deposit the specified minimum SNT amount.
   - The deposit is held in a smart contract until the recipient responds or a predefined timeout period elapses.

3. **Recipient Response**:
   - If the recipient replies to the message or accepts the contact request, the deposited SNT is transferred to the recipient.
   - If the recipient does not respond within the timeout period, the deposited SNT is returned to the sender.

4. **Timeout Mechanism**:
   - A configurable timeout period (e.g., 7 days) is set for the recipient to respond.
   - If the recipient does not respond within this period, the deposit is automatically refunded to the sender.

5. **User Experience**:
   - The user interface will clearly indicate the SNT deposit requirement before the sender sends the message or contact request.
   - The recipient will receive a notification of the incoming message or contact request, along with the option to accept or ignore it.

6. **Smart Contract Implementation**:
   - A smart contract will manage the deposits, timeouts, and transfers.
   - The contract will handle the logic for holding the deposit, transferring it to the recipient upon response, and refunding it to the sender if no response is given.

7. **Security and Fairness**:
   - The system will ensure that the deposit is only forfeited to the recipient if they respond, preventing abuse.
   - The smart contract will be audited to ensure security and fairness.

8. **Optional Feature**:
   - Users can opt out of the TtT feature by setting their minimum SNT deposit to zero, allowing anyone to contact them without a deposit.

This implementation ensures that the TtT feature effectively reduces spam while providing a fair and transparent system for both senders and recipients.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision 
## Decision

To improve the polish and oversight of the mobile app, we will introduce a new role called ""Section Lead"" for each major section of the application. The responsibilities of a Section Lead include:

1. **Oversight and Quality Assurance:**
   - Ensure that the design and development of their section meet high standards of quality and align with the overall vision and goals of the mobile app.
   - Conduct regular code reviews and design audits to maintain consistency and catch potential issues early.

2. **Coordination and Communication:**
   - Act as the primary point of contact for their section, facilitating communication between the mobile development team, the design team, and other stakeholders.
   - Coordinate with the design team to ensure that the visual and interaction designs are implemented accurately and effectively.
   - Collaborate with the backend and other relevant teams to ensure seamless integration and functionality.

3. **Project Management:**
   - Manage the roadmap and sprint planning for their section, ensuring that tasks are prioritized and completed on time.
   - Track progress and report on the status of their section to the team lead and other stakeholders.
   - Identify and address any bottlenecks or resource constraints that may impact the development of their section.

4. **Mentorship and Support:**
   - Provide guidance and support to other team members working on their section, helping them to grow and develop their skills.
   - Foster a collaborative and inclusive environment within the team, encouraging open communication and continuous improvement.

5. **User Feedback and Iteration:**
   - Gather and analyze user feedback related to their section, using this information to inform future improvements and updates.
   - Work with the design and product teams to iterate on features and address any usability issues.

### Implementation Plan

1. **Selection Process:**
   - The team lead will identify and select individuals who have demonstrated strong technical and leadership skills, as well as a deep understanding of their respective sections.
   - The selection process will involve input from the design team and other relevant stakeholders to ensure a good fit.

2. **Role Definition and Training:**
   - Clearly define the responsibilities and expectations for the Section Lead role.
   - Provide training and resources to help Section Leads understand their new responsibilities and how to effectively manage their sections.

3. **Integration and Support:**
   - Integrate the Section Lead roles into the existing team structure, ensuring that they have the necessary tools and support to succeed.
   - Establish regular check-ins and feedback loops to monitor the effectiveness of the new role and make adjustments as needed.

By introducing Section Leads, we aim to enhance the overall quality and polish of the mobile app, improve coordination and communication, and ensure that each section is managed with the attention and care it deserves.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision 
## Decision

We will use a dedicated `ProductOptionValue` entity to represent each value of a `ProductOption`. This approach provides several benefits:

1. **Normalization and Denormalization:**
   - **Efficiency:** By using a separate entity, we can leverage API Platform's built-in normalization and denormalization processes more efficiently. This means that when a `ProductOption` is fetched, its values can be included in the response without additional queries.
   - **Flexibility:** The `ProductOptionValue` entity can have its own attributes, such as `value`, `position`, and `locale`, making it more flexible and easier to manage.

2. **Data Integrity:**
   - **Validation:** Using a dedicated entity allows us to enforce data integrity and validation rules at the database level. For example, we can ensure that each `ProductOptionValue` is unique within a `ProductOption`.
   - **Consistency:** It ensures that the values are consistently managed and can be easily referenced and updated.

3. **Usability:**
   - **Ease of Use:** Clients can easily manage `ProductOption` values by interacting with the `ProductOptionValue` entity. This makes the API more intuitive and user-friendly.
   - **Standardization:** It aligns with RESTful principles, where each resource (in this case, `ProductOptionValue`) has its own endpoint for CRUD operations.

4. **Scalability:**
   - **Performance:** By normalizing the values into a separate entity, we can optimize performance, especially when dealing with large datasets. This approach allows for efficient querying and indexing.
   - **Extensibility:** If we need to add more features or attributes to `ProductOptionValue` in the future, it will be easier to do so without affecting the `ProductOption` entity.

### Implementation Details

- **Entities:**
  - `ProductOption`: Represents the product option itself.
  - `ProductOptionValue`: Represents individual values of the product option. Each `ProductOptionValue` will have a reference to its parent `ProductOption`.

- **API Endpoints:**
  - `/api/product_options`: Endpoint for managing `ProductOption` entities.
  - `/api/product_option_values`: Endpoint for managing `ProductOptionValue` entities.

- **Relationships:**
  - A `ProductOption` can have multiple `ProductOptionValue` entities.
  - Each `ProductOptionValue` belongs to one `ProductOption`.

- **Serialization:**
  - When fetching a `ProductOption`, include its `ProductOptionValue` entities in the response.
  - When creating or updating a `ProductOption`, allow the client to specify its `ProductOptionValue` entities.

This decision will provide a robust, efficient, and scalable solution for managing product options and their values in the Sylius API.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision 
## Decision

To provide a clear, easy to use, and consistent way of handling customer locales in Sylius, we will implement the following changes:

1. **Locale-Specific API Endpoints:**
   - Introduce locale-specific endpoints for resources that are locale-aware. For example, instead of returning all translations, we will allow clients to request data for a specific locale.
   - The endpoint URLs will include the locale as a part of the URL, e.g., `/api/products/{locale}/` or as a query parameter, e.g., `/api/products/?locale={locale}`.

2. **Default Locale Handling:**
   - If no locale is specified in the request, the API will default to the store's default locale. This ensures backward compatibility and provides a fallback mechanism.

3. **Consistent Embedding of Translations:**
   - Ensure that all endpoints that return locale-aware resources consistently embed the translation for the requested locale. This means that if a client requests a product in a specific locale, the response will include only the relevant translation.
   - For example, a product endpoint will return:
     ```json
     {
       ""id"": 1,
       ""name"": ""Product Name in English"",
       ""description"": ""Product Description in English"",
       ""translations"": {
         ""en_US"": {
           ""name"": ""Product Name in English"",
           ""description"": ""Product Description in English""
         },
         ""fr_FR"": {
           ""name"": ""Nom du produit en franais"",
           ""description"": ""Description du produit en franais""
         }
       }
     }
     ```
   - When a specific locale is requested, the `name` and `description` fields will be populated with the corresponding translation, and the `translations` field will still contain all available translations for reference.

4. **Client-Side Localization:**
   - Provide clear documentation and examples for frontend developers on how to handle and request locale-specific data from the API. This includes best practices for constructing API requests and handling the responses.

5. **API Versioning:**
   - Introduce versioning for the API to ensure that existing integrations and clients can continue to use the current behavior while new clients can take advantage of the improved locale handling.

By implementing these changes, we will provide a more consistent and user-friendly experience for both backend and frontend developers, ensuring that locale-aware resources are handled in a standardized and efficient manner.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision
We will use Architecture Decision Records (ADRs) to document decisions that affect the development of the Terraform Controller. ADRs will be stored in a dedicated directory within the project repository, and each ADR will be written in a markdown file. The ADRs will follow the template described by Michael Nygard, which includes the following sections: Status, Context, Decision, and Consequences.

### Key Points:
- **Lightweight and Accessible:** ADRs are lightweight and easy to create, making it simple for contributors to document their decisions.
- **Discoverability:** ADRs will be stored in a dedicated directory (e.g., `docs/adr/`) within the project repository, ensuring they are easily discoverable.
- **Integration with Development Workflow:** ADRs can be created and reviewed as part of the regular development process, such as during code reviews or pull requests.
- **Historical Context:** ADRs will provide historical context and rationale for decisions, helping future contributors understand the reasoning behind specific implementations or choices.

### Example Directory Structure:
```
terraform-controller/
 docs/
    adr/
       0001-record-of-architectural-decisions.md
       0002-decision-on-feature-x.md
       ...
 src/
 tests/
 ...
```

### Example ADR Template:
```markdown
# 0001-record-of-architectural-dedecisions

## Status
Accepted

## Context
Decisions that affect the development of Terraform Controller that are not captured via a proposal need to be captured in some way. We need a method that is lightweight and easy to discover the decision that have been made. The record of decisions will help future contributors to the project to understand why something has been implemented or is done a certain way.

## Decision
We will use Architecture Decision Records (ADRs) to document decisions that affect the development of the Terraform Controller. ADRs will be stored in a dedicated directory within the project repository, and each ADR will be written in a markdown file.

## Consequences
- ADRs will provide a clear and consistent way to document decisions.
- Future contributors will have a reference to understand the reasoning behind specific choices.
- The process of creating and reviewing ADRs will be integrated into the existing development workflow.
```

By adopting this approach, we ensure that all significant decisions are documented, providing a clear and accessible record for current and future contributors.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision 
## Decision

We will use webhooks to detect pull request changes.

### Rationale

1. **Real-Time Notifications**: Webhooks provide real-time notifications, which means we can trigger actions (e.g., running tests, updating a dashboard) immediately when a pull request is created, updated, or merged. This reduces the latency compared to polling.

2. **Reduced Load on GitHub API**: Polling requires frequent calls to the GitHub API, which can lead to rate limiting and increased load on both our system and GitHub's servers. Webhooks, on the other hand, are event-driven and only send data when an event occurs, reducing the number of API calls.

3. **Simplicity and Reliability**: Setting up webhooks is straightforward and reliable. GitHub will automatically send a POST request to a specified URL whenever a relevant event occurs. This simplifies the implementation and maintenance of the system.

4. **Event-Driven Architecture**: Webhooks align well with an event-driven architecture, which is more scalable and efficient for handling asynchronous events like pull request changes.

### Implementation Steps

1. **Set Up Webhook in GitHub**:
   - Go to the repository settings.
   - Navigate to the ""Webhooks"" section.
   - Add a new webhook with the URL of the endpoint that will handle the webhook payload.
   - Select the events you want to trigger the webhook (e.g., `pull_request`).

2. **Create Endpoint to Handle Webhook Payload**:
   - Set up a server or use a cloud function to handle incoming webhook requests.
   - Parse the payload to determine the type of event and the details of the pull request.
   - Trigger the appropriate actions based on the event (e.g., run tests, update status, notify stakeholders).

3. **Security Considerations**:
   - Use a secret token to verify the authenticity of the webhook payload.
   - Ensure the endpoint is secure and only accessible by authorized services.

By using webhooks, we can efficiently and reliably detect pull request changes, enabling us to respond quickly and reduce the overhead on our systems.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision 
## Decision

We will allow cross-namespace references in the tf-controller API, but we will implement additional safeguards and best practices to mitigate the security risks associated with such references. Here are the detailed steps:

### 1. **Namespace Isolation and Permissions Management**
   - **RBAC (Role-Based Access Control):** Implement fine-grained RBAC policies to ensure that only authorized users and service accounts can create, update, or delete cross-namespace references.
   - **Service Accounts:** Use dedicated service accounts for tf-controller and other components that need to access resources across namespaces. These service accounts should have the minimum necessary permissions.
   - **Network Policies:** Consider using network policies to restrict communication between namespaces, ensuring that only necessary traffic is allowed.

### 2. **Validation and Enforcement**
   - **API Validation:** Add validation to the tf-controller API to ensure that cross-namespace references are only allowed in specific, well-defined fields (e.g., `Terraform.spec.sourceRef`, `Terraform.spec.dependsOn[]`, `.data.resources[]`).
   - **Admission Controllers:** Implement an admission controller to enforce these rules at the Kubernetes API level. This ensures that any attempt to create or update a resource with an invalid cross-namespace reference is rejected.

### 3. **Documentation and Best Practices**
   - **Documentation:** Clearly document the use cases and best practices for cross-namespace references in the tf-controller API. This should include examples of how to set up RBAC and service accounts securely.
   - **Guidelines:** Provide guidelines for users on how to minimize the use of cross-namespace references and how to secure their deployments when they are necessary.

### 4. **Audit and Monitoring**
   - **Audit Logs:** Enable audit logging for all cross-namespace operations to track and review who is making changes and when.
   - **Monitoring and Alerts:** Set up monitoring and alerting to detect and respond to any unauthorized or suspicious cross-namespace activity.

### 5. **Future Considerations**
   - **Gradual Phasing Out:** Consider a long-term plan to gradually phase out the need for cross-namespace references by improving the design and architecture of the system. This could involve refactoring certain components or introducing new features that reduce the need for cross-namespace dependencies.
   - **Community Feedback:** Engage with the community to gather feedback on the use of cross-namespace references and to identify potential improvements or alternative solutions.

By implementing these measures, we can maintain the flexibility and functionality that cross-namespace references provide while significantly reducing the associated security risks.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision 
## Decision

To address the challenges related to the deletion of Terraform resources, we will implement a caching mechanism for Workspace BLOBs. This will ensure that the TF-Controller can efficiently manage and delete single Terraform objects, even when they are tied to other resources like Source objects, Secrets, and ConfigMaps.

### Key Points of the Decision

1. **Caching Mechanism for Workspace BLOBs:**
   - Implement a caching layer within the TF-Controller to store and retrieve Workspace BLOBs.
   - The cache will store the generated Workspace BLOBs, which contain all necessary information from the associated Source, Secrets, and ConfigMaps.
   - This will allow the TF-Controller to quickly access the required data for finalization procedures without needing to re-download and re-process the Source BLOB each time.

2. **Enhanced Deletion Process:**
   - When a user initiates the deletion of a single TF object, the TF-Controller will first check the cache for the corresponding Workspace BLOB.
   - If the BLOB is found in the cache, the TF-Controller will use it to perform the deletion process.
   - If the BLOB is not found in the cache, the TF-Controller will download the Source BLOB, generate the Workspace file system, and store the new BLOB in the cache before proceeding with the deletion.

3. **Handling Dependencies:**
   - For resources with dependencies, the TF-Controller will use the cached Workspace BLOBs to ensure that all dependent resources are properly managed and deleted in the correct order.
   - This will prevent the TF object from getting stuck in an inconsistent state due to missing or incomplete information.

4. **Namespace Deletion:**
   - Once the single object and dependent resource deletions are working correctly, the TF-Controller will be enhanced to handle namespace deletions.
   - The caching mechanism will be leveraged to ensure that all Terraform objects and their dependencies within a namespace are deleted efficiently.

### Implementation Steps

1. **Cache Initialization:**
   - Initialize a cache (e.g., using an in-memory cache like Redis or an on-disk cache) to store Workspace BLOBs.
   - Ensure the cache has a reasonable expiration policy to manage memory usage and keep the cache up-to-date.

2. **BLOB Generation and Caching:**
   - Modify the tf-runner to generate and compress the Workspace file system as a BLOB.
   - Store the generated BLOB in the cache with a unique key based on the Source, Secrets, and ConfigMaps used to create it.

3. **Deletion Workflow:**
   - When a deletion request is received, the TF-Controller will first check the cache for the relevant Workspace BLOB.
   - If the BLOB is found, use it to perform the deletion.
   - If the BLOB is not found, download the Source BLOB, generate the Workspace file system, store the new BLOB in the cache, and then proceed with the deletion.

4. **Dependency Management:**
   - Implement logic to handle dependencies by ensuring that all dependent resources are deleted in the correct order using the cached Workspace BLOBs.

5. **Namespace Deletion:**
   - Extend the deletion workflow to handle namespace deletions, ensuring that all Terraform objects and their dependencies within the namespace are deleted efficiently.

### Benefits

- **Improved Efficiency:** The caching mechanism will reduce the need to repeatedly download and process Source BLOBs, leading to faster and more efficient deletions.
- **Consistency:** By using the cached Workspace BLOBs, the TF-Controller can ensure that all necessary information is available, preventing inconsistencies and making it easier for users to manage their infrastructure.
- **Scalability:** The caching mechanism will help scale the TF-Controller to handle a larger number of Terraform objects and namespaces, improving overall performance and reliability.

This decision will address the current challenges and provide a robust solution for managing Terraform resource deletions.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision 
## Decision

Given the changes in Zarf v0.15, we will update our E2E testing strategy to take advantage of the new flexibility and address the additional requirements. The following steps will be taken:

1. **Remove Dependency on Repository Secrets:**
   - Since the creation of the initialization package no longer requires repository secrets, we will remove the dependency on `peter-evans/slash-command-dispatch@v2` for running E2E tests. This will allow any contributor to run E2E tests without needing maintainer approval.

2. **Expand Test Coverage:**
   - **Kubernetes Distributions:** We will add support for testing against different Kubernetes distributions (e.g., kind, minikube, EKS, GKE, AKS) to ensure Zarf works across a variety of environments.
   - **Linux Distributions:** We will also expand our testing to include different Linux distributions (e.g., Ubuntu, CentOS, Alpine) to ensure compatibility with various host systems.

3. **Local Testing:**
   - **Local Development Environment:** We will enable the ability to run at least some of the E2E tests locally using tools like `kind` or `minikube`. This will provide quicker feedback loops for developers and reduce the reliance on EC2 instances.
   - **Documentation and Setup Scripts:** We will create detailed documentation and setup scripts to help contributors set up their local testing environment easily.

4. **GitHub Actions:**
   - **Automated Workflows:** We will update our GitHub Actions workflows to include the expanded test coverage. This will include matrix builds to test against multiple Kubernetes and Linux distributions.
   - **Triggering Tests:** We will configure the workflows to automatically run on PRs and on a regular schedule to catch regressions early.

5. **Test Infrastructure:**
   - **CI/CD Pipeline:** We will enhance our CI/CD pipeline to handle the increased load and complexity of the expanded test suite. This may involve optimizing resource usage and parallelizing tests where possible.
   - **Test Reporting:** We will improve test reporting to provide clear and actionable feedback to contributors. This will include detailed logs, test results, and failure analysis.

By implementing these changes, we aim to improve the robustness and reliability of Zarf, while also making it easier for contributors to develop and test new features.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision 
## Decision

We will implement a mechanism within Zarf to create and manage a temporary, in-cluster registry for the purpose of pushing and managing images. This approach will allow Zarf to function in unknown environments without relying on external dependencies or pre-existing registries.

### Key Components of the Solution:

1. **Temporary In-Cluster Registry:**
   - **Creation:** Zarf will create a temporary, in-cluster registry using a lightweight, self-contained solution such as `registry:2` (the official Docker registry image).
   - **Deployment:** The registry will be deployed as a Kubernetes deployment and service, making it accessible within the cluster.
   - **Cleanup:** The temporary registry will be automatically cleaned up after the images have been pushed and the workloads are running.

2. **Image Pushing:**
   - **Push Images to Temporary Registry:** Zarf will push the required images to the temporary in-cluster registry.
   - **Update Workload Definitions:** Zarf will update the Kubernetes workload definitions (e.g., Deployments, StatefulSets) to use the images from the temporary in-cluster registry.

3. **TLS Trust Management:**
   - **Self-Signed Certificates:** The temporary registry will use self-signed certificates.
   - **Trust Injection:** Zarf will inject the self-signed certificate into the CRI configuration on each node. This can be done by:
     - **Node Configuration:** Modifying the node's root certificate authorities.
     - **CRI Configuration:** If the CRI supports it, modifying the CRI's configuration to trust the self-signed certificate.

4. **Node Affinity and Taints:**
   - **Optional Node Affinity/Taints:** If necessary, Zarf can apply node affinity or taints to ensure that the initial bootstrap process runs on a specific node, simplifying the TLS trust management.

### Implementation Steps:

1. **Deploy Temporary Registry:**
   - Create a Kubernetes deployment and service for the temporary registry.
   - Ensure the registry is accessible within the cluster.

2. **Push Images:**
   - Use `docker` or `crictl` to push the required images to the temporary registry.
   - Update the Kubernetes workload definitions to use the images from the temporary registry.

3. **Inject TLS Trust:**
   - Generate a self-signed certificate for the temporary registry.
   - Inject the self-signed certificate into the CRI configuration on each node.

4. **Bootstrap Workloads:**
   - Deploy the updated workloads using the images from the temporary registry.
   - Ensure the workloads are running successfully.

5. **Cleanup:**
   - Once the workloads are running, clean up the temporary registry and its associated resources.

### Benefits:

- **Flexibility:** Zarf can operate in any Kubernetes environment, regardless of whether a registry exists.
- **Simplicity:** No external dependencies or pre-existing infrastructure is required.
- **Security:** The use of self-signed certificates ensures secure communication within the cluster.

### Considerations:

- **Resource Usage:** The temporary registry will consume some resources, but this should be minimal and temporary.
- **Node Access:** Zarf will need appropriate permissions to modify the CRI configuration on each node.

By implementing this solution, Zarf will be able to handle the complexities of image management in Kubernetes clusters, ensuring a smooth and reliable deployment process in any environment.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision 
## Decision

We will implement and maintain a Software Bill of Materials (SBOM) for all software components that run on government hardware. This decision is driven by the need to comply with Executive Order 14028, which mandates SBOMs for software used in federal information systems.

### Implementation Plan

1. **Tool Selection**:
   - We will use tools like `cyclonedx`, `spdx-sbom-generator`, or `syft` to generate SBOMs. These tools are widely adopted and support various formats such as SPDX, CycloneDX, and SWID.
   - For containerized applications, we will use `syft` and `grype` to generate and scan SBOMs.

2. **Integration into CI/CD Pipeline**:
   - Integrate SBOM generation into our continuous integration (CI) and continuous deployment (CD) pipelines.
   - Ensure that SBOMs are generated automatically during the build process and stored in a secure and accessible location.

3. **Format and Standards**:
   - Use the SPDX format for SBOMs, as it is one of the most widely supported and standardized formats.
   - Ensure that the SBOM includes all necessary metadata, such as component names, versions, licenses, and dependencies.

4. **Validation and Verification**:
   - Implement automated checks to validate the completeness and accuracy of the generated SBOMs.
   - Regularly review and update the SBOMs to reflect any changes in the software components.

5. **Documentation and Training**:
   - Provide documentation and training for developers and operations teams on how to generate, validate, and maintain SBOMs.
   - Ensure that all team members understand the importance of SBOMs and their role in maintaining them.

6. **Compliance and Reporting**:
   - Establish a process for regular reporting and compliance checks to ensure that all software running on government hardware has an up-to-date SBOM.
   - Work with legal and compliance teams to ensure that all SBOMs meet the requirements of EO14028 and other relevant regulations.

### Benefits

- **Compliance**: Ensures that we meet the requirements of EO14028 and other federal regulations.
- **Transparency**: Provides a clear and detailed list of all software components, including open-source libraries and their dependencies.
- **Security**: Helps in identifying and mitigating potential security vulnerabilities by providing a comprehensive view of the software supply chain.
- **Traceability**: Facilitates tracking and managing software components throughout their lifecycle.

### Next Steps

- **Tool Evaluation**: Evaluate and select the appropriate SBOM generation tool.
- **Pipeline Integration**: Integrate the selected tool into the CI/CD pipeline.
- **Training and Documentation**: Develop and distribute training materials and documentation.
- **Pilot Program**: Conduct a pilot program to test the SBOM generation and validation process.
- **Full Rollout**: Implement the SBOM generation and management process across all relevant projects.

By following this plan, we will ensure that our software meets the SBOM requirements and maintains the highest standards of security and compliance.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision
## Decision

We will introduce a new Zarf feature to automatically handle the mutation of image paths, secrets, and other references for K8s manifests, even when using secondary gitops tools like Flux or ArgoCD. This will reduce the burden on package authors and ensure consistent and reliable deployment across different environments.

### Key Points:
1. **Automatic Mutation**:
   - Zarf will automatically mutate image paths, secrets, and other references in K8s manifests to use the internal Zarf Registry and other internal services (e.g., Git server).
   - This mutation will be applied during the `zarf prepare` step, ensuring that the manifests are correctly configured before being handed off to gitops tools.

2. **Compatibility with GitOps Tools**:
   - The new feature will be designed to work seamlessly with popular gitops tools such as Flux and ArgoCD.
   - Zarf will generate the necessary configuration files and patches that can be applied by these tools, ensuring that the manifests are correctly mutated and ready for deployment.

3. **Enhanced `zarf prepare` Command**:
   - The `zarf prepare` command will be enhanced to include the automatic mutation of manifests.
   - It will also provide an option to output the mutated manifests, allowing package authors to review and validate the changes if needed.

4. **CRD Support**:
   - The mutation process will support Custom Resource Definitions (CRDs) and other advanced Kubernetes resources.
   - This will ensure that all references to the internal Zarf Registry and other services are correctly updated, even in complex CRDs.

5. **Documentation and Best Practices**:
   - Detailed documentation will be provided to guide package authors on how to use the new feature.
   - Best practices and examples will be included to help package authors understand how to integrate Zarf with their gitops workflows.

6. **Backward Compatibility**:
   - The new feature will be designed to be backward compatible with existing Zarf packages.
   - Package authors who prefer to manually mutate their manifests will still have the option to do so, but the new feature will provide a more streamlined and automated approach.

### Implementation Steps:
1. **Update `zarf prepare` Command**:
   - Enhance the `zarf prepare` command to include the automatic mutation of image paths, secrets, and other references.
   - Add options to output the mutated manifests for review and validation.

2. **Integrate with GitOps Tools**:
   - Develop the necessary hooks and integrations to work with popular gitops tools like Flux and ArgoCD.
   - Generate the required configuration files and patches that can be applied by these tools.

3. **Support for CRDs**:
   - Extend the mutation process to support CRDs and other advanced Kubernetes resources.
   - Ensure that all references to the internal Zarf Registry and other services are correctly updated.

4. **Testing and Validation**:
   - Conduct thorough testing to ensure the new feature works as expected with various gitops tools and Kubernetes resources.
   - Validate the backward compatibility with existing Zarf packages.

5. **Documentation and Training**:
   - Update the documentation to include detailed information on the new feature.
   - Provide training and best practices to help package authors transition to the new approach.

By implementing this new feature, we aim to simplify the deployment process for Zarf packages and ensure that they work seamlessly with gitops tools, reducing the burden on package authors and improving the overall reliability and consistency of deployments.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision 
## Decision

We will simplify the current approach by removing the `stage2` Go binary and instead use a lightweight shell script or a Kubernetes Job to perform the necessary operations. This will reduce the overhead and complexity of the system.

### Key Changes:
1. **Remove `stage2` Go Binary**: The Go binary will be replaced with a shell script or a Kubernetes Job that performs the following tasks:
   - Start a Docker registry in `rw` mode.
   - Push the `registry:2` crane tarball into the registry.
   - Switch the Docker registry to `r` mode.

2. **Reduce ConfigMaps**: Since the `stage2` binary is no longer needed, the number of ConfigMaps required for the operation will be reduced, making the initialization process faster and more efficient.

3. **Simplify Build and CI**: By eliminating the need to compile and manage another binary, the build and CI processes will be simplified, reducing build time and maintenance overhead.

4. **Use Lightweight Solutions**: Instead of using a full-featured Docker registry to host a single image, we will use a more lightweight solution that is better suited for the task. This can be achieved by using a minimal Docker registry or a similar lightweight tool.

### Implementation Steps:
1. **Create Shell Script or Kubernetes Job**:
   - Develop a shell script or a Kubernetes Job that starts the Docker registry in `rw` mode, pushes the `registry:2` crane tarball, and then switches the registry to `r` mode.
   - Ensure the script or job is idempotent and handles any potential errors gracefully.

2. **Update Documentation**:
   - Update the documentation to reflect the new approach and provide clear instructions on how to use the new shell script or Kubernetes Job.

3. **Test and Validate**:
   - Thoroughly test the new implementation to ensure it works as expected and does not introduce any new issues.
   - Validate the performance and efficiency improvements.

By implementing these changes, we aim to streamline the process, reduce overhead, and improve the overall efficiency of the system.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision

We will introduce unit testing in a defined and limited capacity within Zarf. This approach allows us to target specific areas of the codebase that are difficult to test with end-to-end tests, while also managing the maintenance burden of the tests.

### Rationale

1. **Coverage for Critical Areas**:
    - By selectively introducing unit tests, we can ensure that critical and hard-to-test areas of the codebase are covered. This is particularly important as Zarf moves into being a library for other products, where the robustness and reliability of our interfaces become even more crucial.

2. **Balanced Maintenance Burden**:
    - Introducing unit tests in a limited capacity helps manage the maintenance burden. We can focus on writing high-quality, meaningful tests for the most critical parts of the code, rather than trying to cover every line of code, which can lead to a lot of low-value tests.

3. **Alignment with Current Practices**:
    - This approach aligns well with our current testing practices, which already include end-to-end testing. By adding unit tests in a targeted manner, we can complement our existing testing strategy without overcomplicating it.

4. **Flexibility and Adaptability**:
    - This approach provides flexibility. As the codebase evolves, we can add or modify unit tests as needed, ensuring that they remain relevant and effective. It also allows us to adapt to changes in the project's requirements and priorities.

### Implementation Plan

1. **Identify Critical Areas**:
    - Conduct a review of the codebase to identify areas that are difficult to test with end-to-end tests but are critical to the functionality of Zarf. These areas might include complex business logic, core interfaces, and utility functions.

2. **Write Unit Tests**:
    - Write unit tests for the identified areas. Focus on writing tests that are clear, concise, and easy to maintain. Use mocking and stubbing where necessary to isolate the code under test.

3. **Integrate with CI/CD**:
    - Integrate the new unit tests into the CI/CD pipeline to ensure they are run automatically with every commit and pull request. This will help catch issues early and ensure that the tests are always up-to-date.

4. **Documentation and Guidelines**:
    - Document the guidelines for writing and maintaining unit tests. This should include best practices, examples, and a clear explanation of when and how to write unit tests. This documentation will help ensure that all contributors understand the expectations and can write effective tests.

5. **Review and Refine**:
    - Regularly review the unit tests and the overall testing strategy to ensure they continue to meet the project's needs. Be open to feedback and make adjustments as necessary to improve the quality and effectiveness of the tests.

By following this approach, we can enhance the reliability and maintainability of Zarf while keeping the testing strategy manageable and aligned with our current practices.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision 
## Decision

We will adopt option 2: Run Syft against files and dataInjections.

### Rationale:
- **Accuracy and Relevance**: By focusing on files and dataInjections, we ensure that the SBOM includes only the artifacts that are actually part of the Zarf package. This reduces the risk of including extraneous or irrelevant dependencies, such as those from git repositories.
- **Common Use Cases**: Files and dataInjections are the most common ways for users to include additional artifacts in Zarf packages. This approach aligns with the typical use cases and provides a more practical and useful SBOM.
- **Simplicity and Usability**: This method is straightforward to implement and understand, making it easier for both developers and users. It avoids the complexity and potential confusion that could arise from scanning the entire build directory.
- **Balance of Coverage and Noise**: While this approach may not cover every possible way an artifact can be included, it strikes a good balance between providing comprehensive information and avoiding unnecessary noise in the SBOM.

### Implementation Steps:
1. **Identify Files and DataInjections**: During the package build process, identify all files and dataInjections that are being included in the Zarf package.
2. **Run Syft**: Use Syft to generate the SBOM for these identified files and dataInjections.
3. **Integrate SBOM**: Integrate the generated SBOM into the Zarf package metadata, ensuring it is easily accessible to users.

### Future Considerations:
- **User Flexibility**: In the future, we can consider adding an option for users to specify additional directories or files to be included in the SBOM, providing more flexibility while maintaining the core functionality.
- **Monitoring and Feedback**: Monitor user feedback and usage patterns to determine if additional features or adjustments are needed to improve the SBOM generation process.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision

We will introduce a new feature in Zarf that allows for the deployment of Zarf packages in connected environments without the need for a Zarf init package. This feature will be designed to maintain the declarative nature of Zarf while providing flexibility for users in connected environments.

### Key Components of the Decision

1. **Declarative Deployment Specification**:
   - Introduce a new YAML-based specification for deploying Zarf packages directly, without the need for an init package.
   - This specification will allow users to define the desired state of their K8s resources and dependencies in a declarative manner.

2. **Simplified Initialization**:
   - Provide a command or flag (e.g., `zarf deploy --no-init`) that allows users to skip the initialization step and directly deploy the specified Zarf package.
   - Ensure that the necessary dependencies and configurations are handled automatically or through user-provided specifications.

3. **Enhanced CLI and Documentation**:
   - Update the Zarf CLI to support the new deployment method.
   - Provide comprehensive documentation and examples to guide users on how to use this feature in connected environments.

4. **Validation and Error Handling**:
   - Implement validation checks to ensure that the deployment can proceed without an init package.
   - Provide clear error messages and guidance if the deployment cannot proceed due to missing dependencies or other issues.

5. **Backward Compatibility**:
   - Ensure that the new feature does not break existing workflows and remains backward compatible with the current Zarf init package-based deployments.
   - Allow users to choose between the two methods based on their environment and requirements.

### Implementation Steps

1. **Design the New YAML Specification**:
   - Define the structure and fields required for the new YAML-based deployment specification.
   - Ensure it is flexible enough to handle various deployment scenarios and dependencies.

2. **Update the Zarf CLI**:
   - Add the new `--no-init` flag to the `zarf deploy` command.
   - Implement the logic to parse the new YAML specification and deploy the package accordingly.

3. **Automate Dependency Management**:
   - Develop a mechanism to automatically manage and install any required dependencies in the connected environment.
   - Provide options for users to specify additional dependencies or configurations if needed.

4. **Testing and Validation**:
   - Conduct thorough testing to ensure the new feature works as expected in various connected environments.
   - Validate that the feature does not interfere with the existing init package-based deployments.

5. **Documentation and User Guidance**:
   - Create detailed documentation and examples to help users understand and use the new feature.
   - Provide best practices and troubleshooting guides for common issues.

By implementing this decision, Zarf will become more versatile and useful in both disconnected and connected environments, providing a consistent and declarative way to manage K8s deployments.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision 
## Decision

To address the limitations and expand the flexibility of the `scripts` noun in components, we will introduce a more structured and extensible approach. This will allow for more granular control and better support for various use cases. The new structure will include the following enhancements:

1. **Script Phases**: Define distinct phases for scripts to be executed, such as `prepare`, `before`, `after`, and potentially `cleanup` or `on-failure`. This will provide a clear and consistent way to organize script execution.

2. **Script Configuration**: Allow each script to have a configuration block that can include:
   - **Command**: The actual command to be executed.
   - **Environment Variables**: A map of environment variables to be set during the script execution.
   - **Working Directory**: Specify the working directory for the script.
   - **Timeout**: Set a timeout for the script to prevent it from running indefinitely.
   - **Conditionals**: Define conditions under which the script should run (e.g., only if a certain file exists).

3. **Script Types**: Support different types of scripts, such as shell scripts, Python scripts, or even inline commands. This will allow for more flexibility in the types of logic that can be executed.

4. **Script Dependencies**: Allow scripts to specify dependencies on other scripts or components, ensuring that they are executed in the correct order.

5. **Script Validation**: Implement a validation mechanism to ensure that the scripts are well-formed and meet the required criteria before they are executed.

6. **Documentation and Examples**: Provide clear documentation and examples to help users understand how to define and use the new script structure effectively.

### Example Configuration

```yaml
components:
  - name: example-component
    scripts:
      prepare:
        - command: ""echo 'Preparing component...'""
          env:
            EXAMPLE_VAR: ""value""
          workingDir: ""/path/to/working/directory""
          timeout: 30s
      before:
        - command: ""echo 'Before deployment...'""
          condition: ""file_exists('/path/to/some/file')""
      after:
        - command: ""echo 'After deployment...'""
          dependencies:
            - ""prepare[0]""
            - ""before[0]""
```

By implementing these changes, we will provide a more robust and flexible system for managing scripts within components, allowing teams to better leverage the capabilities of the package system.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision 
## Decision

Given the trade-offs between speed, memory usage, and reliability, we will implement a more robust solution for local image support in Zarf. The new approach will use **Crane with the `--no-memory-cache` option** to avoid the OOM (Out of Memory) issues while still providing a reasonable performance.

### Rationale:
- **Memory Usage**: Crane's `--no-memory-cache` option significantly reduces memory usage by avoiding loading the entire tar stream into memory, thus preventing OOM conditions.
- **Performance**: While this option is slower than the current implementation, it is still faster than some of the other methods we tested, such as using the Docker CLI directly.
- **Reliability**: Using Crane with `--no-memory-cache` ensures that Zarf can handle large images without crashing, which is critical for a reliable user experience.
- **Future Compatibility**: This approach aligns with our future plans for OCI imports and avoids issues with legacy formats produced by Docker.

### Implementation Details:
- **Update Zarf to Use Crane with `--no-memory-cache`**: Modify the existing code to include the `--no-memory-cache` flag when using Crane to load images from the local Docker daemon.
- **User Documentation**: Update the documentation to inform users about the change and the potential performance impact. Provide guidance on how to optimize their local development environment to mitigate any slowdowns.
- **Testing and Validation**: Conduct thorough testing to ensure that the new implementation works reliably across different scenarios and environments.

### Performance Considerations:
- **Caching**: Encourage users to leverage caching where possible to reduce the time required for subsequent image loads.
- **Optimization Tips**: Provide tips and best practices for users to optimize their local Docker setup, such as increasing the available memory or using a more powerful machine for development.

By making this change, we aim to provide a more stable and reliable local development experience for Zarf users, even when working with large images.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision

To address the complexities of deploying Big Bang with Zarf, we will implement a multi-step approach that simplifies the process and reduces the storage overhead. The decision is based on the following key points:

1. **Flux Integration**:
   - Ensure that Flux is installed as part of the Zarf package or as a prerequisite before deploying Big Bang. This can be achieved by including Flux in the Zarf package or providing clear documentation and scripts to install Flux.

2. **Image Management**:
   - Instead of including all possible images in the Zarf package, we will dynamically pull only the necessary images based on the `values.yaml` file provided by the user.
   - Implement a mechanism to scan the `values.yaml` file and determine which images are required for the specific deployment. This can be done using a script or a custom tool that parses the `values.yaml` and generates a list of required images.
   - Use a caching mechanism to store frequently used images locally, reducing the need to pull them from remote repositories repeatedly.

3. **Git Repository Management**:
   - Similar to image management, dynamically include only the necessary git repositories based on the `values.yaml` file.
   - Develop a script or tool that scans the `values.yaml` file and determines which git repositories are required for the specific deployment.
   - Use a caching mechanism to store frequently used git repositories locally, reducing the need to clone them from remote repositories repeatedly.

4. **Values Injection**:
   - Simplify the injection of the `values.yaml` file into the Big Bang deployment structure by providing a template or a script that automatically injects the `values.yaml` into the appropriate location.
   - Ensure that the `values.yaml` file is validated and any missing or incorrect values are flagged and handled gracefully.

5. **Documentation and User Experience**:
   - Provide clear and detailed documentation on how to deploy Big Bang with Zarf, including the steps to install Flux, manage images and git repositories, and inject the `values.yaml` file.
   - Offer example `values.yaml` files and best practices for different deployment scenarios to help users get started quickly.

By implementing these steps, we aim to make the deployment of Big Bang with Zarf more efficient, manageable, and user-friendly. This approach will reduce the storage overhead, simplify the deployment process, and provide a better overall experience for end users.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision 
## Decision

We will adopt the OCI (Open Container Initiative) standard for distributing and publishing Zarf packages. This approach leverages existing, well-established infrastructure and tools that are widely used and trusted in the industry, such as Docker registries and other OCI-compliant registries.

### Key Points:
1. **OCI Compliance**: Zarf packages will be published to and fetched from OCI-compliant registries. This allows users to leverage existing registry infrastructure, which is familiar and widely supported.
2. **Package Signing**: We will implement a signing mechanism for Zarf packages. This will use cryptographic signatures to ensure the integrity and authenticity of the packages. The signing process will be integrated into the Zarf CLI, allowing package creators to sign their packages before publishing.
3. **Verification**: Zarf will include built-in verification capabilities to check the signatures of downloaded packages. This ensures that the package has not been tampered with and comes from a trusted source.
4. **User Experience**: The Zarf CLI will be enhanced to support the new publishing and verification features. Users will be able to publish and pull packages using simple commands, and the CLI will handle the signing and verification processes seamlessly.

### Implementation Steps:
1. **Update Zarf CLI**:
   - Add commands to sign and verify Zarf packages.
   - Integrate with OCI registries for publishing and fetching packages.
2. **Documentation**:
   - Provide detailed documentation on how to sign and publish Zarf packages.
   - Include examples and best practices for maintaining package security.
3. **Testing and Validation**:
   - Conduct thorough testing to ensure the new features work as expected.
   - Validate the security and integrity of the signed packages.

By adopting the OCI standard and implementing package signing, we will provide a scalable, secure, and user-friendly way to distribute and publish Zarf packages.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision 
## Decision

To better support airgap development and enhance the flexibility of Zarf's git server capabilities, we will implement the following changes:

1. **Enhance Git Server Support:**
   - Introduce a more generic and flexible git server integration that supports both Flux `GitRepository` objects and native git operations.
   - Ensure that the git server can be used for cloning, pushing, and pulling repositories directly from the command line or through other git clients.

2. **Add Artifact and Library Management:**
   - Integrate a package manager or artifact repository (e.g., Nexus, Artifactory) into Zarf to host and manage dependencies for various coding languages.
   - Provide a seamless way to upload, download, and manage artifacts and libraries within the airgap environment.

3. **Simplify Development Workflow:**
   - Create a streamlined process for developers to set up and use the git server and artifact repository.
   - Provide documentation and examples on how to configure and use these tools in an airgap environment.

4. **Ensure Compatibility and Security:**
   - Ensure that the new features are compatible with existing Zarf configurations and do not introduce security vulnerabilities.
   - Implement robust access controls and authentication mechanisms to secure the git server and artifact repository.

5. **Testing and Validation:**
   - Develop a comprehensive testing strategy to validate the new features in both development and production environments.
   - Engage with the community to gather feedback and make iterative improvements.

By implementing these changes, Zarf will provide a more versatile and developer-friendly solution for managing dependencies and code repositories in airgap environments.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision 
## Decision

To make Zarf a ""Generally Available"" v1.x.x product, we will focus on the following key areas:

1. **Stability and Reliability:**
   - **Versioning and Backwards Compatibility:** Adopt semantic versioning (SemVer) to clearly communicate changes. Major versions (e.g., 1.0.0 to 2.0.0) will include breaking changes, while minor and patch versions (e.g., 1.0.0 to 1.1.0 or 1.0.0 to 1.0.1) will maintain backwards compatibility.
   - **Change Management:** Implement a more structured change management process. Breaking changes will be announced well in advance, and migration guides will be provided to help users transition smoothly.

2. **Enhanced Testing:**
   - **Automated Testing:** Expand the automated testing suite to cover a wider range of scenarios, including edge cases and integration tests. This will help catch issues early and ensure that new features and bug fixes do not break existing functionality.
   - **Continuous Integration/Continuous Deployment (CI/CD):** Integrate CI/CD pipelines to run tests automatically on every commit and pull request. This will help maintain a high level of code quality and reduce the risk of introducing bugs.

3. **Documentation and User Support:**
   - **Comprehensive Documentation:** Improve and expand the documentation to cover all aspects of Zarf, including installation, configuration, usage, and troubleshooting. This will help users understand how to use Zarf effectively and resolve common issues.
   - **User Guides and Tutorials:** Create detailed user guides and tutorials to help new users get up and running quickly. These should include best practices and examples of real-world use cases.
   - **Community and Support Channels:** Establish and maintain active community and support channels (e.g., forums, Slack, GitHub discussions) where users can ask questions, report issues, and share knowledge.

4. **Security and Compliance:**
   - **Regular Security Audits:** Conduct regular security audits and vulnerability assessments to identify and address potential security issues.
   - **Compliance Certifications:** Work towards obtaining relevant compliance certifications (e.g., SOC 2, ISO 27001) to provide assurance to users that Zarf meets industry standards for security and data protection.

5. **User Feedback and Community Involvement:**
   - **Feedback Mechanisms:** Implement mechanisms for users to provide feedback, such as surveys, user testing, and feature requests. This will help us understand user needs and priorities.
   - **Community Contributions:** Encourage and facilitate community contributions by providing clear guidelines for contributing, maintaining an open and welcoming community, and recognizing and rewarding contributors.

By focusing on these areas, we aim to make Zarf a robust, reliable, and user-friendly tool that can be trusted for mission-critical operations.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision 
## Decision

We will introduce a new concept called `hooks` in Zarf to address the issues with ECR and other external registries. These `hooks` will be a part of the Zarf core and will provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. This approach will allow us to handle the specific requirements of ECR (and potentially other external registries) without requiring package creators or deployers to manage these details.

### Key Points of the Decision:

1. **Introduction of `hooks`**:
   - `Hooks` will be a new feature in Zarf that allows cluster maintainers to register custom functionality to run at specific points in the deployment lifecycle.
   - `Hooks` will be system-agnostic and can be used to handle various registry-specific requirements, such as token refresh and repository creation.

2. **Token Refresh**:
   - A `hook` will be created to automatically refresh ECR authentication tokens. This `hook` will be triggered periodically (e.g., every 10 hours) to ensure that the tokens do not expire.
   - The `hook` will be responsible for updating the necessary secrets and configurations in the cluster to keep the tokens up-to-date.

3. **Repository Creation**:
   - Another `hook` will be created to automatically create ECR image repositories before pushing images. This `hook` will be triggered during the package deployment process.
   - The `hook` will check if the required repositories exist and create them if they do not. This ensures that the `push-to-create` issue is resolved without requiring manual intervention.

4. **Integration with Zarf Core**:
   - `Hooks` will be integrated into the Zarf core, making them available to all clusters initialized with Zarf.
   - The `hooks` will be configurable and can be enabled or disabled based on the needs of the cluster maintainer.

5. **User Experience**:
   - Package creators will not need to include ECR-specific `actions` in their packages. They can focus on the core functionality of their packages.
   - Cluster maintainers will have the flexibility to configure `hooks` to handle the specific requirements of their chosen external registry, ensuring a smooth deployment experience.

### Implementation Plan:

1. **Design and Specification**:
   - Define the `hook` interface and the lifecycle events where `hooks` can be registered.
   - Specify the behavior and configuration options for the ECR-specific `hooks`.

2. **Development**:
   - Implement the `hook` framework in the Zarf core.
   - Develop the ECR-specific `hooks` for token refresh and repository creation.

3. **Testing**:
   - Thoroughly test the `hooks` to ensure they work correctly in various scenarios.
   - Validate that the `hooks` do not interfere with the normal operation of Zarf and its packages.

4. **Documentation**:
   - Update the Zarf documentation to include information about `hooks` and how to configure them.
   - Provide examples and best practices for using `hooks` with different external registries.

By introducing `hooks`, we aim to provide a flexible and robust solution that enhances the Zarf ecosystem while maintaining the simplicity and portability of Zarf packages.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision 
## Decision

We will use gRPC to decouple transformations from Zarf and synchronize the TypeScript code with the Go code.

### Rationale

1. **Performance and Efficiency**:
   - gRPC is designed for high-performance communication, using HTTP/2 for transport and Protocol Buffers (protobuf) for message serialization. This makes it efficient in terms of both bandwidth and latency.
   - WASM, while powerful, introduces additional complexity and overhead, especially for a use case that primarily involves inter-service communication.
   - REST and JSON, while widely used, are less efficient compared to gRPC due to the overhead of JSON serialization and HTTP/1.1.

2. **Strong Typing and Contract Definition**:
   - gRPC uses Protocol Buffers, which provide a strong, language-agnostic contract definition. This ensures that the Go and TypeScript code can be kept in sync more easily.
   - The generated code from protobuf definitions can be used directly in both Go and TypeScript, reducing the risk of discrepancies between the two implementations.

3. **Simplicity and Maintainability**:
   - gRPC simplifies the process of maintaining a single, consistent API across different languages. This is particularly important as Pepr takes on more responsibilities from the Zarf Agent.
   - REST and other HTTP-based solutions would require more boilerplate code and manual handling of data serialization and deserialization.

4. **Scalability**:
   - gRPC's design supports bidirectional streaming, which can be useful for more complex interactions in the future.
   - The ability to add new services and methods without breaking existing clients is a significant advantage of gRPC.

### Implementation Plan

1. **Define Protobuf Messages and Services**:
   - Define the necessary protobuf messages and service definitions to cover the transformation logic.
   - Ensure that the protobuf definitions are comprehensive and well-documented to facilitate easy understanding and maintenance.

2. **Generate Code**:
   - Use the `protoc` compiler to generate Go and TypeScript code from the protobuf definitions.
   - Integrate the generated code into the respective projects (Zarf and Pepr).

3. **Implement gRPC Server and Client**:
   - Implement the gRPC server in Go, which will handle the transformation logic.
   - Implement the gRPC client in TypeScript, which will call the gRPC server to perform transformations.

4. **Testing and Validation**:
   - Write comprehensive tests to ensure that the gRPC communication works as expected.
   - Validate the performance and reliability of the gRPC-based solution in a staging environment before rolling it out to production.

5. **Documentation and Training**:
   - Document the new gRPC-based architecture and provide training for the development team to ensure smooth adoption.
   - Update any relevant documentation and developer guides to reflect the changes.

By adopting gRPC, we can achieve a more efficient, maintainable, and scalable solution for decoupling transformations from Zarf and synchronizing the TypeScript and Go code.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision 
## Decision

To address the inconsistencies in Zarf's package handling, we will standardize and enhance the functionality for all supported package sources. This will ensure that all package types can be created, deployed, inspected, and removed consistently. The following changes will be implemented:

1. **Split Tarball Packages:**
   - **Inspection:** Add support for inspecting split tarball packages.
   - **Removal:** Add support for removing split tarball packages.

2. **In-Cluster (Deployed) Packages:**
   - **Inspection:** Add support for inspecting in-cluster packages by name.
   - **Removal:** Ensure that in-cluster packages can be removed by name.

3. **HTTPs URLs:**
   - **Inspection:** Add support for inspecting packages from HTTPs URLs.
   - **Removal:** Add support for removing packages deployed from HTTPs URLs.

4. **General Enhancements:**
   - **Consistent CLI Interface:** Ensure that the CLI commands for creating, deploying, inspecting, and removing packages are consistent across all package types.
   - **Error Handling and Feedback:** Improve error handling and provide clear feedback to users when performing package operations.
   - **Documentation:** Update the documentation to reflect the new and enhanced capabilities, including examples and best practices.

By implementing these changes, Zarf will provide a more robust and user-friendly experience, ensuring that all package types are handled consistently and efficiently.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision 
## Decision

To address the issues with Zarf's composability and import chains, we will implement the following changes:

1. **Refactor Import Chain Handling:**
   - **Centralize Import Logic:** Move the import chain logic to a centralized location to ensure consistency and maintainability.
   - **Enhance Test Coverage:** Improve test coverage to include deeper import chains, ensuring that all layers are properly tested and validated.
   - **Optimize Performance:** Optimize the import chain handling to handle larger numbers of layers efficiently.

2. **Integrate OCI Skeletons:**
   - **Rethink Composability:** Re-evaluate how OCI skeletons are integrated into the composability model. Ensure that they are seamlessly integrated and do not break the existing composability features.
   - **Update Documentation:** Provide clear documentation on how OCI skeletons interact with the composability features, including examples and best practices.

3. **Centralize Component Filtering:**
   - **Implement Centralized Filtering:** Move the `only` filter implementation to a central location to ensure consistent behavior across all components.
   - **Improve Filter Logic:** Enhance the filter logic to handle create-time filters more robustly, ensuring that only the specified components are included in the final package.
   - **Add Unit Tests:** Add unit tests to cover the `only` filter functionality, ensuring that it works as expected in various scenarios.

By implementing these changes, we aim to improve the robustness, performance, and usability of Zarf's composability features, making it easier for package creators to build and manage complex packages.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision 
## Decision

To improve the package development lifecycle and reduce the time and effort required for debugging and testing, we will introduce a new `zarf package dev` command. This command will be designed to streamline the development process in connected environments, making it easier to iterate on `zarf.yaml` without the need to reset the cluster or repeatedly create and deploy the package.

### Key Features of `zarf package dev`:

1. **Automatic Rebuild and Redeploy**:
   - The `zarf package dev` command will automatically rebuild and redeploy the package whenever changes are detected in the `zarf.yaml` file.
   - This will eliminate the need to manually run `zarf package create` and `zarf package deploy` after each change.

2. **Incremental Builds**:
   - The command will support incremental builds, meaning that only the necessary components will be rebuilt and redeployed. This will significantly reduce the time required for each iteration.
   - Dependencies that have not changed will not be re-fetched, further optimizing the build process.

3. **Cluster Management**:
   - The `zarf package dev` command will manage the cluster state, ensuring that the cluster is properly initialized and ready for deployment.
   - If the cluster needs to be reset, the command will handle this automatically, but it will do so in a way that minimizes the impact on the development workflow.

4. **Integration with `YOLO` Mode**:
   - The `zarf package dev` command will leverage the existing `YOLO` mode to skip certain heavy dependencies (like Docker images) during development.
   - This will make the development process faster and more efficient, especially for small changes.

5. **CLI Flags**:
   - The `zarf package dev` command will be exposed via CLI flags, making it easy for developers to use.
   - For example: `zarf package dev --watch <dir>`

### Implementation Steps:

1. **Add `zarf package dev` Command**:
   - Implement the `zarf package dev` command in the Zarf CLI.
   - Ensure that the command can watch for changes in the `zarf.yaml` file and trigger a rebuild and redeploy as needed.

2. **Incremental Build Logic**:
   - Add logic to the build process to determine which components need to be rebuilt and which can be reused.
   - Integrate this logic with the `zarf package dev` command.

3. **Cluster Management**:
   - Implement cluster management within the `zarf package dev` command to handle initialization and reset as needed.
   - Ensure that the command can detect and handle cluster state changes gracefully.

4. **Integrate with `YOLO` Mode**:
   - Extend the `YOLO` mode to work seamlessly with the `zarf package dev` command.
   - Ensure that the `YOLO` mode is used by default in the `zarf package dev` command to speed up the development process.

5. **Documentation and Testing**:
   - Update the documentation to include information about the new `zarf package dev` command.
   - Write tests to ensure that the command works as expected and handles various edge cases.

By introducing the `zarf package dev` command, we aim to provide a more efficient and user-friendly development experience for Zarf users, reducing the time and effort required to iterate on their packages.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision 
## Decision

Migrate the documentation site from Docusaurus 2 to a more performant and flexible static site generator, such as Next.js with MDX support.

### Rationale:
- **Performance**: Next.js is known for its performance optimizations, including automatic code splitting, server-side rendering (SSR), and static site generation (SSG). This will lead to faster load times and a better user experience.
- **Flexibility**: Next.js offers more flexibility in terms of customization and integration with other tools and libraries. This will allow us to add more interactive features and improve the overall design of the documentation site.
- **Dependency Management**: Next.js has a more robust and modern dependency management system, which can help reduce the issues we've been facing with Docusaurus 2.
- **Community and Ecosystem**: Next.js has a large and active community, which means better support, more plugins, and a wealth of resources for development and maintenance.

### Implementation Steps:
1. **Setup Next.js Project**:
   - Initialize a new Next.js project.
   - Configure the project to use MDX for markdown-based content.

2. **Content Migration**:
   - Migrate existing Docusaurus content (markdown files) to the new Next.js project.
   - Ensure that all links, images, and other assets are correctly referenced.

3. **Theme and Design**:
   - Replicate or improve the current theme and design using Next.js.
   - Ensure that the new site is responsive and accessible.

4. **Custom Scripts and Plugins**:
   - Port any custom scripts or plugins used in the Docusaurus setup to the Next.js environment.
   - Integrate any necessary third-party libraries or tools.

5. **Testing and Validation**:
   - Thoroughly test the new site to ensure that all pages, links, and features work as expected.
   - Validate the performance improvements and ensure that the site is SEO-friendly.

6. **Deployment**:
   - Set up a deployment pipeline for the new Next.js site.
   - Update DNS and other configurations to point to the new site.

7. **Documentation and Training**:
   - Update internal documentation to reflect the new setup and processes.
   - Provide training or documentation for team members on how to contribute to the new site.

By migrating to Next.js, we aim to address the current limitations and provide a more robust, performant, and maintainable documentation site.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision 
## Decision

To address the issues with the current in-house logging solution and improve the overall logging experience, we will adopt a more robust and flexible logging framework. Specifically, we will transition to using a well-established logging library such as `logrus` or `zap`, which are widely used and provide better support for structured logging, different log levels, and thread safety.

### Key Changes and Improvements:

1. **Logging Library Transition**:
   - Replace the in-house logging solution with `logrus` (or `zap`).
   - `logrus` is chosen for its simplicity and ease of use, while `zap` is known for its performance and flexibility. We will evaluate both and choose the one that best fits our needs.

2. **Log Levels and Output**:
   - Define and use standard log levels: `DEBUG`, `INFO`, `WARN`, `ERROR`, and `FATAL`.
   - Ensure that `FATAL` logs output to STDERR and exit the program, but do so in a way that allows for proper context handling and clean-up.
   - Use `ERROR` for non-fatal errors that should be logged but not cause the program to exit.

3. **Context Handling and Clean-Up**:
   - Implement a deferred function or a context manager to handle clean-up and context management before exiting the program.
   - Example:
     ```go
     func main() {
         defer func() {
             if r := recover(); r != nil {
                 log.Errorf(""Recovered from panic: %v"", r)
                 // Perform any necessary clean-up here
             }
         }()
         
         // Main program logic
         if err := someFunction(); err != nil {
             log.Fatalf(""Fatal error: %v"", err)
         }
     }
     ```

4. **Thread Safety**:
   - Ensure that the logging library is thread-safe and can handle concurrent access.
   - Remove the global state for components like the progress bar and spinner, and instead pass them as parameters or use a context to manage their state.

5. **Disabling Logging Components**:
   - Provide configuration options to disable specific logging components (e.g., progress bar, spinner) via environment variables or command-line flags.
   - Example:
     ```go
     var (
         showProgressBar = flag.Bool(""show-progress-bar"", true, ""Show the progress bar"")
     )

     func main() {
         flag.Parse()
         if *showProgressBar {
             // Initialize and use the progress bar
         } else {
             // Disable the progress bar
         }
     }
     ```

6. **Unit Testing**:
   - Use a logger that can be easily mocked or replaced during unit testing.
   - Example:
     ```go
     type Logger interface {
         Debugf(format string, args ...interface{})
         Infof(format string, args ...interface{})
         Warnf(format string, args ...interface{})
         Errorf(format string, args ...interface{})
         Fatalf(format string, args ...interface{})
     }

     var log Logger = &logrus.Logger{}

     func TestSomeFunction(t *testing.T) {
         mockLogger := &MockLogger{}
         log = mockLogger
         // Run tests
     }
     ```

### Implementation Steps:

1. **Evaluate and Choose a Logging Library**:
   - Evaluate `logrus` and `zap` based on performance, features, and ease of use.
   - Select the most suitable library for our needs.

2. **Refactor Existing Code**:
   - Replace all instances of the in-house logging solution with the new logging library.
   - Update log level usage and ensure proper context handling and clean-up.

3. **Add Configuration Options**:
   - Add command-line flags and environment variables to control logging behavior and component visibility.

4. **Update Unit Tests**:
   - Refactor unit tests to use the new logging library and mock it where necessary.

By making these changes, we will have a more robust, flexible, and maintainable logging solution that enhances the user experience and simplifies development and testing.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision 
## Decision

To ensure a stable and predictable experience for Zarf users, we will introduce an explicit schema versioning system starting with the v1 release. This will allow us to clearly define and manage the lifecycle of features and deprecations.

### Key Points:
1. **Schema Versioning**:
   - Introduce a `schemaVersion` field in the `zarf.yaml` file to explicitly define the version of the schema being used.
   - The initial version will be `1.0.0` for the v1 release.

2. **Deprecation Management**:
   - For deprecated keys, continue to warn users during the `create` process but do not automatically delete these keys from the packaged `zarf.yaml`.
   - Maintain backward compatibility for deprecated keys within the v1 schema. Deprecated keys will remain functional until the next major version (v2).

3. **Automatic Migrations**:
   - Implement automatic migrations for deprecated keys that can be directly mapped to their replacements (e.g., `setVariable` to `setVariables`).
   - These migrations will occur during the `create` process, but the original deprecated key will still be preserved in the packaged `zarf.yaml` to ensure compatibility with older Zarf versions.

4. **Documentation and Communication**:
   - Clearly document all deprecated keys and their replacements in the Zarf documentation.
   - Provide a migration guide for users to update their `zarf.yaml` files to the latest schema.
   - Communicate deprecation plans and timelines through release notes and community channels.

5. **Long-term Support**:
   - Establish a policy that deprecated features will remain supported throughout the v1 lifecycle.
   - Plan for a clear transition strategy for the next major version (v2), including a defined deprecation period and end-of-life dates for deprecated features.

### Example `zarf.yaml` with Schema Version:

```yaml
schemaVersion: 1.0.0
# Other configuration fields...
```

### Example Migration Process:

1. **Detection of Deprecated Keys**:
   - During the `create` process, detect and log any use of deprecated keys.
   - Warn the user about the deprecation and provide guidance on how to update the `zarf.yaml`.

2. **Automatic Migrations**:
   - If a deprecated key can be automatically migrated, perform the migration and include both the old and new keys in the packaged `zarf.yaml`.
   - For example, if `setVariable` is used, it will be converted to `setVariables` and both keys will be included in the output.

3. **User Notification**:
   - Notify the user that the deprecated key has been migrated and provide instructions on how to update their `zarf.yaml` to remove the deprecated key.

By implementing these changes, we aim to provide a more robust and predictable experience for Zarf users, while also ensuring that deprecated features are managed in a transparent and controlled manner.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
