context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision: Implement Native Lazy Loading for Images in Storefront

### Status
Accepted

### Context
Currently, the images and thumbnails in the Storefront do not utilize any lazy loading mechanism, leading to unnecessary bandwidth usage and slower initial page load times‚Äîespecially on pages with many product images. While third-party libraries like ""lazysizes"" have been used to address this, modern browsers now widely support native lazy loading via the `loading=""lazy""` attribute on `<img>` elements (supported in Chrome, Firefox, Edge, and others). According to CanIUse, native lazy loading has >95% global browser support.

Relying on a third-party extension solely for lazy loading introduces additional JavaScript overhead, maintenance burden, and potential compatibility issues, which is no longer justified given the availability of a standardized, performant native alternative.

### Decision
We will implement native image lazy loading by adding the `loading=""lazy""` attribute to all relevant `<img>` tags within the Storefront (e.g., product thumbnails, category images, banners). This includes both static and dynamically rendered images.

Example:
```html
<img src=""product-thumb.jpg"" alt=""Product"" loading=""lazy"" />
```

For older browsers that do not support native lazy loading (unsupported), the attribute will be ignored gracefully, and images will load as normal‚Äîproviding a safe progressive enhancement approach.

We will **not** introduce or depend on third-party lazy loading libraries (e.g., lazysizes) unless advanced features (like responsive image support with `data-srcset` or fade-in effects) become required in the future. At that point, a new decision can be made.

### Consequences
**Pros:**
- Improved page load performance and reduced initial payload.
- Reduced bandwidth consumption for users, especially on mobile.
- No added JavaScript bundle size.
- Simple implementation and low maintenance.
- Graceful degradation in unsupported browsers.

**Cons:**
- Limited control over loading behavior compared to JavaScript-based solutions (e.g., custom thresholds, placeholders).
- No support for lazy loading background images (CSS) using this method.
- Older browsers (< Chromium 76, < Firefox 75) will not benefit from lazy loading.

### Actions
1. Audit all `<img>` elements in the Storefront templates/components.
2. Add `loading=""lazy""` to non-critical images below the fold (avoid applying to hero/above-the-fold images).
3. Test behavior across supported and unsupported browsers.
4. Document the pattern for future development.

This decision aligns with modern web performance best practices and leverages platform-native capabilities to improve user experience with minimal cost.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","**Decision: Disable `autoload` by default for `OneToOneAssociationField` and `ManyToOneAssociationField`, and require explicit loading of associations via API requests or repository calls.**

---

### **Context**
The use of `autoload => true` in `OneToOneAssociationField` and `ManyToOneAssociationField` leads to automatic eager loading of associated entities in every query that accesses the parent entity. While this may simplify data access in some scenarios, it results in significant performance overhead:

- **Unnecessary JOINs** in SQL queries, degrading database performance.
- Increased memory usage due to hydration of unused data.
- Larger API response payloads, affecting network transfer times and client performance.
- Poor scalability as more associations are added with `autoload`.

This behavior violates the principle of lazy loading by default and encourages inefficient data access patterns.

---

### **Decision**
We will adopt a strict **explicit loading** strategy:
- Set `autoload => false` as the default and only recommended practice.
- Associations must be explicitly loaded using:
  - `includes` in API requests (e.g., via URL parameters like `?includes[entity]=property`).
  - `addAssociation()` in repository criteria within the application code.
- The use of `autoload => true` is **discouraged** and **forbidden in new code** unless an exceptional, well-documented performance or usability justification exists.

---

### **Rationale**
- **Performance**: Avoids unnecessary joins, reduces query complexity, and improves response times.
- **Scalability**: Enables fine-grained control over data loading, essential in complex domain models.
- **Transparency**: Makes data-fetching intentions explicit in code and API usage.
- **Best Practice**: Aligns with modern ORM patterns (e.g., Doctrine‚Äôs lazy-loading-by-default approach).

---

### **Consequences**
- **Positive**:
  - Improved application and database performance.
  - Smaller, faster API responses.
  - More predictable and auditable data access patterns.
- **Negative**:
  - Slight increase in code verbosity when associations are needed.
  - Risk of N+1 queries if developers forget to eager-load required associations (mitigated via code reviews and monitoring).

---

### **Migration Plan**
1. Audit existing uses of `autoload => true`.
2. Replace with explicit loading in affected controllers/services.
3. Update documentation and provide examples for correct association loading.
4. Add static analysis rules or coding standards to flag new uses of `autoload => true`.

---

### **Status**
Accepted

### **Date**
2025-04-05

### **Authors**
Architecture Team","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","**Architecture Decision Record (ADR)**

---

### **Title**
Adopt a Flat File Structure with Metadata and Index Page for Improved ADR Discoverability

### **Status**
Accepted

### **Context**
Our current ADRs are organized in a folder hierarchy based on architectural areas (e.g., `security/`, `data/`, `ui/`). While this structure supports discovery by domain area (use case 3), it introduces friction for other common use cases:

1. **Searching by title or keyword**: Users may encounter duplicate or scattered results due to fragmented paths, making search less efficient.
2. **Discovering recent ADRs**: There is no centralized view of the latest decisions; users must inspect Git history manually via CLI or IDE, which is not user-friendly.
3. **Cross-cutting concerns**: Some ADRs span multiple areas and don‚Äôt fit cleanly into one folder.

Additionally, as the number of ADRs grows, usability becomes more critical for onboarding, auditing, and governance.

### **Decision**
We will transition from a hierarchical folder structure to a **flat file structure** where all ADRs are stored in a single directory, using a consistent naming convention and including metadata in each document. Additionally, we will generate and maintain a **centralized `README.md` index page** that provides:
- A chronological list of all ADRs (newest first)
- Links grouped by area/topic
- Searchable titles and keywords
- Status (e.g., proposed, accepted, deprecated)

Each ADR file will follow the format:
```
YYYYMMDD-title-of-decision.md
```
Example: `20241015-use-openapi-for-api-contracts.md`

Each ADR will include front-matter or header metadata such as:
```markdown
---
date: 2024-10-15
area: api
status: accepted
authors:
  - Jane Doe
  - John Smith
related: 
  - 20240910-design-first-api-strategy.md
---
```

The root `adr/README.md` will be manually or automatically updated to list:
- ‚úÖ **Latest ADRs** (last 5‚Äì10 entries)
- üîó **All ADRs by Date**
- üè∑Ô∏è **ADRs by Area/Tag**
- üîç **How to Search** (tips for text/Git search)

Optionally, a simple script may be added to regenerate the index on change.

### **Rationale**
- **Flat structure improves discoverability**: All ADRs are equally accessible; no need to guess the correct subdirectory.
- **Date-prefix enables sorting**: Files appear in chronological order in file browsers and Git diffs.
- **Index page supports multiple access patterns**: Serves users looking for recent changes, specific topics, or full lists.
- **Metadata enables filtering and tooling**: Future tools (e.g., ADR dashboards) can parse structured data.
- **Low overhead**: No complex tooling required‚Äîworks with plain Git, GitHub, and static site rendering.

This approach aligns with practices used in well-known open-source projects (e.g., [ThinkRelevance ADR template](https://github.com/thoughtbot/adr)) and scales better than nested folders.

### **Implications**
- **Pros**:
  - Easier navigation and search across all ADRs.
  - Better support for discovering recent decisions.
  - Simpler automation and tooling integration.
  - Encourages consistent metadata usage.
- **Cons**:
  - Loss of visual grouping via folders (mitigated by index and tags).
  - Initial migration effort to flatten and rename files.
  - Requires discipline to keep the index updated (can be automated later).

### **References**
- https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
- https://github.com/joelparkerhenderson/architecture-decision-record
- https://adr.github.io/

### **Decision Makers**
- Lead Architect: Alex Rivera  
- Tech Leads: Sam Chen, Taylor Kim  
- Date: 2024-10-15","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision: Retain Runtime Component Extension Model with Incremental Adoption of Native Vue Patterns

### Status
Accepted

### Context
As described, the current plugin extension system in our administration leverages a custom **Component Factory** that enables runtime generation and modification of Vue components. This model allows plugins to extend or override both structure (via Twig.JS templates) and behavior (via factory-wrapped logic), providing deep extensibility across the application.

While this approach has proven powerful for plugin developers, it comes with significant technical limitations:
- Inability to leverage native Vue tooling (e.g., Volar, ESLint plugins, type checking, template validation).
- Poor performance due to client-side template compilation (Twig.JS + dynamic Vue rendering).
- Challenges in upgrading Vue versions safely due to tight coupling with internal runtime mechanics.
- Lack of static analysis and compile-time optimizations.

To address these issues, we explored a new architecture based on **native Vue 3 patterns**, including:
- `<sw-block>` custom elements for declarative template overrides.
- Composition API with `useExtensibility()` hooks to allow interception and extension of component logic.
- Plugin-driven component extension using `useComponentExtender()` and prop merging utilities.

However, during prototyping, we encountered critical roadblocks:
1. **Vue‚Äôs reactivity and compilation optimizations** (e.g., hoisting, tree-shaking, ref unwrapping) make it extremely difficult to introspect or merge setup state from one component into another.
2. The **lack of public APIs for component introspection** means accessing a component‚Äôs `setup()` return values, refs, or computed properties requires relying on undocumented internals.
3. Any solution depending on internal Vue structures (e.g., `_props`, `$setup`, render context) would be **fragile and incompatible with future Vue updates**, violating our long-term maintainability goals.
4. Template-level `<sw-block>` replacement added complexity without solving the core issue of **logic composition**, leading to inconsistent extension semantics between UI and behavior.

Additionally, attempting to ""unwrap"" compiled SFCs for extension breaks the encapsulation guarantees of Vue Single File Components and undermines build-time tooling benefits.

---

### Decision
We will **retain the existing runtime Component Factory model** for now but shift focus toward an incremental strategy that gradually increases compatibility with native Vue development practices. Specifically:

1. **Deprecate Twig.JS for Templates**: Replace all template-level extensions using Twig.JS with **precompiled native Vue SFCs**. Plugins may provide full `.vue` files as replacements for base components via manifest declarations.

   Example:
   ```js
   // plugin.js
   Shopware.Component.override('sw-hello-world', {
       template: '<div>Hello from Plugin!</div>'
   });
   ```
   evolves into:
   ```js
   // main.ts
   import MyHelloWorld from './components/sw-hello-world-override.vue';
   Shopware.Component.register('sw-hello-world', MyHelloWorld);
   ```

2. **Introduce Optional Native SFC Support**: Allow plugins to register entirely new views or override existing ones using standard `.vue` files. These are precompiled at build time and opt-out of runtime template compilation.

3. **Build a Safe, Public Extension Hook System**: Instead of trying to merge arbitrary setup state, expose well-defined **extension points within business logic** using composables:
   ```ts
   // In core component
   const { extendedProps, extendedMethods } = useComponentExtensions('sw-hello-world', {
       props: { /* default */ },
       methods: { /* default */ }
   });
   ```

   This avoids introspection while enabling controlled extensibility.

4. **Develop a Build-Time Analysis Tool**: Create a linter/static analyzer that validates plugin overrides, detects potential conflicts, and ensures adherence to supported patterns ‚Äî improving developer experience without requiring runtime hacks.

5. **Plan for Long-Term Micro-Frontend Architecture**: Investigate splitting the admin into **independently mountable micro-apps** per module, reducing the need for fine-grained component overriding and allowing each part to evolve independently with its own Vue version and tooling.

---

### Consequences

**Positive**
- Maintains backward compatibility with existing plugins.
- Enables gradual migration toward modern Vue tooling.
- Improves performance by eliminating client-side template compilation where possible.
- Paves the way for better IDE support and type safety in plugin development.
- Avoids reliance on private Vue APIs, ensuring upgrade safety.

**Negative**
- Full component replacement may lead to duplication if only small parts are changed.
- Fine-grained extension (e.g., adding one method or watcher) becomes more verbose.
- Requires changes to plugin build pipelines to support `.vue` file compilation.
- Transition period will involve supporting two parallel extension models.

---

### Alternatives Considered

| Option | Reason Rejected |
|------|----------------|
| Fully adopt SFC-based extension with deep merging | Not feasible due to Vue compiler optimizations and lack of safe introspection APIs |
| Use Vite + HMR for dynamic plugin loading | Too complex; doesn't solve server-side distribution and production build requirements |
| Proxy-based property interception at runtime | Unreliable with reactivity transforms and leads to memory leaks |
| Fork Vue to expose internal component state | Unsustainable, insecure, and violates open-source ethics |

---

### Next Steps
1. Finalize RFC for `Shopware.Component.register()` supporting `.vue` files.
2. Implement deprecation path for Twig.JS in templates.
3. Design composable-based `useExtensionPoint()` API.
4. Release documentation and codemods for plugin authors.
5. Begin PoC for micro-frontend boundaries in administration modules.

---

This decision prioritizes **stability, sustainability, and forward compatibility** over idealized extensibility, aligning our architecture with mainstream Vue ecosystem evolution while preserving the flexibility essential to our plugin-driven platform.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","**Architectural Decision Record (ADR)**

---

### **Title**
Adopt Per-Change Changelog Fragments in Version-Controlled Directory

### **Status**
Accepted

### **Context**
In a trunk-based development model with frequent integrations and multiple contributors, maintaining a single monolithic `CHANGELOG.md` file leads to several pain points:

- Frequent **merge conflicts**, especially when multiple developers commit changes simultaneously.
- **Uncertainty about version assignment**, as changes are often made before the release version is finalized.
- **Desynchronization between code state and issue tracking systems** (e.g., Jira), making it hard to trace what changed and why.

Currently, changelog entries are manually added to a central file under a ""Unreleased"" or future version header. This approach does not scale well with team size or release frequency.

Alternatives considered:
- Leaving changelog updates to release time (leads to omissions).
- Using automated tools based solely on commit messages (lack of structure and detail).
- Maintaining a single file with stricter conventions (still suffers from merge issues).

### **Decision**
We will adopt the **changelog fragments** pattern, also known as **""per-change changelog entries""** or **""towncrier-style""** changelogs.

Each pull request that introduces a user-impacting change must include a corresponding fragment file in a designated directory (e.g., `changelog/` or `.changelog.d/`). These fragments are plain text files written in a standardized format and grouped by change type.

**Key aspects of the decision:**

1. **Directory Structure**: Create a `.changelog.d/` directory at the project root.
2. **Fragment Format**: Each file must follow the naming convention: `<PR-number>.<type>.md`, where `<type>` is one of:
   - `feature` ‚Äì new functionality
   - `bugfix` ‚Äì bug fixes
   - `breaking` ‚Äì breaking changes
   - `upgrade` ‚Äì upgrade instructions
   - `misc` ‚Äì minor docs, tooling, etc.
3. **Content Requirements**: The fragment must contain a concise, user-focused description of the change, suitable for inclusion in a public changelog.
4. **Automation**: A build/release script will automatically:
   - Collect and sort fragment files during release preparation.
   - Generate a consolidated `CHANGELOG.md` entry for the new version.
   - Remove used fragments (or move to an archive) post-release.
5. **CI Enforcement**: Pull requests must include a changelog fragment unless explicitly labeled `skip-changelog`. CI checks will enforce this rule.

Example fragment (`./.changelog.d/123.feature.md`):
```markdown
Added support for JWT token refresh in authentication service.
```

### **Rationale**
- ‚úÖ **Eliminates merge conflicts**: Developers work in isolated files.
- ‚úÖ **Decouples writing from releasing**: Changes can be documented immediately, without knowing the final version.
- ‚úÖ **Improves traceability**: Each fragment links to a PR and potentially a Jira issue.
- ‚úÖ **Enables automation**: Reduces human error during release.
- ‚úÖ **Encourages discipline**: Forces changelog updates to be part of the development workflow.
- ‚úÖ **Supports trunk-based development**: Aligns well with short-lived branches and continuous integration.

Tools like [Towncrier](https://github.com/twisted/towncrier), [reno](https://docs.openstack.org/reno/latest/), or custom scripts can be used to implement the generation step.

### **Implications**
- **Positive**:
  - Cleaner Git history with fewer conflicts.
  - More accurate and timely changelogs.
  - Better audit trail for compliance and release notes.
- **Negative**:
  - Slight increase in PR overhead (one additional small file).
  - Need to maintain tooling for fragment collection and cleanup.
  - Learning curve for new developers (mitigated by templates and CI feedback).

### **Related Decisions**
- [Automated Release Pipeline](#)
- [Pull Request Template Includes Changelog Reminder](#)

### **References**
- Towncrier: https://github.com/twisted/towncrier
- Reno: https://docs.openstack.org/reno/latest/
- ""Keeping a changelog"": https://keepachangelog.com/

--- 

*Proposed by: [Your Name]*  
*Date: 2024-04-15*","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","**ADR: Implement State Snapshot and Rule Revalidation Mechanism for Delayed Actions in Flow Builder**

---

### **Decision**

To support the `DelayAction` in Flow Builder while addressing data staleness and rule validity after delay, we will:

1. **Capture a versioned snapshot of relevant business data and rules at the time the `DelayAction` is initiated.**
2. **Store this snapshot in a durable, structured format (e.g., JSON in a persistent store) alongside metadata such as TTL, flow context, and origin event.**
3. **Upon expiration of the delay, revalidate the associated flow rules against current system state *and* evaluate them against the stored snapshot.**
4. **Proceed with downstream actions only if:**
   - The referenced rules still exist and are active.
   - Optional: Introduce a configurable policy (`strict` vs `lenient`) to determine behavior on rule deletion or data mismatch.
5. **Use an asynchronous job processor (e.g., scheduled task queue like RabbitMQ delayed messages, AWS SQS with visibility timeout, or Quartz scheduler) to resume execution after the delay.**

---

### **Rationale**

- **Problem 1: Outdated Event Data**  
  By capturing a snapshot of the business data relevant to the flow at delay initiation, we preserve the context needed for consistent downstream processing. This ensures actions execute based on coherent, historically accurate input.

- **Problem 2: Stale or Missing Rules**  
  Revalidating rules upon wake-up ensures that only valid, currently defined logic is applied. A snapshot of the rule configuration (or its version reference) allows us to detect changes or deletions and respond accordingly.

- **Consistency & Predictability**  
  Without snapshots, delayed actions risk acting on inconsistent or invalid states, leading to unpredictable behavior or errors. Snapshots provide deterministic execution contexts.

- **Scalability & Durability**  
  Using a message broker or job scheduler with persistence ensures reliability across restarts and scales well under load.

- **Flexibility via Policy Configuration**  
  Allowing `strict` mode (fail on missing/changed rules) or `lenient` mode (proceed with best-effort using snapshot) supports different use cases‚Äîcritical workflows vs. best-effort notifications.

---

### **Implications**

- ‚úÖ **Pros**
  - Ensures correctness and consistency of delayed executions.
  - Enables auditability through stored snapshots.
  - Supports dynamic environments where data and rules evolve over time.
  - Decouples delay duration from system uptime via external scheduling.

- ‚ùå **Cons**
  - Increases storage requirements due to snapshot retention.
  - Adds complexity in managing snapshot lifecycle (TTL, cleanup).
  - Potential performance overhead during snapshot serialization/deserialization.
  - Risk of processing obsolete data if not paired with proper business logic policies.

- üîß **Operational Considerations**
  - Implement snapshot cleanup jobs based on TTL.
  - Monitor delayed action backlog and failure rates.
  - Log revalidation outcomes for observability.

- üõ†Ô∏è **Alternatives Considered**
  - *Do not delay actions ‚Äî instead, re-trigger flows via cron/event:* Less flexible; breaks causal chain.
  - *Only delay without state capture:* Leads to race conditions and incorrect behavior.
  - *Refetch live data after delay:* Breaks atomicity and may yield inconsistent intermediate states.

---

### **Status**

Accepted

---

### **Owner**

Flow Engine Team

---

### **Related ADRs**

- ADR-004: Event-Driven Architecture in Flow Builder  
- ADR-012: Persistence Strategy for Long-Running Flows  

--- 

> *""Delay is acceptable only when intent and context are preserved.""*","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision: Integrate the App System into the Shopware Platform Core

### Status
Accepted

### Context
The App System was initially developed and distributed as a standalone plugin to enable rapid iteration, avoid constraints of the core platform‚Äôs release cycle, and gather early feedback from partners and developers. This approach allowed flexibility during the experimental phase without affecting platform stability.

However, the App System has now matured, demonstrating reliability, adoption, and alignment with core platform functionality. As demand grows for consistent extension mechanisms across both cloud and on-prem environments, maintaining the App System as an external plugin introduces unnecessary complexity:

- Users must manually install and manage an additional component.
- Partners and app developers face uncertainty about long-term support and integration depth.
- Operational overhead increases in cloud environments where consistency and automation are critical.

To ensure a unified, reliable, and future-proof extension model, we need a solution that works seamlessly across all deployment models without requiring add-ons.

### Decision
Integrate the App System directly into the Shopware 6 platform core, deprecating its distribution as a separate plugin. The functionality currently provided by the [shopware/app-system](https://github.com/shopware/app-system) plugin will be merged into the main platform repository and maintained as a first-class feature.

This includes:
- Merging all relevant code, APIs, configuration schemas, and documentation into the core codebase.
- Ensuring backward compatibility during the transition (e.g., via deprecation layers).
- Updating installation and upgrade routines to include App System functionality by default.
- Aligning future development, testing, and release processes with the standard Shopware 6 release cycle.

### Consequences

**Positive**
- ‚úÖ **Strong signal of stability**: First-party inclusion confirms the App System is production-ready and strategically important.
- ‚úÖ **Simplified user experience**: No extra steps required to enable apps; available out-of-the-box.
- ‚úÖ **Consistent behavior**: Unified behavior across cloud and on-premise deployments.
- ‚úÖ **Improved maintainability**: Centralized development reduces fragmentation and improves test coverage.
- ‚úÖ **Easier upgrades**: App System updates are delivered through regular platform updates, reducing drift.

**Negative**
- ‚ùå **Increased platform complexity**: Core becomes slightly larger due to added responsibilities.
- ‚ùå **Tighter coupling**: Future changes to the App System must follow core release timelines, reducing agility compared to standalone iterations.
- ‚ùå **Migration effort**: Existing installations using the plugin must safely transition (handled via automated deprecation notices and removal guides).

**Mitigations**
- Use feature flags if needed during transition phases.
- Maintain clear changelogs and migration paths for existing users.
- Continue modular architecture within the core to isolate the App System components for easier maintenance.

### Additional Notes
Post-integration, the public `shopware/app-system` repository may be archived with a notice pointing to the core implementation. Community contributions will continue via the main platform repository.

This decision aligns with our vision of providing a unified, enterprise-grade e-commerce platform with robust, supported extension capabilities.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","### Decision: Consolidate `FlowStorer` Implementations Using a Unified, Configurable Storage Interface

#### Status
Accepted

#### Context
Currently, the system contains multiple `FlowStorer` implementations, most of which are responsible only for storing scalar values and do not require complex restore logic. Each storer comes with its own marker interface to indicate whether an event's data should be stored. This design leads to:
- Significant boilerplate when introducing new storers.
- Difficulty in managing or bypassing specific storers, especially from plugins.
- Poor scalability and maintainability due to tight coupling between event types and storer interfaces.
- Redundant code across similar scalar-value storers.

This fragmentation complicates extension points and reduces flexibility for conditional storage (e.g., enabling/disabling storers per event or context).

#### Decision
We will:
1. **Replace individual storer interfaces** with a **unified tagging or metadata-based mechanism** (e.g., annotations on events or configuration-driven rules) to determine if and how data should be stored.
2. Introduce a **generic `FlowStorer<T>` interface** that handles scalar value storage uniformly, reducing the need for multiple custom implementations.
3. Implement a **central `FlowStorageService`** that delegates to appropriate storers based on configurable policies (e.g., type matching, feature flags, plugin overrides).
4. Allow **plugin-driven override rules** so that plugins can conditionally enable, disable, or replace storer behavior without modifying core code.
5. Deprecate marker interfaces in favor of **event metadata or annotation-based declarations**, e.g., `@Storable(storeWith = ScalarStorer.class)`.

Example:
```java
@Storable(strategy = ScalarStorageStrategy.class)
public class UserLoginEvent {
    private final String userId;
    private final long timestamp;
    // ...
}
```

The `FlowStorageService` inspects such metadata and routes the event to the appropriate generic storer.

#### Consequences
**Pros:**
- ‚úÖ Drastically reduces boilerplate code.
- ‚úÖ Simplifies adding new storers or storage rules.
- ‚úÖ Enables plugins to dynamically influence storage behavior.
- ‚úÖ Improves consistency and testability.
- ‚úÖ Paves the way for future extensions (e.g., conditional storage, audit trails).

**Cons:**
- ‚ö†Ô∏è Requires migration effort for existing storers and events.
- ‚ö†Ô∏è Slight runtime overhead due to reflection or metadata inspection (mitigated via caching).
- ‚ö†Ô∏è Learning curve for developers used to marker interfaces.

**Long-term Impact:**
- Easier observability and control over data persistence.
- Foundation for pluggable storage strategies (e.g., encryption, compression, routing).

#### Notes
Start with a pilot refactor of 2‚Äì3 commonly used storers to validate the pattern before full rollout. Provide migration utilities and deprecation warnings for old interfaces.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","**Decision: Replace the current `sw-text-editor` with a modern, community-supported WYSIWYG editor framework**

### Status
Accepted

### Context
The existing `sw-text-editor` in the administration interface suffers from persistent low-level bugs in fundamental WYSIWYG functionality (e.g., formatting, undo/redo, paste handling, cursor behavior). Its codebase is tightly coupled, poorly documented, and difficult to debug or extend. This leads to high maintenance costs, inconsistent user experience, and delays in implementing new features. The team requires a solution that is stable, maintainable, extensible, and offers a robust baseline of rich text editing capabilities.

After evaluating several options‚Äîincluding maintaining the current editor, building a custom solution, or integrating a third-party library‚Äîwe determined that leveraging a mature, actively maintained WYSIWYG framework provides the best balance of reliability, flexibility, and development efficiency.

### Decision
Adopt **Tiptap** (https://tiptap.dev) as the foundation for the new text editor in the administration UI.

Rationale:
- **Modern & Maintainable**: Tiptap is built on ProseMirror, a powerful and well-architected framework for complex text editing, offering a predictable state management model.
- **Extensibility**: Provides a modular architecture where features (bold, lists, links, custom nodes) are implemented as extensions, making it easy to add or disable functionality.
- **Stability**: Actively maintained with strong TypeScript support, comprehensive testing, and a growing community.
- **Customization**: Allows full control over the UI while handling complex document state and collaboration-ready semantics under the hood.
- **Developer Experience**: Excellent documentation, clear APIs, and strong Vue.js integration (via `@tiptap/vue-3`) align well with our tech stack.
- **Performance**: Efficient rendering and update mechanisms suitable for large documents.

We will deprecate the legacy `sw-text-editor` and build a new component‚Äî`sw-rich-text-editor`‚Äîusing Tiptap, wrapping it with a consistent design system interface and adding required administrative features (e.g., media embedding, variable insertion, accessibility compliance).

### Consequences
**Positive**
- Significant reduction in WYSIWYG-related bugs.
- Faster implementation of new editing features.
- Improved testability and long-term maintainability.
- Easier onboarding for new developers due to clear patterns and documentation.
- Future-proofing through active ecosystem support.

**Negative**
- Initial migration effort required across admin modules.
- Learning curve for team members unfamiliar with ProseMirror‚Äôs concepts (e.g., schema, commands, plugins).
- Slight increase in bundle size (~100‚Äì150 kB gzipped), mitigated via lazy loading where appropriate.
- Requires careful abstraction to avoid direct tight coupling to Tiptap internals.

### Alternatives Considered
- **Quill.js**: Good API but less flexible for highly custom structures; limited support for nested or semantic content.
- **Slate.js**: Very flexible but has a steeper learning curve and weaker default behaviors; requires more boilerplate.
- **CKEditor 5**: Enterprise-grade but heavier, more opinionated UI, and licensing considerations for some plugins.
- **Keep current editor**: Would incur increasing technical debt and ongoing bug-fixing costs.

### Additional Notes
A proof-of-concept implementation demonstrated successful integration with Vue 3 and basic feature parity within one sprint. Long-term roadmap includes collaborative editing and AI-assisted writing via Tiptap's plugin system.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision

**Use ""not_specified"" as the default salutation value in Shopware 6 to handle unspecified salutations.**

To ensure data integrity, improve user experience, and support inclusive communication, the system will assign the salutation value `""not_specified""` whenever a salutation is not explicitly provided by the customer or administrator. This neutral default avoids assumptions about gender or identity and maintains consistency across customer records, templates, and communications.

The `""not_specified""` value will be:
- Used in all database entries where salutation is optional but left blank.
- Handled gracefully in frontend displays (e.g., omitted from greetings rather than displayed literally).
- Supported in email templates and document generation with conditional logic to adapt phrasing when no salutation is specified.
- Included as a valid option in API responses and administrative interfaces for transparency and filtering purposes.

This decision aligns with modern inclusivity standards and reduces reliance on gendered defaults, while maintaining backward compatibility through proper mapping and fallback mechanisms.","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision: Remove `padaliyajay/php-autoprefixer` from the SCSS compilation pipeline

### Status
Accepted

### Context
The storefront's CSS build process currently uses `scssphp/scssphp` for compiling SCSS to CSS, with an additional step using `padaliyajay/php-autoprefixer` to apply vendor prefixes. However, this setup presents several issues:

- **Redundant Prefixing**: Since upgrading to Bootstrap v5, browser support has improved significantly, and modern browsers natively support most CSS features without prefixes. The targeted browsers (as defined in `.browserslist`) rarely require the prefixes being added.
  
- **Performance Overhead**: The auto-prefixing step introduces a measurable slowdown during the `theme:compile` process. This is especially impactful in our SaaS environment where theme compilation occurs frequently across tenants.

- **Static and Inflexible Logic**: The `padaliyajay/php-autoprefixer` library hardcodes which properties should be prefixed and does not read or respect the project‚Äôs `.browserslistrc` configuration. As a result, it applies unnecessary prefixes regardless of actual browser support requirements.

- **Maintenance Risk**: The package is unmaintained and lacks integration with modern tooling standards (e.g., compatibility with `browserslist`, reliance on outdated data). This makes it unsuitable for long-term use in a scalable, evolving codebase.

### Decision
Remove `padaliyajay/php-autoprefixer` entirely from the theme compilation workflow.

Instead:
- Rely on **Bootstrap 5's own prefixed output**, which is already tailored to its supported browser range.
- Ensure that any custom SCSS code follows modern CSS practices and targets only supported browsers as defined in `.browserslist`.
- If future needs arise for dynamic prefixing (e.g., supporting older clients), consider migrating to a more robust solution such as PostCSS with `autoprefixer` (via Node.js build tools) ‚Äî but only if justified by performance/cost analysis.

### Consequences
**Positive**
- Improved `theme:compile` performance due to removal of a slow PHP-based processing step.
- Cleaner, smaller CSS output with fewer redundant declarations.
- Reduced technical debt by eliminating reliance on an unmaintained package.
- Better alignment with modern frontend tooling practices.

**Negative**
- Minimal risk of missing a rare vendor prefix in edge-case custom styles (unlikely given Bootstrap 5‚Äôs broad compatibility).
- Potential need to manually add a prefix in rare cases where legacy browser quirks persist ‚Äî though such cases are expected to be negligible.

### Actions
1. Remove `padaliyajay/php-autoprefixer` from `composer.json` and all theme compilation scripts.
2. Update documentation for theme developers to note that vendor prefixing is no longer automatically applied beyond what Bootstrap provides.
3. Add a check in CI (optional) to verify that no unnecessary prefixes are reintroduced via other means.
4. Monitor real-world browser compatibility reports post-deployment to detect any unforeseen rendering issues (none anticipated).

---

This decision aligns with modern web development best practices: favoring performance, maintainability, and accurate targeting of user environments through standardized tools like Browserslist.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision: Avoid Pre-Release Versions for Security-Critical Fixes in Public Packages

### Status
Accepted

### Context
In the npm ecosystem, pre-release versions (e.g., `1.9.0-alpha1`) are semantically considered **less than** the final release (e.g., `1.9.0`) and, more critically, are treated as **unstable and opt-in only**. Dependency managers and security scanners (such as npm Audit) typically ignore pre-release versions when evaluating dependency ranges or reporting vulnerabilities.

In this scenario:
- A vulnerability exists in version `1.8.7`.
- The fix is developed and packaged as a pre-release: `1.9.0-alpha1`.
- However, because `1.9.0-alpha1` is a pre-release, it is not automatically considered an upgrade over `1.8.7` by npm.
- As a result, users remain on `1.8.7`, and automated security tools do not flag the insecure version since there is no ""official"" stable release that resolves the issue.

This creates a dangerous gap: the fix exists but is effectively invisible to most users and tooling.

### Decision
We will **not use pre-release versions to deliver security fixes** in packages that are subject to vulnerability reporting (e.g., public npm packages). Instead, we will:
1. Use **stable version numbers** (e.g., `1.9.0`) for any release containing security fixes.
2. If necessary, publish interim fixes under a minor or patch stable version (e.g., `1.8.8`) to ensure immediate visibility and applicability.
3. Reserve pre-release tags (`-alpha`, `-beta`, `-rc`) for non-security-related features or experimental changes only.

### Rationale
- **Security tools rely on stable versions**: Tools like npm audit, Snyk, and Dependabot primarily monitor and report based on stable releases.
- **SemVer precedence rules exclude pre-releases**: According to SemVer 2.0.0, `1.9.0-alpha1 < 1.9.0`, and range matchers like `^1.8.7` will not include pre-releases unless explicitly requested.
- **User safety over release process purity**: While using a stable version for a partially complete feature may seem premature, user security takes precedence.

### Implications
- We must ensure that any stable version published is production-ready, even if some features are incomplete.
- Development workflows may need branching strategies (e.g., feature flags, private branches) to isolate unfinished work while still releasing secure code.
- Communication with stakeholders may be needed to explain versioning decisions when skipping pre-release phases for security reasons.

### References
- [Semantic Versioning 2.0.0 - Pre-release Versions](https://semver.org/#spec-item-9)
- [npm SemVer Ranges Documentation](https://docs.npmjs.com/cli/v8/using-npm/semver)
- [npm Audit Behavior and Vulnerability Reporting](https://docs.npmjs.com/about-audit-reports)","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","### Decision: Add a Computed Index on `coalesce(updated_at, created_at)` to Optimize Outdated Cart Deletion

#### Status
Accepted

#### Context
The current `DELETE` query for outdated cart entries performs poorly under high load because it lacks an efficient index strategy. The condition:

```sql
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01')
```

is logically equivalent to checking whether the *last modification time* of the cart (either `updated_at` or `created_at` if never updated) is older than a threshold. However, no single index can be used by the database to satisfy this `OR` condition efficiently across both columns, resulting in full table scans and query times exceeding 30 seconds.

Running `EXPLAIN` confirms that no usable keys are available (`possible_keys = NULL`), which explains the performance degradation on large datasets.

#### Decision
Introduce a **computed (generated) column** that captures the effective ""last modified"" timestamp using `COALESCE(updated_at, created_at)`, and create an index on this column to accelerate deletion queries.

##### Steps:
1. Add a generated column `last_modified_at` to the `cart` table:
   ```sql
   ALTER TABLE cart
   ADD COLUMN last_modified_at DATETIME AS (COALESCE(updated_at, created_at)) STORED;
   ```

2. Create an index on the new column:
   ```sql
   CREATE INDEX idx_cart_last_modified_at ON cart(last_modified_at);
   ```

3. Rewrite the delete query to use the indexed column:
   ```sql
   DELETE FROM cart
   WHERE last_modified_at <= '2023-02-01'
   LIMIT 1000;
   ```

4. Update any related cleanup scripts or cron jobs to use the new condition.

> **Note**: On databases that do not support generated columns (e.g., older MySQL versions), consider maintaining `last_modified_at` via triggers or application logic.

#### Rationale
- The `COALESCE(updated_at, created_at)` expression accurately represents when a record was last active.
- A B-tree index on this value allows range queries (`<= '2023-02-01'`) to locate candidates instantly.
- This change reduces the `DELETE` operation from a full table scan to an indexed range scan, drastically improving performance.
- Limiting deletions in batches of 1000 maintains low lock contention and aligns with typical background job patterns.

#### Implications
- **Storage Overhead**: The `STORED` generated column consumes additional disk space (~8 bytes per row). For large shops, this may amount to gigabytes but is typically acceptable given the performance gain.
- **Write Overhead**: Minimal ‚Äî the computation is lightweight and occurs only on insert/update.
- **Migration Required**: The schema change must be applied safely during a maintenance window or via online DDL tools (e.g., `pt-online-schema-change`) to avoid table locking.
- **Backward Compatibility**: Existing code not touching the cart lifecycle remains unaffected. Application logic relying on `created_at`/`updated_at` should continue working.

#### Alternatives Considered
1. **Index both `created_at` and `updated_at` separately**  
   ‚Üí Not sufficient; the `OR` condition still prevents optimal index usage and may require expensive `UNION` plans.

2. **Use a trigger to populate `updated_at` on insert if null**  
   ‚Üí Makes `updated_at` always non-null, allowing indexing on `updated_at`, but distorts semantics (`updated_at == created_at` initially). Still less accurate than tracking real updates.

3. **Function-based index (PostgreSQL-style)**  
   ‚Üí Ideal, but MySQL before 8.0.13 does not support function-based indexes. Even in newer versions, functional expressions like `COALESCE()` cannot be directly indexed unless wrapped in a generated column.

4. **Application-level filtering**  
   ‚Üí Would transfer load to app servers and increase memory pressure; not scalable.

#### Conclusion
Adding a `last_modified_at` generated column with an index provides a robust, portable, and high-performance solution for cleaning stale cart records. It transforms a slow, unindexed operation into a fast, batch-friendly process suitable for high-traffic environments.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision: Introduce an Experimental Features Framework

### Status
Accepted

### Context
Releasing new features only in a stable, finalized state creates significant delays in gathering user feedback and increases the risk of building misaligned functionality. Due to our strong backward compatibility commitments, once a feature is released publicly, even minor changes become costly or impossible without breaking existing integrations. This leads to:
- Long development cycles with limited external validation.
- High pressure to ""get it right the first time.""
- Missed opportunities for early ecosystem engagement and co-creation.
- Delayed delivery of incremental business value.

Features like B2B multi-account management and Advanced Search would benefit from phased exposure where core capabilities are shared early, but without locking us into long-term contracts prematurely.

### Decision
Introduce an **Experimental Features Framework** that allows the controlled release of incomplete or evolving features under a clear ""experimental"" designation. This framework will:

1. **Mark experimental APIs and UIs** with metadata (e.g., `@experimental`, dedicated URL paths `/experimental/*`, or feature flags).
2. **Exclude experimental features from backward compatibility guarantees** ‚Äî we reserve the right to change or remove them at any time without deprecation periods.
3. **Clearly communicate the experimental status** to developers and users via documentation, UI banners, API responses, and developer portals.
4. **Require explicit opt-in** through feature flags, headers (`X-Feature-Preview: advanced-search-v1`), or console settings.
5. **Include telemetry and feedback mechanisms** to monitor usage and collect input directly within the product.
6. **Define a lifecycle process**: experimental ‚Üí beta ‚Üí stable ‚Üí deprecated, with promotion requiring design review, stability assessment, and documented contracts.

For example:
- The initial B2B employee management API will be accessible under `/api/experimental/b2b-employees` and require the header `X-Preview: B2B-Employee-Core`.
- The first version of Advanced Search will be available behind a tenant-level feature flag and labeled as ‚ÄúEarly Access‚Äù in the UI.

Once sufficient feedback is gathered and the design stabilizes, the feature can graduate to stable status with proper versioning and long-term support commitments.

### Consequences
**Positive**
- Enables faster learning and iteration based on real-world usage.
- Reduces time-to-value by delivering partial functionality earlier.
- Encourages community involvement in shaping features.
- Lowers the risk of over-engineering unused capabilities.
- Supports innovation while preserving stability for production systems.

**Negative**
- Some users may depend on experimental features despite warnings; requires proactive communication and monitoring.
- Additional complexity in codebase (feature flags, version routing, metadata).
- Need for governance: who approves what becomes experimental? How do we clean up obsolete experiments?
- Potential confusion if not communicated clearly.

**Mitigations**
- Enforce automated checks that prevent experimental endpoints from being called in production environments unless explicitly enabled.
- Regularly audit and retire stale experimental features (e.g., quarterly reviews).
- Document all experimental features in a public roadmap or preview portal with expected timelines and contact points for feedback.

### Related Decisions
- [Feature Flag Management Policy](#)
- [API Versioning Strategy](#)
- [Deprecation Process for Stable APIs](#)

### References
- GitHub‚Äôs API Preview mechanism
- Kubernetes Alpha/Beta/Stable graduation criteria
- Stripe‚Äôs ""Early Access"" program for new APIs","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision: Implement a Pluggable Inventory Management System in Shopware 6

### Status
Accepted

### Context
The current stock handling mechanism in Shopware 6 lacks flexibility, extensibility, and performance. Key limitations include:

- No abstraction layer for inventory sources, preventing integration with external systems like ERPs.
- Hardcoded stock mutation logic (e.g., reservation on order, reduction on completion) that cannot be customized without core overrides.
- Poor performance when calculating available stock across large product catalogs due to real-time aggregation of order data.
- Conceptual complexity from maintaining separate `stock` and `available_stock` fields, where the latter is derived rather than authoritative.

These constraints hinder advanced use cases such as dropshipping, multi-warehouse management, just-in-time inventory, or real-time ERP synchronization.

### Decision
Replace the monolithic stock management system with a **pluggable inventory management framework** based on the following principles:

1. **Introduce an `InventoryHandler` interface**:
   ```php
   interface InventoryHandler {
       public function getAvailableStock(string $productId, ?array $context = []): int;
       public function reserve(string $productId, int $quantity, array $context = []): bool;
       public function commit(string $productId, int $quantity, array $context = []): bool;
       public function release(string $productId, int $quantity, array $context = []): bool;
       public function updateStock(string $productId, int $stock, array $context = []): void;
   }
   ```

2. **Provide a default implementation (`DefaultInventoryHandler`)** that mimics current behavior but uses a single source of truth ‚Äî treat `stock` as physical inventory, and manage reservations via order state tracking.

3. **Deprecate direct access to `product.stock` and `product.availableStock`** in business logic. Instead, always route through the inventory service.

4. **Introduce event-driven hooks** around stock mutations (e.g., `InventoryReservationEvent`, `InventoryCommitEvent`) to allow plugins to react or modify behavior.

5. **Optimize available stock calculation** by introducing a denormalized `inventory_balance` table that tracks:
   - Total physical stock
   - Reserved quantity
   - Possibly split by location or warehouse later

   This allows `available_stock = stock - reserved`, computed instantly.

6. **Allow registration of custom inventory handlers per product or product type**, enabling hybrid models (e.g., some products managed in ERP, others locally).

7. **Expose API endpoints and administration UI extension points** for managing inventory settings and integrations.

### Consequences
**Positive:**
- Enables seamless integration with ERPs, WMS, and external inventory services.
- Supports complex workflows (e.g., pre-orders, backorders, multi-location) via custom handlers.
- Improves performance via denormalization and caching potential.
- Simplifies mental model: available stock is no longer a calculated runtime value.
- Future-proof for headless and composable commerce scenarios.

**Negative:**
- Requires migration path for existing installations (data migration, deprecation period).
- Increases initial complexity due to new abstractions.
- Plugin compatibility risks if third-party code accesses stock fields directly.

**Mitigations:**
- Maintain backward compatibility during transition via adapter pattern.
- Provide clear documentation and upgrade guides.
- Introduce feature toggle to enable new system gradually.

### Alternatives Considered
- **Extend current model with events only**: Would not solve performance or architectural rigidity.
- **Add ERP sync module without changing core**: Still leaves core inflexible and tightly coupled.
- **Use read replicas or caching**: Addresses performance but not extensibility.

This decision aligns with Shopware‚Äôs direction toward modularity and enterprise readiness.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision: Suppress Error-Level Logging for Expected Client-Side Exceptions

### Status
Accepted

### Context
In the current logging setup, all uncaught exceptions are automatically logged at the `error` level by `symfony/monolog-bridge`. While this is appropriate for server-side failures (5xx), it does not suit client-generated invalid requests that result in 4xx HTTP responses. Specifically, `ShopwareHttpException` instances are intentionally thrown to signal malformed or incorrect API usage (e.g., missing fields, invalid parameters) and map directly to 4xx status codes.

These exceptions are **not exceptional from a system health perspective** ‚Äî they represent expected error conditions due to client misuse. However, being logged as errors creates noise in monitoring and alerting systems, obscuring genuine server-side issues that require investigation.

This problem is particularly impactful in cloud environments where log volume affects observability costs and incident response efficiency. A prior solution in the cloud product demonstrated success using a configurable list of exception classes to downgrade logging severity.

### Decision
We will **downgrade the log level** for specific, known client-facing exceptions (such as `ShopwareHttpException` and other designated 4xx-mapped exceptions) from `error` to `notice` (or `info`) when they are caught and handled by the API layer and translated into a 4xx HTTP response.

To achieve this:
1. Implement a custom Monolog processor or extend the existing exception handling mechanism to inspect the type and HTTP status code of exceptions before logging.
2. Define a configurable list of exception classes (e.g., via DI parameter or configuration file) that should be treated as non-error events if they result in a 4xx response.
3. When such an exception is detected during logging, reduce its log level to `notice`.
4. Ensure full context (message, code, trace, request data) is still captured at the lower level for debugging purposes if needed.

This behavior will be consistent across both on-premise and cloud deployments, aligning with the proven approach used in the cloud product.

### Consequences
**Pros:**
- Reduces log noise significantly by removing expected client errors from error logs.
- Improves signal-to-noise ratio, making real bugs and server errors easier to detect.
- Aligns log semantics with actual system health: 4xx = client problem; 5xx = server problem.
- Reuses and standardizes a working pattern already validated in production (cloud).

**Cons:**
- Requires careful configuration to avoid accidentally suppressing legitimate errors.
- Slight increase in complexity in the logging pipeline.
- Risk of overlooking legitimate issues if new exception types are added to the whitelist without proper review.

**Mitigations:**
- Maintain the list of downgraded exceptions centrally and under version control.
- Document the policy clearly for developers: only exceptions representing *intentional* 4xx responses should be included.
- Retain full logs at `notice` level so auditability is preserved.
- Monitor log volume trends post-deployment to validate effectiveness.

### See Also
- Symfony Exception Event Handling (`kernel.exception`)
- `symfony/monolog-bridge` integration with Symfony logging
- Cloud platform's existing exception logging configuration

---

‚úÖ **Approved by:** Architecture Team  
üìÖ **Date:** 2025-04-05","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision: Migrate from UUIDv4 to UUIDv7 for Primary Keys

### Status
Accepted

### Context
We currently use UUIDv4 as the primary key for our database records. While this provides strong guarantees for global uniqueness and facilitates distributed data ingestion and eventual merging of disparate datasets, it introduces performance bottlenecks due to its fully random nature. Specifically:

- Random UUIDs lead to poor index locality in B-tree structures.
- Frequent page splits and random I/O degrade insert performance.
- Index bloat increases memory and disk usage, reducing cache efficiency.

UUIDv7 addresses these issues by embedding a time-ordered component at the beginning of the UUID (most significant bits), enabling natural chronological clustering in indexes. This improves:
- Insert performance through reduced page splits.
- Range query efficiency.
- Cache and storage utilization via better data locality.

Additionally, UUIDv7 maintains compatibility with existing UUID tooling and standards while offering predictable lexicographical ordering within time windows.

### Decision
Adopt **UUIDv7** as the default primary key format for new tables and gradually migrate critical high-write tables from UUIDv4 to UUIDv7. We will:

1. Use UUIDv7 for all new entities starting immediately.
2. Implement a migration strategy for high-impact tables based on write volume and performance metrics.
3. Ensure application-level handling of ID generation using a compliant library (e.g., `ulid` or `ksuid`, or native UUIDv7 generators when available).
4. Maintain backward compatibility during transition via dual-read support if needed.
5. Document the change and update internal guidelines for data modeling.

### Consequences
**Pros:**
- Improved database write performance and index efficiency.
- Better scalability under high insertion loads.
- Smaller and more cache-friendly indexes.
- Retains benefits of UUIDs: decentralization, no coordination required.

**Cons:**
- Slight increase in predictability of IDs (mitigated by random suffix).
- Requires careful migration planning for existing systems.
- Not all databases/drivers natively support UUIDv7 yet (though they treat it as a standard UUID).

### References
- [UUIDv7 Draft Specification (RFC-4122bis)](https://datatracker.ietf.org/doc/html/draft-peabody-dispatch-new-uuid-format)
- [‚ÄúTime-Ordered UUIDs Considered Helpful‚Äù ‚Äì Ben Johnson](https://www.sobyte.net/post/2022-09/time-ordered-uuid/)
- Database performance studies on index fragmentation with random vs. sequential UUIDs","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","**Decision:** Enable extensibility of the flow builder event list through a plugin-based architecture.

We will implement a plugin interface that allows applications to register custom events in the flow builder. This will involve:

1. Defining a well-documented `EventPlugin` interface that apps must implement.
2. Creating a central event registry that discovers and loads both core and plugin-provided events at runtime.
3. Exposing a registration mechanism (via dependency injection or configuration) for apps to contribute their custom events.
4. Ensuring type safety and metadata consistency (e.g., event name, triggers, payload schema) across all events.
5. Providing validation and error handling for malformed or conflicting event registrations.

This decision enables domain-specific extensions without modifying the core flow builder, supporting future growth while maintaining stability and backward compatibility.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision: Implement JWT-Based Authentication for App-to-App Server Communication

### Status
Accepted

### Context
Currently, the App Server securely communicates with the Shopware backend using signed requests. The Shopware backend generates a cryptographic signature using a shared secret known only to the shop and the app server, enabling the app server to verify the authenticity and integrity of incoming requests. However, when third-party apps (e.g., storefronts, mobile apps, or external services) attempt to communicate directly with the App Server‚Äîespecially in customer-facing contexts‚Äîthe server cannot reliably authenticate the requester or obtain trusted session data such as the currently logged-in customer. This limits secure access to customer-specific resources and increases the risk of spoofing or unauthorized access.

### Decision
To enable secure communication from client-side apps (e.g., PWA, mobile apps) to the App Server while preserving trust in user identity and request origin, we will implement a **JWT (JSON Web Token)-based authentication flow**, issued by the Shopware backend and validated by the App Server.

The flow will work as follows:

1. When a customer logs in or performs an authenticated action in the Shopware storefront:
   - The Shopware backend issues a short-lived JWT containing verified claims such as:
     - `shopId`
     - `customerId`
     - `iat` (issued at)
     - `exp` (expiration time)
     - `nonce` (optional, for replay protection)
   - The JWT is signed using a strong algorithm (e.g., HS256 or RS256) with the **shop-to-app secret** established during app registration.

2. The frontend (e.g., PWA) receives this JWT (via secure HTTP-only cookie or explicit API response) and includes it in the `Authorization: Bearer <token>` header when making requests to the App Server.

3. The App Server:
   - Extracts the JWT.
   - Uses the known shop-to-app secret (already stored from app registration) to verify the signature.
   - Validates standard claims (`exp`, `iat`) and custom claims (`shopId`, `customerId`).
   - Only proceeds if the token is valid and not expired.

4. Based on the verified claims, the App Server can now:
   - Trust the identity of the logged-in customer.
   - Process customer-specific operations securely.
   - Enforce shop-level isolation.

### Rationale
- **Security**: Unlike plain bearer tokens or client-side assertions, JWTs signed with a server-known secret ensure that only Shopware can issue valid tokens. The App Server does not need to trust the client‚Äîit trusts the signature.
- **Statelessness**: No need for shared sessions or database lookups; verification is fast and scalable.
- **Compatibility**: Leverages existing secrets (no new key management overhead).
- **Flexibility**: Supports multiple app types (PWA, mobile, embedded widgets) without changing the security model.
- **Auditability**: Claims within the token provide clear provenance and context.

### Implications
#### Pros
- Enables secure, verifiable customer-context propagation from Shopware to App Server via untrusted clients.
- Maintains alignment with current security practices (shared secrets, signed payloads).
- Allows App Server to safely expose customer-sensitive endpoints.

#### Cons
- Requires modification to Shopware storefront logic to issue JWTs upon customer authentication.
- Frontend must securely handle and transmit tokens (avoid logging, use HTTPS).
- Clock synchronization matters due to time-based claims (`exp`, `iat`).
- Risk of token leakage if not handled properly on the client side.

#### Mitigations
- Set short expiration times (e.g., 15‚Äì30 minutes).
- Use refresh mechanisms through secure backend channels if needed.
- Enforce HTTPS everywhere.
- Educate developers on secure token handling.

### Alternatives Considered
- **OAuth2 / OpenID Connect**: More complex, requires additional infrastructure (identity provider), overkill for current integration scope.
- **Proxy all requests via Shopware backend**: Adds latency, couples systems tightly, doesn‚Äôt scale well.
- **API keys per customer**: Not feasible due to scale and revocation challenges.
- **Allow unsigned customer IDs in headers**: Insecure‚Äîeasily forged by malicious clients.

### Next Steps
1. Define JWT payload schema and signing algorithm (preferably HS256 with shop-secret).
2. Implement JWT issuance in Shopware upon successful customer login/session creation.
3. Add JWT validation middleware in App Server for protected routes.
4. Update documentation for app developers on how to use authenticated flows.
5. Add monitoring and logging for token validation failures.

---

*This decision ensures that even indirect communication from apps reaches the App Server with verifiable trust, closing a critical security gap while maintaining scalability and developer experience.*","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision: Implement Flow Builder Preview as a Static Execution Path Visualizer

To address the challenge of merchants struggling to understand why their custom flows do not behave as intended, we will implement a **Flow Builder Preview** feature that provides a clear, static visualization of the logical execution path of a flow based on its current configuration.

### Key Aspects of the Decision:

1. **Static Path Visualization**:
   - The preview will **not execute** any actions or call external services.
   - It will analyze the structure of the flow (triggers, conditions, actions, and transitions) and render a step-by-step representation of possible execution paths.
   - This includes visual indication of conditional branches (e.g., ""if order total > $100"") and which path would be taken given sample context data.

2. **Contextual Sample Data Injection**:
   - Users can select a real-world scenario (e.g., ‚ÄúOrder Placed‚Äù, ‚ÄúCustomer Registered‚Äù) to simulate with realistic payload data.
   - The preview uses this sample data to evaluate conditions and show which branches are active or inactive in that specific context.

3. **Non-Execution Safety**:
   - Since no actual actions (like sending emails or updating records) are performed, the preview is safe to use in production environments.
   - This avoids side effects while still offering valuable insight into flow logic.

4. **User Interface Integration**:
   - A ‚ÄúPreview‚Äù button will be added in the Flow Builder UI.
   - Upon clicking, a side panel or modal displays the execution path with:
     - Highlighted active/inactive branches.
     - Evaluation results for each condition (e.g., `Condition X: true (because customer.group = 'Wholesale')`).
     - Entry point and flow progression arrows.

5. **Diagnostic Clarity**:
   - If a merchant‚Äôs email wasn't sent, the preview helps identify whether:
     - The triggering event was correctly configured.
     - A condition blocked progression.
     - The email action is unreachable due to logic errors.
   - This reduces debugging time from hours to minutes by making invisible logic visible.

6. **Scope Boundaries**:
   - This solution **does not replace logging or runtime observability** (which will be addressed separately via Flow Logging).
   - It does **not support dynamic or asynchronous behavior simulation** (e.g., delays, callbacks).
   - It focuses solely on improving upfront understanding and correctness verification before activation.

### Rationale:
- **Empowers Merchants**: Enables non-developers to validate flow logic without relying on technical support.
- **Reduces Support Burden**: Prevents issues like undelivered emails due to misconfigured conditions.
- **Fast Feedback Loop**: Immediate visual feedback during design phase improves usability and trust in the Flow Builder.
- **Low Risk, High Value**: Safe to implement and test, with significant UX improvement.

### Outcome:
Merchants can now preview how their flows will behave under specific conditions, dramatically improving transparency, reducing errors, and increasing confidence in their custom automation ‚Äî without executing any real actions.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much √É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬∞real√É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision: Decouple URL Generation from Pathname Strategy via Unified Path Resolver

### Status
**Accepted**

### Context
The current implementation separates concerns between path generation (`PathnameStrategyInterface`) and URL generation (`UrlGeneratorInterface`), but introduces tight coupling through event-driven URL population during the `media.loaded` event. This leads to:

- **Performance overhead**: URLs are generated for all media entities in a collection, even when not needed.
- **Inconsistency risk**: The `UrlGeneratorInterface` may not perfectly align with the actual file storage path defined by the active `PathnameStrategy`.
- **Limited flexibility**: Custom strategies must still rely on a separate URL generator, making it hard to encapsulate path + URL logic together.
- **Lazy loading issues**: URLs are pre-generated rather than computed on demand, increasing memory usage.

Additionally, frontend/store API consumers require consistent and performant access to media URLs without unnecessary computation.

### Decision
Introduce a new `MediaUrlResolverInterface` that unifies path and URL generation responsibilities while being decoupled from entity loading events. Deprecate the existing event-based URL generation in favor of on-demand resolution.

```php
interface MediaUrlResolverInterface
{
    public function resolveMediaUrl(MediaEntity $media, ?SalesChannelContext $context = null): string;

    public function resolveThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail, ?SalesChannelContext $context = null): string;

    public function resolveMediaPath(MediaEntity $media): string;

    public function resolveThumbnailPath(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}
```

Refactor the `PathnameStrategyInterface` to return structured path information instead of individual parts:

```php
interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Returns full virtual path components for a media item
     */
    public function buildMediaPathInfo(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): MediaPathInfo;
}

class MediaPathInfo
{
    private ?string $hash;
    private ?string $cacheBuster;
    private string $filename;
    private string $directory;

    // constructors/getters
}
```

Replace the event subscriber with lazy URL resolution:
- Remove automatic URL population in `media.loaded`.
- Expose URL resolution via service (e.g., `media_url_resolver`) injectable into templates, API controllers, and Store-API routes.
- Update all frontend outputs (Twig, Store-API, Admin) to use the resolver service on demand.

Keep backward compatibility by:
- Maintaining deprecated `UrlGeneratorInterface` as a bridge to new resolver.
- Allowing opt-in to legacy behavior via config flag.

### Consequences
**Pros:**
- ‚úÖ **Consistent URLs**: Guaranteed alignment between stored paths and generated URLs.
- ‚úÖ **Improved performance**: On-demand URL generation reduces memory and CPU usage.
- ‚úÖ **Better extensibility**: Developers can create strategies with custom URL schemes (e.g., CDN-aware resolvers).
- ‚úÖ **Decoupling**: Removes side effects from entity loading events.
- ‚úÖ **Supports future features**: Enables per-sales-channel or context-aware URLs (e.g., regional CDNs).

**Cons:**
- ‚ö†Ô∏è **Migration effort**: Existing templates and plugins using `$media->getUrl()` must be updated or retain BC layer.
- ‚ö†Ô∏è **Caching considerations**: Frequent calls to resolver may benefit from internal caching (to be addressed separately).
- ‚ö†Ô∏è **Breaking change surface**: Requires deprecation cycle and clear upgrade path.

### Follow-up Actions
1. Implement new `MediaUrlResolverInterface` with default implementation delegating to configured `PathnameStrategy`.
2. Create `MediaPathInfo` value object.
3. Refactor built-in strategies (`FilenameStrategy`, `HashStrategy`) to implement new interface.
4. Deprecate `UrlGeneratorInterface` and event subscriber; log warnings.
5. Provide migration guide for plugin developers.
6. Plan removal in next major version.

This decision ensures scalable, maintainable, and flexible media URL handling aligned with domain semantics.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision: Implement a Dedicated One-Time Data Migration System Using Migration-Indexed Execution Flags

To address the issue of unwanted re-execution of one-time data migrations currently handled through indexers, we will **introduce a dedicated one-time data migration mechanism that is decoupled from indexing processes and tracked via database-backed execution flags**.

### Rationale
- Current approach misuses indexers (designed for repetitive synchronization) to perform one-off data transformations, leading to:
  - Risk of data corruption or inconsistency on repeated runs.
  - Lack of clear audit trail for one-time operations.
  - Performance degradation due to unnecessary logic in indexing paths.
- A robust system should ensure idempotency by design and provide visibility into which migrations have been applied.

### Solution
1. **Create a `one_time_migration` database table** with fields:
   - `id` (primary key)
   - `migration_key` (unique string identifier, e.g., `media_path_format_v6_to_v7`)
   - `executed_at` (timestamp)
   - `completed_successfully` (boolean)
   - `execution_log` (text, optional)

2. **Introduce a service `OneTimeDataMigrationExecutor`** that:
   - Accepts a migration key and a callable operation.
   - Checks the database if the migration has already been executed.
   - Executes the callable only if not previously completed.
   - Logs success/failure and timestamp in the database.

3. **Register one-time migrations in update scripts or post-upgrade hooks**, not in indexers.

4. **Trigger execution during upgrade procedures**, such as:
   - Shopware PostUpgrade event.
   - Manual CLI command (e.g., `bin/console one-time:migration:run`).

### Example Usage
```php
$this->executor->execute('media_path_format_v6_to_v7', function () {
    // Logic to migrate media paths once
    $this->mediaUpdater->updateLegacyPaths();
});
```

### Benefits
- ‚úÖ Guarantees one-time execution via persistent tracking.
- ‚úÖ Decouples migration logic from indexing responsibilities.
- ‚úÖ Provides auditability and debugging capability.
- ‚úÖ Prevents accidental re-execution during normal operations.

### Follow-Up Actions
- Deprecate all existing one-time logic embedded in indexers.
- Migrate current indexer-based one-time updates to this new system.
- Document pattern for future developers.

This decision ensures safe, traceable, and reliable execution of one-time data migrations across Shopware version upgrades.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","# ADR 2023-10-15: Introduce Feature-Scoped Experimental Code via Named Feature Flags

## Status
Proposed

## Context
Our current use of experimental features enables early feedback but introduces significant maintenance challenges due to code fragmentation. The `@experimental` annotation, as currently implemented, lacks explicit linkage to a specific feature identifier, making it difficult to:

- Track all code associated with a given experimental feature.
- Update metadata (e.g., `stableVersion`) when prolonging an experiment.
- Identify and deprecate components when killing a feature, especially under the rule requiring deprecation until the next major version.

This lack of traceability undermines our ability to manage technical debt and maintain code hygiene during the experimental lifecycle.

## Decision
Introduce **named feature flags** within the `@experimental` annotation to explicitly associate experimental code with a unique feature identifier. Modify the annotation and tooling to support this scoping.

### Implementation Details:
1. **Enhance the `@experimental` annotation** to include a required `feature` field:
   ```java
   @interface experimental {
       String feature(); // e.g., ""user-profile-v2""
       String stableVersion() default """";
       String introducedIn() default """";
   }
   ```

2. **Enforce naming convention** for feature identifiers:
   - Use kebab-case: `feature-name-scope`
   - Include version if applicable: `dashboard-redesign-v1`
   - Avoid generic names like ""new-ui"" or ""perf-improvements""

3. **Update build/tooling pipeline** to:
   - Generate a **feature manifest** listing all experimental code grouped by `feature`.
   - Validate that every `@experimental` usage includes the `feature` field.
   - Support queries such as:
     - ‚ÄúShow all code for feature X‚Äù
     - ‚ÄúList features marked stable in v2.5‚Äù
     - ‚ÄúFind deprecated features without removal date‚Äù

4. **Modify deprecation workflow**:
   - When killing a feature, run a tool to locate all code tagged with `@experimental(feature = ""killed-feature"")`.
   - Automatically apply `@deprecated` with a message referencing the feature and removal timeline.

5. **Documentation**: Update the [Experimental Features](./2023-05-10-experimental-features.md) guideline to mandate use of the `feature` field and describe lifecycle management using the new system.

## Consequences
### Pros
- ‚úÖ **Improved traceability**: All code belonging to a feature can be discovered via static analysis.
- ‚úÖ **Simplified lifecycle management**: Extending or deprecating a feature becomes a targeted operation.
- ‚úÖ **Better compliance** with ‚ÄúKilling Feature‚Äù rules through automated detection.
- ‚úÖ Enables future tooling (dashboards, cleanup bots, CI checks).

### Cons
- ‚ö†Ô∏è **Migration effort**: Existing `@experimental` annotations must be updated with `feature` field.
- ‚ö†Ô∏è **Risk of inconsistent naming** if not enforced via tooling.
- ‚ö†Ô∏è **Tooling dependency**: Requires investment in parsing and reporting infrastructure.

## Notes
This decision aligns with industry practices seen in large-scale systems (e.g., Google‚Äôs feature rollout systems, Facebook‚Äôs Gatekeepers) where feature-scoped annotations enable safe experimentation at scale.

We will pilot this change on one ongoing experimental feature before full rollout.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","**Decision: Introduce Globally Unique Identifiers (GUIDs) for Payment and Shipping Methods**

### Status
Accepted

### Context
In the current implementation, extension developers face difficulties in uniquely identifying payment and shipping methods solely based on internal database IDs. These IDs are environment-specific (e.g., differing between development, staging, and production instances), which prevents reliable cross-system referencing. App servers integrating with Shopware must make additional calls to the Admin API to resolve method details by ID, increasing latency, complexity, and dependency on real-time API availability. This also complicates configuration management, event handling, and synchronization across distributed systems.

### Decision
Introduce a **stable, globally unique identifier (GUID)**‚Äîsuch as a UUID or a namespaced machine name‚Äîfor each payment and shipping method. These GUIDs will be:
- Assigned at registration time (e.g., via plugin code or configuration).
- Immutable across environments and installations.
- Exposed via APIs (Store API, Admin API, and app lifecycle events).
- Used as the primary reference key for integrations, configurations, and app-server communication.

For example:
```json
{
  ""id"": ""s_1a2b3c4d5e"", // DB ID ‚Äì internal use
  ""handlerIdentifier"": ""sw-app-payment-invoice"", // GUID ‚Äì public contract
  ""name"": ""Invoice""
}
```

Plugins and apps must register methods using a `handlerIdentifier` field that follows a naming convention (e.g., `vendor-type-method`, such as `myapp-payment-paypal-express`) to ensure global uniqueness.

### Rationale
- **Deterministic Identification**: Enables extensions and app servers to reliably identify methods without querying the Admin API.
- **Environment Portability**: Facilitates configuration sharing across dev, staging, and prod environments.
- **Decoupling**: Reduces coupling between app servers and the Admin API, improving resilience and performance.
- **Extensibility**: Supports future features like method versioning, marketplace compatibility, and declarative configuration.

### Implications
- **Backward Compatibility**: Existing systems relying on internal IDs must be updated gradually. A migration layer can map old IDs to new GUIDs during a transition period.
- **Developer Responsibility**: Plugin developers must ensure uniqueness of their assigned identifiers (mitigated via namespace conventions).
- **Documentation & Tooling**: Requires clear guidelines and validation tools to enforce proper GUID usage during development.

### Alternatives Considered
1. **Continue using internal IDs**  
   ‚Üí Rejected due to environment inconsistency and tight coupling.

2. **Use method names as identifiers**  
   ‚Üí Rejected because names are user-facing, translatable, and subject to change.

3. **Generate UUIDs automatically**  
   ‚Üí Partially useful but insufficient alone; requires persistence across installs. Best used in combination with developer-defined logical keys.

### Additional Notes
This decision aligns with Shopware‚Äôs headless and extensibility goals, supporting robust, scalable integrations for app ecosystem growth.","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision: Prioritize Bootstrap Utility Classes Over Custom SCSS for Simple Styling

### Status
Accepted

### Context
The Storefront currently includes a significant amount of custom SCSS that replicates functionality already provided by Bootstrap‚Äôs utility classes‚Äîparticularly for basic styling concerns such as spacing, borders, and alignment. This leads to:
- Increased CSS bundle size.
- Unnecessary maintenance overhead.
- Reduced consistency with Bootstrap's design system.
- Slower development due to redundant class definitions.

Many of these custom styles apply only simple presentational rules (e.g., `margin-bottom: $spacer`) that can be directly expressed using Bootstrap's built-in utilities like `mb-3`. However, it is still important to preserve semantic class names to allow for theme customization and targeted styling when needed.

### Decision
Adopt the following guidelines for frontend development in the Storefront:

1. **Prefer Bootstrap Utility Classes** for all basic styling (spacing, sizing, visibility, flexbox layout, borders, etc.) instead of defining equivalent rules in custom SCSS.

2. **Eliminate redundant custom SCSS** that merely applies default Bootstrap spacings or other utility-equivalent styles. Replace them with appropriate utility classes directly in the HTML templates.

3. **Retain semantic class names** in HTML for key elements to support extensibility by themes or plugins, even if no custom CSS is initially applied. These classes should not have associated SCSS unless advanced styling is required.

   Example:
   ```html
   <div class=""shipping-modal-actions d-flex justify-content-end gap-2 p-3"">
       <button class=""btn btn-light shipping-abort"">Abort</button>
       <button class=""btn btn-primary shipping-submit"">Submit</button>
   </div>
   ```
   - Utilities handle layout/styling (`d-flex`, `gap-2`, `p-3`).
   - Semantic classes (`shipping-modal-actions`, `shipping-abort`) remain for potential overrides.

4. **Allow custom SCSS selectively** when:
   - Complex layouts cannot be achieved cleanly with utilities.
   - Reusable components require encapsulated styles.
   - Performance or readability would suffer from excessive utility class usage (""utility soup"").

5. **Refactor existing components incrementally** to align with this approach during active development or maintenance.

### Consequences
**Positive:**
- Smaller, cleaner SCSS files with reduced duplication.
- Faster UI implementation using utility-first patterns.
- Easier theming through preserved semantic hooks.
- Improved consistency with Bootstrap standards.

**Negative:**
- Slight increase in HTML verbosity due to multiple utility classes.
- Requires developer discipline and awareness of available utilities.
- Initial effort required to audit and refactor legacy SCSS.

### References
- [Bootstrap 5.2 Utilities](https://getbootstrap.com/docs/5.2/utilities/)
- Utility-First CSS principles (e.g., Tailwind CSS methodology)
- Frontend maintainability best practices

### See Also
- ADR-004: Use of Component Libraries
- ADR-012: Theme Extensibility Guidelines","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision: Adopt a Contract-Based, API-First End-to-End Testing Strategy Using Playwright and Behavior-Driven Development (BDD) with Gherkin

### Status
Accepted

### Context
The current Cypress-based E2E test suite is tightly coupled to the test environment state, leading to flakiness, slow execution, and poor determinism. The lack of standardized test design and inability to run tests against cloud environments undermines confidence in automation and hinders scalability. To support reduced manual testing, we require a more robust, reliable, and maintainable test automation strategy that aligns with real product requirements and can execute across environments‚Äîincluding staging and production-like cloud environments.

### Decision
We will transition from the current Cypress-based E2E suite to a **Playwright**-powered, **API-first end-to-end testing framework**, combined with **Behavior-Driven Development (BDD)** using **Gherkin syntax**. This new approach will emphasize:

1. **Decoupling from environment state** via contract testing and API-driven setup.
2. **Deterministic and fast execution** by minimizing UI interactions and using direct API calls for preconditions.
3. **Clear, requirement-driven test cases** written in Gherkin (`.feature` files), making them readable by non-technical stakeholders.
4. **Environment-agnostic execution** through configurable endpoints and secrets management.
5. **A standardized test strategy** based on testing user behaviors, not implementation details.

#### Key Components of the New Approach:
- **Test Framework**: [Playwright](https://playwright.dev/) (over Cypress) due to:
  - Faster execution and better handling of modern SPAs.
  - Built-in auto-waiting, parallelization, and flakiness reduction mechanisms.
  - Strong API testing capabilities alongside browser automation.
  - Cross-browser and headless support out of the box.
- **Test Design**: BDD with Gherkin (using Cucumber or equivalent integration with Playwright).
  - Feature files (e.g., `login.feature`) describe behavior in plain language.
  - Step definitions map Gherkin steps to reusable automation code.
  - Encourages collaboration between QA, product, and dev teams.
- **Test Flow**:
  - Use **API calls** (via Playwright‚Äôs request context) to set up preconditions (e.g., create users, seed data).
  - Perform minimal, critical **UI validation** only to confirm behavior.
  - Assert against APIs or databases where possible for reliability.
- **Contract Validation**:
  - Integrate lightweight contract tests to ensure backend stability across environments.
  - Prevent breaking changes that could destabilize E2E runs.
- **Execution Model**:
  - Tests are environment-agnostic; target URL, credentials, and config are injected at runtime.
  - CI/CD pipelines will run suites against multiple environments (including cloud) using isolated test tenants/data.

### Rationale
- **Determinism & Reliability**: By reducing reliance on UI interactions and managing state via APIs, tests become less flaky and more predictable.
- **Speed**: Skipping lengthy UI flows for setup drastically reduces execution time.
- **Readability & Collaboration**: Gherkin makes tests accessible to product owners and BAs, ensuring alignment with requirements.
- **Scalability**: Playwright supports parallel execution and is well-suited for large test suites.
- **Cloud Readiness**: The decoupled, configuration-driven model enables execution against any environment, including cloud-hosted instances.

### Implications
- **Migration Effort**: Existing Cypress tests will be gradually rewritten or replaced based on priority.
- **Learning Curve**: Teams will need training on BDD, Gherkin, and Playwright.
- **Tooling Investment**: Need to integrate Playwright with BDD tools (e.g., `cucumber-js`, custom reporters) and improve reporting.
- **Shift in Culture**: Requires closer collaboration between QA, development, and product to define and maintain behavior specs.

### Alternatives Considered
| Option | Why Rejected |
|-------|-------------|
| Stay with Cypress + fixes | Would not resolve fundamental architectural flaws (state coupling, speed, flakiness). Incremental fixes offer limited ROI. |
| Cypress + BDD add-ons | Possible, but Cypress is slower and less resilient than Playwright; long-term maintenance concerns remain. |
| Puppeteer + custom framework | Higher maintenance burden; Playwright offers superior tooling and community support. |
| Full contract testing (Pact) instead of E2E | Insufficient for validating integrated user journeys; we still need behavioral validation. |

### Next Steps
1. Set up a PoC with Playwright + Gherkin for a core user journey (e.g., user registration ‚Üí login ‚Üí dashboard).
2. Define a test strategy document outlining BDD practices, folder structure, and naming conventions.
3. Train QA and development teams on BDD and Playwright.
4. Build shared utilities for API setup, authentication, and data cleanup.
5. Integrate into CI/CD to run against cloud environments with isolated data contexts.
6. Deprecate legacy Cypress tests incrementally as new ones are validated.

---

*This decision enables us to build a sustainable, scalable, and collaborative test automation practice aligned with business goals.*","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","**Decision: Encapsulate database transaction handling within a dedicated service for flow actions to ensure consistent and observable transaction behavior**

---

### Status
Accepted

### Context
Currently, flow actions that require database operations must manually manage transactions. This leads to several issues:
- Inconsistent error handling: If a transaction commit fails within a flow action, the error is caught and suppressed by the flow dispatcher.
- Silent failure: Only a vague log message is produced, giving no clear indication of what went wrong.
- Transaction poisoning: In nested transaction scenarios (common in Shopware due to lack of savepoint support by default), a failed inner commit marks the entire connection as ""rollback only"". The outer transaction then unexpectedly fails on commit, with no clear origin of the root cause.

This undermines data integrity, complicates debugging, and violates the principle of least surprise.

---

### Decision
Introduce a **`FlowTransactionHandler`** service responsible for managing transaction boundaries in flow actions. This service will:
- Provide a safe API for flow actions to perform database operations within a transactional context.
- Ensure that if a transaction fails, it is explicitly rolled back and the error is re-thrown after logging.
- Prevent silent suppression of transaction errors by not catching `Throwable` types indiscriminately in the flow dispatcher.
- Integrate with Shopware‚Äôs connection model and respect its limitations around savepoints.

Flow actions must use this service instead of directly managing transactions via the entity manager or connection.

Example usage in a flow action:
```php
public function handleFlow(FlowEvent $event): void
{
    $this->transactionHandler->transact(function () use ($event) {
        // DB operations
        $this->entityManager->persist($someEntity);
        $this->entityManager->flush();
    });
}
```

Additionally:
- Update the flow dispatcher to allow critical exceptions (e.g., `TransactionException`) to propagate when appropriate, so calling code can react.
- Log detailed context (e.g., flow name, action type, event metadata) when a transaction fails.

---

### Consequences
**Positive:**
- Improved reliability: Failed transactions are no longer silently ignored.
- Better observability: Clear logs and traceable error origins help debugging.
- Predictable behavior: Avoids ""phantom"" rollbacks caused by poisoned connections.
- Centralized control: Easier to audit, test, and extend transaction logic.

**Negative:**
- Requires refactoring existing flow actions to use the new handler.
- Slight performance overhead due to additional indirection (negligible in practice).
- Learning curve for developers used to manual transaction management.

---

### Alternatives Considered
1. **Do nothing**  
   ‚Üí Leads to continued data inconsistency and operational fragility. Rejected.

2. **Modify flow dispatcher to re-throw all errors**  
   ‚Üí Too coarse; could break legitimate error suppression patterns. Rejected.

3. **Use savepoints if available**  
   ‚Üí Not feasible by default in Shopware MySQL setups; adds complexity without solving root cause. Partial mitigation only. Rejected.

---

### References
- [Shopware Connection Documentation](https://developer.shopware.com/docs)
- Doctrine DBAL Transaction Management Guide
- Martin Fowler ‚Äì Service Layer Pattern","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","### Decision: Disable Vue 3 Compatibility Mode in Core Administration Codebase

#### Status
Accepted

#### Context
Vue 3 introduced a compatibility mode (also known as ""Legacy Mode"") to facilitate the migration from Vue 2. While this mode enables the use of Vue 2 APIs and behaviors within Vue 3, it is intended solely as a temporary bridge. Maintaining compatibility mode indefinitely prevents teams from fully leveraging Vue 3's performance improvements, compositional patterns (e.g., Composition API), and modern reactivity system.

In our case, the compatibility mode has been kept enabled to reduce friction for plugin developers during the transition from Vue 2 to Vue 3. This allowed core administration and third-party plugins to be migrated independently across different major versions. However, continued reliance on compatibility mode introduces technical debt, limits future optimizations, and delays full alignment with Vue 3 best practices.

Additionally, keeping compatibility mode active increases long-term maintenance costs and risks creating inconsistencies when plugins eventually upgrade.

#### Decision
We will disable Vue 3 compatibility mode in the core administration codebase in the next major release. This requires:

1. **Full migration** of all internal components to Vue 3 standards:
   - Replace `Vue.extend`, `Vue.component`, and other global APIs.
   - Migrate options API usage where beneficial to composition API (especially for reusable logic).
   - Update event handling (`$on`, `$off`, `$once` removal) and lifecycle hooks (e.g., `beforeDestroy` ‚Üí `beforeUnmount`).
   - Ensure proper usage of new reactivity APIs (`ref`, `reactive`, etc.).

2. **Deprecation communication**:
   - Issue clear deprecation warnings in the current minor version when compatibility features are used internally.
   - Publish detailed migration guides for internal and external developers.

3. **Plugin support strategy**:
   - Maintain backward compatibility *at the API contract level* where possible (e.g., props, emitted events).
   - Provide migration tooling or codemods to assist plugin authors.
   - Offer an extended grace period or optional compatibility wrappers for plugins that lag behind.

4. **Testing & validation**:
   - Run comprehensive end-to-end tests after disabling compatibility mode.
   - Verify that all critical admin workflows function correctly under strict Vue 3 semantics.

#### Consequences
**Positive**:
- Enables full utilization of Vue 3‚Äôs performance and developer experience benefits.
- Reduces technical debt and aligns the codebase with modern Vue standards.
- Encourages ecosystem-wide upgrade momentum.
- Simplifies future framework upgrades by removing legacy abstraction layers.

**Negative**:
- Increases short-term effort for both core and plugin teams.
- May temporarily fragment plugin compatibility if not well-coordinated.
- Requires careful planning and communication to avoid breaking third-party integrations.

#### Alternatives Considered
- **Keep compatibility mode enabled indefinitely**: Rejected due to increased technical debt, reduced performance, and blocking future Vue upgrades.
- **Gradual per-component migration without disabling mode**: Rejected because it delays the inevitable and provides no incentive to complete the migration.

#### Additional Notes
This decision does not prevent individual plugins from using their own compatibility strategies during their migration window. The core responsibility is to lead by example and provide stable, forward-looking foundations. Disabling compatibility mode signals commitment to modern development standards and long-term sustainability.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","**Decision: Upgrade to Vuex 4.1.0 and implement a local workaround for the getter reactivity bug**

### Status
Accepted

### Context
The current version of Vuex in use is affected by a critical bug in version `4.1.0` that breaks getter reactivity under specific conditions. While downgrading to `4.0.2` was considered, it introduces separate, blocking issues related to module initialization, making it an unsuitable rollback target. No patch version between `4.0.2` and `4.1.0` resolves both problems. Remaining on an older, incompatible version is not sustainable due to dependency constraints and missing improvements.

Additionally, Vuex `4.1.0` includes important fixes and better compatibility with newer Vue 3 ecosystems. The getter reactivity issue, while impactful, occurs only in well-defined scenarios (e.g., nested getters accessing dynamically registered modules before initialization), allowing for targeted mitigation.

### Decision
Proceed with upgrading to Vuex `4.1.0` and apply a **local workaround** in affected code areas to restore reactivity. The workaround consists of:
- Using `store.getters` within computed properties with defensive null/undefined checks.
- Introducing `$store.subscribe` or `watchEffect` where necessary to manually track changes in problematic getters.
- Ensuring all dynamic modules are registered before any getter access during component setup.
- Adding inline comments and TODOs to flag temporary workarounds for future removal when a patched Vuex version is released.

Additionally, subscribe to the Vuex GitHub repository for updates and contribute to the relevant issue to help expedite an official fix.

### Consequences
**Pros:**
- Enables use of a more stable base version (`4.1.0`) with other critical fixes.
- Maintains forward compatibility with the Vue 3 ecosystem.
- Avoids regressions from `4.0.2` that impact module system integrity.
- Workaround is isolated and documented, minimizing long-term tech debt.

**Cons:**
- Temporary increase in code complexity due to manual reactivity handling.
- Risk of forgetting to remove workarounds once Vuex releases a fix.
- Slight performance overhead in some components using manual watchers.

### Follow-up Actions
- Create a tracking ticket for removing workarounds when a fixed Vuex version is released.
- Add a project-wide comment convention (`// FIXME: Vuex getter reactivity workaround`) for easy grepping.
- Consider pinning Vuex version to `4.1.0` explicitly to prevent accidental upgrades until resolution.

This decision balances immediate stability with forward progress, ensuring system integrity while awaiting a permanent upstream fix.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","**Architectural Decision Record (ADR)**

---

**Title:** Replace Traditional Design Patterns with a Plugin and Event-Driven Architecture for Extensibility

**Status:** Accepted

**Context:**

Our current architecture relies on PHP design patterns‚Äîspecifically **Decorator**, **Adapter**, and **Factory**‚Äîto enable extensibility and customization by third-party developers. While these patterns have served us well in encapsulating behavior and enabling polymorphism, they introduce significant challenges:

1. **Backward/Forward Compatibility Burden:**  
   Any modification to core classes or interfaces risks breaking third-party implementations. Maintaining compatibility across versions requires meticulous care and often results in technical debt.

2. **Limited Subprocess Extension:**  
   These patterns do not naturally support extending internal steps of a process unless each step is extracted into its own class and interface‚Äîa practice that leads to:

3. **Code Proliferation:**  
   The necessity to extract every customizable subprocess results in an explosion of interfaces, abstract classes, and boilerplate code. This harms code readability, increases maintenance costs, and raises the barrier to entry for new developers.

We require a more scalable, maintainable, and future-proof mechanism for allowing third-party extensibility without coupling plugins tightly to our internal class hierarchy.

**Decision:**

We will transition from reliance on structural design patterns (Decorator, Adapter, Factory) for extensibility toward a **plugin and event-driven architecture**, using **events/hooks and a registry-based plugin system**.

Specifically:
- Introduce a **domain event system** where key lifecycle points emit events (e.g., `OrderCreated`, `PaymentProcessing`, `UserRegistered`).
- Allow third-party plugins to register **listeners or subscribers** to these events.
- Define **extension points (hooks)** within core processes where external logic can be injected via a central registry.
- Support **service-level decoration through a declarative plugin system**, where plugins can wrap or enhance services without inheritance or manual decorator chains.
- Provide a **plugin manifest format** and loading mechanism to manage third-party extensions safely and predictably.

This approach decouples extension logic from core implementation details, reducing tight coupling and minimizing the surface area affected by internal changes.

**Rationale:**

- **Improved Compatibility:**  
  Since plugins react to stable, versioned events or hook contracts rather than concrete classes, internal refactoring has less impact on third-party code.

- **Fine-Grained Extensibility:**  
  Events and hooks allow interception at specific subprocess stages without requiring extraction into separate classes.

- **Reduced Code Bloat:**  
  Eliminates the need for numerous adapter/decorator classes and abstract factories. Behavior is added through simple callback functions or lightweight listener classes.

- **Easier Onboarding:**  
  Plugin model is more intuitive and widely understood (e.g., WordPress, Symfony bundles, Laravel service providers), lowering the learning curve.

- **Runtime Flexibility:**  
  Plugins can be enabled/disabled dynamically, and their execution order can be controlled via priorities.

**Implications:**

‚úÖ *Pros:*
- Greater long-term maintainability.
- Easier evolution of internal architecture without breaking extensions.
- More natural expression of cross-cutting concerns (logging, analytics, integrations).

‚ùå *Cons:*
- Requires initial investment to identify and formalize extension points.
- Event payload immutability and versioning must be carefully managed.
- Debugging distributed logic across listeners may become harder if not properly traced.

‚ö†Ô∏è *Migration Strategy:*
- Begin by identifying high-impact extension points currently using Decorator/Adapter patterns.
- Gradually expose them as events or hooks alongside existing mechanisms.
- Deprecate old extension methods over one or two major versions.
- Provide migration guides and tooling for third-party developers.

**Related Decisions:**
- [ADR-001: Use PSR-14 for Event Dispatcher Interoperability]
- [ADR-005: Define Plugin Lifecycle and Autoloading Mechanism]

**Notes:**
This shift aligns with modern PHP ecosystems (e.g., Symfony, Shopware, Magento 2) that favor event dispatching and modular plugins over deep inheritance hierarchies. It positions us to support marketplace-style extensions in the future.

--- 

**Decision Owner:** Chief Architect  
**Date:** 2025-04-05","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","**Decision: Introduce a Dedicated Jest Runner with Compat Mode Disabled for Component Tests**

To support the incremental migration away from React compat mode, we will introduce a new Jest test runner configuration that explicitly disables compat mode. This runner will be used for component tests, enabling us to verify that components function correctly in a strict, forward-compatible environment.

### Context
Our current component tests run under React‚Äôs compat mode, which masks potential issues related to upcoming breaking changes in React (e.g., automatic batching, stricter useEffect semantics). As part of our long-term goal to fully adopt modern React behavior, we must ensure all components work without compat mode.

Running all tests without compat mode immediately is risky due to the volume of existing components. Instead, we adopt a phased approach by introducing a separate test execution path.

### Decision
1. **Create a New Jest Runner Configuration**:  
   Add a new Jest configuration file (e.g., `jest.config.no-compat.js`) that:
   - Extends the base Jest config.
   - Sets `react.compat.mode` to `false` via test environment or global setup.
   - Targets a subset of component tests intended to be compat-mode-free.

2. **Use Environment Override or Setup Script**:  
   Use a custom test environment or `setupFilesAfterEnv` script to disable compat mode at runtime:
   ```js
   // setup-no-compat.js
   React.unstable_disbleCOMpatMode?.(); // or equivalent when available
   ```

3. **Gradual Test Migration**:  
   - Initially, run only a small set of updated component tests using this runner.
   - Gradually move more component tests to this configuration as they are verified to work without compat mode.

4. **CI Integration**:  
   Add a new CI job that runs tests using the no-compat runner, ensuring visibility and preventing regressions in non-compat environments.

5. **Documentation & Naming**:  
   Clearly document the purpose of the new runner and label tests or directories accordingly (e.g., `__tests_no_compat__/` or file suffixes).

### Rationale
- **Safety**: Allows incremental validation without breaking the entire test suite.
- **Visibility**: Provides clear signal on which components are ready for a post-compat world.
- **Forward Compatibility**: Aligns with React‚Äôs long-term direction and reduces future technical debt.

### Consequences
- **Increased CI Time**: An additional test job will increase pipeline duration slightly.
- **Configuration Overhead**: Maintaining two Jest configs requires discipline.
- **Migration Effort**: Teams must actively migrate tests, requiring coordination and tracking.

### Next Steps
- Implement the new Jest config.
- Pilot with one component library or module.
- Monitor failures and adjust components accordingly.
- Establish tracking (e.g., checklist or GitHub label) for full migration.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","**Decision: Implement Comprehensive, Component-Level Health Checks with Tiered Monitoring and Pre-Deployment Validation**

To proactively detect functional errors before system rollouts and ensure early identification of component failures in production, we will implement a tiered health checking strategy that includes:

1. **Component-Level Health Checks**:  
   Each critical component (e.g., databases, APIs, message queues, external service integrations) must expose its own health endpoint indicating its operational status (e.g., using HTTP `/health` with statuses like `UP`, `DOWN`, `DEGRADED`). These checks will validate connectivity, responsiveness, and key functionality.

2. **Aggregated System Health Endpoint**:  
   The system will provide a top-level health endpoint that aggregates the status of all components. This endpoint will distinguish between:
   - **Critical components**: Failure causes overall system status to be `DOWN`.
   - **Non-critical components**: Failure results in `DEGRADED` status but allows the system to remain `UP`.

3. **Pre-Deployment Health Validation in CI/CD Pipeline**:  
   Integrate automated health checks into the staging and pre-production environments. Deployment to production will be blocked if any critical component fails its health check during the final validation phase.

4. **Canary Releases with Health Check Monitoring**:  
   Roll out new versions to a small subset of users or servers first, and monitor health metrics closely before full deployment. Automated rollback will be triggered if health thresholds are breached.

5. **Real-Time Alerting and Observability**:  
   Connect health check outputs to monitoring tools (e.g., Prometheus, Grafana, Datadog) to generate real-time alerts on component degradation or failure, enabling rapid response.

**Rationale**:  
This decision addresses the context where undetected functional errors reached production by ensuring visibility into component health throughout the lifecycle. By validating health not just at the system level but at the component level‚Äîand enforcing checks before and after deployment‚Äîwe reduce the risk of undetected failures impacting users.

**Status**: Accepted  
**Date**: 2024-04-27  
**Author**: Architectural Review Board","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","**Architectural Decision Record (ADR)**

---

**Title:** Allow Major Feature Flag Activation for Additional Test Namespaces in Unit Test Suite  
**Status:** Accepted  
**Date:** 2025-04-05  

---

### **Context**

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` currently restricts the activation of major feature flags during unit testing to test classes residing **only** in the `Shopware\Tests\Unit` namespace. This limitation prevents other valid unit test suites‚Äîsuch as those in `Shopware\Tests\Integration\Unit`, `Shopware\Tests\Custom\Unit`, or third-party bundles using similar conventions‚Äîfrom enabling major features even when running in a unit-test context.

This restriction leads to inconsistent behavior and reduced reusability of test utilities and configuration logic across different parts of the codebase that follow unit-testing practices but reside under alternative namespaces.

Additionally, as Shopware evolves and supports more modular or plugin-oriented development, enforcing a single hardcoded namespace becomes an architectural bottleneck.

---

### **Decision**

Modify the `TestPreparationStartedSubscriber` to **allow major feature flag activation for any test class identified as part of the unit test suite**, based on configurable or heuristic criteria rather than a strict namespace match.

Specifically:

1. **Introduce a flexible namespace pattern or configuration option** (e.g., via environment variable or XML config) to define which namespaces qualify as ""unit tests"" for feature flag purposes.
   - Example: `UNIT_TEST_NAMESPACE_PATTERN=""^Shopware\\\\Tests\\\\(\\w+)\\\\Unit($|\\\\)""`

2. **Fallback to current behavior** (`Shopware\Tests\Unit`) if no custom pattern is defined, ensuring backward compatibility.

3. Optionally, consider using PHPUnit's built-in test suite categorization (e.g., groups or attributes) to determine test type instead of relying solely on namespace.

4. Update documentation to reflect how developers can extend or customize this behavior in plugins or custom test setups.

---

### **Rationale**

- **Flexibility**: Supports modular development and custom test organization without requiring core modifications.
- **Consistency**: Ensures that all unit-style tests‚Äîregardless of sub-namespace‚Äîare treated equally regarding feature flag availability.
- **Maintainability**: Reduces hardcoding and moves toward configuration-driven behavior.
- **Future-proofing**: Accommodates evolving test structures in monorepos, plugin ecosystems, or split test suites.

Rejecting this change would result in technical debt, repeated workarounds, and friction for contributors outside the main codebase.

---

### **Consequences**

**Positive:**
- Enables broader adoption of major features in unit-like test environments.
- Supports better test architecture in plugins and external bundles.
- Easier evolution of internal test structure without breaking feature flag logic.

**Negative:**
- Slight increase in complexity due to pattern matching or configuration parsing.
- Risk of misconfiguration leading to unintended activation of major features in integration-like contexts (mitigated by clear docs and defaults).

---

### **Related Issues**
- SW-12345: Feature flags not activating in custom unit test directories
- SW-12678: Plugin unit tests fail to enable next-major functionality

---

### **Approver(s)**
- @core-arch-team  
- @phpunit-extension-maintainer

--- 

> *‚ÄúDesign for the 95%, configure for the rest.‚Äù*","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","# Architectural Decision Record: Automated Migration from Vue 2 Options API to Vue 3 Composition API

## Status
Accepted

## Context
Our Vue.js application is currently built using the Vue 2 Options API, which limits code reusability, makes complex component logic harder to manage, and provides suboptimal TypeScript integration. With the adoption of Vue 3 across our technology stack, we aim to leverage the advantages of the Composition API‚Äîsuch as improved logical code organization, better type inference with TypeScript, and enhanced composability of reusable logic via custom composables.

Migrating a large number of components manually would be labor-intensive, error-prone, and inefficient. An automated approach is necessary to ensure consistency, reduce migration time, and minimize human errors during the transition.

Additionally, maintaining backward compatibility during a gradual migration may be required, especially if parts of the application remain in maintenance mode or are used in production during the transition.

## Decision
We will adopt an **automated tooling strategy** to convert existing Vue 2 Options API components to the Vue 3 Composition API using [`vue-codemod`](https://github.com/voieapp/vue-codemod) (or similar actively maintained codemod tools like [`jscodeshift`](https://github.com/facebook/jscodeshift) with Vue-specific transformers).

Specifically:
- Use `vue-codemod` with the `options-to-composition` transformation to automatically rewrite Options API syntax into equivalent Composition API code.
- Run the transformation in a **staged, incremental manner**, starting with low-risk, standalone components.
- Manually review and test all transformed components to verify correctness, particularly around lifecycle hooks, `this` references, and reactivity.
- Supplement automated conversion with custom scripts for project-specific patterns (e.g., plugin integrations, mixins, or legacy helpers).
- Adopt [Vue‚Äôs `<script setup>`](https://vuejs.org/api/sfc-script-setup.html) syntax for all new and migrated components to align with modern Vue best practices.

During migration, maintain dual compatibility where needed by:
- Using [`@vue/composition-api`](https://github.com/vuejs/composition-api) for Vue 2 projects requiring early adoption of Composition API features.
- Ensuring build pipeline supports both syntaxes temporarily.

## Rationale
### Why automate?
- **Efficiency**: Hundreds of components can be processed rapidly.
- **Consistency**: Automation ensures uniform output and reduces human error.
- **Scalability**: Enables parallel migration efforts across teams.

### Why `vue-codemod`?
- Purpose-built for Vue migrations.
- Actively maintained and community-tested.
- Supports common Options API patterns (data, methods, computed, lifecycle hooks).
- Outputs clean, readable Composition API code compatible with `<script setup>`.

### Why not manual rewrite?
While a full manual rewrite offers maximum control, it is impractical due to:
- High development cost.
- Risk of introducing bugs unrelated to functionality.
- Delayed realization of benefits from Composition API.

### Why not migrate everything at once?
A big-bang migration introduces significant risk. A phased approach allows:
- Continuous delivery during migration.
- Incremental testing and validation.
- Rollback capability per component.

## Implications
### Positive
- Accelerates migration timeline significantly.
- Encourages consistent use of modern Vue patterns.
- Improves long-term maintainability and developer experience.
- Better support for TypeScript and IDE tooling post-migration.

### Negative / Risks
- Automated tools may misinterpret complex or non-standard component structures (e.g., deeply nested mixins, dynamic options).
- Some edge cases (like `this.$refs`, `provide/inject`, or `watch` with strings) require manual fixes.
- Learning curve for developers unfamiliar with Composition API.
- Temporary increase in technical complexity due to mixed codebase.

### Mitigations
- Conduct pilot migration on non-critical components first.
- Implement thorough unit and integration testing after each batch.
- Provide internal documentation and training on the Composition API.
- Maintain a migration checklist and code review template.

## Additional Notes
- After migration, deprecate use of Options API in coding standards.
- Update ESLint rules and CI pipelines to enforce Composition API usage for new files.
- Plan eventual removal of any compatibility shims (e.g., `@vue/composition-api`) once fully on Vue 3.

## References
- [Vue 3 Composition API Guide](https://vuejs.org/guide/reusability/composables.html)
- [Migration Build for Vue 3](https://v3-migration.vuejs.org/)
- [`vue-codemod` GitHub Repository](https://github.com/voieapp/vue-codemod)
- [Shopware Documentation on System Updates](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html)","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJs√É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬Øs inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in Vue√É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬Øs templating and reactivity model.","**Architectural Decision Record (ADR)**

---

### **Title**  
Replace TwigJs Block System with Native Vue.js Component-Based Architecture Using `sw-block` and `sw-block-parent`

---

### **Status**  
Accepted

---

### **Context**  
The current frontend architecture uses **TwigJs** to implement a block-based templating system, enabling plugin developers to override or extend UI components via Twig‚Äôs template inheritance. While this approach has provided extensibility, it presents several limitations:

- **Limited reactivity**: TwigJs templates are rendered statically and do not integrate seamlessly with Vue.js reactivity.
- **Performance overhead**: The hybrid use of server-side templating (via Twig) and client-side rendering (via Vue) introduces complexity and runtime inefficiencies.
- **Developer experience**: Mixing templating languages increases the learning curve and reduces consistency in component development.
- **Modernization gap**: The industry trend favors fully reactive, component-driven frameworks like Vue.js over static template inheritance.

Vue.js offers a more powerful, dynamic, and maintainable solution through its component model. To preserve and enhance the existing extensibility features (e.g., allowing plugins to override parts of the UI), we propose introducing two new Vue components:  
- `sw-block`: Represents a named, reusable UI section that can be overridden.  
- `sw-block-parent`: Serves as a container that renders `sw-block` instances and supports content extension/inheritance logic in Vue.

This shift aligns with our long-term goal of a fully decoupled, reactive frontend architecture.

---

### **Decision**  
We will deprecate the TwigJs-based block system and replace it with a native Vue.js implementation using the new components:

1. **Introduce `sw-block`**  
   - A wrapper component that defines a named block region in a parent component.
   - Supports scoped slots for dynamic content projection.
   - Allows registration under a global name for discoverability and potential override mechanisms.

2. **Introduce `sw-block-parent`**  
   - A resolver component that dynamically renders the appropriate `sw-block` implementation based on naming and registration.
   - Enables plugin systems to register alternative versions of a block (e.g., via a registry pattern).
   - Facilitates ""extend"" behavior by optionally rendering default slot content alongside overridden content.

3. **Implement a Block Registry**  
   - Maintain a global registry (e.g., via Vue app config or plugin) where blocks can be registered or overridden by plugins.
   - Support priority-based resolution to manage override order.

4. **Migration Path**  
   - Maintain backward compatibility during transition using a compatibility layer (e.g., render TwigJs blocks inside Vue components temporarily).
   - Provide migration guides and codemods for plugin developers.

5. **Deprecation Plan**  
   - Mark TwigJs block usage as deprecated in documentation.
   - Remove TwigJs dependency in the next major version.

---

### **Consequences**

#### **Positive**
- ‚úÖ Improved reactivity and performance through full integration with Vue‚Äôs rendering pipeline.
- ‚úÖ Better developer experience with consistent Vue syntax and tooling (e.g., DevTools, TypeScript support).
- ‚úÖ Enhanced flexibility via scoped slots, dynamic components, and programmatic block resolution.
- ‚úÖ Future-proof architecture aligned with modern frontend practices.
- ‚úÖ Easier testing and debugging within component trees.

#### **Negative**
- ‚ö†Ô∏è Initial development cost to build and test the new components and registry.
- ‚ö†Ô∏è Migration effort required for core and third-party plugins.
- ‚ö†Ô∏è Potential runtime complexity in block resolution logic if not well-documented.

#### **Neutral**
- üü° Requires clear conventions for naming and registering blocks to avoid conflicts.

---

### **Alternatives Considered**
1. **Keep TwigJs blocks**  
   - Pros: No migration needed.  
   - Cons: Blocks progress toward a fully reactive UI; limits interactivity and integration.

2. **Use Vue slots only (no custom components)**  
   - Pros: Simpler, idiomatic Vue.  
   - Cons: Lacks centralized override mechanism needed for plugin ecosystem.

3. **Adopt Web Components instead**  
   - Pros: Framework agnostic.  
   - Cons: Higher complexity; doesn‚Äôt solve the need for dynamic overrides in a Vue-centric codebase.

---

### **Related Decisions**
- [ADR-001] Frontend Framework Standardization on Vue.js  
- [ADR-005] Plugin System for Component Overrides  

---

### **Notes**  
This decision enables a scalable, maintainable path forward for a plugin-extensible admin interface. The `sw-block` and `sw-block-parent` components will become foundational elements in the Vue-driven UI architecture.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the component√É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬Øs internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `√É¬É√Ç¬Ç√É¬Ç√Ç¬Ä 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","**Decision: Replace the asterisk symbol next to every price with a standardized, reusable tax-inclusive pricing label that appears contextually near price displays, and update accompanying legal text for clarity and compliance.**

### Status
Accepted

### Context
The current implementation appends an asterisk (`*`) to every displayed product price in the storefront (e.g., `‚Ç¨50,00 *`), which references a footnote in the page footer stating:  
`* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

This approach has several downsides:
- **Visual clutter**: The repeated asterisk creates noise, especially on product listing pages with many items.
- **Poor accessibility**: Screen readers may read the asterisk literally, confusing users unless properly marked up (which is often inconsistent).
- **User experience**: Users must scroll to the footer to understand the meaning, breaking their flow.
- **Scalability**: If additional disclaimers are needed, managing multiple symbols becomes unwieldy.
- **Design rigidity**: Hard to adapt across devices or locales without duplication.

### Decision
Replace the per-price asterisk with a **contextual, reusable UI component**‚Äîa discreet but visible **""incl. VAT"" label**‚Äîthat appears adjacent to or beneath price information. This label will:
- Be implemented as a shared frontend component (e.g., `<PriceInfoLabel />`).
- Appear consistently next to prices (e.g., below or inline: ‚Äú‚Ç¨50,00‚Äù + ‚Äúincl. VAT‚Äù).
- Include a tooltip or link to full terms (shipping, delivery charges) when space allows (e.g., on desktop).
- Retain compliance by linking to or summarizing the full legal text in the footer.

Additionally:
- Remove the asterisk from individual prices.
- Update the footer text to:  
  _‚ÄúAll prices include VAT. Shipping costs and possible delivery charges apply ‚Äî see our [Shipping & Returns] policy.‚Äù_
- Ensure the new label is translatable and respects regional tax disclosure requirements.

### Rationale
- **Improved UX**: Reduces visual noise while keeping critical tax information visible where users expect it.
- **Better Accessibility**: Semantic HTML (e.g., `aria-label`, tooltips with `role=""tooltip""`) improves screen reader support.
- **Maintainability**: A single reusable component ensures consistency and simplifies future updates.
- **Compliance**: Legal requirements for tax transparency are still met, with clearer separation between tax and shipping disclosures.
- **Flexibility**: Easier to adapt for markets with different tax rules (e.g., excluding VAT in B2B views).

### Consequences
**Positive:**
- Cleaner, more professional storefront appearance.
- Enhanced usability and accessibility.
- Easier localization and regulatory adaptation.

**Negative:**
- Requires frontend refactoring to replace all instances of `*` with the new component.
- Design adjustments may be needed for responsive layouts (e.g., hiding tooltip on mobile).
- Temporary inconsistency during rollout if not deployed globally at once.

### Approval
Architect: @LeadFrontendArchitect  
Legal/Compliance: @LegalAdvisor  
UX Lead: @UXDirector  

**Date:** 2024-04-15","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision

**Replace DomPDF with an accessible PDF generation solution that supports WCAG-compliant output, and implement structured, semantic HTML templates to ensure proper document accessibility.**

### Rationale

DomPDF lacks critical accessibility features required by WCAG 2.1 (e.g., proper tagging for screen readers, logical reading order, alternative text support, color contrast control), making it unsuitable for generating accessible documents. Continuing its use would result in non-compliance with legal standards such as EN 301 549, Section 508, or the European Accessibility Act.

Instead, we will migrate to a PDF engine that supports **tagged PDFs** and **accessibility metadata**, such as:

- **mPDF** ‚Äì Supports tagged PDFs, Unicode, and WCAG-friendly output when paired with semantic HTML/CSS.
- **PrinceXML** ‚Äì Full support for accessible PDFs from HTML, including ARIA and structural semantics (commercial license required).
- **WeasyPrint** ‚Äì Open-source option with growing accessibility support via proper CSS and HTML semantics.

Additionally, we will enforce the following development practices:
- Use of semantic HTML (`<h1>‚Äì<h6>`, `<p>`, `<table>` with headers, etc.) in document templates.
- Programmatic inclusion of alt text for images.
- High-contrast color schemes compliant with WCAG AA.
- Logical tab order and reading sequence.
- Testing using automated tools (e.g., axe, Pa11y) and PDF accessibility checkers (e.g., PAC, Adobe Acrobat Pro).

### Consequences

**Positive:**
- Documents meet WCAG 2.1 Level AA requirements.
- Improved usability for users relying on assistive technologies.
- Reduced legal and compliance risks.
- Future-proofing for evolving accessibility regulations.

**Negative:**
- Migration effort required to update templates and backend integration.
- Potential licensing cost if choosing PrinceXML.
- Slight performance overhead depending on new library.

### Status

‚úÖ Accepted

### Date

2024-04-05

### Authors

Architecture Team, Accessibility Working Group","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","**Decision: Restrict extension of core rule classes and introduce a plugin-safe rule interface**

To address the growing complexity and maintenance burden of the rule system, we will limit direct inheritance of core rule classes (e.g., `Rule`, `AndRule`, `OrRule`) by making them final or sealing their inheritance. Instead, we will introduce a well-defined, stable plugin interface ‚Äî such as a `CustomRuleInterface` ‚Äî that allows plugins to implement custom logic in a controlled and forward-compatible manner.

This change improves long-term maintainability by:
- Reducing the surface area affected by internal changes to the rule engine.
- Enabling safer refactoring and performance optimizations in core rules.
- Encouraging plugin developers to compose behavior rather than override it.
- Supporting backward compatibility through explicit extension contracts.

Plugins requiring custom conditions must now use the new interface and register their rules via a dedicated factory or DI tag, ensuring consistency and discoverability.

This decision aligns with the goals outlined in RFC #5785 to simplify evolution of the rule system while preserving extensibility within supported boundaries.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision: Deprecate and Remove the Custom Iterator Helper in Favor of Native JavaScript Iteration Patterns

### Status
Accepted

### Context
The `src/helper/iterator.helper.js` utility was introduced to provide a unified way to iterate over various iterable structures such as Arrays, Maps, FormData, and NodeLists. However, this abstraction has proven to be unnecessary and counterproductive for several reasons:

- Modern JavaScript provides robust, standardized iteration mechanisms (e.g., `for...of`, `forEach`, `for...in`, spread syntax) that work across all common data types.
- The helper often wraps native methods without adding meaningful logic, increasing cognitive load without benefit.
- Developers are already familiar with native iteration patterns, whereas the custom helper requires additional documentation and learning.
- Inconsistent usage across the codebase leads to confusion‚Äîsome files use native loops, others use the helper.
- Most iterated values (like `NodeList`) are already natively iterable, making the helper redundant.
- Every usage introduces an extra import, increasing bundle size and module coupling unnecessarily.

### Decision
We will **deprecate and remove** the `Iterator` helper class in favor of **native JavaScript iteration constructs**. Going forward:

- New code must use native iteration patterns:
  - `array.forEach()`, `for...of`, `for...in`, or array methods like `map`, `filter`, etc.
  - Use spread syntax (`[...iterable]`) when converting to arrays is needed.
  - Leverage built-in iterability of DOM collections like `NodeList`, `HTMLCollection`, `Map`, `Set`, etc.
- Existing uses of `Iterator.iterate()` should be replaced during active development or refactoring.
- A codemod may be created to automate migration where possible.
- The helper will be marked as deprecated in documentation and via JSDoc comments before removal in a future major release.

### Consequences
**Positive:**
- Reduces code complexity and cognitive overhead.
- Aligns with web standards and improves developer onboarding.
- Eliminates unnecessary abstraction and dependency.
- Encourages more performant and type-appropriate looping strategies (e.g., `for...of` vs `forEach`).
- Improves consistency by standardizing on native language features.

**Negative:**
- Requires effort to migrate existing code (mitigated by gradual replacement and potential codemods).
- Short-term increase in technical debt during transition if not fully enforced immediately.

### Notes
This change supports our broader goal of reducing custom abstractions in favor of well-supported, standard JavaScript practices. It also aligns with modern browser support, where native iterability is widely available.

---

**Next Steps:**
1. Mark `iterator.helper.js` as deprecated with warning comments.
2. Update internal documentation and linting rules to discourage usage.
3. Create a codemod script to auto-replace common `iterate()` patterns.
4. Remove the file in the next major version.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","# Architectural Decision Record (ADR)

---

## **Title**  
Defer Flow Execution After Business Process Completion

## **Status**  
Proposed

## **Context**  

Currently, flows (user-defined automation logic) are executed synchronously during the execution of business processes‚Äîsuch as checkout completion or product updates. These flows are triggered via a decorator around Symfony‚Äôs `EventDispatcher`, which intercepts domain events and immediately invokes associated flow executions.

This synchronous execution model presents several critical issues:

1. **Risk of Business Process Interruption**:  
   If a flow throws an unhandled fatal error, it can abort the entire business process‚Äîeven after core domain logic has successfully completed. This leads to poor user experience and potential data inconsistency.

2. **Performance Degradation**:  
   Flows often perform expensive side effects (e.g., sending emails, calling external APIs). Executing them inline blocks the main request thread, increasing response time and reducing system throughput.

3. **Operational Complexity**:  
   Debugging is difficult due to deeply nested call stacks caused by the decorator-based interception. Stack traces include irrelevant flow execution details, obscuring the actual business logic flow.

4. **Tight Coupling**:  
   The current design tightly couples flow execution to event dispatching, limiting flexibility for future enhancements such as retry mechanisms, observability, or prioritization.

These concerns motivate a shift toward decoupling flow execution from the primary business process lifecycle.

> *Note: This change is gated behind the feature flag* `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` *and will not affect existing behavior until explicitly enabled.*

## **Decision**  

We will defer the execution of flows until **after** the business process has successfully completed. Specifically:

- Domain events will still trigger flow conditions **immediately**, but instead of executing flows synchronously, we will schedule them for **asynchronous deferred execution**.
- A dedicated Symfony event listener (replacing the current decorator) will collect eligible flows during the request lifecycle.
- Upon successful completion of the business process (i.e., transaction commit, response ready), these flows will be dispatched to a **message queue** (e.g., RabbitMQ, Amazon SQS) or executed in a post-commit hook if queuing is unavailable.
- Flow execution will occur in an isolated environment (e.g., via a console command consumer or serverless function), separate from the main request cycle.

This approach ensures that:
- Business processes cannot be rolled back or failed due to flow errors.
- User-facing latency is reduced, as no flow work blocks the HTTP response.
- Error handling, retries, and monitoring can be implemented specifically for flows.
- Debugging becomes easier with cleaner stack traces and explicit entry points.

## **Consequences**  

### **Positive**

‚úÖ **Improved Reliability**:  
Business processes complete independently of flow success/failure.

‚úÖ **Better Performance**:  
Removes costly operations (email, webhooks) from the critical path.

‚úÖ **Enhanced Observability & Maintainability**:  
Dedicated listeners and execution environments simplify logging, tracing, and debugging.

‚úÖ **Scalability & Extensibility**:  
Lays the foundation for advanced capabilities like flow prioritization, rate limiting, retries, and distributed execution.

‚úÖ **Safer Upgrades**:  
Feature-flagged rollout allows incremental adoption and rollback if needed.

### **Negative**

‚ùå **Eventual Consistency**:  
Side effects (e.g., confirmation emails) are no longer immediate. This may require UX adjustments (e.g., messaging about delayed notifications).

‚ùå **Increased Infrastructure Complexity**:  
Requires message broker setup and maintenance for reliable async processing.

‚ùå **Potential Duplication or Loss**:  
Needs idempotency controls and proper error handling in consumers to avoid missed or duplicated flow runs.

‚ùå **Development Overhead**:  
Developers must now test both sync domain logic and async flow behaviors separately.

### **Neutral / Mitigated**

üîß **Transaction Boundaries**:  
Flows previously relying on active database transactions may need adaptation. Solution: Ensure all required data is serialized or reloaded at execution time.

üîß **Local Development Experience**:  
Async behavior might complicate local testing. Mitigation: Provide development modes where flows run synchronously post-commit (but still after business process).

## **Implementation Steps (High-Level)**

1. Introduce a `FlowExecutionScheduler` service responsible for collecting flows during event handling.
2. Replace the `EventDispatcher` decorator with a dedicated event listener that registers flows without executing them.
3. Hook into Doctrine‚Äôs `onFlush` or use a post-response kernel event to dispatch scheduled flows **only on successful completion**.
4. Serialize flow definitions and context, then publish to a message queue.
5. Build a worker command (`bin/console flow:consume`) to process queued flows.
6. Add monitoring, retry logic, and dead-letter queues for failed executions.
7. Roll out behind feature flag; monitor impact before full release.

## **References**

- [Martin Fowler - Event Sourcing](https://martinfowler.com/eaaDev/EventSourcing.html)
- [12-Factor App - Admin Processes](https://12factor.net/admin-processes)
- Symfony Messenger component documentation
- Feature flag: `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS`

---

## **Decision Makers**  
[To be filled: e.g., Lead Architects, CTO]

## **Date**  
2025-04-05","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","**Decision: Adopt the Architectural Decision Record (ADR) format based on Michael Nygard's lightweight ADR template**

### Status
Proposed

### Context
Architectural decisions‚Äîsuch as technology selection, system decomposition, and integration patterns‚Äîare made continuously during a project‚Äôs lifecycle. Without proper documentation, these decisions become tribal knowledge, increasing onboarding time, risking regression, and reducing transparency. We need a consistent, lightweight, and version-controlled way to record key architectural choices.

Several formats exist for capturing ADRs, including:
- Michael Nygard‚Äôs ADR template (lightweight, markdown-based)
- RFC-style documents (more formal, often used in larger orgs)
- Wiki pages (flexible but harder to version and link)

We require a solution that is:
- Easy to write and read
- Version-controlled alongside code
- Traceable to implementation
- Minimal overhead to encourage adoption

### Decision
We will use **Michael Nygard‚Äôs ADR format**, stored as Markdown files in a dedicated `docs/adr` directory within the main repository. Each ADR will follow this structure:

```markdown
## Title
## Status
(e.g., Proposed, Accepted, Deprecated, Superseded)

## Context
The forces and constraints that led to this decision.

## Decision
The specific choice we made.

## Consequences
The good and bad outcomes‚Äîtechnical, operational, and organizational.
```

New ADRs will be added with sequential numeric prefixes (e.g., `001-use-adrs.md`, `002-choose-web-framework.md`) to maintain order.

### Rationale
- **Lightweight**: Markdown is easy to edit and review via pull requests.
- **Version-controlled**: ADRs live in Git, so they evolve with the codebase.
- **Traceable**: Pull requests referencing ADRs can link decisions to implementation.
- **Widely adopted**: The format is well-known in the industry, easing onboarding.
- **Scalable**: Works for small teams and large projects alike.

Alternatives like RFC repositories or wiki systems introduce extra tooling and fragmentation.

### Implications
- ‚úÖ **Positive**
  - Improved onboarding and knowledge sharing.
  - Better audit trail for future refactoring.
  - Encourages deliberate, documented design discussions.

- ‚ö†Ô∏è **Negative**
  - Requires discipline to maintain.
  - Risk of outdated or incomplete records if not enforced.

### Next Steps
1. Create `docs/adr` directory.
2. Add the first ADR (this one).
3. Include ADR creation in the definition of done for significant architectural changes.
4. Add a team guideline encouraging ADRs for non-trivial decisions.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision

**Develop a new TUF Repository Toolkit as a separate, modular, and composable library that provides high-level orchestration tools for building TUF repositories, while cleanly separating concerns between metadata manipulation, storage, policy, and application logic.**

### Rationale

To address the limitations of both the current Metadata API (too low-level) and the legacy `python-tuf` implementation (bloated, inflexible, and poorly structured), we will create a **TUF Repository Toolkit** that:

1. **Builds on the existing Metadata API** ‚Äî leverages its clean, modern interface for reading, writing, and signing individual metadata files.
2. **Adopts a modular, dependency-injected design** ‚Äî avoids global state and allows users to plug in their own:
   - Storage backends (filesystem, cloud, database)
   - Key management systems
   - Timestamp/snapshot policies
   - Automation rules (e.g., expiry refreshes, delegation strategies)
3. **Provides composable components instead of monolithic tools** ‚Äî offers reusable abstractions such as:
   - `RepositoryLayout`: defines metadata hierarchy and file locations
   - `RoleManager`: handles key rotation, signing, and threshold logic
   - `ExpiryPolicy`: configurable auto-expiry and refresh behavior
   - `ChangeSet`: batch operations with atomicity guarantees
4. **Supports both CLI and server-side use cases** ‚Äî enables lazy loading, partial metadata loading, and streaming operations suitable for large-scale repositories.
5. **Clearly separates library vs. application responsibilities** ‚Äî the toolkit assists with common patterns but does not make opinionated choices about performance-critical behaviors (e.g., bulk loading all metadata).

### Example Usage Sketch

```python
from tuf.repository_toolkit import RepositoryBuilder
from myapp.storage import S3Backend
from myapp.policy import AutoSnapshotPolicy

repo = RepositoryBuilder(
    layout=MyCustomLayout(),
    storage=S3Backend(bucket=""my-tuf-repo""),
    policy=AutoSnapshotPolicy(threshold_bytes=1024*1024),
)

# Only load root and targets when needed
with repo.edit(""targets"") as targets:
    targets.add_target(""app-v2.0.tar.gz"", length=... )
```

### Consequences

- ‚úÖ **Improved maintainability**: Smaller, testable modules replace 7000+ lines of intertwined code.
- ‚úÖ **Better scalability**: Avoids loading all metadata; supports incremental updates.
- ‚úÖ **Greater flexibility**: Applications can customize behavior without forking.
- ‚ö†Ô∏è **New surface area to maintain**: Requires disciplined API design and versioning.
- ‚ûï **Enables future tooling**: Foundation for GUIs, CI/CD integrations, audit tools.

This decision ensures that developers get practical help implementing TUF repositories without inheriting the technical debt of older implementations.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision: Migrate Primer React Components from Ambient Type Definitions to Native TypeScript Implementation

### Status
Accepted

### Context
Primer React was initially developed in JavaScript without first-class TypeScript support. To enable adoption in TypeScript projects, ambient type declarations (`index.d.ts`) were introduced in July 2019 as a pragmatic workaround. While this approach successfully unblocked early TypeScript usage, it has led to several ongoing challenges:

- **Type-Implementation Drift**: Types are manually maintained and frequently fall out of sync with actual component APIs.
- **Increased Maintenance Burden**: Every change in components requires a parallel update in `index.d.ts`, increasing review complexity and risk of errors.
- **Poor Developer Experience**: Users encounter incorrect or missing types, leading to confusion, workarounds, and numerous open issues (e.g., #906, #540, #520, #534).
- **Limited Type Safety**: Ambient declarations cannot leverage inferred types, generics, or advanced TypeScript features based on runtime logic.

As the library grows, these issues compound, making long-term sustainability questionable.

### Decision
We will incrementally migrate Primer React components from JavaScript with ambient types to **native TypeScript implementation** (`.ts` and `.tsx` files). This includes:

1. **Rewriting components in TypeScript**, removing reliance on `index.d.ts`.
2. **Removing the ambient `index.d.ts`** file once all exports are covered by inline types.
3. **Adopting a gradual migration strategy**, converting one package or component at a time to minimize disruption.
4. **Enforcing type checks in CI** using strict TypeScript settings to prevent regressions.
5. **Using tooling like `tsc --noEmit` and `@typescript-eslint`** to ensure code quality during and after migration.

New components must be written in TypeScript. Existing JavaScript files will be converted as they are modified or when type inaccuracies are reported.

### Rationale
- **Accuracy & Maintainability**: Co-located types reduce drift and ensure that API changes automatically reflect in types.
- **Better DX**: Developers get precise autocompletion, refactoring support, and compile-time error detection.
- **Future-Proofing**: Enables use of advanced TypeScript patterns (e.g., conditional types, utility types) for more expressive component APIs.
- **Industry Standard**: Most modern React libraries (e.g., MUI, Chakra UI, Radix UI) are built in TypeScript for these reasons.

Alternatives considered:
- **Continue maintaining ambient types**: Rejected due to unsustainable overhead and recurring bugs.
- **Auto-generate types from JSDoc**: Rejected because it still lacks full fidelity and doesn't scale well with complex React patterns (e.g., polymorphic components).

### Consequences
**Positive**
- Improved type accuracy and reliability.
- Reduced number of TypeScript-related bug reports.
- Easier onboarding for new contributors familiar with TypeScript.
- Stronger foundation for building design system tooling (e.g., doc generation, lint rules).

**Negative**
- Upfront engineering cost to convert existing codebase.
- Learning curve for contributors less familiar with TypeScript.
- Potential for merge conflicts during transition if not coordinated.

**Mitigations**
- Use codemods where possible to automate file conversion.
- Provide internal documentation and pair programming sessions for team enablement.
- Coordinate migrations per package to avoid widespread conflicts.

### References
- [Original ambient types addition](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa)
- Issue #906: ""Incorrect prop types for Button""
- Issue #540: ""TypeScript: Modal component missing onClose prop""
- TypeScript Handbook: [Ambient Declarations](https://www.typescriptlang.org/docs/handbook/declaration-files/introduction.html)","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example √É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬É√É¬Ç√Ç¬∫](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example √É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬É√É¬Ç√Ç¬∫](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision: Standardize on Box-Based Component Composition in Primer React

### Context
Primer React currently uses multiple patterns for building components, primarily:
1. **`styled-components`** ‚Äì leveraging `styled()` with attribute injection and template literal styles.
2. **`Box`-based composition** ‚Äì using the primitive `<Box>` component with the `sx` prop to apply design system-aware styles.

While both approaches are functional, they introduce inconsistency in implementation patterns, increase the learning curve for contributors, and complicate long-term maintenance.

---

### Decision
We will standardize on the **`Box`-based component composition pattern** for all new and refactored components in Primer React. This means:

- Prefer using `<Box as={...}>` with the `sx` prop to build new components.
- Avoid creating new styled-components unless absolutely necessary (e.g., complex dynamic styling not feasible via `sx`).
- Refactor existing `styled-component` implementations to use `Box` during active maintenance or major version updates.
- Continue supporting existing `styled` components until deprecated in a major release.

---

### Rationale

| Factor | Why Box Wins |
|-------|--------------|
| **Consistency** | Using `Box` across components ensures a uniform API and reduces cognitive load for developers. |
| **Theming & Tokens** | The `sx` prop is tightly integrated with the design system (`theme`), making token usage (spacing, colors, radii) safer and more predictable. |
| **Accessibility & Props Handling** | `Box` correctly forwards props and supports accessible defaults; `styled-components` can inadvertently drop or misroute attributes if not carefully managed. |
| **Bundle Size** | Reducing reliance on `styled-components` can lower bundle impact by minimizing CSS-in-JS runtime overhead (especially static styles). |
| **Flexibility** | `Box` supports dynamic rendering via the `as` prop and composes cleanly with `sx`, `merge`, and spreading. |
| **Future-Proofing** | Aligns with modern trends toward utility-first and primitive-based UI systems (e.g., Dripsy, Panda CSS, Radix UI). |

> Exceptions may be made for performance-critical components requiring highly optimized static styles, or when advanced `styled-components` features (e.g., global keyframes, component selectors) are essential.

---

### Consequences

**Positive**
- Easier onboarding for new team members due to reduced pattern fragmentation.
- Improved consistency in how styles are applied and overridden.
- Better alignment with Primer‚Äôs design system via `sx`.
- Simplified testing and debugging (fewer tagged template literals to parse).

**Negative**
- Some legacy `styled` components will remain temporarily, leading to short-term duplication of patterns.
- Minor runtime cost of additional wrapper elements (mitigated by React 18+ optimizations).
- Requires discipline to avoid inline style bloat in `sx` objects.

---

### Example (Recommended Pattern)

```tsx
const Avatar: React.FC<AvatarProps> = ({ size = 20, alt = '', square = false, sx: sxProp = {}, ...props }) => {
  const styles: BetterSystemStyleObject = {
    display: 'inline-block',
    overflow: 'hidden',
    lineHeight: 'condensedUltra',
    borderRadius: getBorderRadius({ size, square }),
  }

  return (
    <Box
      as=""img""
      alt={alt}
      sx={merge(styles, sxProp)}
      {...props}
    />
  )
}
```

---

### Status
**Accepted**

### Date
2025-04-05

### Authors
Architectural Review Board, Primer React Team","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision

We will continue to use the `drafts` directory as a stable mechanism for introducing experimental, in-progress, or parallel versions of components within `@primer/react`. This approach provides clear separation between stable and unstable APIs, enables incremental adoption, supports side-by-side versioning, and reduces risk during component evolution.

Access to components under `@primer/react/drafts` is considered opt-in and subject to the following guidelines:

- **Stability**: Components in `drafts` are not covered by semantic versioning guarantees. Breaking changes may occur in patch or minor releases.
- **Usage Intent**: Usage of `drafts` components in production applications must be coordinated with the Primer team. They are intended for early collaboration, internal experimentation, or phased rollouts‚Äînot general consumption.
- **Documentation**: All `drafts` components must be documented internally with clear status (e.g., ‚Äúexperimental‚Äù, ‚Äúin review‚Äù, ‚Äúcandidate for promotion‚Äù) and a path forward.
- **Lifecycle Management**: Each `drafts` component must have an owner and a defined timeline for either graduation to the main export or removal.

Over time, we may automate detection of `drafts` usage in consumer codebases to proactively communicate upcoming changes or deprecations.

This pattern aligns with our goals of safe component evolution, modular development, and controlled API surface growth.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision: Limit Snapshot Testing to Shallow Rendering and Use Explicit Unit Tests for Behavioral Intent

### Status
Accepted

### Context
Snapshot testing has proven effective for catching unintended changes in UI output, particularly in React component suites. However, as the codebase scales, we've observed several anti-patterns:
- Large, deeply nested snapshots that are hard to read and review.
- Brittle tests due to incidental changes (e.g., event handler function references, generated IDs, or styling class names).
- Lack of clarity about test intent ‚Äî when a snapshot fails, it's often unclear whether the change is functional or cosmetic, and whether it should be accepted or fixed.

Additionally, teams spend significant time reviewing large diffs without understanding the significance of the changes, leading to ""snapshot fatigue"" where developers blindly accept changes just to make tests pass.

### Decision
We will **restrict full DOM tree snapshot testing** of React components using tools like `react-test-renderer` or `@testing-library/react`'s deep rendering. Instead, we adopt the following practices:

1. **Use shallow rendering snapshots only when appropriate**  
   If snapshot testing is used, it must be with shallow rendering (via libraries like Enzyme‚Äôs `shallow()`), limiting the output to the direct child components and avoiding deep tree serialization. This reduces noise and size.

2. **Prefer explicit unit tests for behavior and structure**  
   Replace broad snapshot assertions with targeted, intent-driven tests that verify:
   - Correct props passed to child components.
   - Presence of key elements (e.g., `expect(screen.getByRole('button')).toBeInTheDocument()`).
   - Expected text content or attribute values.
   - Event handlers are attached correctly (e.g., `onClick` invocation).

3. **Allow full snapshots only with justification**  
   Full render snapshots may be used sparingly for critical visual components (e.g., design system primitives) but must include:
   - A clear comment explaining why a snapshot is necessary.
   - Use of serializers (e.g., `jest-serializer-html`) to clean up irrelevant details like React-specific keys or function strings.
   - Regular cleanup and review of obsolete snapshots.

4. **Introduce property-based testing where applicable**  
   For components with multiple states or inputs, use parameterized tests to validate expected outcomes across variations rather than relying on monolithic snapshots.

5. **Enforce linting rules**  
   Add ESLint rules (e.g., via `jest-dom`, `testing-library`) to discourage `.toMatchSnapshot()` usage in new files unless exempted.

### Example

‚úÖ Preferred ‚Äì Intent-revealing test:
```tsx
test('renders link with correct href and label', () => {
  render(<Link page=""http://example.com"">Example</Link>);
  const link = screen.getByRole('link', { name: 'Example' });
  expect(link).toHaveAttribute('href', 'http://example.com');
});
```

üö´ Avoided ‚Äì Overbroad snapshot:
```tsx
test('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://example.com"">Example</Link>).toJSON();
  expect(tree).toMatchSnapshot(); // Hard to reason about, includes implementation noise
});
```

### Consequences
**Pros:**
- Tests become more maintainable and readable.
- Failures clearly indicate what broke and why.
- Reduces risk of accepting meaningful regressions due to diff overload.
- Encourages thinking about component contracts and behavior over static output.

**Cons:**
- Requires more upfront effort to write descriptive tests.
- Slight increase in test boilerplate.
- Learning curve for team members accustomed to snapshot-first workflows.

### Review Period
This decision will be reviewed after 6 months, based on metrics including:
- Number of snapshot-related PR rejections or confusion.
- Test flakiness reports.
- Developer survey on test clarity and confidence.

---

> **Summary**: Prioritize *intent* over *implementation*. Snapshot tests obscure what we're actually verifying; explicit assertions keep our tests focused, understandable, and resilient.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision: Use Conditional Development-Only Warnings with Dead Code Elimination

To address the concern of including developer warnings in production bundles, we will adopt a pattern that ensures `console.warn()` calls and related development-time checks are automatically stripped from production builds.

### Chosen Approach

Use a helper utility that wraps `console.warn()` and is conditionally executed only in development mode, leveraging **dead code elimination** via environment variables.

```ts
// utils/warning.ts
export function warning(condition: boolean, message: string) {
  if (__DEV__ && !condition) {
    console.warn(message);
  }
}

// Usage in components
warning(
  typeof onClick === 'function',
  '@primer/react Button: onClick must be a function, received ' + typeof onClick
);
```

Alternatively, for deprecation warnings:

```ts
export function deprecatedWarning(component: string, oldProp: string, newProp?: string) {
  if (!__DEV__) return;

  const message = newProp
    ? `${component}: The ""${oldProp}"" prop is deprecated. Use ""${newProp}"" instead.`
    : `${component}: The ""${oldProp}"" prop is deprecated and will be removed in a future version.`;

  console.warn(message);
}
```

### Implementation Details

1. **Define `__DEV__` Environment Variable**  
   Set `__DEV__` equal to `process.env.NODE_ENV !== 'production'` during build time (already standard in most tooling like Webpack, Vite, etc).

2. **Enable Tree Shaking & Dead Code Elimination**  
   Ensure the build process:
   - Replaces `__DEV__` with a literal boolean (`true`/`false`)
   - Enables minification (e.g., Terser) to remove unreachable branches

   Example Webpack/Terser configuration:
   ```js
   new TerserPlugin({
     terserOptions: {
       dead_code: true,
       global_defs: {
         __DEV__: process.env.NODE_ENV !== 'production'
       }
     }
   })
   ```

3. **Strip Logs in Production**  
   With this setup, code inside `if (__DEV__)` blocks will be completely removed from production bundles when minified.

### Benefits

- ‚úÖ `console.warn()` and associated logic are excluded from production builds
- ‚úÖ No runtime performance cost in production
- ‚úÖ Clear, reusable pattern for emitting dev-only warnings
- ‚úÖ Works seamlessly with existing bundlers and treeshaking
- ‚úÖ Supports type checking and linting

### Trade-offs

- ‚ùó Requires consistent use of the pattern ‚Äî direct `console.warn()` calls must be audited and avoided
- ‚ùó Slight increase in bundle size during development (negligible)

### Enforcement

- Add ESLint rule to ban bare `console.warn()` calls in favor of `warning()` utility
- Document the pattern in contribution guidelines and code reviews

---

**Status**: Accepted  
**Date**: 2025-04-05  
**Relevant Tools**: Webpack, Vite, Rollup, Terser, ESLint","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision: Adopt a Centralized Live Region Manager for Consistent Announcements in Primer React

To ensure reliable and consistent accessible announcements across Primer React components, we will implement a **centralized Live Region Manager** that programmatically controls live region announcements via a React context or singleton service pattern. This manager will expose a simple API for declarative and imperative announcements while handling the underlying complexities of ARIA live regions.

### Chosen Solution

We will create a `LiveRegion` context/provider system with the following features:

1. **Dedicated Persistent Live Region Elements**: One or more hidden but always-mounted `<div>` elements with appropriate ARIA attributes (`aria-live=""polite""` and/or `aria-live=""assertive""`, `aria-atomic=""true""`, `role=""status""` or `alert` as needed) placed at the root of the application (e.g., within a `PrimerProvider`).

2. **Announcement API**: Expose a hook (`useLiveRegion()`) that allows components to trigger announcements:
   ```tsx
   const { announcePolite, announceAssertive } = useLiveRegion();
   
   // Usage
   announcePolite('Form submitted successfully.');
   announceAssertive('Error: Unable to save file.');
   ```

3. **Controlled Mutation**: Instead of relying on DOM injection or visibility toggles, the manager updates the text content of the persistent live region element(s), ensuring screen readers detect changes reliably.

4. **Debouncing & Coalescing**: Implement minimal debouncing (e.g., 100ms) to prevent rapid-fire announcements from interrupting each other or being dropped.

5. **Support for Multiple Roles**: Provide variants for different semantics:
   - `announcePolite()` ‚Üí uses `aria-live=""polite""` + `role=""status""`
   - `announceAssertive()` ‚Üí uses `aria-live=""assertive""` + `role=""alert""`

6. **Fallback Mechanism**: Ensure the live region container is rendered early in the document (e.g., just after `<body>`) to maximize compatibility, even if JavaScript hasn't fully loaded.

### Rationale

- **Reliability**: By keeping the live region element persistently mounted and only updating its text content, we avoid the common pitfall of dynamically injecting live regions into the DOM, which many screen readers fail to recognize.
  
- **Consistency**: Provides a single, well-documented pattern across all Primer React components, reducing accessibility bugs due to inconsistent implementation.

- **Flexibility**: Supports both immediate feedback (e.g., form submission) and dynamic updates (e.g., search result count) through a unified interface.

- **Compliance**: Aligns with WAI-ARIA best practices and real-world testing results showing that persistent live regions with content mutation are most effective.

- **Developer Experience**: Simplifies authoring accessible components by abstracting complex ARIA behaviors behind an easy-to-use API.

### Implementation Notes

- The actual live region DOM nodes should be inserted unconditionally near the root of the app (ideally outside any conditional rendering trees).
- Use `data-testid` or similar for testing purposes without exposing internals.
- Document usage clearly in Primer‚Äôs accessibility guidelines.

### Alternatives Considered

| Option | Why Not Chosen |
|-------|----------------|
| Direct use of `role=""status""` or `aria-live` on component elements | Unreliable when elements are mounted/unmounted or shown/hidden; inconsistent across screen readers. |
| Using `aria-live` with dynamic insertion | As noted in the context, this often fails because screen readers don‚Äôt observe newly added live regions. |
| Third-party libraries (e.g., `react-aria-live`) | Adds external dependency; prefer internal control and alignment with GitHub‚Äôs design system needs. |

### Future Considerations

- Support for internationalization (i18n) in announcements.
- Queueing mechanism for high-volume announcement scenarios.
- Integration with error tracking or telemetry for failed announcements (if detectable).

---

By adopting this centralized pattern, Primer React ensures robust, predictable, and accessible live announcements across GitHub‚Äôs UI surface, improving the experience for assistive technology users.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"**Decision:** Adopt a microservices architecture with containerization using Docker and orchestration via Kubernetes to enhance scalability, maintainability, and deployment agility.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision: Adopt Testing Library with Interactive Storybook Integration for Unified Component Testing

### Status
Accepted

### Context
Our current testing setup uses Jest for unit tests and Chromatic for visual regression testing, with Storybook serving as a manual playground for component interactions. This has led to two key issues:
- **Duplication of effort**: Test scenarios are manually recreated in both Storybook stories and Jest test files.
- **Poor debuggability and visibility**: Jest tests lack visual feedback, making them hard to understand, debug, and maintain‚Äîespecially for team members less familiar with programmatic testing.

We need a more cohesive, efficient, and observable testing workflow that reduces redundancy and improves developer experience.

### Decision
Introduce **React Testing Library (RTL)** as the standard for component testing, tightly integrated with **Storybook's `play` functions** and **Chromatic snapshots**, creating a unified testing model where:

1. **Stories become executable test cases** using Storybook‚Äôs `play` functions to simulate user interactions directly within the browser.
2. **Jest + RTL tests reuse existing Storybook stories** via `@storybook/testing-react`, eliminating markup duplication.
3. **Visual testing is enhanced** by running Chromatic on stories with interaction sequences, not just static states.
4. **Debugging is improved** through real-time visual rendering during test development and failure inspection via Chromatic.

### Rationale
- **Eliminates duplication**: By sourcing component render logic from stories, Jest tests avoid recreating props and wrappers.
- **Improves observability**: Developers can visually validate behavior by viewing stories with interactions instead of reading CLI output.
- **Enables interactive testing**: Storybook‚Äôs `play` functions allow defining user flows (e.g., click, type) that can be run automatically in CI via `chromatic --interactions`.
- **Leverages existing tools**: No new tools required‚Äîjust better integration between Storybook, RTL, Jest, and Chromatic.
- **Developer-friendly**: Team members can write and debug tests using the browser UI, lowering the barrier to contribution.

### Implementation Steps
1. Refactor existing Jest tests to use `@storybook/testing-react`‚Äôs `renderFromStory` or `composeStories` to instantiate components from stories.
2. Convert key interactive test scenarios into Storybook stories with `play` functions using RTL syntax.
3. Enable interaction testing in Chromatic by configuring `interactionTestThreshold` and ensuring play functions run in CI.
4. Deprecate standalone Jest-rendered component setups that duplicate story markup.
5. Train team on writing `play` functions and reusing stories in tests.

### Example
```tsx
// Button.stories.tsx
export const ClickableButton = () => <Button>Submit</Button>;
ClickableButton.play = async ({ canvasElement }) => {
  const canvas = within(canvasElement);
  await userEvent.click(canvas.getByText('Submit'));
};
```

```ts
// button.test.ts
import { composeStories } from '@storybook/testing-react';
import * as ButtonStories from './Button.stories';

const { ClickableButton } = composeStories(ButtonStories);

test('triggers action on click', async () => {
  const { container } = render(<ClickableButton />);
  await userEvent.click(screen.getByText('Submit'));
  // expect something to happen
});
```

### Consequences
- ‚úÖ Reduced code duplication between stories and tests.
- ‚úÖ Visual debugging via Storybook during development.
- ‚úÖ Automated interaction testing via Chromatic.
- ‚ùå Initial migration cost to refactor tests and stories.
- ‚ùå Requires discipline to keep stories and tests in sync (but now they share source, reducing drift).

### Status
Accepted","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","### Decision: Adopt a Monorepo Tool to Unify Dependency Management and Workspace Orchestration

#### Status
Accepted

#### Context
The `primer/react` repository consists of multiple workspaces (e.g., top-level package, `docs`, `examples/*`) that currently operate in isolation with respect to dependency management. Contributors must manually run `npm install` in each workspace or rely on the `setup` script to automate this. Inter-package dependencies are managed via fragile mechanisms such as relative file paths (`file:../`), manual `npm link`, or symlinking, which are error-prone, non-portable, and not scalable.

Additionally, project scripts under the `script/` directory vary in complexity and often require their own dependencies, but they lack a consistent execution environment or dependency isolation strategy. As these grow, maintaining them becomes harder without proper tooling.

This fragmentation leads to:
- Inconsistent developer experiences.
- Brittle local development and CI workflows.
- Difficulty in sharing code or configurations across packages.
- Lack of visibility into inter-dependencies.
- Poor scalability for future growth.

#### Decision
Adopt **[pnpm](https://pnpm.io/)** as the monorepo package manager with workspace support enabled. Restructure the project into a unified pnpm workspace by:
1. Introducing a root `pnpm-workspace.yaml` file to define all workspaces (e.g., `packages/*`, `docs`, `examples/*`).
2. Migrating from individual `npm install` calls to a single `pnpm install` at the root that bootstraps all workspaces.
3. Replacing relative/file-based dependencies and `npm link` usage with direct workspace references using version ranges like `^` or `workspace:*`.
4. Refactoring complex scripts in the `script/` directory into dedicated internal workspace packages under `tools/*` or `scripts/*`, each with its own `package.json` if needed, allowing them to declare and manage their own dependencies.
5. Using `pnpm`‚Äôs built-in [script running capabilities](https://pnpm.io/cli/run) (`pnpm --filter <pkg> run <script>`) to execute commands across workspaces consistently.

Example:
```yaml
# pnpm-workspace.yaml
packages:
  - '.'
  - 'docs'
  - 'examples/*'
  - 'tools/**'
```

Update cross-dependencies:
```json
// docs/package.json
{
  ""dependencies"": {
    ""@primer/react"": ""workspace:*""
  }
}
```

Introduce tooling workspace:
```json
// tools/build-docs/package.json
{
  ""name"": ""@primer/tools-build-docs"",
  ""private"": true,
  ""dependencies"": {
    ""esbuild"": ""^0.19.0"",
    ""glob"": ""^8.0.0""
  },
  ""bin"": ""./index.js""
}
```

Execute via:
```bash
pnpm --filter docs run dev
pnpm --filter '@primer/tools-*' run test
```

#### Rationale
- **pnpm** offers efficient disk usage via hard linking and robust workspace support with filtering, making it ideal for monorepos.
- It natively supports `workspace:` protocol, enabling seamless linking between local packages without manual `link` or `file:` hacks.
- A single `pnpm install` at the root ensures reproducible installs across environments.
- Decentralized script management via `tools/*` workspaces improves maintainability and allows scripts to have precise dependencies.
- Better integration with modern tooling (TypeScript, Turborepo, etc.) in the future.

Alternatives considered:
- **Lerna + npm/yarn**: Adds complexity without solving core installation inefficiencies; largely superseded by native workspace features in modern managers.
- **Yarn Workspaces**: Similar capabilities, but pnpm‚Äôs strictness and superior performance (faster installs, smaller disk footprint) make it preferable.
- **Turborepo**: Excellent for build orchestration but works best *on top of* a package manager like pnpm. Can be layered later.

#### Implications
- ‚úÖ Unified, fast, and reliable dependency installation with `pnpm install`.
- ‚úÖ Automatic linking of local packages eliminates need for `npm link` or file dependencies.
- ‚úÖ Clear ownership and dependency boundaries for scripts and tools.
- ‚úÖ Enables future optimizations (caching, task pipelines) via tools like Turborepo or Nx.
- ‚ö†Ô∏è Requires contributors to install `pnpm` (can be enforced via `corepack` or install script).
- ‚ö†Ô∏è Initial migration effort required to convert existing setup and update documentation.

#### Next Steps
1. Add `packageManager` field to root `package.json` (e.g., `""packageManager"": ""pnpm@8.15.0""`).
2. Install pnpm via `corepack enable` or standard installer.
3. Create `pnpm-workspace.yaml` and validate workspace discovery.
4. Replace file-relative and linked deps with `workspace:*`.
5. Move complex scripts into `tools/` with own `package.json`.
6. Update CI and `setup` script to use `pnpm install`.
7. Document new workflow for contributors.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision: Restrict Public API Surface by Introducing a Dedicated `public` Directory

To prevent unintended exposure of internal implementation details in `@primer/react`, we will **limit the public API surface** by introducing a dedicated `public/` directory (or `exports/`) that explicitly defines which modules are part of the public API.

### Decision

We will restructure the codebase so that only files under a new `src/public/` directory are included in the package's `exports` field and thus exposed as part of the public API. All other files in `src/`‚Äîsuch as utilities, private components, or internal hooks‚Äîwill remain internal and not be accessible via `import` from outside the package unless explicitly re-exported through the public interface.

#### Implementation Plan:
1. Create a new `src/public/` directory.
2. Move or re-export only those modules intended for public consumption into this directory.
3. Update the `package.json` `exports` field to map paths under `'./lib-esm/*'` to `src/public/*` instead of `src/*`.
4. Adjust Rollup configuration to bundle only the `public/` entry points.
5. Add lint rules or tooling (e.g., `no-restricted-imports`) to prevent accidental imports from non-public paths within the package itself.
6. Document the convention: ""Only files in `src/public/` are considered public; all other files are internal.""

#### Example Updated Export Pattern:
```json5
{
  ""exports"": {
    // ...
    ""./lib-esm/*"": {
      ""import"": ""./src/public/*.js"",
      ""require"": ""./lib-cjs/*.js""
    }
  }
}
```

### Rationale
- **Prevents Accidental Exposure**: Without an explicit boundary, any file in `src/` can be imported by consumers using deep import paths (e.g., `@primer/react/lib-esm/internal/utils`). This creates maintenance burdens and breaks encapsulation.
- **Improves Maintainability**: A clear separation between public and private APIs allows refactoring internals without fear of breaking downstream users.
- **Aligns with Best Practices**: Modern packages (e.g., Material UI, Radix UI) use similar patterns to control their public surface area.

### Consequences
- ‚úÖ **Positive**: Clear contract with consumers; safer refactors; reduced bundle size risks due to deep imports.
- ‚ö†Ô∏è **Negative**: Additional overhead in organizing exports; requires discipline to maintain boundaries.
- ‚ùå **Mitigated Risk**: Deep imports might still work if Rollup outputs individual files, but they will be unsupported and can be warned against via documentation and tooling.

This change ensures that our public API is intentional, not incidental.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","# Architectural Decision Record: Migrate from Runtime CSS-in-JS to Static CSS with CSS Variables

## Status
**Accepted**

## Context
The current styling architecture in our applications relies heavily on runtime CSS-in-JS (e.g., `@primer/react`'s `sx` prop), which introduces significant performance bottlenecks:

1. **Slow initial page loads** ‚Äì Styles are injected during JavaScript execution, preventing parallelization of style delivery and increasing time-to-interactive, especially on slower devices.
2. **Inefficient server-side rendering** ‚Äì Requires double-rendering to collect styles, adding ~20% overhead to SSR times.
3. **Expensive style updates** ‚Äì Dynamic style recomputation on every render leads to poor re-render performance, particularly at scale (e.g., lists with 1000+ components).

Additionally, the current approach limits our ability to implement dynamic theming and consistent design system guardrails.

Performance benchmarks show:
- 60% faster initial render with static `.css` files vs. runtime injection
- 20‚Äì60% faster updates when avoiding runtime style generation
- SSR time increased by 450ms due to style collection

We also aim to leverage **CSS variables** for improved theme adaptability and runtime theme switching without re-rendering.

## Decision
Migrate from runtime CSS-in-JS (primarily `sx` prop) to a model based on:
1. **Static CSS extraction** using build-time CSS or CSS Modules for base component styles
2. **CSS custom properties (variables)** for theming and dynamic token resolution
3. **Predefined utility/utility-like classes** where appropriate, inspired by Primer Primitives
4. **Limited, optimized use of runtime styling** only when absolutely necessary (e.g., highly dynamic layout values)

This includes:
- Phasing out widespread use of the `sx` prop in favor of semantic class names and CSS variables
- Adopting CSS Modules or similar locally-scoped static solutions for component styles
- Defining a shared design token system mapped to CSS variables (e.g., `--color-fg-default`, `--space-3`)
- Updating Primer Primitives to emit class names and support CSS variable-based theming

## Consequences

### Pros
- ‚úÖ **Improved performance**: Faster FCP, TTI, and update cycles by eliminating runtime style calculation
- ‚úÖ **Better SSR efficiency**: Eliminates need for double-rendering; styles can be pre-generated
- ‚úÖ **True theming support**: CSS variables enable dynamic, run-time theme switching without React re-renders
- ‚úÖ **Smaller JS bundles**: Reduces dependency on CSS-in-JS libraries and generated styles
- ‚úÖ **Stronger design system contracts**: Encourages use of constrained tokens via CSS variables, improving consistency

### Cons
- ‚ö†Ô∏è **Migration effort required**: Existing components using `sx` will need refactoring
- ‚ö†Ô∏è **Learning curve**: Teams must adopt new patterns for styling and theming
- ‚ö†Ô∏è **Slight loss of flexibility**: Highly dynamic inline-style-like usage no longer encouraged
- ‚ö†Ô∏è **Tooling changes needed**: Build pipeline must support CSS extraction and variable transforms

## Notes
This decision aligns with industry trends toward ""zero-runtime"" styling solutions (e.g., Linaria, Tailwind, vanilla-extract) and supports GitHub‚Äôs long-term goals for performance and theming.

We will establish codemods and documentation to assist teams in migrating away from pervasive `sx` usage, while preserving targeted runtime styling for exceptional cases.

## References
- [primer/discussions#1928](https://github.com/github/primer/discussions/1928)
- [gh/gh#241090](https://github.com/github/github/pull/241090)
- [gh/gh#243269](https://github.com/github/github/pull/243269)","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","# ADR 015: Replace Storybook Play Functions with Dedicated Playwright Test Files

## Status
Accepted

## Context
In [ADR 009](./adr-009-interaction-tests.md), we adopted Storybook's `play` functions as the recommended approach for writing interaction-based tests. The intention was to colocate interactive test logic with component stories to improve maintainability and ensure that user interactions were documented alongside visual examples.

However, practical experience has revealed several limitations:
- **Test execution complexity**: Running `play` function tests requires spinning up the entire Storybook environment, which increases test runtime and infrastructure overhead.
- **Debugging challenges**: Failures in `play` functions are harder to debug due to layered abstraction between Storybook, Jest, and Playwright.
- **Limited test isolation**: `play` functions execute within the Storybook iframe context, restricting access to global browser APIs and making setup/teardown operations less predictable.
- **Tooling misalignment**: Playwright is designed for end-to-end testing in real browser contexts, but `play` functions run in a constrained environment that doesn't fully leverage Playwright's capabilities.
- **Developer experience**: Colocating complex interaction logic with stories can make stories harder to understand and maintain, especially when multiple interaction scenarios exist for a single component.

Additionally, our current testing suite has grown to include numerous edge cases requiring sophisticated setup‚Äîbeyond what `play` functions elegantly support.

## Decision
We will deprecate the use of Storybook `play` functions for interaction testing and instead adopt dedicated Playwright test files organized alongside component implementations.

Interaction tests will be written as standalone `.spec.ts` files using Playwright's test API, targeting specific Storybook URLs or local development servers. These tests will:

- Reside in the same directory as the component or adjacent `__tests__` folder
- Use Playwright's page object model and assertions directly
- Be executed via Playwright CLI (rather than through Storybook's test runner)
- Target deployed Storybook builds or locally served instances during CI/CD

Example structure:
```
components/Button/
‚îú‚îÄ‚îÄ Button.tsx
‚îú‚îÄ‚îÄ Button.stories.tsx
‚îî‚îÄ‚îÄ __tests__/
    ‚îî‚îÄ‚îÄ Button.interactions.spec.ts
```

Example test:
```ts
import { test, expect } from '@playwright/test';

test('shows dropdown when button is clicked', async ({ page }) => {
  await page.goto('/iframe.html?id=button--dropdown');
  await page.click('#dropdown-button');
  await expect(page.locator('#dropdown-menu')).toBeVisible();
});
```

Storybook `play` functions should be removed from existing stories unless they serve a pure documentation purpose (e.g., demonstrating an interaction in Storybook Canvas).

## Consequences
### Positive
- ‚úÖ Faster feedback loop with direct Playwright execution
- ‚úÖ Better debugging experience using Playwright Inspector and trace viewer
- ‚úÖ Full access to Playwright's API and browser context
- ‚úÖ Clear separation between documentation (stories) and testing logic
- ‚úÖ Easier parallelization and integration into CI/CD pipelines
- ‚úÖ Consistent pattern across all interaction tests

### Negative
- ‚ùå Loss of colocation between some interactive examples and their tests
- ‚ùå Additional configuration needed to map tests to correct Storybook URLs
- ‚ùå Migration effort required to convert existing `play` function tests

### Mitigations
- Maintain descriptive story titles and args to preserve documentation value
- Adopt naming conventions to clearly link tests to stories (e.g., `ComponentName--story-variant.interactions.spec.ts`)
- Create shared utilities to resolve Storybook URLs from component and story metadata
- Gradually migrate existing `play` functions during active development of affected components

## References
- [Playwright Test API](https://playwright.dev/docs/api/class-test)
- [Storybook Interaction Testing](https://storybook.js.org/docs/react/writing-tests/interaction-testing)
- [Playwright for Visual Regression](https://playwright.dev/docs/test-snapshots)","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","# Architectural Decision Record: Safe Responsive Props for SSR

## Title
Provide a server-side rendering (SSR)-safe mechanism for responsive props to prevent layout shifts

## Status
Accepted

## Context
We support responsive props that accept either a static value or an object mapping breakpoints (`narrow`, `regular`, `wide`) to values. These are resolved client-side using `useResponsiveValue` and `matchMedia`, which works well in-browser but introduces a critical issue during server-side rendering:

- On the server, there is no viewport information
- `useResponsiveValue` must return a fallback value (typically based on a default breakpoint)
- During hydration, if the actual client viewport differs from the server's fallback, the resolved prop value changes
- This causes a post-hydration layout shift when the responsive prop affects layout or styling

This undermines user experience and violates core SSR principles of visual stability.

## Decision
Introduce a new hook `useSSRResponsiveValue` that guarantees consistent server/client behavior by synchronizing breakpoint resolution through:
1. **Server-rendered `<meta>` tag**: Inject a hidden meta tag with the effective breakpoint during SSR
2. **Client-side hydration sync**: Read this initial breakpoint on first render before `matchMedia` listeners attach
3. **Deferred updates**: Only allow `matchMedia` to update the value after initial mount (via `useEffect`)

Additionally, standardize responsive prop types using a generic `ResponsiveProp<T>` type.

```ts
// Type definition
type Breakpoint = 'narrow' | 'regular' | 'wide';
type ResponsiveProp<T> = T | Record<Breakpoint, T>;

// Hook usage
function Stack({ gap }: { gap: ResponsiveProp<StackGap> }) {
  const resolvedGap = useSSRResponsiveValue(gap, 'regular'); // second arg is fallback breakpoint
  // ...
}
```

The hook implementation:

```tsx
function useSSRResponsiveValue<T>(
  value: ResponsiveProp<T>,
  fallbackBreakpoint: Breakpoint = 'regular'
): T {
  const [resolvedValue, setResolvedValue] = useState<T>(() => {
    // Safely access document during SSR
    if (typeof document === 'undefined') {
      return isResponsive(value) ? value[fallbackBreakpoint] : value;
    }

    // Read server-rendered breakpoint hint
    const meta = document.querySelector('meta[name=""breakpoint""]');
    const initialBreakpoint = (meta?.getAttribute('content') as Breakpoint) || fallbackBreakpoint;

    return isResponsive(value) ? value[initialBreakpoint] : value;
  });

  useEffect(() => {
    if (!isResponsive(value)) {
      return;
    }

    const updateValue = () => {
      let bp: Breakpoint = 'narrow';
      if (window.matchMedia('(min-width: 768px)').matches) bp = 'regular';
      if (window.matchMedia('(min-width: 1024px)').matches) bp = 'wide';

      setResolvedValue(value[bp]);
    };

    updateValue();

    const mediaQueries = [
      window.matchMedia('(min-width: 768px)'),
      window.matchMedia('(min-width: 1024px)')
    ];

    const listener = () => requestAnimationFrame(updateValue);
    mediaQueries.forEach(mq => mq.addEventListener('change', listener));

    return () => {
      mediaQueries.forEach(mq => mq.removeEventListener('change', listener));
    };
  }, [value]);

  return resolvedValue;
}
```

And ensure server frameworks inject the correct meta tag based on user-agent/device hints or adaptive serving strategies:

```html
<meta name=""breakpoint"" content=""regular"" />
```

## Consequences
**Positive:**
- ‚úÖ Eliminates layout shifts caused by mismatched server/client responsive prop resolution
- ‚úÖ Maintains full responsive functionality on the client after hydration
- ‚úÖ Backward compatible ‚Äì existing `useResponsiveValue` can be deprecated gradually
- ‚úÖ Provides a clear, standardized pattern for future components

**Negative:**
- ‚ùå Requires coordination between server framework and component library for meta tag injection
- ‚ùå Slight increase in complexity for handling initial vs. dynamic updates
- ‚ùå Initial client render uses hinted breakpoint until `useEffect` runs (but this matches server)

**Neutral/Mitigated:**
- The reliance on a meta tag is a small payload cost but enables isomorphic consistency
- Device detection on server isn't perfect, but even coarse matching (mobile/tablet/desktop) significantly reduces layout shift frequency compared to fixed fallback

## Notes
Consider eventually deprecating the original `useResponsiveValue` in favor of this SSR-safe version once adoption is complete. Documentation should emphasize using `ResponsiveProp<T>` for any prop intended to support responsiveness.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","### Decision: Support Graceful Deprecation of Props with Runtime Warnings and Automated Migration Guidance

When a prop or component API is deprecated in favor of a more accessible or flexible alternative, we will:

1. **Maintain backward compatibility** during a deprecation period by supporting the old API alongside the new one.
2. **Emit a runtime warning** in development mode when the deprecated prop or pattern is used, clearly indicating:
   - That the API is deprecated.
   - What the replacement is.
   - How to migrate (with code examples).
   - The planned removal timeline (if applicable).

3. **Preserve functionality** of the deprecated API using internal fallbacks or transformations (e.g., automatically wrapping `title` in an appropriate heading element with correct accessibility attributes), ensuring no immediate breakage.

4. **Document the deprecation** in public documentation, including:
   - A migration guide.
   - Side-by-side comparisons of old vs. new usage.
   - Accessibility and flexibility benefits of the new API.

5. **Enable automated migration** via codemods where feasible, allowing teams to upgrade at scale.

6. **Remove the deprecated API** in the next major version, following semantic versioning principles.

---

#### Example Implementation

```tsx
// ActionList.Group.tsx
const Group = ({ title, children, ...props }: GroupProps) => {
  // Warn if using deprecated `title` prop
  if (process.env.NODE_ENV !== 'production') {
    if (title != null) {
      console.warn(
        '`title` prop in ActionList.Group is deprecated. Use <ActionList.GroupHeading> instead.\n\n' +
          'Before:\n' +
          '<ActionList.Group title=""Group title"">\n' +
          '  ...\n' +
          '</ActionList.Group>\n\n' +
          'After:\n' +
          '<ActionList.Group>\n' +
          '  <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>\n' +
          '  ...\n' +
          '</ActionList.Group>'
      );
    }
  }

  return (
    <div {...props}>
      {title ? <GroupHeading as=""h2"">{title}</GroupHeading> : null}
      {children}
    </div>
  );
};
```

---

#### Rationale

- **Developer Experience**: Clear warnings guide authors to modern, more accessible patterns without sudden breakage.
- **Accessibility**: The new API (`GroupHeading`) allows semantic HTML structure (e.g., proper heading levels), improving screen reader navigation.
- **Flexibility**: Authors can now customize the heading element (`as=""h1""`, `as=""h3""`, etc.) based on document structure.
- **Maintainability**: Controlled deprecation reduces technical debt and aligns the codebase with current best practices.

---

#### Status

‚úÖ Accepted

#### Date

2025-04-05

#### Labels

deprecation, accessibility, api-design, breaking-change-prep","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision

Adopt the de facto standard of supporting proxy configuration via lowercase environment variables `http_proxy`, `https_proxy`, and `no_proxy` for self-hosted runners.

### Rationale

- **Widespread Convention**: The use of `http_proxy`, `https_proxy`, and `no_proxy` (lowercase) is a well-established pattern across widely used command-line tools and runtimes such as curl, wget, git, Python, Perl, and Docker. This ensures familiarity for system administrators and DevOps engineers.
- **Interoperability**: By aligning with these standards, the self-hosted runner will seamlessly integrate into existing enterprise environments where these variables are already configured at the system or container level.
- **Simplicity and Predictability**: Relying on environment variables avoids the need for additional configuration files or UI settings, reducing complexity and operational overhead.
- **Case Sensitivity Handling**: While some tools prefer uppercase (e.g., `HTTPS_PROXY`), most implementations first check uppercase variants and fall back to lowercase. To maximize compatibility, the runner should follow this precedence:  
  `HTTP_PROXY` ‚Üí `http_proxy`, `HTTPS_PROXY` ‚Üí `https_proxy`, respecting `no_proxy` in both cases, but document primary support for lowercase.

### Implementation Guidance

- The runner should read proxy settings from environment variables at startup.
- Support both `http://` and `https://` proxies as appropriate.
- Respect `no_proxy` for bypassing proxies for specific hosts, IP addresses, or domains.
- Document the supported variables and format clearly in deployment and configuration guides.

### Outcome

Enterprises requiring proxy configurations can deploy self-hosted runners using familiar, standardized environment-based settings, enabling smoother adoption and integration into restricted network environments.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","**Decision: Change Default Command Echoing Behavior to Opt-In**

To improve clarity and reduce noise in user logs, we will change the default behavior of command echoing in the system from opt-out to **opt-in**.

### Rationale
- By default, command echoing produces verbose output that is often irrelevant to successful workflows, leading to cluttered and harder-to-navigate logs.
- Users debugging issues benefit significantly from seeing executed commands, but this should be a conscious choice rather than a default.
- Maintaining visibility during failures ensures users retain critical context for troubleshooting without requiring debug mode.

### Decision Details
1. **Default Behavior**:  
   Command echoing is **disabled by default** unless:
   - The `ACTIONS_STEP_DEBUG` secret is set to `true`, **or**
   - The user explicitly enables echoing using `::echo::on`.

2. **Failure Handling**:  
   If a command fails (non-zero exit code), its command line **will be echoed** regardless of the current echo setting. This ensures failure diagnostics include what was executed.

3. **User Control**:  
   Users may control echoing via special workflow commands:
   - `::echo::on` ‚Äì Enables echoing of subsequent commands.
   - `::echo::off` ‚Äì Disables echoing of subsequent commands.

4. **Backward Compatibility**:  
   This change may affect users who relied on implicit command visibility. However, since it primarily reduces output (not functionality), and debug use cases are preserved via `ACTIONS_STEP_DEBUG` or explicit opt-in, the impact is considered low-risk and aligned with usability best practices.

### Implementation
- Update runner logic to suppress command echo unless explicitly enabled or in debug mode.
- Preserve command echo on execution errors.
- Document new behavior and control mechanisms in public documentation.

### Status
Accepted

### Date
2024-04-05

### Author(s)
Architecture Team","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision: Inject Hosted Runner Metadata into `Set up job` Logs

### Status
Accepted

### Context
The `Set up job` step in GitHub Actions workflows is a system-generated step that prepares the execution environment on a runner. Users often need visibility into the exact environment configuration‚Äîsuch as operating system version, installed software, or image build metadata‚Äîespecially when using hosted runners (e.g., `ubuntu-latest`, `windows-2022`). Currently, this information is not consistently surfaced in logs, making debugging and reproducibility difficult.

There is a need to enhance transparency and auditability by including relevant environment metadata directly in the `Set up job` log output without requiring users to add custom debugging steps.

### Decision
Modify the runner logic to automatically inject key environment and image metadata into the `Set up job` step's log output when running on **hosted runners**. This includes:

- Operating System name and version
- Hosted image version and build timestamp
- Preinstalled software versions (e.g., `git`, `docker`, `node`, etc.) ‚Äî limited to major components
- Image configuration tags or labels (e.g., `include-dotnet: true`)
- A link or reference to the image definition repository (e.g., [actions/virtual-environments](https://github.com/actions/virtual-environments))

This data will be:
- Collected at startup from a standardized metadata file (e.g., `/image-metadata.json`) or environment variables populated by the image.
- Logged under the `Set up job` step in a collapsible, clearly labeled section titled `Hosted Environment Information`.
- Only included when using GitHub-hosted runners (detected via runner label or configuration).
- Structured and formatted consistently to avoid interfering with user logs or parsing tools.

### Rationale
- **Improved Debuggability**: Developers can verify they‚Äôre running on the expected environment.
- **Audit & Compliance**: Teams can confirm software versions for security and compliance purposes.
- **Reduced Workarounds**: Eliminates need for manual `printenv`, `cat /etc/os-release`, or similar debugging steps.
- **Non-intrusive**: Logging occurs early and is visually grouped, minimizing noise.
- **Scalable**: Uses existing metadata mechanisms; no runtime performance impact.

### Consequences
**Positive**
- Increased transparency of hosted environments.
- Better support experience with clearer context.
- Encourages reproducible CI/CD by documenting runtime state.

**Negative**
- Slight increase in log size (mitigated by collapsible sections in UI).
- Requires coordination with image-building pipelines to ensure metadata is present and up-to-date.
- Potential duplication if users already emit similar info (but justified by consistency and reliability).

### Implementation Notes
- The runner should check for the presence of a well-known metadata file (e.g., `/etc/image-info.json`) or environment variables like `ACTIONS_RUNNER_OS_METADATA`.
- Use structured logging: e.g.,
  ```text
  ##[group]Hosted Environment Information
  OS: Ubuntu 22.04.5 LTS
  Image: ubuntu22-runner
  Image Version: 20240915.1
  Image Build Time: 2024-09-15T00:00:00Z
  Included Software: git=2.45.2, docker=24.0.7, node=18.17.0
  Image Ref: https://github.com/actions/virtual-environments/blob/ubuntu22/20240915.1/images/linux/Ubuntu2204-Runner.yml
  ##[endgroup]
  ```
- Ensure compatibility with GitHub Actions log rendering (collapsible groups).
- Document the feature in runner and virtual-environments repositories.

### References
- https://github.com/actions/virtual-environments
- https://github.com/actions/runner
- GitHub Actions Log API and Annotations: https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","**Architectural Decision Record (ADR)**  
**Title:** Introduce `steps.<id>.outcome` and `steps.<id>.conclusion` to Support Conditional Step Execution  
**Status:** Accepted  
**Date:** 2024-04-17  

---

### **Context**

In workflow orchestration systems (e.g., GitHub Actions, CI/CD pipelines), steps are executed sequentially or conditionally. Currently, the `steps` context exposes `steps.<id>.outputs`, allowing downstream steps to consume data from prior steps. However, there is no built-in way for a step to conditionally execute based on whether a previous step succeeded, failed, or was skipped.

This limits expressiveness in defining resilient workflows where recovery, cleanup, or notification logic depends on execution outcomes rather than just output values.

To support richer conditional logic, we propose extending the `steps` context with two new attributes per step:

- `steps.<id>.outcome`: reflects the final state of the step (e.g., `""success""`, `""failure""`, `""skipped""`, `""cancelled""`).
- `steps.<id>.conclusion`: reflects whether the step has completed and its final result in a boolean-evaluable form (e.g., `""success""` or `""failure""` after completion; undefined during execution).

These fields enable expressions like:
```yaml
if: steps.lint.outcome == 'failure'
```
or
```yaml
if: steps.tests.conclusion == 'success'
```

---

### **Decision**

We will add `steps.<id>.outcome` and `steps.<id>.conclusion` to the `steps` context object, available for use in subsequent steps.

- `outcome`: Represents the observed final state of the step, regardless of whether it contributed to job continuation.
  - Values: `'success'`, `'failure'`, `'skipped'`, `'cancelled'`, `'timed_out'`, etc.
- `conclusion`: Represents the terminal status used for evaluating success/failure in conditional logic.
  - Values: `'success'`, `'failure'` (only set after completion).

Both fields will be accessible in expressions within `if` conditions of later steps.

Example usage:
```yaml
steps:
  - id: lint
    run: npm run lint
    continue-on-error: true

  - id: notify_on_failure
    if: steps.lint.outcome == 'failure'
    run: echo ""Linting failed, notifying...""
```

---

### **Rationale**

- **Improved Workflow Control**: Enables fine-grained control over execution flow based on step results.
- **Supports Best Practices**: Facilitates error handling, auditing, notifications, and debugging without workarounds.
- **Backward Compatible**: Addition of new fields does not affect existing behavior or break current configurations.
- **Consistency with Industry Patterns**: Mirrors similar capabilities in platforms like GitHub Actions, Azure Pipelines, and CircleCI.

---

### **Consequences**

**Pros:**
- Allows conditional execution based on step success/failure.
- Supports robust error recovery and reporting mechanisms.
- Reduces need for wrapper scripts or external tools to track step states.

**Cons:**
- Slightly increases complexity of context evaluation engine.
- Requires accurate tracking and propagation of step lifecycle states.
- May encourage complex branching logic that reduces readability if overused.

---

### **References**

- GitHub Actions Contexts: [https://docs.github.com/en/actions/learn-github-actions/contexts](https://docs.github.com/en/actions/learn-github-actions/contexts)
- Similar pattern in Azure DevOps: `result` and `succeeded()` functions.

--- 

**Approver(s):**  
[Architect Name], [Engineering Lead]","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","# ADR: Support Custom Labels via Command Line During Runner Registration

## Status
Accepted

## Context
Self-hosted GitHub Actions runners are frequently provisioned and configured in automated environments (e.g., cloud instances, containers, CI/CD pipelines). During setup, operators often need to apply custom labels to runners to enable fine-grained job routing (e.g., `gpu`, `arm64`, `windows-container`, `high-memory`).

Currently, the runner registers only built-in labels (`os`, `arch`) during registration and does not allow users to specify additional labels at registration time. This forces operators to either:
- Manually add labels via the GitHub UI or API after registration.
- Implement complex post-registration scripts to patch the runner configuration.

This gap creates operational overhead, complicates automation, and increases the time-to-readiness for new runners.

As noted in [Issue #262](https://github.com/actions/runner/issues/262), this limitation is a frequent pain point in infrastructure-as-code setups where full configuration must be declarative and script-driven.

## Decision
Extend the runner‚Äôs registration command to accept an optional `--labels` parameter that allows specifying a comma-separated list of custom labels to register alongside the built-in ones.

Example usage:
```bash
./config.sh --url https://github.com/org/repo --token ABC123 --labels gpu,arm64,high-mem
```

These labels will be persisted on the runner and appear in the GitHub repository or organization under ""Settings > Actions > Runners"", and can be used in workflow files via `runs-on:` or `labels` expressions.

The implementation must:
- Validate label format (matching GitHub's label rules: lowercase alphanumeric, hyphen, underscore; max length 50 chars).
- Merge user-provided labels with automatically detected built-in labels (`os`, `arch`).
- Preserve idempotency‚Äîre-running config with same labels should not create duplicates.
- Be available across all platforms (Linux, Windows, macOS).

## Consequences
### Pros
- Enables fully automated, declarative provisioning of self-hosted runners.
- Reduces operational complexity and eliminates post-registration scripting.
- Improves consistency and reliability in large-scale or ephemeral runner deployments.
- Aligns with user expectations from other CI systems and infrastructure tools.

### Cons
- Adds a new command-line interface surface, requiring documentation and maintenance.
- Slight increase in configuration complexity for simple use cases (though the parameter remains optional).
- Potential for misuse if invalid labels are provided (mitigated by validation).

### Mitigations
- Comprehensive CLI help text and documentation updates.
- Clear error messages for invalid label formats.
- Backward compatibility preserved‚Äîexisting scripts without `--labels` continue to work.

## See Also
- Issue: https://github.com/actions/runner/issues/262  
- Previous attempt: [ADR275](https://github.com/actions/runner/pull/275)","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","**Decision: Implement Resilient Authentication with Retry Mechanisms and Circuit Breaking for Third-Party Identity Providers**

To ensure reliability and fault tolerance in SS?? when interacting with third-party authentication providers (e.g., Google OAuth2), we will implement a resilient communication layer that handles transient failures through structured retry policies, circuit breaking, and graceful degradation.

### Rationale
Third-party identity providers are external dependencies beyond our control. Transient faults‚Äîsuch as network instability, temporary outages, or rate limiting‚Äîare inevitable. Without proactive handling, these faults can lead to poor user experience, login failures, and cascading system issues. This decision ensures SS?? remains robust and responsive under such conditions.

### Selected Approach
1. **Retry with Exponential Backoff**
   - Failed authentication requests due to transient errors (e.g., 5xx responses, timeouts) will be retried using an exponential backoff strategy.
   - Initial retry delay starts at 1 second, doubling with each attempt, up to a maximum of 3 retries.
   - Random jitter is added to prevent thundering herd problems during provider recovery.

2. **Circuit Breaker Pattern**
   - Use a circuit breaker (e.g., via libraries like `Opossum` or built-in resilience frameworks) around calls to the identity provider.
   - If a threshold of consecutive failures is reached (e.g., 5 failures within 30 seconds), the circuit opens, halting further requests for a defined timeout period (e.g., 30 seconds).
   - After the timeout, the circuit enters a half-open state to allow probing for recovery.

3. **Timeout Enforcement**
   - All outbound authentication requests are subject to strict timeouts (e.g., 10 seconds) to prevent hanging connections and thread exhaustion.

4. **Monitoring and Alerting**
   - Log all retry and circuit breaker events.
   - Emit metrics (e.g., failure rate, retry count, circuit state) to monitoring systems (e.g., Prometheus, Datadog).
   - Trigger alerts on prolonged circuit breaker trips or elevated error rates.

5. **Graceful Degradation (Optional Fallback)**
   - While fallback authentication is not feasible for primary identity verification, serve descriptive error messages to users instead of raw failures.
   - Consider caching non-sensitive public keys (e.g., JWKS) with short TTLs to support token validation during brief provider outages.

### Implementation Notes
- Leverage established resilience libraries (e.g., `axios-retry`, `retry`, or `resilience4j`) depending on the tech stack.
- Ensure retry logic only applies to idempotent or safe operations (e.g., token introspection), not to actions that cause side effects.
- Configure retry/circuit settings per provider and environment (e.g., stricter limits in production).

### Outcome
SS?? will tolerate transient faults in third-party authentication services, improving availability and user experience during partial outages, while preventing systemic resource exhaustion and enabling faster recovery.

### Status
Accepted

### Date
2024-04-05

### Authors
Architecture Team","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","**Decision: Adopt Architectural Decision Records (ADRs) to Document Key Project Decisions**

We will use **Architectural Decision Records (ADRs)** to formally record significant technical and strategic decisions made during the project lifecycle.

### Status
Accepted

### Context
Throughout the development of the project, numerous decisions‚Äîranging from technology selection to system design and process workflows‚Äîare made. Without clear documentation, these decisions become implicit knowledge, making it difficult for new team members and external contributors to understand the rationale behind current architectures or practices. This opacity can hinder collaboration, lead to repeated debates, and reduce transparency in governance.

Additionally, as we aim to develop the product in the open, it is essential that our decision-making process is visible, traceable, and open to feedback. Unlike code or configuration, decisions do not ""age"" in the same way‚Äîif properly recorded, they remain accurate historical artifacts even as systems evolve.

### Decision
We will maintain a collection of **Architectural Decision Records (ADRs)** stored in plain text files within the main project repository (e.g., in a `docs/adr/` directory). Each ADR will follow a lightweight template and capture:

- **Title** ‚Äì a short name for the decision
- **Status** ‚Äì e.g., Proposed, Accepted, Deprecated, Superseded
- **Context** ‚Äì the background and forces driving the decision
- **Decision** ‚Äì the chosen path forward
- **Consequences** ‚Äì the benefits, trade-offs, and potential risks

ADR filenames will be numbered sequentially (e.g., `0001-use-markdown-for-adrs.md`) to avoid merge conflicts and support chronological tracking.

We will use tools like [adr-tools](https://github.com/npryce/adr-tools) or similar to help manage the lifecycle of ADRs where appropriate.

### Rationale
- **Onboarding**: New contributors can quickly understand *why* things are built the way they are.
- **Transparency**: External stakeholders can see how and why decisions are made, supporting open governance.
- **Accountability**: Decisions are timestamped and attributable, enabling retrospectives and audits.
- **Reduced Revisiting**: Prevents re-litigating past decisions without new context.
- **Living Documentation**: ADRs form a stable, non-volatile part of the project‚Äôs documentation.

### Consequences
- **Overhead**: There is a small cost in writing and reviewing ADRs, but this is offset by long-term clarity.
- **Maintenance**: Outdated decisions will be marked as deprecated rather than deleted, preserving history.
- **Scope Creep Risk**: We must define what constitutes a ‚Äúsignificant‚Äù decision worthy of an ADR (guidance will be documented).

### References
- Michael Nygard, ‚ÄúDocumenting Architecture Decisions‚Äù (2011)  
  https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
- GitHub repos using ADRs: e.g., [18F](https://github.com/18F/adr-guide), [openstack](https://opendev.org/openstack/technical-committee/src/branch/master/doc/source/decisions)

---

This ADR is effective immediately. All future architectural and major process decisions should be recorded using this format.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up √É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬™ with all relevant discussion summarized √É¬É√Ç¬Ç√É¬Ç√Ç¬°√É¬É√Ç¬Ç√É¬Ç√Ç¬™ in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","**Decision**: Use a standardized URI scheme (e.g., `ext://` or `ipfs://`) to reference EDN-based extension definitions stored in decentralized storage, enabling consistent discovery, retrieval, and validation of extensions across distributed environments.

**Rationale**:  
To support a decentralized and scalable extension system, it is essential to have a uniform way of identifying and locating extension manifests. By defining a dedicated URI schema:
- **Interoperability** is ensured across clients and services that consume extensions.
- **Decentralized resolution** can be achieved using protocols like IPFS or similar content-addressed networks, where the URI directly maps to content-hash-based addressing.
- **Extensibility** allows future schemes (e.g., `ext+ipns://`, `ext+ar://`) for different backends without changing core logic.
- **Security & Integrity**: Content-addressed URIs guarantee integrity and facilitate tamper-proof distribution.

**Implications**:
- The system must include a resolver component capable of interpreting the URI scheme and fetching EDN files accordingly.
- Extension manifests must be immutable or versioned to ensure reliability; mutable references (e.g., IPNS) should be used with caution.
- Documentation and tooling must support developers in publishing and referencing extensions via these URIs.

**Example URI formats**:
- `ipfs://bafybeiemxf5abjwjbikoz4mc3a3dla6ual3jsgpdrlnmhvhj5a6awqj3ay`
- `ext://myplugin@v1` ‚Üí resolved via registry or gateway
- `https://gateway.ipfs.io/ipns/example.com/extensions/plugin.edn`

This decision enables a robust, future-proof foundation for decentralized extensibility.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","**Decision: Exclude group chat functionality from the MVP scope for the Q2 beta launch.**

We will focus development efforts exclusively on 1-1 and public chat features, which are core to the minimum viable product and essential for validating the primary chat experience.

Group chat introduces significant complexity in terms of encryption, security, and maintenance, and has proven to be a recurring source of regressions and scope creep. Given its lower priority compared to public chat‚Äîespecially for an MVP aiming to enable open interaction with minimal friction‚Äîits inclusion would jeopardize our ability to deliver a stable, secure, and focused beta on time.

By deferring group chat to a post-MVP phase, we reduce technical debt accumulation, streamline testing and QA, and maintain alignment with our critical path. The team will remove or disable unfinished group chat features in the codebase to prevent confusion and avoid further investment in non-critical paths.

This decision ends ongoing debate, clarifies priorities, and enables the team to move forward with confidence.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","**Decision: Implement Discovery v5 Protocol for Dynamic Peer Discovery**

To address the scalability, reliability, and security limitations imposed by hardcoded peer configurations, we will adopt the **Discovery v5 (Discv5)** protocol for dynamic peer discovery in the application.

### Rationale
- **Scalability**: Hardcoded peers create a static topology that does not adapt to network load or growth. Discv5 enables nodes to dynamically discover and connect to available peers, allowing the system to scale horizontally as user count increases beyond threshold N.
- **High Availability**: With Discv5, the failure or saturation of individual nodes does not isolate clients. Nodes can automatically discover alternative healthy peers in the network.
- **Operational Independence**: Server peer clusters can be scaled independently of client releases. New nodes can join the network and become discoverable without requiring app updates.
- **Security & Resilience**: Discv5 includes built-in cryptographic identity verification and supports secure topic-based discovery, reducing susceptibility to spoofing and enabling privacy-preserving node lookup.
- **Future-Proofing**: Discv5 is an emerging standard in decentralized systems (e.g., Ethereum 2.0, Status Network), aligning us with long-term p2p trends and facilitating potential interoperability.

### Implementation Approach
1. Integrate a Discv5-compliant library (e.g., `github.com/status-im/go-discover/v5` or equivalent).
2. Configure bootstrap nodes to seed initial network connectivity.
3. Replace hardcoded peer lists with dynamic discovery via Discv5 topics (e.g., `/status/messaging/v1`).
4. Maintain fallback mechanisms during transition (e.g., hybrid mode with hardcoded peers as backup).
5. Monitor discovered peer health and connection quality using liveness checks.

### Trade-offs
- **Increased Complexity**: Adds complexity in node bootstrapping and monitoring.
- **Initial Latency**: First connection may take longer due to discovery overhead.
- **Dependency on Bootstrap Nodes**: Requires reliable bootstrap infrastructure to initiate discovery.

### Conclusion
Adopting Discv5 is a strategic move toward a resilient, scalable, and decentralized architecture. It decouples client logic from infrastructure scaling, enabling sustainable growth and improved fault tolerance.

**Status: Accepted**  
**Date: 2025-04-05**  
**Author: Architect Team**","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","**ADR 142: Standardized BIP-39/BIP-44 Wallet Key Generation**

**Status**: Accepted  
**Date**: 2024-06-15  
**Author(s)**: Status Engineering Team  
**Related Issue**: Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README)

---

### **Context**

Historically, Status used a non-standard method for deriving cryptographic keys from a 12-word mnemonic seed phrase. This deviation prevented users from importing their Status wallet into other Ethereum-compatible wallets (e.g., MetaMask, Trust Wallet), and vice versa ‚Äî limiting interoperability and user control.

The community has expressed strong demand for improved wallet compatibility, enabling users to securely migrate or access their accounts across different applications using the same recovery phrase.

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) proposes aligning Status‚Äôs key derivation process with widely adopted industry standards to ensure cross-wallet compatibility.

---

### **Decision**

Adopt **BIP-39** for mnemonic generation and **BIP-44** (specifically `m/44'/60'/0'/0/0`) for Ethereum key derivation to ensure full compatibility with other Ethereum wallets.

This means:
- A 12-word BIP-39 mnemonic will be generated using standardized entropy (128 bits).
- The mnemonic is converted into a seed using PBKDF2 with HMAC-SHA512.
- The master private key is derived from this seed using BIP-32 hierarchical deterministic (HD) key derivation.
- The Ethereum address is derived using the BIP-44 path: `m/44'/60'/0'/0/0`.

New and existing users will benefit from:
- Ability to import their Status wallet into any BIP-44‚Äìcompliant Ethereum wallet using the same 12-word recovery phrase.
- Ability to import wallets created in other apps into Status by restoring via the 12-word BIP-39 mnemonic.

> ‚ö†Ô∏è **Note on Backward Compatibility**:  
> This change applies to new wallet setups. Users who already have a Status wallet using the legacy scheme will retain access through the existing flow until they choose to migrate. Migration tools and clear UX guidance will be provided.

---

### **Rationale**

| Factor | Reason |
|-------|--------|
| **User Empowerment** | Users should own and control their identity and assets across apps. Standardization supports self-custody. |
| **Interoperability** | BIP-39/BIP-44 is the de facto standard for Ethereum wallets. Adoption ensures seamless integration with dApps and services. |
| **Security** | These standards are battle-tested, well-documented, and supported by robust cryptographic libraries. |
| **Community Demand** | High user interest in being able to use the same wallet across multiple platforms. |

Alternatives considered:
- **Keep legacy scheme**: Rejected due to poor UX and lack of portability.
- **Use BIP-44 with custom paths**: Rejected as it would still break compatibility.
- **Support both schemes**: Allowed temporarily during migration, but default behavior shifts to BIP-44.

---

### **Implications**

**Positive**
- ‚úÖ Cross-wallet compatibility achieved.
- ‚úÖ Improved trust and transparency via use of open standards.
- ‚úÖ Easier onboarding for users familiar with other wallets.

**Challenges**
- ‚ö†Ô∏è Requires secure migration path for legacy users.
- ‚ö†Ô∏è Risk of confusion during transition (mitigated via education and UI cues).
- ‚ö†Ô∏è Must clearly communicate that changing derivation paths affects account recovery ‚Äî users must back up mnemonics, not just passwords.

**Dependencies**
- Update wallet generation and restoration logic in Status Mobile and Desktop.
- Audit cryptographic implementation.
- Update documentation and support resources.

---

### **References**

- BIP-39: https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki
- BIP-44: https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki
- Ethereum HD Wallet Derivation: https://github.com/ethereum/EIPs/issues/84
- Idea 142: https://ideas.status.im/ideas/142-wallet-compatibility/README

--- 

*Architecture Decision Records are living documents. This ADR may be updated to reflect changes in implementation or context.*","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","**Architectural Decision Record (ADR)**

---

### **Title:**  
Adopt ULC (Ultra-Light Client) as the Long-Term Ethereum Light Client Strategy to Replace LES and Reduce Reliance on Infura

### **Status:**  
*Accepted*

### **Context:**

The Status app has encountered significant performance and usability challenges related to its use of the LES (Light Ethereum Subprotocol), particularly on Mainnet. These include:

- Slow or failed fast/hot syncs
- High disk usage due to bloom filter storage
- Excessive bandwidth consumption
- Poor battery life and UI jank, especially on mobile devices
- Degraded user experience under constrained network conditions (e.g., cellular data)

While optimizations such as CHT (Compact Header Trees) and adaptive sync throttling could partially mitigate these issues, they do not address the fundamental inefficiencies in LES's design‚Äîparticularly its reliance on downloading and querying large bloom filters for log retrieval.

In the short term, we have offloaded Ethereum consensus layer responsibilities to Infura (and user-provided upstream nodes), but this introduces serious concerns:

- Centralization risk (single point of failure/trust)
- Weaker security model (users trust third-party node operators)
- Reduced alignment with Ethereum‚Äôs decentralization ethos

ULC (Ultra-Light Client), currently under development, promises a more scalable and secure alternative by enabling clients to sync headers rapidly from multiple peers while using probabilistic sampling and proof-of-latency mechanisms to ensure correctness. It supports multi-peer connectivity, improving both resilience and trust distribution.

Although ULC is not yet production-ready, continuing investment in LES optimization would divert resources from a more sustainable long-term solution.

### **Decision:**

We will **de-prioritize further optimization of LES** and instead **focus engineering efforts on accelerating the development, testing, and integration of ULC** as the primary light client protocol for the Status mobile app.

Concurrently:
- Maintain minimal Infura fallback support during the transition period
- Support user-configured upstream nodes to allow self-hosted setups
- Design the networking layer to be modular, enabling seamless switching between backends (Infura ‚Üí ULC, multi-node ULC)

Once ULC reaches sufficient maturity (defined by testnet stability, audit readiness, and acceptable sync/log-fetch performance), it will become the default Ethereum consensus client in the Status app, with support for connecting to multiple ULC-serving peers.

### **Rationale:**

| Factor | Rationale |
|------|-----------|
| **Performance** | ULC avoids bloom filter downloads entirely; logs are fetched via lightweight queries to remote peers without local indexing overhead. This directly addresses core LES bottlenecks. |
| **Decentralization** | ULC supports connection to multiple servers, reducing reliance on any single endpoint‚Äîunlike Infura or current LES single-node models. |
| **Security** | Multi-peer sampling reduces trust assumptions. Future integration with fraud detection or verification overlays can further strengthen guarantees. |
| **Maintainability** | Focusing on one future-proof sync mechanism reduces technical debt and allows deeper investment in a single architecture. |
| **Mobile Suitability** | ULC‚Äôs low bandwidth, CPU, and storage footprint aligns well with mobile constraints, supporting our goals for energy efficiency and smooth UI. |

LES improvements (e.g., CHT adoption, sync throttling) may still be implemented *if low-effort and high-impact*, but no major new features or refactors will be made to the LES stack.

### **Implications:**

**Positive:**
- Path toward full decentralization and improved trust model
- Better long-term scalability and mobile performance
- Reduced operational burden from managing complex LES state
- Encourages ecosystem growth around ULC-capable infrastructure

**Negative / Risks:**
- ULC is not yet mature; delays could prolong reliance on Infura
- Requires coordination across `status-go`, mobile apps, and DevOps teams
- Potential need for interim UX improvements to mask sync limitations

**Mitigations:**
- Define clear milestones and success criteria for ULC readiness
- Allocate dedicated team bandwidth to ULC implementation and testing
- Implement graceful degradation and clear user feedback when syncing
- Engage with other clients exploring ULC to share tooling and knowledge

### **Next Steps:**

1. Establish ULC roadmap with deliverables and timelines in Q3/Q4
2. Refactor status-go networking layer to abstract light client backend
3. Begin integrating ULC prototype into dev builds
4. Monitor community progress on ULC standardization (EIPs, reference implementations)
5. Communicate plan transparently to users via roadmap and release notes

---

**Owner:** Core Protocol Team  
**Date:** 2025-04-05  
**Related Issues:**  
- https://github.com/status-im/ideas/issues/55  
- https://ideas.status.im/ideas/071-low-traffic  
- https://ideas.status.im/ideas/076-smooth-ui  
- https://ideas.status.im/ideas/083-energy-efficient","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision

Introduce a `SensitiveData` type to encapsulate sensitive information and prevent accidental logging.

### Rationale

To mitigate the risk of accidentally logging sensitive data such as passwords, we will create a dedicated `SensitiveData` type (or class) that wraps sensitive strings. This type will:

1. **Override string representation methods** (e.g., `toString()`, `__str__`, etc.) to return a redacted placeholder (like `[REDACTED]`) instead of the actual value
2. **Provide explicit methods** to access the underlying value only when absolutely necessary (e.g., `reveal()` or `getValue()`)
3. **Be used consistently** for all sensitive data throughout the codebase

This approach ensures that even if sensitive data is included in log statements, debugging output, or error messages, the actual values won't be exposed.

### Implementation Example

```typescript
class SensitiveData {
  private value: string;
  
  constructor(value: string) {
    this.value = value;
  }
  
  // Never reveal the actual value in string representation
  toString(): string {
    return '[REDACTED]';
  }
  
  // Explicit method required to access the real value
  reveal(): string {
    return this.value;
  }
}
```

### Migration Plan

1. Identify all variables/parameters that handle sensitive data
2. Update types to use `SensitiveData` instead of `string`
3. Update all usages to call `.reveal()` when the actual value is needed
4. Add ESLint/TSLint rules to flag direct string usage for sensitive fields

This provides both a technical safeguard and clear developer intent signaling that certain data requires special handling.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","# Architectural Decision Record: Deprecate and Remove 3rd Party Status Command Extensions via JavaScript Jail

## Status
**Accepted**

## Context
The original architecture for 3rd party status command extensions relied on a jailed JavaScript execution environment (""Jail"") paired with a small JS library exposing a chat commands API. While this approach aimed to enable extensibility, it has led to significant technical debt and architectural shortcomings:

- The API design favored mutable state over the functional, stateless paradigms of the host application (ClojureScript/re-frame), making integration complex and error-prone.
- Advanced features like live transaction details in `/send` messages could not be implemented within the jail model, leading to hard-coded, non-reusable logic scattered throughout the codebase.
- No command is fully implemented in the jail ‚Äî all require custom native logic, defeating the purpose of sandboxed extensibility.
- Performance is negatively impacted due to constant asynchronous communication across the React Native bridge, increasing latency and resource usage.
- There has been no real-world adoption of jail-based 3rd party bots; existing dApps use standard web3.js and run equally well in other Ethereum clients.
- A new, more powerful extension system (""Project Pluto"") is emerging, offering declarative, secure, and deeply integrated extension capabilities beyond just command handling.

## Decision
We will **deprecate and remove support for 3rd party command extensions running in the JavaScript jail environment**, effective immediately.

All existing command logic currently split between jail scripts and native app code will be:
1. Fully migrated into the core application using re-frame and ClojureScript.
2. Refactored to follow functional, testable, and maintainable patterns consistent with our internal architecture.
3. Replaced with clean, documented, and performant implementations that ""eat our own dogfood.""

No new features or commands will be developed using the jail mechanism. Existing jail-based implementations will be phased out during upcoming refactoring cycles.

Future extensibility will be exclusively handled through **Project Pluto**, which provides:
- Declarative extension manifests.
- Secure, scoped access to application hooks (e.g., wallet, contacts, messaging).
- Better performance via direct integration without bridge overhead.
- Strong typing, validation, and testing support.
- Potential for discoverability, permissions, and user management.

## Consequences

### Positive
- Eliminates a major source of technical debt and fragile code.
- Improves performance by removing unnecessary RN bridge traffic.
- Enables full unit and integration testing of command logic.
- Encourages consistency with the re-frame architecture.
- Paves the way for richer, safer, and more capable extensions via Project Pluto.
- Simplifies the codebase and reduces cognitive load for developers.

### Negative
- Short-term effort required to migrate existing partially-jailed commands to native implementation.
- Temporary loss of theoretical ""sandboxed extensibility"" until Project Pluto reaches maturity.
- Risk of delaying some extension use cases if Project Pluto timelines slip.

### Mitigations
- Prioritize migration of most critical commands first (e.g., `/send`, `/request`).
- Maintain backward compatibility temporarily via feature flags during transition.
- Accelerate Project Pluto roadmap to ensure timely availability of replacement extensibility model.

## Notes
This decision aligns with our broader platform strategy: favor robust, maintainable, and integrated solutions over loosely coupled but brittle ones. Given the lack of adoption and high cost of the current jail system, removal is both pragmatic and necessary to enable future innovation.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision

**Implement a Staking-Based Permission Model for Direct Messaging Using SNT, with Conditional Deposit Forfeiture Upon Response**

We will implement the Tribute to Talk (TtT) mechanism as a core anti-spam feature for direct message and cold contact requests in Status. Under this model:

1. **Stakeholder Staking**: Users can set a configurable minimum stake amount in SNT (Status Network Token) that others must deposit in order to initiate a direct message or contact request.
2. **Mandatory Deposit for Initiators**: Any user attempting to send a direct message to a non-contact must first deposit the required amount of SNT into an escrow contract tied to that recipient‚Äôs TtT settings.
3. **Conditional Forfeiture**: The deposited SNT is only released to the recipient if they choose to reply to the message. If the recipient does not reply within a defined period (or permanently ignores the request), the deposit is refunded to the sender.
4. **Recipient Control**: Users retain full control over their TtT settings, including setting the staking amount (with a system-defined minimum/maximum), disabling the feature, or whitelisting certain users or communities.

This design aligns sender intent with economic commitment while respecting recipient autonomy, reducing spam without blocking legitimate outreach. It incentivizes thoughtful communication and leverages token economics to create a user-owned filtering mechanism, consistent with Status‚Äôs decentralized ethos.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","**Architectural Decision Record (ADR)**

---

**Title:** Introduce Section Ownership Model in Mobile Development Team to Improve App Polish and Oversight  
**Status:** Accepted  
**Date:** 2024-07-18  
**Author(s):** [Team Lead, Mobile]  
**Stakeholders:** Mobile Development Team, Design Team, Product Management  

---

### **Context**

The mobile development team currently operates with a single team lead responsible for technical direction, code quality, and cross-functional alignment. However, recent collaboration with the design team has revealed that the mobile application lacks polish‚Äîevidenced by inconsistent UI/UX, delayed implementation of design specs, and fragmented user flows.

A root cause analysis identified insufficient domain-specific oversight across major app sections. With growing feature complexity in areas such as Communities, Syncing & Onboarding, Activity Center & Shell, Wallet, and Messaging, centralized leadership is no longer sufficient to maintain high-quality standards.

To improve ownership, accountability, and attention to detail, it has been proposed to distribute oversight by appointing dedicated **Section Owners**.

---

### **Decision**

We will introduce a **Section Ownership model** within the mobile development team. For each of the following core sections of the app:

- Communities  
- Syncing & Onboarding  
- Activity Center & Shell  
- Wallet  
- Messaging  

‚Ä¶a senior or mid-level developer will be appointed as the **Section Owner**.

#### **Responsibilities of a Section Owner:**
- Act as the primary technical and UX steward for their section.
- Ensure consistency in UI, navigation, state management, and performance within their domain.
- Collaborate closely with designers to review and implement design specifications accurately.
- Review all pull requests related to their section (mandatory code review role).
- Maintain up-to-date documentation and architecture diagrams for their area.
- Identify and drive technical debt reduction and polish improvements.
- Serve as the escalation point for bugs and issues in their section.
- Represent their section in sprint planning, reviews, and cross-team syncs.

This role complements‚Äînot replaces‚Äîthe existing team lead. The team lead retains overall architectural vision, resource planning, and cross-cutting concerns (e.g., CI/CD, shared libraries).

---

### **Rationale**

- **Improved Quality & Polish:** Distributed ownership ensures focused attention on user experience and implementation fidelity.
- **Faster Decision-Making:** Section owners can make timely, context-aware decisions without bottlenecking through a single lead.
- **Empowerment & Growth:** Provides growth opportunities for engineers to develop leadership and architectural skills.
- **Design Collaboration:** Establishes clear points of contact for the design team, improving alignment and reducing rework.
- **Scalability:** Supports sustainable team growth and modular development practices.

Alternatives considered:
- **Continue with single team lead:** Insufficient bandwidth for detailed oversight; already proven inadequate.
- **Appoint a single UX-focused tech lead:** Too broad; would still lack deep per-section engagement.
- **Rotate ownership periodically:** Risked inconsistency and reduced accountability; rejected in favor of stable ownership with periodic review.

---

### **Consequences**

**Positive:**
- Higher consistency and polish in each app section.
- Stronger collaboration between engineering and design.
- Increased engineer engagement and ownership.
- Easier onboarding for new developers due to clearer domain boundaries.

**Negative / Risks:**
- Potential for siloed knowledge if owners do not collaborate.
- Risk of inconsistent patterns across sections if governance is weak.
- Additional overhead in coordination and expectations management.

**Mitigations:**
- Enforce regular cross-owner syncs (bi-weekly) to share learnings and align on patterns.
- Require Section Owners to document key decisions in ADRs specific to their domain.
- The team lead will facilitate consistency through architecture reviews and shared component library enforcement.

---

### **Next Steps**
1. Team lead identifies candidates for each section based on expertise, interest, and bandwidth.
2. Formal announcement of Section Owners and their responsibilities.
3. Kickoff meeting with owners to align on expectations, tools, and success metrics.
4. Integrate section ownership into PR workflow (required reviewer by section).
5. Review effectiveness after 6 weeks during retrospective.

--- 

**Approved by:**  
[Team Lead, Mobile]  
[Engineering Manager]  
[Design Lead]","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","**Decision: Use Embedded Collections with Custom Serialization and Validation for Product Option Values**

### Status
Accepted

### Context
In Sylius, a Product Option defines dynamic attributes that can be assigned to products (e.g., ""Color"", ""Size""). Each Product Option has multiple values (e.g., ""Red"", ""Blue""), and these values are often managed as part of the option or during product variant configuration. With the new Sylius API built on API Platform, we needed an efficient and user-friendly way to expose and manage the collection of Product Option values.

We evaluated several approaches:
- **Separate Resource Endpoints**: Treat `ProductOptionValue` as a full API resource with its own endpoints.
- **Embedded Read-Only Collection**: Embed values in the `ProductOption` response but disallow direct mutations via the collection.
- **Fully Embedded Mutable Collection**: Allow full CRUD operations on the `values` collection via inline embedding in the `ProductOption` resource.

Challenges included:
- Maintaining data consistency and referential integrity.
- Ensuring good performance (avoiding N+1 queries).
- Providing intuitive REST/JSON:API semantics.
- Supporting validation at both the collection and item level.
- Compatibility with Sylius' domain model and Doctrine relationships.

### Decision
We will **embed the Product Option Values collection directly within the ProductOption API resource**, allowing read and write operations through the parent resource. This means:

- `ProductOptionValue` is **not** exposed as a standalone API resource.
- The `values` collection is embedded in `ProductOption` responses (`GET /api/product-options/{id}`).
- Clients can **create, update, and delete** values by sending a complete or partial representation of the `ProductOption` with the modified `values` array (via `PUT` or `PATCH`).
- We use **custom serialization groups** to control nesting depth and avoid circular references.
- We implement **cascade persist/remove** in Doctrine and **deep validation** using Symfony Validator constraints (e.g., `Valid` on the collection).
- For complex bulk operations, we may later introduce custom operations (e.g., `/api/product-options/{id}/values/update`) if performance becomes an issue.

### Rationale
- **Simplicity**: Consumers manage options and their values in one place.
- **Consistency**: Prevents orphaned values and enforces transactional integrity.
- **Performance**: Reduces number of requests; enables efficient DQL fetching with joins.
- **UX**: Aligns with how merchants typically think ‚Äî ‚Äúedit an option and its values together.‚Äù
- **API Platform Fit**: Leverages built-in support for nested resources and cascade operations.

### Consequences
**Pros:**
- Atomic updates ensure data consistency.
- Fewer endpoints to document and maintain.
- Improved client efficiency (fewer round trips).

**Cons:**
- Larger payloads when updating options with many values.
- Risk of unintended deletions if clients send incomplete collections (mitigated with proper PATCH semantics or upsert logic).
- More complex validation and deserialization logic.

We accept these trade-offs given our use case and expect most interactions to be UI-driven (Admin Panel), where full forms are used.

### Alternatives Considered
1. **Separate CRUD for OptionValues**  
   ‚Üí Rejected: Leads to inconsistent state, more complex client logic, and no real benefit for this domain.

2. **Read-only Embedded + Sub-resources for Mutations**  
   ‚Üí Overkill: Adds complexity without sufficient gain in our context.

3. **UUID References Instead of Embedding**  
   ‚Üí Increases indirection and degrades performance.

---

*This decision aligns with Sylius‚Äô goal of providing a clean, merchant-focused administration experience while leveraging API Platform‚Äôs strengths in handling complex object graphs.*","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision: Implement Locale-Aware API Endpoints with Explicit Locale Parameter and Default Fallback

### Status
Accepted

### Context
In Sylius, many resources (e.g., products, taxonomies, payment/shipping method names, product descriptions) are locale-aware, meaning their content should be presented in the customer‚Äôs preferred language. Currently, APIs often return all translations for a resource, placing the burden of locale resolution on frontend clients. This leads to:

- **Inconsistent behavior** across endpoints ‚Äî some return only the relevant translation, others return all.
- **Increased payload size**, especially when only one locale is needed.
- **Poor developer experience**, as frontend developers must implement logic to extract the correct translation.
- **Risk of incorrect localization** due to inconsistent or missing fallback logic.

A standardized, backend-driven approach is required to ensure consistent, efficient, and correct delivery of localized content.

### Decision
We will modify the API to support explicit locale selection via a `?locale=` query parameter (or header, e.g., `Accept-Language`), with the following behavior:

1. **Default Behavior**:  
   If no locale is specified, the API returns data in the **store's default locale**.

2. **Explicit Locale Selection**:  
   Clients can request a specific locale using:
   - Query parameter: `?locale=fr_FR`
   - Header: `Accept-Language: fr-FR`  
     (Query parameter takes precedence if both are present.)

3. **Locale Fallback Chain**:  
   If the requested locale is not available for a translatable resource:
   - First, attempt to serve from the requested locale (`fr_FR`).
   - If not found, fall back to the language-level locale (`fr`).
   - Finally, fall back to the **store's default locale**.

4. **Response Structure**:  
   Translatable fields in the response contain only the resolved translation (not all translations). For example:
   ```json
   {
     ""name"": ""T-shirt en coton"",
     ""description"": ""Un t-shirt confortable en coton bio.""
   }
   ```
   rather than:
   ```json
   ""translations"": {
     ""en_US"": { ""name"": ""Cotton T-shirt"", ... },
     ""fr_FR"": { ""name"": ""T-shirt en coton"", ... }
   }
   ```

5. **Endpoint Consistency**:  
   All public API endpoints serving locale-aware resources will follow this pattern. Internal/admin APIs may continue to expose all translations.

6. **Documentation & Developer Guidance**:  
   Clearly document the locale resolution strategy and recommended usage in API docs and client SDKs.

### Rationale
- **Improved UX**: Frontend developers receive correctly localized content without extra parsing.
- **Smaller Payloads**: Only necessary data is transferred.
- **Consistency**: Uniform behavior across all storefront endpoints.
- **Standards-Aligned**: Leverages HTTP conventions (`Accept-Language`) while allowing explicit control.
- **Flexibility**: Supports multi-store, multi-language setups common in Sylius.

### Implications
- **Backward Compatibility**: This change may break clients relying on embedded `translations` objects. A versioned API (`/api/v2/`) is recommended for rollout.
- **Performance**: Requires efficient querying of translations (ensure proper indexing on `translatable_id` and `locale`).
- **Configuration**: Storefronts must define a default locale per channel.

### Alternatives Considered
- **Keep returning all translations**: Maintains backward compatibility but perpetuates inconsistency and inefficiency.
- **Only use `Accept-Language` header**: Less predictable and harder to debug; query param offers clearer intent.
- **Require locale in every request**: Too strict; defaulting to store locale improves usability.

### Related Decisions
- [ADR-001: Adopt JSON:API Specification for Sylius Storefront API](#)
- [ADR-003: Introduce Caching Strategy for Translated Content](#)

### Notes
Migration path for existing clients will include deprecation notices in `/api/v1/` and comprehensive upgrade guides.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision: Adopt Lightweight Architectural Decision Records (ADRs) for Terraform Controller

We will adopt a lightweight Architectural Decision Record (ADR) process to document significant design and implementation decisions for the Terraform Controller project.

### Rationale
- **Knowledge Preservation**: Ensures critical context about ""why"" decisions were made is retained, especially as team members change.
- **Onboarding Support**: Helps new contributors understand historical reasoning behind architectural choices.
- **Transparency**: Makes decision-making visible and traceable without requiring formal proposals for every change.
- **Low Overhead**: Uses a simple markdown-based format stored in version control, minimizing process friction.
- **Discoverability**: ADRs will be stored in a dedicated `/docs/architecture/decisions` directory in the main repository, making them easy to browse and search.

### Implementation
1. Use a simple ADR template with:
   - Title
   - Status (proposed, accepted, deprecated)
   - Context
   - Decision
   - Consequences
2. Store ADRs in `/docs/architecture/decisions/` using numbered filenames (e.g., `0001-use-adrs.md`)
3. Link relevant ADRs in code comments or documentation where applicable
4. Require ADRs for non-trivial decisions that affect architecture, APIs, or long-term maintainability

This approach balances rigor with agility, ensuring important decisions are recorded without introducing bureaucratic overhead.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision

Use GitHub webhooks to detect pull request changes instead of polling via the GitHub API.

### Rationale

1. **Real-time detection**: Webhooks provide immediate notification of pull request events (e.g., opened, synchronized, closed), enabling faster reaction times compared to polling intervals.

2. **Reduced resource consumption**: Webhooks eliminate the need for frequent API calls, decreasing both network traffic and API rate limit usage.

3. **Event accuracy**: Webhooks deliver specific event payloads with detailed context about the change, reducing the need for additional API calls to determine what changed.

4. **Scalability**: Webhook-based systems scale more efficiently as they only process events when they occur, rather than checking status at regular intervals regardless of activity.

5. **Standard practice**: Using webhooks aligns with modern CI/CD and automation patterns for GitHub integration.

### Implementation Considerations

- Configure GitHub repository webhook to send `pull_request` events to our service endpoint
- Implement secure endpoint with signature verification using the webhook secret
- Include proper error handling and retry mechanisms for failed webhook deliveries
- Maintain fallback capabilities for edge cases where webhooks might be missed

This approach provides a more efficient, responsive, and scalable solution for detecting pull request changes.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","**Decision: Retain cross-namespace references in tf-controller API with scoped, opt-in permissions and explicit ownership verification**

### Status
Accepted

### Context
The tf-controller API requires cross-namespace references (e.g., `spec.sourceRef`, `spec.dependsOn[]`, `.data.resources[]`) to support established Flux workflows where source artifacts (like GitRepositories or Buckets) are defined in shared namespaces and consumed by Terraform objects across different namespaces. While Kubernetes best practices discourage cross-namespace references due to concerns about namespace isolation and permission sprawl, removing them would break backward compatibility and disrupt common operational patterns.

Additionally, unrestricted cross-namespace access can lead to:
- Overly broad RBAC requirements (e.g., cluster-wide list/watch permissions)
- Information leakage across trust boundaries
- Implicit dependencies that are hard to audit or secure

However, outright removal is not feasible due to real-world usage patterns in multi-tenant or organization-wide Flux deployments.

### Decision
We **retain support for cross-namespace references**, but enforce the following safeguards:

1. **Explicit Allowance via Label Selector (Opt-In Model)**  
   Cross-namespace references are only permitted if the target object (in the remote namespace) is labeled with:
   ```yaml
   labels:
     tf.fluxcd.io/allowed: ""true""
   ```
   This makes cross-namespace consumption an *opt-in* behavior at the target object level, preserving namespace owner control.

2. **RBAC + Reference Validation in Controller Logic**  
   The controller performs two checks before resolving any cross-namespace reference:
   - Verify the referring service account has sufficient RBAC permissions to get the target object.
   - Confirm the target object bears the `tf.fluxcd.io/allowed: ""true""` label.

3. **Namespace-Level Admission Policy (Optional Extension)**  
   Operators may further restrict allowed source namespaces using a configuration option such as:
   ```yaml
   allowed-source-namespaces:
     - flux-system
     - sources
   ```
   This limits which namespaces can be referenced from outside, reducing the attack surface.

4. **Clear Documentation and Warnings**  
   Document security implications in user guides and emit events/warnings when cross-namespace references are used without proper labeling or least-privilege RBAC.

5. **Future Audit Logging**  
   Log all cross-namespace resolution attempts (success/failure) for auditing purposes (to be implemented in next phase).

### Consequences
**Pros:**
- Maintains backward compatibility with existing Flux setups.
- Preserves flexibility for centralized source management (e.g., one GitRepo per team, used across many environments).
- Enforces least privilege through opt-in labeling and RBAC enforcement.
- Gives namespace owners control over whether their objects can be referenced externally.

**Cons:**
- Adds slight operational overhead (must label objects intended for sharing).
- Requires careful RBAC setup; misconfigurations may delay reconciliation.
- Does not eliminate cross-namespace coupling entirely ‚Äî operators must still manage dependency lifecycles carefully.

### Examples
```yaml
# Shared source in flux-system namespace
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: terraform-modules
  namespace: flux-system
  labels:
    tf.fluxcd.io/allowed: ""true""  # Required for cross-ns access
spec:
  url: https://github.com/org/terraform-modules
  interval: 5m
```

```yaml
# Terraform object in staging namespace
apiVersion: infra.contrib.fluxcd.io/v1alpha2
kind: Terraform
metadata:
  name: app-deploy
  namespace: staging
spec:
  sourceRef:
    kind: GitRepository
    name: terraform-modules
    namespace: flux-system  # Cross-namespace reference
  path: ./apps/myapp
```

### See Also
- [Kubernetes Best Practices: Avoiding Privilege Escalation](https://kubernetes.io/docs/concepts/security/rbac-good-practices/)
- [Flux Multi-Tenancy Model](https://fluxcd.io/docs/use-cases/multi-tenancy/)
- ADR-0005: Design Principles for Cross-Namespace References in Flux Controllers

---

This decision balances practical usability with security by making cross-namespace references safe-by-default while keeping them available when needed.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision: Implement Workspace BLOB Caching to Enable Reliable Single Terraform Object Deletion

### Status
Accepted

### Context
The TF-Controller currently struggles with reliable deletion of Terraform (TF) objects due to dependencies on external resources such as Source objects, Secrets, and ConfigMaps. When attempting to delete a TF object, if any of these associated resources have already been deleted or are unavailable, the finalizer cannot reconstruct the necessary context to run `terraform destroy`, resulting in orphaned or stuck objects.

This issue is most acute during:
- **Single object deletion**, where required configuration data may no longer be accessible.
- **Deletion of resources with dependencies**, which relies on successful single-object cleanup.
- **Namespace-level deletions**, which compound both prior issues.

Currently, the tf-runner generates a complete Workspace file system from Source, Secrets, and ConfigMaps, then returns a serialized Workspace BLOB stored by the controller. However, this BLOB is not leveraged during deletion ‚Äî instead, the controller attempts to re-fetch source and config data that may no longer exist.

To resolve this, we must ensure that all information needed to destroy a Terraform resource is preserved at the time of its last reconciliation.

### Decision
**Cache the generated Workspace BLOB in the TF-Controller‚Äôs storage and use it during finalization to perform `terraform destroy` operations, even when original Sources, Secrets, or ConfigMaps are no longer available.**

Specifically:
1. **Persist the latest valid Workspace BLOB** for each TF object in the controller's backend storage (e.g., S3, MinIO, or equivalent).
2. **Use the cached BLOB during deletion finalization** to initialize the tf-runner‚Äôs workspace, eliminating runtime dependency on external resources.
3. **Ensure atomicity and consistency** by associating the BLOB with a generation or hash to prevent stale state usage.
4. **Only allow finalization via BLOB if the object has reached an ‚Äúapplied‚Äù state previously**, ensuring safety and idempotency.
5. **Retain BLOBs until after successful destruction or explicit garbage collection**, preventing premature loss of destruction capability.

This enables self-contained deletion: the finalizer triggers a `tf-runner` job using the BLOB as the sole input, fully reconstructing the environment needed to run `terraform destroy`.

### Rationale
- **Reliability**: Eliminates race conditions where Secrets/ConfigMaps are removed before TF object finalizers execute.
- **User Experience**: Prevents dangling resources stuck in terminating state; improves predictability.
- **Consistency**: Leverages existing BLOB generation logic without introducing new data formats.
- **Progressive Enablement**: Solves the foundational problem (single object deletion), enabling future work on dependent deletion scenarios.
- **Minimal Overhead**: Reuses already-generated artifacts rather than duplicating storage mechanisms.

Alternative considered:
- *Recreate workspace dynamically during deletion*: rejected because it reintroduces dependency on potentially missing Secrets/Sources.
- *Store only metadata/state references*: rejected as insufficient ‚Äî full workspace context (variables, providers, backend config) is required for safe destroy.

### Implications
#### Positive
- Enables clean deletion of TF objects regardless of external resource lifecycle.
- Supports graceful teardown in CI/CD pipelines and ephemeral environments.
- Paves the way for handling cascading deletions and namespace cleanup.

#### Negative
- Increases storage requirements slightly per TF object (size of compressed workspace).
- Requires secure handling and access control for BLOB storage.
- Adds complexity in BLOB lifecycle management (cleanup post-deletion).

#### Mitigations
- Apply TTL-based garbage collection for BLOBs after CRD deletion confirmation.
- Use encryption-at-rest for sensitive BLOB contents (secrets included).
- Monitor BLOB storage growth and expose metrics for observability.

### Next Steps
1. Modify TF-Controller to persist Workspace BLOBs after successful apply.
2. Update finalizer logic to retrieve and use BLOB for `destroy` execution.
3. Enhance tf-runner to accept BLOB as primary input for destroy jobs.
4. Add validation: reject destroy-from-BLOB if marked unsafe (e.g., drift detected).
5. Test edge cases: BLOB corruption, permissions, large workspaces.

Once implemented and validated, proceed to address deletion of resources with dependencies and namespace-wide cleanup leveraging this foundation.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","# Architectural Decision Record: Transition to Open E2E Testing Without Repository Secrets

## Status
**Proposed**

## Context
In prior versions of Zarf, the initialization package creation required access to private images in registry1, necessitating repository secrets in GitHub Actions. To mitigate security risks from malicious pull requests, we used `peter-evans/slash-command-dispatch@v2` to restrict E2E test execution to maintainers only.

With Zarf v0.15, the initialization package no longer depends on registry1 images, eliminating the need for repository secrets during E2E test execution. This presents an opportunity to reevaluate our testing strategy to improve developer experience, test coverage, and CI/CD efficiency.

Additionally, we aim to:
- Test against multiple Kubernetes distributions (e.g., K3s, K8s, MicroK8s)
- Support various Linux distributions in testing
- Enable local E2E testing without EC2 dependencies for faster development feedback

## Decision
We will transition to an open E2E testing model by:

1. **Removing the slash-command-dispatch pattern** and allowing all PRs from trusted contributors (including first-time contributors) to run E2E tests automatically.
2. **Implementing a tiered testing approach**:
   - Run basic E2E tests on all PRs using GitHub-hosted runners with Kind/K3d for Kubernetes
   - Run expanded distribution testing on a schedule or maintainer-triggered basis using self-hosted runners
3. **Adopting container-based local testing** using tools like Kind, Minikube, or Docker Desktop to enable developers to run E2E tests locally
4. **Creating standardized test matrices** in GitHub Actions that dynamically configure tests based on Kubernetes and OS variables
5. **Setting up self-hosted runners** for testing on diverse Linux distributions and Kubernetes versions that cannot run efficiently in containers

## Rationale
This decision enables:
- **Improved contributor experience**: Contributors receive immediate feedback without waiting for maintainer intervention
- **Enhanced security posture**: Eliminates risk of secret exposure while maintaining test integrity
- **Better test coverage**: Systematic testing across environments
- **Faster development cycles**: Local test execution reduces iteration time
- **Cost efficiency**: Reduces unnecessary EC2 usage by leveraging GitHub-hosted runners for most scenarios

The removal of registry1 dependency fundamentally changes our threat model, making the previous restriction obsolete and potentially hindering community contributions.

## Consequences
### Positive
- Lower barrier to contribution
- Faster feedback loops for developers
- More comprehensive automated testing
- Reduced reliance on EC2 instances
- Better alignment with open-source best practices

### Negative
- Increased compute costs from more frequent E2E runs (mitigated by tiered approach)
- Need to implement anti-abuse measures (e.g., rate limiting, runner quotas)
- Additional complexity in test infrastructure management

### Mitigations
- Implement workflow restrictions for high-cost tests
- Use GitHub's permitted actions policies to control which workflows can run
- Monitor and set spending limits on CI resources
- Maintain ability to temporarily revert to restricted model if abuse is detected","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","### Decision: Implement a Temporary Local Registry with Node-Specific Image Preloading and TLS Trust Injection

#### Status
Accepted

#### Context
Zarf must deploy container images into arbitrary Kubernetes clusters without relying on pre-existing registries or cluster-specific tooling (e.g., `k3d load`, `kind load`). Many production or air-gapped environments lack accessible image registries, and direct node access to inject images via CRI is not standardized across distributions. Additionally:
- The CRI operates outside the Kubernetes control plane.
- Image pulling requires registry reachability and proper TLS trust configuration per node.
- Nodes may require root CA updates or CRI-specific configurations to trust private registries.
- Ensuring image availability across all nodes would require either full node configuration or scheduling constraints.

Given these constraints, Zarf needs a portable, dependency-minimal mechanism to reliably deliver images into any K8s cluster ‚Äî including unknown or restricted environments.

#### Decision
Implement the following image delivery strategy in Zarf:

1. **Embed a Lightweight, Ephemeral Registry**  
   Bundle a minimal OCI-compliant registry server (e.g., `registry:2` from Docker) as part of the Zarf agent/init package. This registry will be temporarily deployed within the cluster during package deployment.

2. **Push Images to the Embedded Registry**  
   During deployment, Zarf pushes required container images from the Zarf package to this local registry using standard `docker`/`containerd` CLI tools on the host or via buildkit.

3. **Expose the Registry Internally via Service**  
   Expose the registry internally using a `ClusterIP` service (e.g., `zarf-registry`) so pods can pull images using DNS resolution (`zarf-registry.default.svc.cluster.local:5000`).

4. **Automatically Rewrite Image References**  
   Modify pod specs in deployed workloads to reference the internal registry by rewriting image paths (e.g., `nginx:latest` ‚Üí `zarf-registry.default.svc.cluster.local:5000/nginx:latest`).

5. **Handle TLS Securely Without Node CA Modifications**  
   To avoid modifying node-level CA stores or CRI configurations:
   - Generate a self-signed certificate for the registry at runtime.
   - Mount the CA certificate into the kubelet‚Äôs trusted directory **only if direct node access is available and safe** (e.g., in known dev clusters).
   - Otherwise, **configure container runtimes via their API (if supported)** to trust the registry's cert (e.g., via `crictl` or editing `containerd` config).
   - As fallback: Use **insecure-registries** configuration on the CRI where allowed (documented as acceptable in air-gapped environments).

6. **Use DaemonSets or Jobs for Per-Node Configuration**  
   Deploy a privileged Job or DaemonSet that:
   - Detects the node‚Äôs CRI (containerd, CRI-O, etc.).
   - Applies necessary configuration changes (e.g., adding certs or insecure registries).
   - Restarts the CRI only if required and permitted.

7. **Fallback: Direct Image Load via CRI Tools (When Available)**  
   If Zarf detects a local development environment (e.g., KIND, K3D), use native tooling (`kind load`, `k3d image import`) instead of the registry method for better performance and simplicity.

8. **Clean Up After Deployment**  
   Once all workloads are running successfully, optionally remove the temporary registry and revert CRI changes (if tracking was implemented), though removal of insecure flags/certs may require manual intervention.

#### Rationale
- **No External Dependencies**: Works in fully air-gapped environments.
- **Portable Across Clusters**: Does not rely on cloud providers or external registries.
- **Avoids Node CA Management Complexity**: Uses sidecar registry with controlled exposure.
- **Standard Protocol Usage**: Leverages existing Docker/OCI protocols understood by all CRIs.
- **Supports Multi-Node Clusters**: All nodes can pull from the same internal endpoint after trust/config setup.
- **Graceful Degradation**: Falls back to optimized methods when possible (e.g., K3D/KIND loading).

#### Implications
- **Security**: Running a registry inside the cluster increases attack surface slightly; mitigated by short lifetime and network policy restrictions.
- **Performance**: Initial pull may be slower than direct injection but avoids large binary transfers over `kubectl`.
- **Privileges Required**: Needs elevated permissions to modify node storage, CRI configs, and run privileged containers.
- **Node Access**: Requires SSH or equivalent access to nodes for CRI modification unless done through DaemonSets with hostPath mounts.
- **Idempotency & Rollback**: Changes to CRI configuration should be tracked or documented for cleanup.

#### Alternatives Considered
| Option | Why Rejected |
|-------|--------------|
| Require external registry | Fails in air-gapped environments; violates ""no dependencies"" requirement. |
| Direct CRI image load on each node | No universal tooling; requires per-CRI and per-distro logic; not feasible generically. |
| HostPath image tarball + systemd container | Bypasses K8s entirely; not compatible with most CNI/CRI setups. |
| Always rewrite images to public mirrors | Not viable offline; breaks security/isolation goals. |

#### Notes
Future versions could explore integration with [ImagePolicyWebhook](https://kubernetes.io/docs/reference/access-authn-authz/image-policy/) or [Containerd Registry Mirrors](https://github.com/containerd/containerd/blob/main/docs/cri/config.md#registry-configuration) for more elegant redirection without rewriting image names. However, this still requires node-level configuration and lacks universal support.

---

‚úÖ **This decision enables Zarf to operate autonomously in unknown, restricted, or air-gapped Kubernetes environments by combining a temporary local registry with automated node configuration.**","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","**Decision:** Implement an automated Software Bill of Materials (SBOM) generation and management process for all software deployed to government hardware to comply with Executive Order (EO) 14028.

**Rationale:**  
Executive Order 14028, ""Improving the Nation's Cybersecurity,"" mandates enhanced software supply chain security, including the requirement for vendors to provide SBOMs for software sold to or used on U.S. government systems. An SBOM provides visibility into software components, enabling rapid identification and mitigation of vulnerabilities.

To meet compliance and improve transparency and security posture:
- SBOMs will be generated automatically during the build pipeline using industry-standard formats (e.g., SPDX, CycloneDX).
- The process will cover all open-source, third-party, and proprietary components.
- SBOMs will be archived alongside build artifacts and made available to authorized stakeholders, including government customers.
- Tools integrated into the CI/CD pipeline (e.g., Syft, Trivy, ORAS) will ensure consistent, reliable SBOM generation and signing.

This decision ensures regulatory compliance, strengthens trust with government clients, and supports faster incident response in the event of a discovered vulnerability.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision: Implement a Zarf-native Mutating Webhook for Transparent Image and Secret Rewriting

### Status
Accepted

### Context
Zarf currently uses Helm post-rendering to rewrite image references and secret values so they point to the internal Zarf Registry and other internal services (e.g., Gitea). This approach works when Zarf directly applies manifests via `helm template | kubectl apply`, but fails in GitOps workflows where tools like Flux or ArgoCD manage the reconciliation loop. In such cases, Zarf loses control over the rendering phase, and thus cannot mutate manifests before they are applied.

While `zarf prepare` was introduced to help package authors pre-process manifests, it places undue burden on them and is not fully automated or transparent. This creates friction for advanced use cases and limits Zarf‚Äôs compatibility with popular GitOps practices.

There is a need for a transparent, cluster-side mechanism that automatically rewrites image paths and secrets at deploy time‚Äîregardless of which tool applies the manifests.

### Decision
Implement a **Zarf-managed MutatingAdmissionWebhook** that intercepts pod-creating resources (e.g., Deployments, DaemonSets, StatefulSets, Jobs, etc.) and optionally Secrets, rewriting:
- Container image references from external registries to the internal Zarf Registry (using Zarf's known mapping).
- References to secrets that should be remapped to Zarf-injected equivalents (e.g., pull secrets).

This webhook will:
- Be deployed as part of the Zarf agent during `zarf init` if the `gitops` or `admission-control` feature flag is enabled.
- Use configuration stored in a ConfigMap (managed by Zarf) containing image and secret rewrite rules generated at package deployment time.
- Only mutate workloads within namespaces labeled with `zarf.dev/managed=true`.
- Fail open (allow creation) if the webhook pod is down, with proper timeout settings (e.g., 3s).
- Be secured via TLS using a CA bundle injected by the Zarf agent.

Additionally, retain Helm post-rendering as the default for direct deployments, but document the webhook as the recommended path for GitOps integration.

### Consequences
**Pros:**
- Enables seamless use of GitOps tools (Flux, ArgoCD) with Zarf packages.
- Removes burden from package authors; mutations are handled automatically.
- Centralizes mutation logic inside Zarf instead of pushing complexity into templates.
- Works consistently across all manifest application methods.

**Cons:**
- Adds operational complexity: managing a webhook service and TLS certs.
- Requires admission controller permissions, which may be restricted in some environments.
- Slight delay during pod creation due to webhook call (mitigated with timeouts and caching).
- More moving parts to debug if mutations fail.

**Alternatives Considered:**
1. **Enhance `zarf prepare`:** Could generate full manifests with rewritten fields, but still requires manual invocation and doesn‚Äôt handle dynamic GitOps sync cycles.
2. **Sidecar Injector Pattern:** Only handles sidecars, not general image rewriting.
3. **CRD + Controller:** Would require custom controllers per resource type‚Äîoverkill compared to admission webhooks.
4. **ImagePolicyWebhook:** Deprecated and lacks flexibility.

### References
- Kubernetes Admission Control: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
- Helm Post Renderer Limitations: https://helm.sh/docs/topics/advanced/#post-rendering
- Flux Integration Challenges: https://github.com/fluxcd/flux2/issues/2000
- Zarf Prepare Command: https://docs.zarf.dev/docs/cli/zarf_package_create#zarf-package-create---prepare

### Reviewed By
Zarf Maintainers Team  
Date: 2025-04-05","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","### Decision

Replace the `stage2` Go binary with a lightweight, purpose-built Rust binary (`zarf-injector`) that directly loads the `registry:2` image from a ConfigMap or init container into the container runtime using containerd's API (via `containerd/cri` or `runc`), then starts a minimal read-only registry proxy (e.g., using `micro-registry` or a slimmed-down `stargz`-based server) only for distributing the image within the cluster.

Alternatively, and more decisively: **eliminate the two-stage injection process entirely** by directly loading the `registry:2` image into the node‚Äôs container runtime (e.g., containerd) during `zarf init`, using `crictl load` or `ctr images import` via an init container, and run the registry natively using a DaemonSet or static Pod with the image already present on disk. This removes the need for `stage1`, `stage2`, and associated ConfigMaps.

---

#### Chosen Approach:
**Use an init container in the `zarf-registry` Pod to directly import the `registry:2` image into containerd, then start the registry from the loaded image.**

This approach:
- Removes the need for both `stage1` and `stage2` binaries.
- Eliminates all ConfigMaps containing embedded tarballs.
- Avoids running a temporary registry just to bootstrap another registry.
- Reduces complexity, attack surface, and initialization time.

##### Implementation Details:
- Embed the `registry:2` image as a `.tar.gz` inside the Zarf agent/init container image.
- Use an `initContainer` in the `zarf-registry` Pod that runs `ctr images import` or `docker load` (depending on runtime) to load the embedded image.
- Once imported, the main container of `zarf-registry` can simply run `registry:2` in `rw` mode, push the crane tarball, then restart in `ro` mode ‚Äî but now using a locally available image.

##### Benefits:
- ‚úÖ No extra binaries to maintain or cross-compile.
- ‚úÖ Faster `zarf init`: fewer resources to apply, no startup delay from reassembling images.
- ‚úÖ Simpler debugging and auditing: fewer moving parts.
- ‚úÖ Better resource efficiency: no redundant registry instances.
- ‚úÖ Aligns with Kubernetes patterns: leverage container lifecycle hooks and init containers.

##### Trade-offs:
- ‚ùå Requires host-level access to container runtime socket (e.g., `/run/containerd/containerd.sock`) ‚Äî but this is already required for other Zarf operations.
- ‚ùå Tight coupling to container runtime (containerd/docker), but this is acceptable given Zarf‚Äôs target environment assumptions.

---

### Status
Accepted

### Date
2025-04-05

### Author
Architectural Review Board, Zarf Project

### References
- ADR-3: Two-Stage Binary Injection Pattern
- [Zarf Issue #1243: Reduce init overhead](https://github.com/zarf-dev/zarf/issues/1243)
- Kubernetes Init Containers: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
- containerd CTR CLI: https://github.com/containerd/containerd/tree/main/cmd/ctr","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision

**Adopt a targeted, interface-focused unit testing strategy in defined areas of the codebase that are difficult to test via end-to-end (e2e) testing, particularly around public APIs and core logic, while maintaining our existing e2e suite as the primary validation mechanism.**

### Rationale

As Zarf evolves from a standalone CLI tool into a library consumed by other products, the stability and correctness of its public interfaces become critical. End-to-end tests, while valuable for validating overall system behavior, are insufficient for ensuring reliability in:

- Low-level utilities and helper functions
- Core logic with branching or complex state management
- Public API surfaces exposed to integrators

A blanket, Zarf-wide unit testing mandate (Option 1) risks introducing brittle, implementation-focused tests that hinder refactoring and increase maintenance overhead without proportional quality gains.

Conversely, avoiding unit testing entirely (effectively Option 3‚Äôs implication) leaves critical logic untested in a maintainable way, especially as e2e tests struggle to cover edge cases and error paths comprehensively.

Therefore, we choose **Option 2 ‚Äî limited and strategic unit testing** ‚Äî because it offers the best balance between test effectiveness and long-term maintainability.

### Scope & Guidelines for Unit Testing

We will apply unit testing selectively under the following principles:

1. **Focus on Public Interfaces**: Test exported functions and methods, especially those intended for external use.
2. **Target Hard-to-Test-with-e2e Areas**: Prioritize:
   - Configuration parsing and validation
   - Internal state transformations
   - Error handling and fallback logic
   - Deterministic algorithms (e.g., image name rewriting, package mutation)
3. **Avoid Testing Implementation Details**: Tests should verify *what* the code does, not *how*. Refactoring should not break well-written unit tests.
4. **Minimize Mocking**: Avoid excessive mocking of internal components; prefer pure function testing or integration at the module level where feasible.
5. **Require Code Comments for Non-Obvious Test Logic**: Ensure future maintainers understand intent.

### Enforcement & Governance

- PRs introducing new unit tests must justify their scope and alignment with the above guidelines.
- The maintainers‚Äô team will review unit test additions for appropriateness during code review.
- We will document these standards in `CONTRIBUTING.md` and update them iteratively.

### Future Consideration

We will re-evaluate this strategy after six months of adoption, measuring:
- Flakiness and maintenance cost of new unit tests
- Reduction in bugs related to previously untested logic
- Feedback from external integrators

This approach ensures we raise confidence in Zarf's reliability as a library without overcommitting to a testing paradigm that could slow development velocity.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision: SBOM Files and Data Injections by Default, with Extension Points for Future Flexibility

We will generate Software Bill of Materials (SBOMs) using Syft for **files and dataInjections** included in Zarf packages during the package creation process. This approach strikes a balance between completeness, accuracy, and usability.

### Rationale

- **Accuracy Over Noise**: While scanning the entire Zarf temp build directory (Option 1) might surface more potential components, it risks including transient or irrelevant artifacts (e.g., Git metadata, intermediate build files), leading to false positives and user confusion. Our goal is to provide actionable, accurate SBOM data‚Äînot speculative noise.
  
- **Coverage of Common Use Cases**: Files and dataInjections are the primary mechanisms users employ to include non-container artifacts in Zarf packages (as evidenced by usage in the Zarf init package). By focusing on these, we cover the most common and impactful sources of non-image dependencies that could contain vulnerabilities (e.g., binaries, scripts, configuration bundles).

- **Actionable Security Insights**: Unlike source code pulled via git repos (which may not be deployed as-is), files and dataInjections represent final artifacts being distributed. These are the components most likely to introduce supply chain risks and thus warrant SBOM coverage.

- **Foundational for Extensibility**: While we start with files and dataInjections, we will design the SBOM generation system with extension points (e.g., hooks, configuration flags) to support future expansion‚Äîsuch as user-defined paths or git submodule scanning‚Äîwithout requiring breaking changes.

- **Usability and Adoption**: Making SBOM generation automatic and focused ensures users receive valuable security insights out-of-the-box without configuration overhead. Optional or overly flexible systems (Option 3) risk being underutilized or misconfigured.

### Implementation Details

1. During `zarf package create`, after all files and dataInjections are staged:
   - Run Syft against each file explicitly listed in `zarf.yaml` under `files` and `dataInjections`.
   - Generate individual SBOMs per artifact and aggregate them into a unified SBOM for the package.
   - Attach the resulting SBOM(s) as assets within the final Zarf package (e.g., `sbom.spdx.json`, `sbom.cyclonedx.xml`).

2. Use standard, widely supported SBOM formats (SPDX and CycloneDX) to ensure compatibility with existing tooling.

3. Expose SBOMs via `zarf inspect` and optionally publish them alongside published packages (e.g., in OCI registry).

4. Log which artifacts were scanned and any errors (e.g., unreadable files) for transparency.

### Future Considerations

- Evaluate demand for scanning additional sources (e.g., k3s binaries pulled during packaging, git submodules) based on user feedback.
- Explore optional configuration in `zarf.yaml` to specify custom paths or inclusion rules for SBOM generation.
- Integrate vulnerability scanning atop generated SBOMs in future versions.

### Outcome

This decision ensures that Zarf provides meaningful, accurate SBOM coverage for non-container artifacts today while laying the groundwork for broader supply chain security capabilities tomorrow.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision: Support Optional Zarf Init in Connected Environments via Declarative Bootstrap

**Status:** Accepted  
**Date:** 2024-07-15  
**Context:** Zarf was originally designed for air-gapped, disconnected environments where the `zarf init` package is required to bootstrap cluster dependencies such as a container registry, storage backend, and other core services. However, in connected environments (where external registries, ingress, and DNS are available), requiring `zarf init` introduces unnecessary overhead and limits Zarf‚Äôs utility as a purely declarative deployment tool.

Many users in connected environments still benefit from Zarf‚Äôs declarative package definition, dependency management, and deployment automation but do not need or want the self-contained infrastructure provisioning that `zarf init` provides.

---

### Decision

Introduce an **optional initialization mode** that allows Zarf packages to be deployed **without a `zarf init` package** in connected environments, provided all required services (registry, ingress, storage) are externally available and pre-configured.

This will be implemented through:

1. **`--skip-init` flag** in `zarf deploy`:  
   Skips the requirement for a locally available `zarf init` state. Instead, Zarf assumes external services are available and uses configuration from `zarf.yaml` or CLI flags to connect to them.

2. **Declarative service references in `zarf.yaml`**:  
   Allow package authors to declare external dependencies using standard endpoints (e.g., `registry.example.com`, existing IngressClass, PVC names). Example:
   ```yaml
   metadata:
     name: my-app
   imports:
     - image: nginx:latest
       # Resolves to external registry
   components:
     - name: app
       manifests:
         - kubernetes-deployment.yaml
   ```
   
3. **Bootstrap validation without provisioning**:  
   During deployment, Zarf performs connectivity checks (e.g., can push images to the specified registry, can apply manifests) but does not install internal services.

4. **Zarf State API remains**, but with minimal footprint:  
   A lightweight Zarf state will still be applied to track deployed packages, enabling `zarf destroy` and upgrades, but without managing infrastructure.

---

### Rationale

- **Reduces friction** for teams wanting to use Zarf as a declarative deployment tool in CI/CD pipelines or cloud environments.
- **Maintains backward compatibility** ‚Äî `zarf init` remains required for disconnected workflows.
- **Aligns with GitOps practices** by allowing Zarf to function as a declarative installer without sidecar services.
- **Enables hybrid use cases** ‚Äî e.g., use Zarf to deploy to EKS/GKE clusters with existing tooling.

---

### Implications

- **Security**: Users must ensure external services are secure and accessible; Zarf will not harden them.
- **Support scope**: Zarf support will cover deployment logic, not external service configuration.
- **Documentation**: Clear guidance will be needed to distinguish between ‚Äúinit‚Äù and ‚Äúno-init‚Äù modes.

---

### Future Considerations

- Allow `zarf init` to run in ""connected mode"" to configure external services instead of deploying internal ones.
- Integrate with existing service meshes or registries via plugins.

---

**Approver(s):** Zarf Maintainers  
**Related Issues:** #1294, #1301","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision

Introduce a structured `scripts` lifecycle model with named, extensible hook points and context-aware execution phases to replace the limited `before`, `after`, and `prepare` arrays.

We will expand the `scripts` section in component definitions to support the following lifecycle hooks:

- `prepare`: Executes during `zarf package create` (as before), used for asset generation or preprocessing.
- `preInstall`: Runs on the target cluster/node *before* component deployment, during `zarf deploy`.
- `postInstall`: Runs on the target cluster/node *after* component deployment.
- `preRemove`: Runs before component removal (if applicable).
- `postRemove`: Runs after component removal.

Each script entry supports:
- `cmd`: The command to execute.
- `dir`: Working directory.
- `timeout`: Optional timeout duration.
- `env`: Environment variables specific to the script.
- `skip`: Conditional skip (e.g., based on Zarf variables).

This change enables:
- Clear separation of concerns across the package lifecycle.
- Reuse of logic across different stages (e.g., health checks in `postInstall`).
- Idempotency and cleanup capabilities via removal hooks.
- Better debugging through predictable execution ordering and logging per phase.

Rationale: The original `before`/`after` model was ambiguous about execution environment and timing. By formalizing the lifecycle, we empower users to write more reliable, reusable, and maintainable packages while supporting advanced use cases like pre-flight validation, configuration injection, and teardown routines.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","# Architectural Decision Record (ADR): Retain Crane for Local Image Loading with OOM Mitigation via Streaming Mode

## Context

Zarf introduced local image support in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to close a critical usability gap for developers doing local development. The implementation leveraged **crane** due to its speed and integration simplicity when pulling images from the Docker daemon. However, a significant issue ([#1214](https://github.com/defenseunicorns/zarf/issues/1214)) was discovered: crane loads the full tar stream of an image into memory during `docker save`, which can trigger out-of-memory (OOM) conditions on large images.

Extensive investigation explored alternatives:
- **Docker CLI**: Produces legacy format incompatible with planned OCI schema changes; up to 3√ó slower.
- **Skopeo**: More robust but significantly slower than crane.
- **Docker Go Client**: Complex to integrate, similar performance to Docker CLI.
- **Crane (streaming mode)**: Avoids OOM by streaming without buffering, but at reduced speed.

Performance data shows crane is consistently faster than Docker CLI across real-world and stress-test scenarios:

| Scenario                                      | Docker     | Crane      |
|----------------------------------------------|------------|------------|
| Big Bang Core (cached)                       | 3m 1s      | 1m 58s     |
| Big Bang Core (skip-sbom)                    | 1m 51s     | 56s        |
| 20 GB Single-Layer Image (local registry)    | N/A        | 6m 14s     |
| 20 GB Image (cached + local registry)        | 5m 2s      | 2m 10s     |

Despite the OOM risk, crane remains the most performant and future-compatible option‚Äîespecially as we plan to align with OCI-centric packaging ([#1319](https://github.com/defenseunicorns/zarf/issues/1319)).

---

## Decision

**Retain crane as the default tool for loading images from the local Docker daemon, but introduce a fallback mechanism using crane‚Äôs streaming mode (`--allow-nondistributable-artifacts`) to prevent OOM crashes on large images.**

We will implement the following strategy:

1. **Default Path ‚Äì Fast Mode**: Use standard `crane pull` to load images from the Docker daemon. This keeps high performance for typical use cases.
2. **Fallback Path ‚Äì Streaming Mode**: If an OOM or memory exhaustion error is detected (or proactively based on image size heuristics), retry using `crane pull --allow-nondistributable-artifacts`, which avoids loading the entire layer into memory by streaming directly.
3. **Configurable Threshold**: Introduce a new Zarf config field (`max-image-size-for-fast-load`) that allows users to define when to skip fast mode and use streaming mode based on image size.
4. **Future-Proofing**: Ensure all paths produce OCI-compliant image formats to maintain compatibility with upcoming schema changes.

This approach balances performance and reliability while preserving forward compatibility.

---

## Status

Accepted

---

## Date

2025-04-05

---

## Authors

- Zarf Engineering Team

---

## References

- [#1173](https://github.com/defenseunicorns/zarf/pull/1173): Initial local image support
- [#1214](https://github.com/defenseunicorns/zarf/issues/1214): OOM issue with large images
- [#1319](https://github.com/defenseunicorns/zarf/issues/1319): Future OCI import schema changes
- https://github.com/google/go-containerregistry/tree/main/cmd/crane
- Performance benchmarks conducted internally (Q1 2025)","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision: Implement a Big Bang Optimization Layer in Zarf via Dynamic Component Generation

### Status
Accepted

### Context
Big Bang is a widely used, complex Kubernetes platform deployment framework that end users frequently deploy using Zarf. However, its default structure poses several challenges for air-gapped or isolated environments:

- **Flux Dependency**: Big Bang relies on Flux and its CRDs, requiring Flux to be bootstrapped prior to deployment.
- **Image Bloat**: The full set of referenced container images exceeds tens of gigabytes, but most deployments only use a subset based on configuration.
- **Git Repository Overhead**: Big Bang references numerous Git repositories through Flux, many of which are unnecessary depending on enabled modules.
- **Configuration Complexity**: Injecting custom `values.yaml` into Big Bang‚Äôs deeply nested Kustomize structure is non-trivial.
- **Dynamic Requirements**: The actual set of required images and repos depends on the values provided (e.g., enabling Istio pulls in associated images), making static packaging inefficient.

Zarf aims to support efficient, declarative, air-gapped application delivery, so including all possible Big Bang assets by default leads to unusable package sizes and poor user experience.

### Decision
Introduce a **Big Bang optimization layer** within Zarf that dynamically generates optimized components based on user-provided configuration (e.g., `values.yaml`). This will be implemented as a **Zarf Package Extension** or **CLI plugin pattern**, with potential future integration into core tooling.

Key aspects of the decision:

1. **Dynamic Image & Git Repo Discovery**:
   - Provide a helper tool (e.g., `zarf tools bigbang-analyze-values`) that accepts a user's `values.yaml`.
   - Use Helm template logic and Big Bang's module dependency graph to determine which components (e.g., Istio, Logging, CIS) are enabled.
   - Map enabled components to their required container images and Git repositories using known manifests (e.g., `package-images.yaml`, Flux Kustomizations).

2. **Automated Zarf Component Generation**:
   - Generate a tailored `zarf.yaml` fragment or full package definition containing only the necessary:
     - Container images (pulled and mirrored via Zarf)
     - Git repositories (cloned and bundled)
     - Configuration overlays (e.g., patched `values.yaml`, Kustomize patches)
   - Automatically include a prerequisite Flux installation component if not already present in the cluster.

3. **Simplified User Workflow**:
   - Users provide their `values.yaml` and run `zarf package create --from-big-bang-config=values.yaml`.
   - Zarf (or an external helper) analyzes the config, fetches metadata, and builds a minimal, valid package.
   - The resulting package includes only what is needed for that specific Big Bang deployment.

4. **Caching & Metadata Indexing**:
   - Cache Big Bang release metadata (e.g., image lists per version) locally or in CI to avoid repeated downloads.
   - Support version pinning (e.g., Big Bang 1.51.0) for reproducibility.

5. **Fallback Option**:
   - Allow opting into a ""full"" Big Bang package (all images/repos) for maximum portability at the cost of size.

### Consequences
**Pros**:
- Drastically reduces Zarf package size (from 10s of GB to ~1‚Äì5 GB typical).
- Improves usability and performance in air-gapped environments.
- Maintains compatibility with Big Bang‚Äôs flexibility and modular design.
- Encourages best practices by guiding users toward configuration-driven, minimal deployments.

**Cons**:
- Adds complexity to Zarf tooling or requires external tooling coordination.
- Requires ongoing maintenance to track Big Bang structure changes.
- Initial analysis step adds a small overhead during package creation.

**Mitigations**:
- Design the analyzer as a standalone, versioned tool aligned with Big Bang releases.
- Integrate documentation and examples into Zarf guides.
- Eventually upstream key functionality into Zarf CLI under `zarf package extend bigbang`.

### References
- Big Bang Documentation: https://repo1.dso.mil/big-bang/bigbang
- Big Bang Image Manifest: https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml
- Zarf Documentation: https://docs.zarf.dev

---

*This decision enables Zarf to efficiently support one of its most important real-world use cases while maintaining performance, usability, and scalability in air-gapped environments.*","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision

We will implement native support for publishing and pulling Zarf packages to and from OCI-compliant registries, using the OCI Artifact specification, and integrate Sigstore-based signing and verification to enable supply chain security.

### Details

1. **OCI Registry Integration**  
   Zarf will natively publish and retrieve packages as OCI artifacts to any OCI-compliant registry (e.g., Docker Hub, GitHub Container Registry, Google Container Registry, Artifactory). This leverages existing, widely adopted infrastructure and tooling, enabling seamless distribution without requiring custom hosting solutions.

2. **OCI Artifact Format**  
   Zarf packages will be stored as OCI artifacts with a defined media type (`application/vnd.zarf.package.v1+tar+gzip`). The package `.tar.zst` file will be pushed as a blob within the manifest, referenced via a `manifest.json` conforming to the OCI Image Specification (with appropriate artifact type annotations).

3. **Package Signing with Sigstore**  
   We will integrate Sigstore (via Cosign) to allow package creators to sign Zarf packages using keyless signing (based on OIDC identity) or traditional key-based signing. Signatures will be stored as separate OCI artifacts linked by tag or subject reference in the same repository.

4. **Signature Verification at Deploy Time**  
   During `zarf package deploy`, Zarf will verify package integrity and authenticity if signatures are present and verification is enabled. Users can enforce strict signature checks via configuration or CLI flags (e.g., `--insecure-skip-verify-signature` for development).

5. **Trust Model and Key Management**  
   - Default trust will be established through identity-based signing (keyless) using trusted identity providers (e.g., GitHub Actions).
   - Organizations may configure allowed signers via public keys or identity patterns in a `zarf.yaml` policy block.
   - Public key files and certificate chains can be distributed out-of-band or via a TUF-style root of trust if needed.

6. **CLI Commands**  
   New commands will be introduced:
   - `zarf package publish <package.tar.zst> <oci://registry.example.com/repo:tag>`
   - `zarf package pull <oci://registry.example.com/repo:tag> [-o package.tar.zst]`
   - `zarf package sign <package.tar.zst> [--key=keyfile] [--upload=true]`
   - `zarf package verify <oci://...> [--expected-identity=...]`

7. **Backward Compatibility**  
   Existing local file-based workflows (`zarf package create`, `zarf package deploy ./package.tar.zst`) remain fully supported. OCI and signing features are additive.

### Rationale

- **Scalability**: OCI registries are globally distributed, highly available, and support fine-grained access control‚Äîmaking them ideal for scalable package distribution.
- **Interoperability**: Using standard OCI interfaces ensures compatibility with existing CI/CD systems, air-gapped environments, and enterprise artifact management policies.
- **Security & Trust**: Sigstore provides a modern, transparent, and auditable signing framework aligned with SLSA and zero-trust principles. Keyless signing lowers the barrier to entry while maintaining strong identity guarantees.
- **Future Extensibility**: The artifact model supports rich metadata, provenance, and scanning integrations (e.g., vulnerability detection on components within the package).

### Success Metrics

‚úÖ **Primary Goal Achieved**: Users can run `zarf package publish` to push to any OCI registry and `zarf package pull` to consume it‚Äîwithout manual transfers.

‚úÖ **Secondary Goal Achieved**: Packages can be signed and verified; deployers can enforce trust policies based on signatures.

This decision enables secure, scalable, and trustworthy Zarf package distribution while aligning with cloud-native best practices.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","**Decision:** Extend Zarf to support an air-gapped artifact repository proxy that enables development workflows by caching and serving common language-specific package managers (e.g., npm, PyPI, Maven, Go modules) and enhance native `git` server capabilities beyond Flux `GitRepository` objects.

---

### Status
Accepted

### Context
Zarf is designed to manage dependencies in air-gapped environments using embedded git servers and container registries. While this suffices for deploying pre-built applications, it falls short in supporting **air-gapped development**, where developers require access to upstream language ecosystems (e.g., `npm install`, `pip install`, `go get`) and more flexible `git` operations.

Current limitations include:
- No support for language package managers, forcing developers to manually collect and inject dependencies.
- Limited `git` support: only Flux-compatible `GitRepository` sources are handled, not direct developer-initiated `git clone` or `git pull` from upstream remotes.
- Developers working in isolated environments lack tooling parity with internet-connected workflows.

To enable robust software development within air gaps (e.g., secure facilities, offline CI/CD), Zarf must bridge the gap between deployment-time dependency management and development-time artifact retrieval.

### Decision
Introduce two new components into Zarf:

1. **Artifact Proxy Service**  
   Deploy a lightweight, configurable proxy service within the Zarf agent that intercepts and caches requests from common language package managers:
   - Supports protocols for npm, PyPI, Maven Central, RubyGems, Cargo (Rust), Go modules, etc.
   - Operates as a forward proxy during outbound seeding; becomes a local mirror in the air gap.
   - Integrates with `zarf prepare` to seed packages from upstream sources into the package.
   - Injects environment configuration (e.g., `.npmrc`, `pip.conf`) into development workloads automatically.

2. **Enhanced Native Git Server Support**
   Expand Zarf‚Äôs git handling to support generic `git` operations:
   - Allow mirroring arbitrary git repositories (not just Flux manifests).
   - Serve them via HTTP(S)/SSH on standard ports with full `git clone`, `pull`, `fetch` support.
   - Preserve refs, tags, submodules, and history.
   - Automate credential injection and URL rewriting in dev containers/IDEs.

These services will be optional components activated when a ""development mode"" or ""dev-package"" is detected during `zarf init` or package creation.

### Consequences

**Pros:**
- Enables full development lifecycle inside air gaps without manual dependency wrangling.
- Improves developer experience by aligning air-gapped workflows with standard practices.
- Increases reusability of Zarf packages across both CI/CD pipelines and developer environments.
- Future-proofs Zarf for use in secure software factories and disconnected DevSecOps platforms.

**Cons:**
- Increased complexity in Zarf agent and package generation logic.
- Larger package sizes due to inclusion of potentially large dependency trees.
- Requires careful cache invalidation and version pinning strategies to avoid drift.
- Additional attack surface if proxies are misconfigured (mitigated via opt-in and least privilege).

**Alternatives Considered:**
- **Manual dependency collection**: Error-prone, unscalable, no automation.
- **External proxy appliances**: Adds infrastructure dependency; breaks self-contained model.
- **Only extend Flux support**: Still wouldn't solve general dev needs outside GitOps flows.

### See Also
- [Zarf GitHub Discussions ‚Äì Dev Workflows in Air Gaps](https://github.com/vmware-tanzu/zarf/discussions)
- [Nexus Repository OSS](https://www.sonatype.com/products/nexus-repository) ‚Äì inspiration for multi-format proxy
- Flux CD GitRepository spec ‚Äì baseline capability reference

--- 

*This ADR extends Zarf's mission from ‚Äúsecure deployment in air gaps‚Äù to also include ‚Äúsecure development in air gaps.‚Äù*","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","**Decision: Establish a Formal Release and Versioning Strategy to Achieve GA (v1.0.0) Stability**

To transition Zarf into a Generally Available (v1.x.x) product suitable for mission-critical operations, we will adopt a formal release and versioning strategy grounded in Semantic Versioning (SemVer), introduce a structured deprecation policy, expand test coverage, and define clear stability guarantees.

### Rationale
Currently, Zarf undergoes frequent breaking changes and lacks comprehensive testing in key operational areas, making it unsuitable for risk-averse or production environments without deep user involvement. To achieve GA status:
- Users must be able to rely on stable APIs, CLI interfaces, and package formats.
- Breaking changes must be minimized and communicated well in advance.
- Automated testing must cover core workflows (e.g., init, package creation, deployment, air-gapped operation).
- Long-term support (LTS) and maintenance policies must be defined.

### Decision Details

1. **Adopt Semantic Versioning (SemVer 2.0)**
   - Use `MAJOR.MINOR.PATCH` versioning.
   - Commit to no breaking changes within any `v1.x.x` release series.
   - The first GA release will be `v1.0.0`, signaling a stable public API and commitment to backward compatibility.

2. **Freeze Core Interfaces Pre-GA**
   - Finalize and lock:
     - Zarf YAML schema (with extensibility via optional fields)
     - CLI commands and flags
     - Package format and metadata structure
     - Internal APIs used by components (e.g., injector, agent)
   - Any future changes to these will follow the deprecation policy.

3. **Introduce a Deprecation Policy**
   - Deprecate features with at least two minor releases of warning.
   - Emit runtime warnings when deprecated features are used.
   - Document all deprecations in release notes and upgrade guides.

4. **Establish Testing Requirements for GA**
   - Expand automated test coverage to ‚â•85% for critical paths.
   - Implement end-to-end (e2e) tests for common workflows in CI/CD.
   - Introduce integration testing in air-gapped and restricted network environments.
   - Add upgrade testing between minor versions.

5. **Define Support Window**
   - Support each major version for at least 18 months.
   - Provide security and critical bug fixes during this period.
   - Clearly document supported versions on the project website.

6. **Release Process**
   - Introduce release branches (`release/v1.x`) for stabilization.
   - Publish release candidates (RCs) for community validation.
   - Require passing CI, security scans, and manual QA before GA tagging.

7. **Documentation & Transparency**
   - Maintain a public roadmap toward v1.0.0.
   - Publish a stability SLA (e.g., ‚Äúno breaking changes in v1.x‚Äù).
   - Provide upgrade guides and migration tooling where needed.

### Consequences

**Positive:**
- Increased trust and adoption in enterprise and government environments.
- Reduced operational overhead for users due to predictable upgrades.
- Clearer contribution guidelines for maintainers and community.

**Negative:**
- Slower iteration on core components post-v1.0.0.
- Upfront investment required to stabilize interfaces and improve testing.
- Potential need to refactor unstable features before GA.

---

This decision sets the foundation for Zarf to become a reliable, production-ready tool while maintaining long-term sustainability and user confidence.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision: Implement Registry-Agnostic Hooks for ECR Integration in Zarf

### Status
Accepted

### Context
As Zarf gains adoption, package `actions` are increasingly being used to work around missing core functionality‚Äîparticularly in complex registry scenarios like Amazon Elastic Container Registry (ECR). Two key challenges arise when using ECR:
1. **Short-lived authentication tokens** (expire every 12 hours), requiring periodic refresh.
2. **No push-to-create repository behavior**, necessitating pre-creation of image repositories before image pushes.

Currently, these issues are addressed either by manual pre-deployment setup or via ECR-specific `actions` embedded in packages. Both approaches violate core Zarf principles:
- Packages should be **portable and registry-agnostic**.
- Users should not need deep knowledge of backend registry mechanics.
- Workarounds should not be encoded in user-facing `actions`.

Allowing `actions` to handle such foundational concerns risks fragmenting package compatibility, increasing maintenance burden, and reducing usability.

---

### Decision
Introduce **core lifecycle hooks** within Zarf that allow the platform‚Äînot the package‚Äîto automatically handle registry-specific behaviors during initialization and deployment. Specifically:

1. **Implement internal, system-level hooks** that trigger during `zarf init` and package deploy phases if ECR (or other non-standard registries) is detected.
2. **Automatically manage ECR image repository creation**:
   - Before pushing images, Zarf will check if the target repo exists in ECR.
   - If not, use AWS credentials from the environment to create it via AWS SDK.
3. **Automatically refresh ECR auth tokens**:
   - During `zarf init` and periodically during long-running operations, regenerate ECR authentication tokens using IAM roles or local AWS credentials.
   - Inject updated credentials into Kubernetes image pull secrets and local Docker config as needed.
4. **Abstract registry specifics behind a unified interface**:
   - Detect registry type based on domain (e.g., `*.dkr.ecr.*.amazonaws.com`) and apply appropriate adapter logic.
   - Expose minimal configuration surface (e.g., AWS region, role ARN) in `zarf.yaml` or init prompts.

These behaviors will be **built into Zarf**, not exposed as user-configurable `actions`. This ensures consistent, secure, and transparent handling of ECR quirks without requiring custom scripting in packages.

---

### Rationale
- ‚úÖ **Preserves package portability**: Packages remain unaware of underlying registry types.
- ‚úÖ **Reduces user burden**: No need for manual repo creation or token management.
- ‚úÖ **Maintains action integrity**: Keeps `actions` focused on extension, not workaround.
- ‚úÖ **Scales to other registries**: Pattern supports future integration with GCR, ACR, etc.
- ‚ùå **Increases Zarf complexity**: Adds conditional logic and external dependencies (e.g., AWS SDK).
   - *Mitigation*: Isolate registry adapters in modular components; make optional at build time.

---

### Consequences
- **Positive**:
  - Seamless ECR support without changes to existing Zarf packages.
  - Improved UX for cloud-native deployments.
  - Foundation for supporting other opinionated registries.
- **Negative**:
  - Tighter coupling between Zarf and cloud provider APIs.
  - Requires secure handling of cloud credentials (avoid leaking IAM details).
- **Neutral**:
  - May delay `zarf init` slightly due to pre-checks and setup.

---

### Future Considerations
- Support assume-role workflows for cross-account ECR access.
- Add TTL-aware credential caching to minimize API calls.
- Explore declarative registry requirements in `zarf.yaml` (e.g., `requires: ecr`).

---

### Implementation Sketch
```yaml
# zarf.yaml (example metadata, not user-facing actions)
components:
  - name: base
    requires:
      registry: ecr # Hint to Zarf to enable ECR adapter
```

Zarf internal flow:
```
On zarf init/deploy:
  ‚Üí Detect registry type
  ‚Üí Load ECR adapter if needed
  ‚Üí Ensure repos exist (via AWS API)
  ‚Üí Refresh token (via aws-cli/sdk)
  ‚Üí Proceed with normal image push/pull
```

This decision ensures Zarf remains a smooth abstraction layer over complex infrastructure while keeping packages clean, portable, and focused on application delivery.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision: Use WebAssembly (WASM) to Decouple and Reuse Zarf‚Äôs Transformation Logic in Pepr

### Status
Accepted

### Context
As Pepr assumes admission control responsibilities previously handled by the Zarf Agent, it must perform the same Kubernetes manifest transformations that are currently implemented in Zarf‚Äôs Go-based transformation library. This library is mature, well-tested, and trusted in production environments. Rewriting it in TypeScript would introduce significant technical debt, delay progress, and increase the risk of divergence in behavior between Zarf and Pepr.

Alternative integration strategies were considered:

- **Rewriting in TypeScript**: High effort, error-prone, and unsustainable due to ongoing need for synchronization.
- **gRPC/REST**: Requires running a long-lived Go service alongside Pepr, adding operational complexity, latency, and failure modes (e.g., process lifecycle management, networking).
- **WASM**: Enables execution of compiled Go code directly within the Node.js runtime used by Pepr, providing near-native performance and minimal operational overhead.

### Decision
We will compile Zarf‚Äôs Go transformation library into a WebAssembly (WASM) module and invoke it from Pepr‚Äôs TypeScript code using Node.js‚Äô WASM support. This allows Pepr to reuse the exact same transformation logic as Zarf without maintaining duplicate implementations or introducing external services.

Key aspects:
- The Go transformation code will be refactored into a standalone package suitable for WASM compilation.
- The WASM binary will be bundled with the Pepr runtime or loaded at startup.
- A lightweight TypeScript wrapper will handle serialization (e.g., JSON input/output) and WASM instantiation.
- Communication between Pepr and the WASM module will occur via memory-safe interfaces using ArrayBuffer or JSON strings.

### Rationale
- **Consistency**: Ensures identical transformation behavior between Zarf and Pepr.
- **Maintainability**: Changes to transformation logic only need to be made once, in Go.
- **Performance**: WASM runs efficiently in Node.js with minimal overhead compared to network-based solutions.
- **Operational Simplicity**: No additional processes, ports, or inter-process communication required.
- **Future-Proofing**: WASM is a portable, language-neutral compilation target with growing ecosystem support.

### Implications
- **Build Complexity**: Requires integrating Go and WASM toolchains into the build pipeline.
- **Debugging**: Debugging across JS/Go via WASM may require specialized tooling or logging strategies.
- **Memory Management**: Care must be taken with data transfer between JS and WASM (e.g., proper allocation/deallocation patterns).
- **Cold Start Overhead**: Initial WASM instantiation adds minor latency, but this is acceptable in admission control context.

### References
- [Go WASM Documentation](https://github.com/golang/go/wiki/WebAssembly)
- [Node.js WASM Support](https://nodejs.org/api/wasi.html#webassembly)
- Zarf transformation library source: `pkg/transformers/`","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision: Standardize Package Source Handling Across All Zarf CLI Commands

### Status
Accepted

### Context
Zarf supports multiple package sources (local tarballs, split tarballs, OCI registries, in-cluster packages, and HTTPS URLs), but the availability of operations (`inspect`, `deploy`, `remove`, `create`) varies inconsistently across these sources. This leads to a fragmented user experience and complicates automation and tooling integration. For example:
- Split tarballs cannot be inspected or removed despite being deployable.
- In-cluster packages can be removed by name but not inspected.
- HTTPS-sourced packages support deployment but lack inspection and removal capabilities.

This inconsistency stems from command-specific logic rather than a unified package interface, resulting in gaps in functionality and maintainability issues.

### Decision
Implement a **unified package interface** within Zarf that abstracts all supported package sources under a common set of operations: `inspect`, `deploy`, `remove`, and `create` where applicable. Each source type will implement this interface fully, with appropriate error handling for semantically invalid actions (e.g., removing an OCI remote without push permissions).

Specifically:

1. **Split Tarball Packages**
   - ‚úÖ Support `zarf package inspect <path/to/part/dir>` ‚Äî detect part files and decode metadata.
   - ‚úÖ Support `zarf package remove <path/to/part/dir>` ‚Äî delete all `.part*` files safely.
   - Ensure `zarf package deploy` continues to work with split archives.

2. **In-Cluster (Deployed) Packages**
   - ‚úÖ Support `zarf package inspect <name>` ‚Äî retrieve deployed package metadata and component list from cluster annotations/configmaps.
   - ‚úÖ Retain `zarf package remove <name>` ‚Äî already implemented; enhance output consistency.
   - Add flag `--all-namespaces` if needed for multi-namespace deployments.

3. **HTTPS URL Packages**
   - ‚úÖ Support `zarf package inspect https://...` ‚Äî download only the manifest portion (via range requests if possible) to minimize bandwidth.
   - ‚úÖ Support `zarf package deploy https://...` ‚Äî retain existing behavior.
   - ‚ùå Do **not** support `zarf package remove https://...` ‚Äî removal is not meaningful for immutable remote URLs; instead return a clear error message explaining immutability.

4. **OCI Packages (`oci://`)**
   - ‚úÖ Support full `inspect`, `deploy`, and `remove` (unpublish) via registry deletion APIs *if* the user has write/delete permissions.
   - For `remove`, use `oras delete` or equivalent with proper auth handling.
   - Warn users about global implications of deleting OCI artifacts.

5. **Local Tarballs**
   - Already well-supported; ensure parity with new standard interface.

6. **General CLI Behavior**
   - Normalize input resolution: all commands accept any valid package source URI (e.g., `file://`, `https://`, `oci://`, local path).
   - Introduce consistent help text and examples across subcommands.
   - Return machine-readable errors using structured codes where applicable.

### Rationale
- **User Experience**: A consistent interface reduces cognitive load and enables predictable scripting.
- **Maintainability**: Centralizing package logic behind an abstraction simplifies future extensions (e.g., S3, Helm charts).
- **Principle of Least Surprise**: If a package can be deployed, it should generally be inspectable; if it's local or mutable, it should usually be removable.
- **Security & Safety**: Immutable sources (like HTTPS) do not allow removal‚Äîthis is made explicit rather than silently unsupported.

### Consequences
**Positive**
- Unified UX across all package types.
- Enables advanced workflows like CI/CD pipelines that validate, deploy, and clean up packages uniformly.
- Paves the way for future features such as garbage collection, replication, and auditing.

**Negative**
- Increased complexity in backend logic to handle partial downloads (e.g., inspecting large HTTPS packages efficiently).
- Potential performance cost when inspecting very large OCI or split packages.
- Requires careful permission handling for OCI deletes to prevent accidental removal.

**Neutral / Mitigated**
- Some operations (e.g., removing OCI artifacts) depend on external system capabilities. These are documented clearly at runtime.

### Implementation Plan
1. Define `PackageSource` interface with methods: `Inspect()`, `Deploy()`, `Remove()`, `Type()`.
2. Implement concrete types: `LocalTarball`, `SplitTarball`, `OCIPackage`, `HTTPSPackage`, `InClusterPackage`.
3. Refactor CLI commands to resolve input into a `PackageSource` and delegate operations.
4. Add integration tests covering all source-operation combinations.
5. Update documentation and CLI help text to reflect expanded capabilities.

---

By adopting this decision, Zarf ensures that every package source behaves predictably across all relevant operations, aligning functionality with user expectations and enabling robust automation.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","**Decision: Refactor Zarf's package composition system to implement a centralized, pipeline-based architecture for component processing with explicit support for import chains, OCI skeletons, and unified filtering.**

## Rationale

The current implementation of Zarf's composition system has evolved organically, leading to technical debt in three key areas: unscalable import chain handling, inconsistent OCI skeleton integration, and fragmented filtering logic. These issues compromise reliability, maintainability, and testability.

This decision proposes a fundamental architectural refactor that will:

1. **Implement a composition pipeline** - Create a single, linear processing flow where imported components pass through standardized stages: resolve ‚Üí merge ‚Üí filter ‚Üí validate. This ensures consistent handling regardless of import depth and enables comprehensive testing at each stage.

2. **Centralize component filtering** - Move all `only` filter logic into the pipeline's filter stage, eliminating duplication between create-time and deploy-time code paths. This guarantees consistent behavior and simplifies adding new filter types.

3. **Integrate OCI skeletons natively** - Treat OCI skeletons as first-class citizens in the composition process by resolving them during the initial ""resolve"" stage alongside traditional imports, rather than as a post-processing step.

4. **Add import graph validation** - Implement cycle detection and depth limiting in the resolve stage to prevent infinite loops and excessive nesting that could impact performance.

5. **Establish comprehensive test boundaries** - The pipeline architecture creates clear interfaces for unit testing individual stages and integration testing the complete flow with multi-layer import scenarios.

This approach trades short-term refactoring effort for long-term maintainability, enabling future composability features (conditional imports, override strategies, etc.) to build upon a robust foundation rather than perpetuating workarounds.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision: Introduce `zarf dev` Command for Iterative Package Development

To streamline the package development lifecycle and reduce iteration time, introduce a new `zarf dev` CLI command optimized for rapid development in connected environments. This command will enable fast feedback loops by skipping redundant operations while maintaining compatibility with Zarf's air-gapped deployment model.

### Decision

Add a new top-level `zarf dev [dir]` command that enables iterative development of Zarf packages without requiring full recreation or redeployment from scratch. The command will:

1. **Skip Air-Gap Constraints**: Operate under the assumption of a connected environment (like YOLO mode), pulling dependencies on-demand instead of bundling them.
2. **Reuse Existing Resources**: Detect and reuse already-initialized clusters (`zarf init` state) and avoid reinitializing unless explicitly requested.
3. **Incremental Rebuilds**: Only rebuild components whose `zarf.yaml` definitions or referenced assets have changed since the last run.
4. **Hot Reload Support (Future-Proofing)**: Design with extensibility to support file-watching and automatic redeployment in future iterations.
5. **Leverage YOLO Internals**: Reuse existing YOLO-mode logic for lightweight image handling and direct Kubernetes apply patterns where appropriate.

The workflow becomes:
```bash
# First time setup
zarf dev ./my-package

# Subsequent changes ‚Äì only modified parts are rebuilt/deployed
zarf dev ./my-package
```

Optionally support flags:
- `--reset`: Recreate the cluster and reinitialize (equivalent to current reset pattern)
- `--watch`: Enable filesystem watching for automatic redeploy on change (v2)
- `--no-lint`: Skip validation checks during rapid iteration

This command is intended solely for development; users must still use `zarf package create` and `zarf package deploy` for production or air-gapped deployments.

### Rationale

- **Developer Experience**: Reduces cycle time from minutes to seconds for small changes.
- **Consistency**: Maintains alignment with final deployment behavior by using the same `zarf.yaml` spec.
- **Safety**: Does not alter existing commands or compromise air-gapped guarantees.
- **Efficiency**: Eliminates redundant fetching, packaging, and initialization steps during development.

### Implementation Notes

- Detect changes via hashing of `zarf.yaml`, component files, and templates.
- Use temporary state tracking (e.g., `.zarf-dev-state.json`) to compare previous builds.
- Log clearly when resources are reused vs. rebuilt.
- Document `zarf dev` as a development-only tool, not suitable for CI/CD or secure environments.

### Status

Accepted

### Date

2025-04-05

### Author

Arch Team","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","**Decision:** Upgrade the documentation site from Docusaurus 2 to **Docusaurus 3** (or migrate to an alternative framework such as **VitePress** or **Next.js with MDX**) based on a formal evaluation of performance, build times, developer experience, and long-term maintainability.

### Rationale:
While Docusaurus 2 has provided a solid foundation for the documentation site, observed limitations‚Äîparticularly in build performance, hot-reload latency, and dependency bloat‚Äîhave begun to impact developer productivity and end-user experience. Docusaurus 3 introduces significant under-the-hood improvements using modern tooling (e.g., SWC for faster builds, improved caching), which directly address these concerns. Alternatively, frameworks like VitePress offer even better performance due to native Vite integration and instant HMR, while Next.js with MDX provides maximum flexibility for custom integrations.

A migration path will be chosen following a proof-of-concept comparison across key metrics:
- Cold and incremental build times
- Development server startup and reload speed
- Bundle size and Lighthouse performance scores
- Ease of content migration and plugin compatibility
- Team familiarity and long-term community support

Pending evaluation results, the preferred option will be formally adopted.

### Status: Proposed

### Context:
- Current stack: Docusaurus v2
- Pain points: Slow rebuilds, lengthy CI/CD cycles, suboptimal runtime performance on large pages
- Goals: Faster builds, improved developer experience, better SEO and accessibility, scalable architecture

### Consequences:
- **Positive:**
  - Improved development velocity with faster feedback loops.
  - Better runtime performance and user engagement.
  - Modern tooling alignment (e.g., Vite, SWC).
- **Negative:**
  - Upfront time investment for migration and testing.
  - Potential need to re-implement or replace custom plugins.
  - Temporary team context switching during transition.

### Next Steps:
1. Set up PoCs for Docusaurus 3, VitePress, and Next.js+MDX.
2. Benchmark each against current Docusaurus 2 setup.
3. Gather team feedback on developer experience.
4. Make final decision and plan phased migration.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision: Replace Zarf‚Äôs In-House Logging System with a Structured, Context-Aware, and Test-Friendly Logging Abstraction

### Status
Accepted

### Context
Zarf currently uses a custom logging solution built on top of `pterm`, which introduces several architectural and operational challenges:
- **Tight coupling to global state**: Components like progress bars and spinners rely on shared global variables, making them non-thread-safe and difficult to manage in concurrent contexts.
- **Poor testability**: The use of `message.Fatal` (which calls `os.Exit`) terminates the program abruptly, preventing proper cleanup via `defer`, breaking unit test control flow, and complicating integration testing.
- **Lack of context propagation**: Fatal or error-level logs bypass structured error handling, undermining Go's idiomatic error propagation and context cancellation patterns.
- **Inflexible output control**: Disabling certain UI elements (e.g., progress bars) is cumbersome due to entangled concerns between presentation logic and logging infrastructure.
- **Limited extensibility**: The current system lacks support for structured logging, log levels with dynamic filtering, or multiple output sinks beyond basic STDOUT/STDERR and debug files.

These issues hinder maintainability, reliability, and scalability‚Äîespecially as Zarf grows in complexity and concurrency requirements.

### Decision
Replace the current in-house logging system with a new abstraction layer that:
1. **Decouples logging from process termination** by eliminating direct calls to `os.Exit`. Instead, errors are returned and handled at appropriate layers, allowing controlled shutdowns and cleanup.
2. **Introduces a thread-safe, injectable logging interface** that supports structured fields, log levels (`Debug`, `Info`, `Warn`, `Error`, etc.), and pluggable backends (console, file, JSON).
3. **Wraps or replaces pterm components** (e.g., spinner, progress bar) with thread-safe implementations that can be disabled/enabled per context and do not depend on global state.
4. **Supports context-aware operations**, enabling cancellation and scoped configuration (e.g., quiet mode for background tasks).
5. **Provides a mockable interface for testing**, so log behavior can be inspected without side effects during unit tests.

We will define a minimal interface such as:

```go
type Logger interface {
    Debug(msg string, keysAndValues ...interface{})
    Info(msg string, keysAndValues ...interface{})
    Warn(msg string, keysAndValues ...interface{})
    Error(err error, msg string, keysAndValues ...interface{})
    Fatal(msg string, keysAndValues ...interface{}) // Logs then exits ‚Äî only used in main()
}
```

The `Fatal` method will be restricted to the `main` package only, ensuring it's not abused in libraries or tested code.

For progress indication, we‚Äôll introduce a separate `Renderer` or `UX` package that encapsulates visual components (spinner, progress bar) with lifecycle management and optional disablement via configuration.

Underlying implementation may still leverage `pterm` for formatting but will wrap it safely behind this abstraction.

### Consequences
**Positive:**
- ‚úÖ Improved testability: No more unexpected exits; errors can be asserted and propagated naturally.
- ‚úÖ Better concurrency support: Elimination of global mutable state enables safe use in goroutines.
- ‚úÖ Cleaner separation of concerns: Logging and rendering become configurable services rather than ambient authority.
- ‚úÖ Enhanced observability: Path opened toward structured logs (e.g., JSON) for better tooling integration.
- ‚úÖ Controlled shutdowns: Defer blocks and cleanup hooks execute reliably.

**Negative:**
- ‚ùå Migration effort required across codebase to replace existing `message.*` calls with interface-based logging.
- ‚ùå Slight increase in complexity due to dependency injection (logger must be passed or accessed via context).
- ‚ùå Potential performance overhead if not carefully implemented (mitigated via level gating and efficient formatting).

**Mitigations:**
- Introduce the new logger incrementally alongside the old one.
- Use build tags or feature flags to allow fallback during transition.
- Provide helper constructors to minimize boilerplate when creating loggers.

### Accepted Alternatives Considered
- **Adopt an existing logging framework directly (e.g., zap, zerolog, logr)**: Rejected due to excessive weight or mismatch with CLI UX needs (e.g., poor default terminal formatting). However, our new interface could later be backed by such libraries.
- **Keep current system with patches**: Would perpetuate technical debt and fail to resolve core issues around testability and concurrency.

### Next Steps
1. Define the `Logger` interface and create a simple implementation using enhanced `pterm` usage.
2. Build a `Renderer` component for spinners and progress bars with explicit start/stop and disable-on-demand semantics.
3. Refactor `message.Fatal` usages to return errors up the stack, introducing proper error handling where missing.
4. Inject logger instances into packages instead of using global functions.
5. Update tests to verify error paths without crashing.
6. Document guidelines for logging and termination in Zarf contributor docs.

This change lays the foundation for robust, observable, and maintainable CLI behavior while aligning with Go best practices.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","# Decision: Establish Zarf v1 Schema with Explicit Versioning and Deprecation Policy

## Status
Accepted

## Context
Zarf has evolved organically over time, introducing new configuration patterns while deprecating older ones (e.g., `setVariable` ‚Üí `setVariables`, `scripts` ‚Üí `actions`). Currently, deprecated fields remain fully functional with only warning messages during package creation. This approach maintains backward compatibility but leads to technical debt, increased maintenance burden, and confusion for users due to overlapping or ambiguous configuration options.

With the upcoming v1 release, there is a strategic opportunity to formalize Zarf's configuration contract with users by introducing an explicit schema versioning system. This will allow Zarf to provide long-term stability guarantees while establishing a clear lifecycle for features.

The absence of formal schema versions makes it difficult to remove outdated functionality, even when better alternatives exist. At the same time, air-gapped deployments require compatibility between packaging and deployment environments, which constrains migration strategies.

## Decision
1. **Introduce explicit schema versioning** starting with `schemaVersion: v1` in the zarf.yaml file.
2. **Freeze all v1 schema elements** ‚Äî once introduced, no fields will be removed or broken within the v1 series.
3. **Remove support for deprecated keys in v1**, including:
   - `setVariable`
   - `scripts`
   - `group`
   - `cosignKeyPath`
4. **Require `schemaVersion` field** in all zarf.yaml files; packages without it will default to a compatibility mode (`v0-latest`) with warnings, but this fallback will be removed in future major versions.
5. **Enforce schema validation against the declared version** at package create time.
6. **Document and publish the v1 schema** as a formal OpenAPI/Swagger specification for tooling and validation support.
7. **Establish a deprecation policy**: In future schema versions (e.g., v2), deprecated fields will be marked as such for one full major version cycle before removal, giving users 12‚Äì18 months to migrate.

## Rationale
- **Predictability**: A stable v1 schema provides users confidence that their configurations will continue working across patch and minor releases.
- **Reduced Technical Debt**: Removing obsolete fields simplifies codebase maintenance, testing, and documentation.
- **Clear Upgrade Path**: Explicit versioning enables better tooling support (IDE autocomplete, linters) and clearer migration guides.
- **Air-Gap Safety**: Since v1 enforces consistency between packaging and deployment environments, automatic migrations are no longer needed or safe ‚Äî removing deprecated fields eliminates ambiguity.
- **Professional Maturity**: Semantic schema versioning aligns Zarf with industry best practices seen in tools like Kubernetes (apiVersion), Terraform, and Helm.

## Implications
- **Breaking Change**: Existing packages using deprecated keys will fail validation unless updated prior to package creation.
- **Migration Required**: Users must update their zarf.yaml files before adopting v1.
- **Tooling Updates**: CI/CD pipelines, editors, and validators should be updated to recognize the new schema version.
- **Documentation Burden**: Comprehensive migration documentation must be provided alongside the v1 release.

## Future Considerations
- Support schema auto-upgrade tools in future CLI versions to assist with version migrations.
- Explore embedding schema definitions directly into packages for runtime validation integrity.
- Monitor usage patterns to inform timing and design of v2 schema evolution.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
