context,decision,predicted_decision
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","# ADR: Migrate Primer React Components to TypeScript

## Decision

We will incrementally migrate the Primer React codebase from JavaScript to TypeScript. This migration will phase out the current ambient type declaration file (`index.d.ts`) in favor of co-located, source-level type definitions that are intrinsically tied to implementation.

The migration will follow a structured, module-by-module approach:
1. Enable TypeScript compilation in the build tooling with `allowJs: true` and `checkJs: false`.
2. Gradually convert individual components and utilities from `.js` to `.ts`/`.tsx`, ensuring comprehensive type coverage.
3. Remove the standalone `index.d.ts` file once all public API surface area is natively typed.
4. Enforce type correctness via CI/CD pipelines using `tsc --noEmit` checks on pull requests.

## Status

Accepted

## Context

Primer React was initially developed without native TypeScript support. To enable consumption in TypeScript projects, an ambient declaration file (`index.d.ts`) was introduced in July 2019 as a pragmatic workaround. While this allowed early adoption in typed environments, it created technical debt due to the decoupling of types from implementation.

Over time, this separation has led to:
- Type inaccuracies and drift as APIs evolved.
- Increased maintenance burden requiring manual updates across two sources of truth.
- Poor developer experience with incorrect or missing types causing runtime errors.
- Growing number of reported TypeScript-related issues (e.g., #906, #540, #520, #534).

As the library scales, maintaining correct types through ambient declarations is no longer sustainable. The disconnect impedes developer productivity, increases bug risk, and complicates contributions.

Alternative options were considered:
- **Keep ambient types**: Would perpetuate existing problems and scale poorly.
- **Rewrite entirely upfront**: High-risk, resource-intensive, and impractical given active development.
- **Adopt JSDoc-based typing**: Offers partial improvement but lacks full expressiveness and tooling integration of real TypeScript.

An incremental migration balances safety, maintainability, and velocity.

## Consequences

### Positive
- ‚úÖ Types stay in sync with implementation, reducing bugs and improving accuracy.
- ‚úÖ Enhanced IDE support (autocomplete, refactoring, inline documentation).
- ‚úÖ Improved contributor experience with immediate type feedback.
- ‚úÖ Better long-term maintainability and scalability of the codebase.
- ‚úÖ Enables richer type features (generics, utility types, etc.) not possible in ambient declarations.

### Negative
- ‚ö†Ô∏è Initial overhead during transition period with mixed JS/TS files.
- ‚ö†Ô∏è Requires developer familiarity with TypeScript; may necessitate team training.
- ‚ö†Ô∏è Potential for friction in CI/CD pipeline changes and tooling configuration.

### Neutral
- üîÑ Build process will incorporate TypeScript compilation, though output remains compatible with JavaScript consumers.
- üîÑ Public API will remain stable throughout migration; no breaking changes intended.

## References

- [Original index.d.ts commit](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa)
- Issue #906: Incorrect prop types in Button component  
- Issue #540: Missing generics in polymorphic components
- Issue #520: Union type mismatches in Box props
- Issue #534: Optional props incorrectly marked as required","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

Throughout the last few years folks from the Design Infrastructure, Web Systems, and UI Platform teams have discussed the idea of using custom elements for behaviors in Primer React. The main goal of using custom elements in Primer React is to be able to author behaviors once and reuse them in any framework. Several experiments have been conducted which are listed above.","**Decision**: Adopt Custom Elements as the standard for authoring cross-framework reusable behaviors in Primer React.

---

**Context**:  
Over the past few years, teams including Design Infrastructure, Web Systems, and UI Platform have collaboratively explored methods to improve reusability and interoperability of UI behaviors across different frontend frameworks. Multiple experiments have demonstrated that custom elements‚Äîbeing part of the Web Components standard‚Äîare well-suited for encapsulating reusable, framework-agnostic behaviors. These native browser APIs allow behavior logic (e.g., keyboard interactions, focus management, open/close states) to be defined once and used seamlessly not only in React but also in vanilla JS, Vue, Angular, or server-rendered contexts.

**Decision**:  
We will standardize on using **Custom Elements** to implement shared interactive behaviors within Primer React components where appropriate. These custom elements will encapsulate low-level behavioral logic (such as dropdown toggling, modal dialogs, or tabbed navigation state) and be designed to interoperate with React via refs and event listeners, without requiring tight coupling to React‚Äôs rendering model.

This approach enables:
- **Cross-framework reuse**: Behavior implementations can be consumed outside React.
- **Progressive enhancement**: Components remain functional even if JavaScript frameworks fail to load.
- **Encapsulation**: Shadow DOM (when used) ensures style and script isolation.
- **Future-proofing**: Leverages platform-standard APIs with broad browser support.

We will continue to export React components as the primary interface for Primer React consumers, but those components may internally leverage custom elements for behavioral concerns when it benefits maintainability and consistency.

**Consequences**:
- ‚úÖ **Pros**:
  - Single source of truth for complex behaviors.
  - Improved alignment with GitHub‚Äôs broader web platform strategy.
  - Easier integration into non-React parts of GitHub.com.
- ‚ùå **Cons**:
  - Slight increase in complexity when bridging React state and custom element events.
  - Requires discipline in API design to avoid framework-specific assumptions.
  - Learning curve for team members unfamiliar with modern Web Components.

**Next Steps**:
1. Define a lightweight authoring pattern/library for creating behavior-focused custom elements.
2. Document interoperability patterns between React and custom elements (e.g., ref attachment, event handling).
3. Begin migration by extracting shared behaviors (e.g., details-menu, popover) into custom elements.
4. Establish testing standards for cross-framework compatibility.

This decision aligns with our long-term vision of a unified, performant, and interoperable component ecosystem at GitHub.",## Decision
"## Context

Our component prop APIs have, at times been a bit of a mess. We've seen:

- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)
- Explicit plans to change some of those (e.g., the deprecation of Styled System props)
- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)

This ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.","## Decision: Standardize and Document Component Prop APIs with a Tiered, Future-Proof Approach

### Summary
We will define a consistent, well-documented, and maintainable prop API contract across all components in our design system. This includes standardizing commonly used props (e.g., `sx`, `as`, `ref`), deprecating legacy patterns (e.g., Styled System utilities), and establishing clear conventions for forward compatibility.

---

### Key Decisions

#### 1. **Adopt a Unified Prop Contract**
All components must conform to the following standardized prop interface:

| Prop | Type | Required? | Description |
|------|------|-----------|-------------|
| `as` | `React.ElementType` | No | Allows changing the underlying DOM element or component (polymorphic). |
| `sx` | `SxProps` (from theme) | No | Style object using our theme-aware utility (e.g., emotion‚Äôs `sx` or similar). |
| `className` | `string` | No | For external styling overrides; not for internal use. |
| `ref` | `React.Ref` | No | Forwarded appropriately (`React.forwardRef`) for all applicable components. |

> ‚úÖ **Enforcement**: Use TypeScript interfaces or base types (e.g., `ComponentProps`) to codify this across the codebase.

#### 2. **Deprecate Legacy Styled System Props**
We formally **deprecate** direct usage of Styled System-style props (e.g., `color`, `fontSize`, `p`, `m`) at the component level unless explicitly wrapped via a higher-order utility.

- ‚úÖ Replace with `sx` for one-off styling.
- üõ†Ô∏è Provide codemods and lint rules to detect and migrate existing uses.
- üìö Update documentation to discourage direct style props.

> ‚ö†Ô∏è Rationale: Avoids prop pollution, improves tree-shaking, and centralizes styling logic.

#### 3. **Standardize Ref Handling**
All visual components that render DOM elements must:
- Use `React.forwardRef`.
- Accept a `ref` prop typed as appropriate (`HTMLDivElement`, etc.).
- Be documented as ‚Äúcan be ref‚Äôd‚Äù in Storybook/docs.

> üîß Enforcement via ESLint rule: `consistent-ref-forwarding`.

#### 4. **Establish a Public vs. Internal Prop Convention**
- **Public API**: Only documented props are considered public.
- **Internal/Unstable**: Prefixed props (e.g., `__internalVariant`) may exist but must be marked unstable and excluded from auto-generated docs.

> üìù Documentation: Use JSDoc + TSDoc to annotate stability and intent.

#### 5. **Create a Base Component Interface**
Define a shared type (e.g., `BaseUIProps`) to enforce consistency:

```ts
interface BaseUIProps {
  sx?: SxProps;
  as?: React.ElementType;
  className?: string;
  children?: React.ReactNode;
}
```

Components extend this as needed.

---

### Migration Plan (Sequenced)

| Phase | Goal | Timeline |
|-------|------|----------|
| 1 | Audit all components for compliance; document deviations | Q1 |
| 2 | Release v0.x with deprecation warnings for Styled System props | Q2 |
| 3 | Introduce `BaseUIProps` and refactor core components | Q2‚ÄìQ3 |
| 4 | Ship codemods for automated migration | Q3 |
| 5 | Remove deprecated props in next major version | Next breaking release |

---

### Tooling & Enforcement

- **TypeScript**: Leverage strict typing and shared interfaces.
- **ESLint**: Custom rules to flag invalid prop patterns.
- **Prettier + lint-staged**: Enforce formatting on commit.
- **Storybook Docs**: Auto-generate prop tables from TS definitions.
- **Deprecation Warnings**: Runtime warnings for deprecated props in development.

---

### Documentation

- Publish a **Prop API Guidelines** page in our Design System documentation.
- Include examples of:
  - Correct `sx` usage
  - Polymorphism with `as`
  - Ref forwarding pattern
  - What not to do (anti-patterns)

---

### Status
Accepted

### Owner
Design Systems Team

### Related ADRs
- [ADR-004: Theming Strategy](#)
- [ADR-012: Component Composition Model](#)",## Decision
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example √É¬Ç√Ç¬°√É¬É√Ç¬∫](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example √É¬Ç√Ç¬°√É¬É√Ç¬∫](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision: Standardize on Box-based Component Composition in Primer React

### Status
**Accepted**

### Context
Primer React currently supports multiple patterns for building components, primarily:
- `styled-components` with attribute manipulation and template literals
- Composition via the `Box` primitive with the `sx` prop

This divergence leads to:
- Inconsistent code styles across components
- Higher cognitive load for contributors
- Duplicated effort in managing styling concerns
- Challenges in maintaining design system constraints (e.g., theme adherence, accessibility)

The two approaches have trade-offs:
- **Styled-components**: Offers fine-grained control and performance benefits through static extraction, but encourages CSS-in-JS logic that can become difficult to maintain and debug. It also creates a steeper learning curve and tighter coupling between styles and implementation.
- **Box-based**: Promotes composition, reusability, and alignment with Primer‚Äôs design tokens. Encourages use of the `sx` prop which is type-safe, theme-aware, and supports responsive props. Results in more predictable output and better runtime flexibility.

### Decision
We will standardize on the **Box-based component pattern** as the default approach for building new components and refactoring existing ones in Primer React.

All new components should:
- Use `Box` (or other low-level primitives) as the foundation
- Accept an `sx` prop to allow style overrides
- Use `merge` from `@primer/react` to combine default and custom styles
- Leverage `BetterSystemStyleObject` typing for improved type safety
- Prefer composition over inheritance or styled-component extensions

Existing styled-components will be incrementally migrated during maintenance cycles or major version updates.

### Rationale
- ‚úÖ **Consistency**: One idiomatic way to build components reduces confusion
- ‚úÖ **Theme Integration**: `sx` ensures tighter coupling with Primer‚Äôs design tokens
- ‚úÖ **Developer Experience**: Simpler mental model‚Äîespecially for newer contributors
- ‚úÖ **Flexibility**: Runtime composition allows greater adaptability in consuming apps
- ‚úÖ **Type Safety**: Strong typing via `BetterSystemStyleObject` and TSX
- ‚úÖ **Alignment with Future Direction**: Supports eventual migration toward utility-first or constraint-based systems

### Exceptions
Use of `styled-components` may still be acceptable for:
- Performance-critical components where static extraction provides measurable benefit
- Complex visual states requiring dynamic CSS rules not expressible via `sx`
- Third-party integrations where Box abstraction would add unnecessary overhead

These cases require explicit justification and approval from the Primer team.

### Consequences
**Positive**
- Unified codebase improves readability and maintainability
- Easier onboarding for new developers
- Better enforcement of design system rules
- Reduced bundle size over time by minimizing redundant styled-components

**Negative**
- Short-term cost of migrating legacy components
- Slight runtime overhead compared to static styled-components
- Some advanced CSS features (e.g., keyframe animations) require workarounds

### References
- [Primer React Pull Request #2019](https://github.com/primer/react/pull/2019)
- [`sx` prop documentation](https://primer.style/react/sx)
- GitHub‚Äôs evolving UI architecture principles","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
,"Please provide the context for which you need an Architectural Decision Record (ADR). This should include details such as:

- The problem or challenge being addressed
- Relevant system requirements or constraints
- Stakeholder concerns or goals
- Any existing architecture or technology considerations

Once I have this information, I will create a structured ADR with a clear decision, rationale, and implications.","## Decision:

1. Prefer using children for √É¬Ç√Ç¬°√É¬Ç√Ç¬∞content√É¬Ç√Ç¬°√É¬Ç√Ç¬±

2. For composite components, the API should be decided by how much customisation is available for children.

For components that have design decisions baked in, should use strict props. For example, the color of the icon inside a Button component is decided by the `variant` prop on the Button. The API does not allow for changing that.

```jsx
<Button variant=""danger"" leadingIcon={TrashIcon}>
  Delete branch
</Button>
```

On the other hand, if we want consumers to have more control over children, a composite API is the better choice.

```jsx
<ActionList.Item>
  <ActionList.LeadingVisual>
    <Avatar src=""https://github.com/mona.png"" />
  <ActionList.LeadingVisual>
  mona
</ActionList.Item>
```"
"## Context

As we work on maturity of our components, we will sometimes need to build components in a parallel track/bundle without breaking existing components. Eventually, the new components would replace the old ones in the main bundle.","**Decision: Adopt a Parallel Development and Gradual Replacement Strategy Using Feature Flags and Component Versioning**

To support the evolution of our components without disrupting existing functionality, we will implement a parallel development approach where new versions of components are developed alongside the current stable ones. This enables safe experimentation, incremental improvement, and backward compatibility during transition.

### Rationale
- **Minimize Risk**: Running new and old components in parallel reduces the risk of regressions in production.
- **Incremental Adoption**: Teams and users can opt into new components gradually via feature flags or configuration.
- **Backward Compatibility**: Existing integrations continue to work while migration paths are developed.
- **Testing in Production**: Feature flags allow controlled rollouts (canary, A/B testing) with real-world usage data.
- **Clear Upgrade Path**: Once mature, new components replace old ones in the main bundle during a scheduled major release.

### Implementation Approach
1. **Component Versioning**  
   Maintain versioned component names or namespaces (e.g., `ButtonV2`, `NewDataTable`) to coexist with legacy versions.

2. **Feature Flags**  
   Wrap usage of new components with feature flags (e.g., `enableNewDataGrid: boolean`) to control activation at runtime.

3. **Bundle Strategy**  
   Include both old and new components in the main bundle during transition, but mark deprecated components with warnings in logs/docs.

4. **Deprecation Plan**  
   - Mark old components as `@deprecated` with removal timelines in release notes.
   - Provide automated codemods or migration guides for consumers.
   - Remove deprecated components in the next major version after sufficient adoption.

5. **Documentation & Communication**  
   Clearly document which components are in active development, experimental, stable, or deprecated.

### Outcome
This decision enables sustainable component maturity by decoupling development velocity from deployment stability. It supports long-term architectural hygiene while maintaining system reliability.",
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision

We will continue to use the `drafts` directory as the designated space for:

1. **In-progress components** ‚Äî Components that are under active development, not yet stable, and not recommended for production use without explicit coordination with the Primer team.
2. **Parallel component versions** ‚Äî Experimental or next iterations of existing components (e.g., `UnderlineNav@v2`) that need to coexist with the current stable version in the main export.

Access to these components remains scoped under `@primer/react/drafts` to:
- Signal instability and discourage unapproved production usage.
- Enable safe iteration without impacting users of the stable API.
- Facilitate internal testing, design collaboration, and gradual adoption.

Components will only be promoted from `drafts` to the main package exports once they meet our quality, accessibility, documentation, and API stability standards.

We acknowledge that `/drafts` is a temporary mechanism and not a long-term solution for versioning or component governance. Future improvements may include better versioning strategies, feature flags, or module federation patterns ‚Äî but those are out of scope for this decision.

This approach aligns with our goals of maintaining stability while enabling innovation.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision: Limit Snapshot Testing to Shallow Component Output or Implementation-Free Structures; Prefer Explicit Unit Tests for Behavior and Intent

### Status
Accepted

### Context
As the project scales, the number of snapshot tests has increased, particularly around React components. While snapshots offer a quick way to detect unintended changes in output (e.g., rendered markup), they come with significant drawbacks:

- **Large and unwieldy snapshots**: Deeply nested or complex components generate verbose snapshot files that are hard to read, review, and maintain.
- **Brittle by default**: Minor, non-breaking changes (e.g., class name reordering, test-only prop functions like `onMouseEnter={[Function]}`) cause failures without indicating whether the change is meaningful.
- **Obscured intent**: A failing snapshot does not clearly communicate *what behavior* is being tested‚Äîonly *that something changed*. This makes it difficult for developers to assess whether a change is acceptable or a regression.
- **Poor debugging experience**: When a snapshot fails, understanding the significance of the diff often requires cross-referencing implementation, design specs, or other context.

Additionally, storing large amounts of serialized component trees bloats the repository and complicates pull request reviews, especially when multiple snapshots change at once due to shared component updates.

### Decision
Adopt the following guidelines for snapshot testing:

1. **Avoid deep rendering snapshots of full component trees** unless absolutely necessary (e.g., end-to-end layout integration tests in rare cases).
2. **Use snapshots only for stable, serializable, and implementation-agnostic values**, such as:
   - Configuration objects
   - Serializers/deserializers output
   - API response shapes (when strongly typed)
   - Router definitions, static data structures
3. **For UI components, prefer shallow rendering with explicit assertions** using tools like `@testing-library/react` and `jest-dom` matchers:
   ```tsx
   it('renders a link with correct href and text', () => {
     render(<Link page=""http://facebook.com"">Facebook</Link>);
     expect(screen.getByRole('link')).toHaveAttribute('href', 'http://facebook.com');
     expect(screen.getByText('Facebook')).toBeInTheDocument();
   });
   ```
4. **If snapshots are used on components**, limit them to **shallow-rendered output** (via `shallow` from Enzyme or similar) or **exclude dynamic/runtime-only properties** (like functions, symbols, or generated IDs):
   ```js
   expect(tree).toMatchInlineSnapshot({
     props: expect.any(Object) // ignore function props
   });
   ```
5. **Document intent explicitly** in test descriptions. Every test should answer: *What behavior am I protecting?*

6. **Treat widespread snapshot updates with caution**‚Äîautomatically accepting all changes (`-u`) risks masking regressions. Require human review for any snapshot batch update.

### Consequences
**Positive:**
- Tests become more maintainable and focused on actual behavior.
- Easier code reviews: test changes reflect intentional logic updates, not incidental rendering differences.
- Reduced brittleness from irrelevant changes (e.g., event handler identity).
- Encourages writing smaller, composable components with clear interfaces.

**Negative:**
- Slightly more effort to write descriptive unit tests vs. one-liner snapshots.
- Some loss of ""automatic"" regression detection for visual/output structure‚Äîrequires discipline to cover key aspects explicitly.
- Migration cost for existing large snapshot files.

### Alternatives Considered
- **Keep using snapshots broadly**: Leads to technical debt, poor test quality, and eventual erosion of trust in the test suite.
- **Use visual regression testing (e.g., Percy)**: Better suited for visual/UI validation but heavier, slower, and not a replacement for logic/unit coverage.
- **Use inline snapshots only**: Helps visibility but doesn't solve the fundamental issue of unclear intent or brittleness.

### Additional Notes
This decision aligns with guidance from the React Testing Library philosophy: [""The more your tests resemble the way your software is used, the more confidence they can give you.""](https://github.com/testing-library/react-testing-library#guiding-principles)

Snapshots are not banned‚Äîbut treated as a tool with specific, narrow use cases rather than a default testing strategy.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision: Use Conditional Development-Only Warnings with Dead Code Elimination

To address the need for developer warnings during development while ensuring zero runtime cost in production, we will adopt a pattern that leverages **conditional `console.warn()` calls behind `process.env.NODE_ENV !== 'production'` checks**, combined with **tree-shaking and dead code elimination** via modern build tools.

### Chosen Approach

Wrap all developer-focused warnings and runtime assertions in conditional blocks that are stripped out during production builds:

```tsx
if (process.env.NODE_ENV !== 'production') {
  console.warn('[@primer/react] The `legacyProp` prop is deprecated and will be removed in v2.0.')
}
```

Additionally, create a shared utility to standardize warning messages:

```ts
// utils/warning.ts
export function warning(condition: boolean, message: string): void {
  if (!condition && process.env.NODE_ENV !== 'production') {
    console.warn(`[@primer/react] ${message}`)
  }
}

// Usage
warning(
  typeof variant !== 'undefined',
  'The `variant` prop is required for Button components in v1.5+'
)
```

### Rationale

1. **Production Code Purity**: Modern bundlers like Webpack, Vite, or Rollup automatically replace `process.env.NODE_ENV` with a literal string during build time. When set to `'production'`, the condition `process.env.NODE_ENV !== 'production'` becomes `false`, allowing minifiers (e.g., Terser) to eliminate the entire block as unreachable code.

2. **Development Feedback**: Developers receive immediate, contextual feedback in the console during development when using deprecated APIs, incorrect props, or unsupported patterns.

3. **Standardization**: A shared `warning()` utility ensures consistent formatting, branding (`[@primer/react]` prefix), and future extensibility (e.g., deduplication, integration with docs links).

4. **Zero Runtime Overhead in Production**: After minification, these checks are completely removed ‚Äî no function calls, no strings, no performance impact.

### Implementation Notes

- Ensure all project build configurations define `process.env.NODE_ENV` appropriately for each environment.
- Use [Terser's `drop_console`](https://terser.org/docs/options/#compress-options) only as an additional safeguard, not as the primary mechanism, since it removes *all* console statements indiscriminately.
- Consider pairing warnings with actionable guidance (e.g., link to migration guide).
- For TypeScript, ensure type guards used in dev-only checks do not affect runtime type behavior.

### Example

```tsx
function MyComponent({ unstable_feature }) {
  if (process.env.NODE_ENV !== 'production') {
    if (unstable_feature) {
      console.warn('[@primer/react] `unstable_feature` is experimental and may change without notice.')
    }
  }

  return <div>...</div>
}
```

After production build + minification:
```js
function MyComponent({ unstable_feature }) {
  return React.createElement(""div"", null);
}
```

### Alternatives Considered

| Option | Why Not Chosen |
|------|----------------|
| Always-on `console.warn()` | Pollutes production logs; violates goal of clean prod bundles |
| Custom logger with runtime flag | Still includes code and branching logic in production |
| ESLint rules only | Catches issues statically but misses dynamic or runtime deprecations |
| `__DEV__` global constant | Achieves same result but less standard than `NODE_ENV` |

---

‚úÖ **Status:** Accepted  
üìÖ **Date:** 2025-04-05  
üè∑Ô∏è **Tags:** performance, developer-experience, tree-shaking, deprecation","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision: Integrate Playwright for End-to-End and Interactive Component Testing

### Status
Accepted

### Context
Our current testing setup relies on:
- **Jest** for unit and integration tests (logic-focused, no DOM interaction).
- **Chromatic** for visual regression testing (snapshot-based UI checks).
- **Storybook** for component documentation and manual interactive testing.

However, we face two key challenges:
1. **Duplication of effort**: Engineers manually replicate complex component states in Storybook stories and Jest test setups, leading to maintenance overhead and inconsistency.
2. **Poor debuggability**: Jest tests lack visual feedback, making it difficult to understand failures‚Äîespecially for UI-heavy or interactive components‚Äîand reducing accessibility for less technical contributors.

Additionally, our ""unit"" tests often cross into integration territory (e.g., testing user flows), but without real browser interaction, they remain limited in realism and observability.

### Decision
Adopt **Playwright** as the primary tool for testing complex interactive scenarios and component behaviors, replacing or reducing reliance on complex Jest DOM tests for these cases.

We will:
- Use **Playwright** for:
  - End-to-end (E2E) testing of user flows.
  - Interactive component testing (replacing complex Jest DOM interaction tests).
  - Automated interaction within Storybook-like scenarios using `@storybook/playwright`.
- Keep **Jest** for:
  - Pure unit testing (functions, utilities, reducers, etc.).
  - Fast, isolated logic validation.
- Keep **Chromatic** for:
  - Visual regression testing (in combination with Playwright-driven states).
- Enhance **Storybook** by:
  - Using stories primarily as renderable test cases for Playwright.
  - Adding `play` functions to automate interactions directly in stories.
  - Reducing manual testing by treating stories as executable specs.

### Rationale
- **Eliminates duplication**: Instead of recreating scenarios in Jest, Playwright can interact directly with Storybook stories, using them as testbeds.
- **Improves debugging**: Playwright provides:
  - Browser UI during test execution (dev mode).
  - Video recordings and screenshots on failure.
  - Step-by-step trace viewer.
- **Realistic testing**: Tests run in actual browsers, simulating real user behavior (clicks, input, navigation).
- **Better collaboration**: Designers and QA can view videos/traces to understand issues without CLI expertise.
- **Future-proofing**: Playwright supports mobile emulation, API testing, and is actively maintained by Microsoft.

### Consequences
**Positive**
- Reduced code duplication between stories and tests.
- Higher confidence in UI behavior through real interaction.
- Easier onboarding and debugging due to visual feedback.
- Stories become living, executable documentation.

**Negative**
- Learning curve for team members unfamiliar with Playwright.
- Increased test runtime compared to Jest (but more valuable coverage).
- Additional setup and maintenance of Playwright configuration and CI runners.

**Mitigations**
- Provide internal workshops and templates.
- Run Playwright tests in parallel and selectively in CI (e.g., on PRs or nightly).
- Use `test.describe.only` and tagging to manage scope during development.

### Implementation Steps
1. Install Playwright: `npx playwright install --with-deps`
2. Set up config (`playwright.config.ts`) targeting localhost and Storybook URL.
3. Write first test using a Storybook story URL (e.g., `/story/button--primary`).
4. Migrate high-complexity Jest DOM interaction tests to Playwright.
5. Add `play` functions to key Storybook stories to automate interactions.
6. Integrate Playwright snapshots with Chromatic where appropriate.
7. Update contribution docs and onboarding materials.

### Approval
Architect: @lead-architect  
Engineering Manager: @eng-mgr  
Date: 2025-04-05","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

While [ADR 002](https://github.com/primer/react/blob/main/contributor-docs/adrs/adr-002-behavior-isolation.md) landed on the decision to:

- Share JavaScript behaviour as ""vanilla"" functions which can be used between Primer View Components (PVC) and Primer React (PRC), consumed via React Hooks in PRC.
- Not use Custom Elements to drive behaviour of components.

Our perspective on the ADR has changed since its approval and this document will address those changes.","**Decision: Re-evaluate and partially revise ADR 002 to allow for Custom Elements in specific cross-framework interoperability scenarios, while maintaining vanilla JavaScript functions as the primary mechanism for shared behavior.**

---

While ADR 002 established a clear direction to share JavaScript behavior via vanilla functions and avoid Custom Elements for driving component logic, practical experience and evolving ecosystem requirements have revealed limitations in that approach‚Äîparticularly around seamless integration across disparate frameworks (e.g., React, Vue, plain HTML/CSS) and improved encapsulation of complex interactive behaviors.

Since the original decision, we‚Äôve observed:

- Increasing demand for truly framework-agnostic components that work natively in multiple contexts without wrapper logic.
- Improved maturity and performance of Custom Elements (especially with regard to hydration, accessibility, and tooling support).
- Use cases where DOM-driven behaviors (e.g., progressive enhancement, server-rendered islands of interactivity) benefit from declarative registration via Custom Elements.
- Growing alignment with platform-native patterns, reducing the burden on consumers to manually initialize components.

Therefore, we **revise** ADR 002 as follows:

1. ‚úÖ **Retain vanilla JavaScript functions as the default method** for sharing behavior between Primer View Components (PVC) and Primer React (PRC), especially when used within React contexts via Hooks.
2. ‚ûï **Allow Custom Elements for specific use cases** where:
   - Cross-framework or no-framework usage is a primary requirement.
   - Automatic DOM upgrade and lifecycle management provide significant ergonomics benefits.
   - The component encapsulates self-contained, stateful behavior that aligns well with web standards (e.g., `<tab-panel>`, `<details-dialog>`).
3. üõë **Do not replace existing PRC components with Custom Elements** unless there is a compelling interop need.
4. üîß **Ensure all Custom Elements are built using standard-compliant, lightweight wrappers** around shared vanilla behaviors‚Äîavoid duplicating logic.
5. üìö **Document clear guidelines** for when to use each pattern and how they interoperate.

This hybrid strategy enables us to maintain consistency within React applications while expanding Primer‚Äôs reach and usability beyond them, aligning with long-term goals of ecosystem resilience and web platform alignment.",## Decision
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","# Architectural Decision Record: Adopt a Monorepo Tool for Workspace Management and Dependency Orchestration

## Status
**Proposed**

## Context
The `primer/react` repository currently consists of multiple isolated workspaces with no centralized dependency management or standardized way to handle inter-package dependencies. Contributors must manually run `npm install` in each workspace or rely on the custom `setup` script. Inter-dependencies between workspaces (e.g., `docs` depending on `@primer/react`) are managed inconsistently using relative paths, file dependencies, or manual `npm link` commands‚Äîleading to unreliable development environments and potential version drift.

Additionally, utility scripts under the `script/` directory have grown in complexity but lack their own dependency isolation or executable context, often relying on top-level installed packages or global assumptions about the environment.

These issues result in:
- Non-deterministic setup experiences
- Brittle cross-workspace imports
- Difficulty sharing code or configs across packages
- Poor discoverability and maintainability of project scripts
- Challenges in running, testing, or versioning shared tooling

## Decision
Adopt **[pnpm](https://pnpm.io/)** as the package manager and introduce a **monorepo configuration** using pnpm workspaces to unify dependency management, enable reliable inter-package linking, and streamline contributor workflows.

Specifically:
1. Introduce a `pnpm-workspace.yaml` at the root to define all workspaces (`packages/*`, `examples/*`, `docs`, etc.).
2. Migrate from npm to pnpm by replacing `package-lock.json` with `pnpm-lock.yaml` and updating CI/config files accordingly.
3. Use pnpm‚Äôs **workspace protocol** (`""dependencies"": { ""@primer/react"": ""workspace:*"" }`) to declare inter-package dependencies instead of relative paths or `file:` references.
4. Move complex scripts from `script/` into a dedicated workspace such as `tools/scripts` or `packages/dev-utils`, allowing them to declare their own dependencies and be executed via `pnpm`.
5. Replace the `npm run setup` script with a single `pnpm install` at the root that installs everything holistically.
6. Leverage pnpm‚Äôs **`public-hoist-pattern`** and selective dependency hoisting to ensure CLI tools (like ESLint, TypeScript) are available where needed without duplication.

## Rationale
### Why pnpm?
- **Efficient & deterministic**: Uses a content-addressable store and hard links, reducing disk usage and installation time.
- **Native monorepo support**: First-class workspace support with fast linking of local packages.
- **Strictness prevents phantom deps**: Unlike npm/yarn, pnpm doesn‚Äôt allow accidental use of undeclared dependencies due to its strict node_modules structure.
- **Backwards compatible**: Can coexist with existing npm patterns and integrates well with most tools.
- **Light migration cost**: Conversion from npm is straightforward; only requires lockfile change and minor CI updates.

### Why not alternatives?
- **Yarn (Berry)**: Powerful but introduces plugin system and `.yarnrc.yml` complexities; migration overhead higher than pnpm.
- **Lerna + npm**: Lerna does not manage dependencies natively and works best when paired with a workspace-aware package manager. Adds abstraction without solving core orchestration problems.
- **Turborepo**: Excellent for build orchestration but works best *on top of* a solid package manager like pnpm. Not a replacement for dependency management.

### Benefits of this approach
- **Faster, simpler setup**: One command (`pnpm install`) replaces multi-step installation process.
- **Consistent, reproducible builds**: All developers and CI systems use the same dependency graph.
- **Reliable local publishing**: Workspace packages are linked automatically using semantic versions or `workspace:*`.
- **Scalable script management**: Scripts become proper packages with dependencies, bin executables, and testability.
- **Improved developer experience**: Auto-completion, type checking, and linting work consistently across workspaces.

## Consequences
### Positive
- Unified, fast, and reliable installation process.
- Clear visibility into all workspaces via `pnpm-workspace.yaml`.
- Easier contribution onboarding.
- Better performance and reduced node_modules size.
- Foundation for future optimizations (e.g., caching with Turborepo, incremental builds).

### Negative
- Requires contributors to install `pnpm` globally (`npm i -g pnpm`), though this can be mitigated with `npx pnpm` or Corepack.
- Initial migration effort: converting lockfile, updating CI, adjusting tooling assumptions.
- Slight learning curve for team members unfamiliar with pnpm.

### Mitigations
- Add `corepack enable` and `corepack prepare pnpm@latest --activate` to ease tooling consistency.
- Document the switch clearly in CONTRIBUTING.md.
- Update GitHub Actions and other CI pipelines to use pnpm.
- Provide a migration checklist and validate all scripts post-migration.

## Additional Notes
Once pnpm is adopted, we unlock future capabilities:
- Use of **Turbo** for task orchestration (e.g., `pnpm run build --filter=@primer/react...`)
- Selective re-running of tests/builds based on changes.
- Publishing automation with `changesets`.

## Approved By
*(To be filled)*

## Date
2025-04-05","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","**Decision: Restructure the `src` directory to separate internal and public modules, and update package exports to explicitly control public API surface**

---

### Status

Accepted

---

### Context

The current export pattern `'./lib-esm/*'` in `@primer/react` inadvertently exposes all files under `src` as part of the public API when they are transformed into the `lib-esm` output. This includes utility functions, private components, and implementation details that are not intended for external consumption. As a result:

- Consumers may import and depend on unstable or internal APIs.
- We lose the ability to refactor internals without fear of breaking changes.
- The package‚Äôs public API is implicit rather than explicitly defined.
- It becomes difficult to enforce module boundaries within the codebase.

This undermines semantic versioning, increases technical debt, and reduces maintainability.

---

### Decision

1. **Restructure `src` to distinguish public and private modules:**
   - Create a dedicated `src/components` (or `src/public`) directory for modules intended for public use.
   - Move internal/utils/helper modules into `src/internal`, `src/utils`, or `src/private`.
   - Use TypeScript `// @private` comments or barrel file filtering to further signal intent.

2. **Update `package.json` exports map to only expose intentional entry points:**
   ```json5
   {
     ""exports"": {
       ""."": {
         ""import"": ""./lib-esm/index.js"",
         ""require"": ""./lib-cjs/index.js""
       },
       ""./components/*"": {
         ""import"": ""./lib-esm/components/*"",
         ""require"": ""./lib-cjs/components/*""
       }
       // Explicitly avoid wildcard exposure of all files
     }
   }
   ```

3. **Avoid deep imports via wildcard exports like `'./lib-esm/*'` unless scoped and intentional.**

4. **Update Rollup configuration** to:
   - Only bundle files under `src/components/*` (or other designated public folders) into ESM/CJS outputs.
   - Exclude `src/internal/**` from public builds.

5. **Add tooling support:**
   - Use ESLint (`no-restricted-imports`) to prevent external consumers from importing internal paths.
   - Add build-time checks or scripts to validate that no internal files are exposed via exports.

---

### Consequences

**Positive:**
- Clear separation between public API and internal implementation.
- Enables safe refactoring of internal code without breaking consumer apps.
- Improves adherence to semantic versioning.
- Provides better documentation and discoverability of public components.

**Negative:**
- Requires upfront effort to reorganize source structure.
- May require updates to existing import paths in consuming applications if they were relying on deep imports.
- Slight increase in build configuration complexity.

**Mitigations:**
- Document the change clearly in release notes.
- Provide codemods if necessary to help users migrate from deep imports.
- Maintain backward compatibility during a deprecation period if needed.

---

### Alternatives Considered

1. **Leave as-is with documentation-only guidance:**
   - ‚ùå Does not prevent misuse; relies on discipline.
   
2. **Use `_` prefix or `internal/` folders without changing exports:**
   - ‚ùå Still allows deep imports and doesn‚Äôt enforce encapsulation via tooling.

3. **Fully namespace internal files under `src/_internals`:**
   - ‚ö†Ô∏è Better than nothing but still depends on convention over enforcement.

Only restricting the `exports` map *and* restructuring source ensures both logical and technical encapsulation.

---

### References

- [Node.js Subpath Exports Documentation](https://nodejs.org/api/packages.html#subpath-exports)
- [React's approach to public vs internal modules](https://github.com/facebook/react/tree/main/packages/react/src)
- [Designing Effective JavaScript APIs](https://medium.com/@kellegous/designing-javascript-apis-d0f0d8cf67a2)","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","# ADR 015: Replace Storybook Play Functions with Dedicated Playwright Test Files

## Status
**Proposed**

## Context
This ADR supersedes [ADR 009 - Interaction Tests](./adr-009-interaction-tests.md).

In ADR 009, we adopted Storybook's `play` functions as the recommended approach for authoring interaction-based tests. While this provided a convenient way to colocate interactive test scenarios with component stories, several limitations have emerged in practice:

1. **Test Visibility and Discoverability**: `play` functions are executed only within the Storybook environment during visual regression runs, making them invisible in standard test workflows (e.g., `npm test`). This reduces their discoverability and makes debugging harder.

2. **Limited Debugging Capabilities**: Debugging failed interactions is challenging because `play` functions don't integrate with Jest's test runner, preventing the use of familiar tools like `--testNamePattern`, focused tests (`.only`), or coverage reporting.

3. **Tooling Fragmentation**: We maintain two distinct testing paradigms‚ÄîJest for unit/integration tests and Storybook `play` functions for interactions‚Äîleading to inconsistent patterns and increased cognitive load.

4. **CI/CD Complexity**: Monitoring, reporting, and retrying interaction tests is more complex when they're embedded in Storybook rather than managed through our primary test runner.

5. **Scalability Concerns**: As interaction test complexity grows (e.g., multi-step flows, assertions beyond visual diffs), the `play` function API becomes increasingly limiting.

Our current testing stack‚ÄîJest for logic and unit testing, Playwright for end-to-end browser testing‚Äîis well-suited to handle component-level interaction tests without requiring special Storybook integration.

## Decision
We will migrate away from using Storybook `play` functions for interaction testing and instead author all interaction tests as dedicated Playwright test files that target our Storybook instances.

Specifically:
- New interaction tests will be written as `.spec.ts` files using Playwright's test runner (`@playwright/test`)
- Tests will load component stories via their URLs (e.g., `http://localhost:6006/?path=/story/components-button--primary`)
- We will use Playwright's full API for interactions, assertions, and test organization
- Existing `play` functions will be gradually migrated to standalone test files
- Storybook remains our source of truth for component visualization, but not for test execution

Example of the new pattern:
```ts
// src/components/Button/Button.interaction.spec.ts
import { test, expect } from '@playwright/test';

test('Button calls onClick when clicked', async ({ page }) => {
  await page.goto('/?path=/story/components-button--primary');
  
  const button = page.locator('button');
  const clickSpy = await page.spyOnEvent('click');
  
  await button.click();
  
  expect(clickSpy).toHaveBeenCalled();
});
```

## Consequences
### Positive
- ‚úÖ All tests run through a unified interface (`npm test`, `npm run test:e2e`)
- ‚úÖ Full access to Playwright's debugging capabilities (trace viewer, step-by-step execution)
- ‚úÖ Better IDE integration and autocomplete support
- ‚úÖ Consistent pattern across unit, integration, and interaction tests
- ‚úÖ Improved CI/CD visibility with standardized test reporting
- ‚úÖ Easier onboarding for developers familiar with Jest/Playwright

### Negative
- ‚ùå Loss of colocation between stories and interaction tests
- ‚ùå Requires additional setup to serve Storybook during test runs
- ‚ùå Migration effort for existing `play` functions

### Mitigations
- Adopt a consistent file naming convention (e.g., `*.interaction.spec.ts`) and co-locate tests with component source
- Use our existing `test:e2e` script which already includes Storybook serving
- Create a codemod to assist with automated migration of simple `play` functions

## References
- [Playwright Test Documentation](https://playwright.dev/docs/intro)
- [Storybook Play Function Guide](https://storybook.js.org/docs/react/writing-stories/play-function)
- [ADR 009 - Interaction Tests](./adr-009-interaction-tests.md)","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision: Use a Fallback-Based Responsive Prop System with Static SSR Rendering

### Status
Accepted

### Context
We support responsive props that allow components to adapt their behavior or appearance based on viewport size using named breakpoints (`narrow`, `regular`, `wide`). These are implemented via the `useResponsiveValue` hook, which relies on `matchMedia` to detect active breakpoints client-side. However, during server-side rendering (SSR), `matchMedia` is unavailable, so the hook must return a fallback value. This leads to a mismatch between the server-rendered output and the hydrated client output when the actual viewport differs from the fallback, causing undesirable layout shifts.

This undermines user experience, harms Core Web Vitals (e.g., Cumulative Layout Shift), and contradicts our goal of delivering fast, stable SSR experiences.

### Decision
We will adopt a **static responsive prop system** for SSR safety by:

1. **Defining a consistent server-side fallback strategy**: All responsive props will resolve to a default breakpoint (e.g., `regular`) during SSR.
2. **Requiring explicit specification of responsive values**: Authors must provide values for all breakpoints *or* use a utility helper that enforces defaults.
3. **Introducing a compile-time or build-time validation layer (optional but recommended)**: Where feasible, use TypeScript patterns or lint rules to warn if responsive props are missing expected breakpoints.
4. **Documenting and standardizing the `regular` breakpoint as the SSR-safe default**: This breakpoint represents a balanced, widely applicable layout and aligns with typical desktop/tablet rendering expectations.

Example:
```tsx
// Safe: Explicit responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }} />

// Safe: Uses SSR fallback (`regular`) for missing breakpoints
<Stack gap={{ regular: 'normal' }} />
```

On the server, `gap` resolves to `'normal'`. On the client, `useResponsiveValue` will dynamically update based on `matchMedia`, but initial hydration matches SSR output ‚Üí no layout shift.

5. **Updating `useResponsiveValue` to accept a configurable SSR fallback**:
```ts
function useResponsiveValue<T>(
  responsiveValue: ResponsiveProp<T>,
  fallbackBreakpoint: Breakpoint = 'regular'
): T {
  // During SSR: return responsiveValue[fallbackBreakpoint] || first available
  // During CSR: subscribe to matchMedia and return current breakpoint value
}
```

6. **Discourage runtime-only responsive logic affecting layout** unless wrapped in a client-only boundary (e.g., `useEffect`, `useLayoutEffect`).

### Consequences
**Pros:**
- Eliminates layout shifts during hydration by ensuring SSR and initial render agree.
- Maintains flexibility for responsive design.
- Simple mental model: ""What you see on the server is what you start with on the client.""
- Improves performance and perceived stability.

**Cons:**
- May result in non-optimal initial layout for viewports far from `regular` (e.g., very narrow mobile).
- Requires discipline from developers to consider fallback behavior.
- Slight increase in bundle size due to retained breakpoint objects even when not fully used.

**Mitigations:**
- Educate teams on designing `regular` as a sensible baseline.
- Consider future enhancements like CSS-native solutions (e.g., container queries, media query-based CSS variables) to shift more logic to the browser without JS involvement.

### Notes
Long-term, we should explore CSS-driven responsive mechanisms (e.g., media queries in style objects, container queries) to reduce reliance on JavaScript for layout decisions. Until then, this approach provides a practical, safe foundation.

### Status
Accepted","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","**ADR: Handle Prop and API Deprecations with Forward-Compatible Migration Paths**

---

### **Decision**

When deprecating a prop or component API in favor of a more accessible, flexible, or semantically appropriate alternative:

1. **Maintain backward compatibility** during a full major version cycle (or equivalent deprecation period).
2. **Introduce the new API** alongside the old one, allowing coexistence.
3. **Deprecate the old API gracefully** by:
   - Adding a `console.warn` in development when the deprecated prop/API is used.
   - Clearly marking the old API as `@deprecated` in JSDoc with a description of the replacement.
   - Documenting the deprecation and migration path in the project‚Äôs public documentation and changelog.
4. **Support automatic detection and transformation** where possible (e.g., via codemods for large-scale migrations).
5. **Remove the deprecated API** only in the next major version, following semantic versioning (SemVer).

In the provided example:
- Deprecate the `title` prop on `<ActionList.Group>`.
- Introduce `<ActionList.GroupHeading>` as a dedicated, accessible heading element that accepts an `as` prop for semantic flexibility.
- Allow both patterns to function during the transition period.
- Warn in development if `title` is used, directing users to use `<ActionList.GroupHeading>` instead.

---

### **Rationale**

- **Accessibility**: The new API enables better DOM structure (e.g., proper heading hierarchy) and screen reader support.
- **Flexibility**: `<ActionList.GroupHeading>` can accept children, custom components, or typography variants ‚Äî not just strings.
- **Clarity**: Separating concerns (group container vs. heading content) improves composability and reduces prop sprawl.
- **Developer Experience**: Clear warnings and codemods reduce friction during upgrades.

---

### **Example Implementation**

```tsx
// During deprecation period
function ActionListGroup({ title, children }: { title?: string; children: ReactNode }) {
  if (process.env.NODE_ENV !== 'production' && title) {
    console.warn(
      '<ActionList.Group `title` prop is deprecated. Use <ActionList.GroupHeading> instead. ' +
        'See migration guide: https://example.com/guides/actionlist-migration'
    );
  }

  return (
    <div role=""group"" aria-labelledby={title ? 'group-title' : undefined}>
      {title ? (
        <ActionList.GroupHeading id=""group-title"" as=""h2"">
          {title}
        </ActionList.GroupHeading>
      ) : null}
      {children}
    </div>
  );
}
```

Eventually remove support for `title` in v2.0.0.

---

### **Status**

Accepted

---

### **Consequences**

- ‚úÖ Smoother migration for consumers.
- ‚úÖ Improved accessibility and extensibility in the new API.
- ‚ö†Ô∏è Increased bundle size temporarily due to dual implementation.
- ‚ö†Ô∏è Maintenance overhead during deprecation window.

--- 

### **Related Decisions**
- ADR-001: Component Composition over Prop Configuration
- ADR-005: Semantic Versioning and Breaking Changes","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

Compilation failures during a CI build should surface good error messages.

For example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply ""tsc exited with exit code 1"".

VSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https://code.visualstudio.com/docs/editor/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.

The problem-matcher concept fits well with ""setup"" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.","### Decision: Integrate Problem Matchers in CI for Rich Compilation Error Reporting

**Date:** 2024-04-17  
**Status:** Accepted  
**Context:**  
In our CI pipeline, when a build fails due to TypeScript compilation errors, the current feedback is often limited to generic exit codes (e.g., `tsc exited with code 1`). This makes it difficult for developers to quickly identify and fix issues without manually inspecting logs. In contrast, local development environments like VSCode provide rich error experiences by parsing compiler output using **problem matchers**‚Äîpatterns that extract file, line, column, and message from tool output and surface them as actionable issues.

GitHub Actions supports a similar concept via **problem matchers**, which can be registered during job execution to parse stderr/stdout and create annotations in pull requests or workflow runs. This capability aligns well with setup actions such as `setup-node`, where we can extend it to not only install tools but also configure intelligent output parsing.

**Decision:**  
We will adopt **problem matchers** in our CI pipelines to transform raw compiler output into structured, actionable errors. Specifically:

1. Use or create a GitHub Actions problem matcher file (e.g., `tsc-problem-matcher.json`) that defines patterns matching TypeScript compiler error output.
2. Modify or extend the `setup-node` action (or introduce a post-install hook) to register the `tsc` problem matcher when Node.js/TypeScript is configured.
3. Ensure this registration applies for the duration of the job, so any subsequent `tsc` invocation automatically produces annotated errors in GitHub‚Äôs UI.

Example implementation:
```bash
# Register the tsc problem matcher before running builds
echo ""::add-matcher::./tsc-problem-matcher.json""
npx tsc --noEmit
```

The matcher JSON would follow the schema documented in [GitHub's documentation](https://github.com/actions/toolkit/blob/main/docs/problem-matchers.md), capturing patterns like:
```
file.ts(10,15): error TS2300: Duplicate identifier 'x'.
```

**Rationale:**  
- Improves developer experience by surfacing precise error locations directly in PR checks and logs.
- Aligns with IDE behavior (like VSCode), reducing context switching.
- Leverages existing, well-documented mechanisms in GitHub Actions.
- Low cost to implement and maintain, especially if reused across repositories.

**Consequences:**  
- ‚úÖ **Positive**: Faster debugging, clearer feedback, fewer ""blind"" rebuilds.
- ‚ö†Ô∏è **Neutral**: Requires initial setup and testing of regex patterns; must handle both full and incremental build outputs correctly.
- ‚ùå **Negative**: Limited to tools with predictable, parsable output. May require customization for non-standard tsc wrappers (e.g., `ttsc`, Babel with TypeScript).

**Future Considerations:**  
- Extend the pattern to other compilers/linters (e.g., ESLint, webpack).
- Contribute upstream to `actions/setup-node` to include built-in support for registering common problem matchers.

**References:**  
- [GitHub Problem Matchers Documentation](https://github.com/actions/toolkit/blob/main/docs/problem-matchers.md)
- [VSCode Task Problem Matchers](https://code.visualstudio.com/docs/editor/tasks#_defining-a-problem-matcher)

---  
*This decision ensures that compilation failures in CI are as informative as they are in local development.*",## Decision
"## Context
run-actions run scripts using a platform specific shell:
`bash -eo pipefail` on non-windows, and `cmd.exe /c /d /s` on windows

The `shell` option overrides this to allow different flags or completely different shells/interpreters

A small example is:
```yml
jobs:
  bash-job:
    actions:
    - run: echo ""Hello""
      shell: bash
  python-job:
    actions:
    - run: print(""Hello"")
      shell: python {0}
```","### Decision: Standardize Script Execution via Configurable Shell Interface with Fallback to Platform-Appropriate Defaults

#### Status
Accepted

#### Context
The system needs to execute arbitrary scripts or commands within a job execution environment, and must support multiple platforms (e.g., Linux/macOS and Windows) with differing native shells. Additionally, certain tasks require non-shell interpreters (e.g., Python, PowerShell, sh, zsh) for running code directly.

Currently:
- On non-Windows platforms, scripts are executed using `bash -eo pipefail` for strict error handling.
- On Windows, `cmd.exe /c /d /s` is used as the default shell.
- The `shell` option allows overriding the execution command, enabling customization of flags or use of alternative interpreters (e.g., `python {0}`, `pwsh -c`, `sh -c`).

Use cases include:
- Running simple shell commands (`echo ""Hello""`).
- Executing scripts in domain-specific interpreters (`print(""Hello"")` in Python).
- Supporting cross-platform workflows without rewriting logic per OS.

#### Decision
We will retain and formalize the current behavior:
1. **Default Shell by Platform**:
   - Non-Windows: Execute `run` actions using `bash -eo pipefail '{command}'`.
     - Ensures strict error handling (exit on error, error in pipeline).
   - Windows: Use `cmd.exe /c /d /s ""{command}""`.
     - `/c` runs command and exits; `/s` enables quotes handling; `/d` skips AutoRun scripts for consistency.

2. **Custom Shell via `shell` Field**:
   - Users may specify a custom shell string in the action step.
   - The placeholder `{0}` in the shell string will be replaced with the path to a temporary script file containing the command(s), if applicable.
   - If no `{0}` is present, the command content is appended directly (typically for `-c` style interpreters).

Examples:
```yaml
- run: echo ""Hello World""
  shell: bash -euxo pipefail {0}  # Custom bash flags

- run: Write-Host ""Hello""
  shell: pwsh -c {0}              # Use PowerShell Core

- run: print(""Hello"")
  shell: python {0}               # Execute via Python interpreter

- run: |
    set -e
    ./build.sh
    ./test.sh
  shell: bash {0}
```

3. **Validation & Escaping**:
   - Commands passed to shells are written to temporary files when `{0}` is used (recommended for multiline or complex scripts).
   - Inline execution (without `{0}`) should properly quote/escape the command to prevent injection.

4. **Documentation Requirement**:
   - Document all default shells and common `shell` overrides.
   - Advise users on security implications of shell injection and best practices.

#### Consequences
**Pros**:
- ‚úÖ Flexibility: Supports any interpreter or shell that can execute text-based scripts.
- ‚úÖ Cross-platform compatibility: Respects platform conventions while allowing overrides.
- ‚úÖ Predictable error handling: Default flags like `-e` and `-o pipefail` help catch errors early.
- ‚úÖ Extensibility: Easy to add new interpreters (e.g., node, ruby, perl) via `shell` configuration.

**Cons**:
- ‚ùå Complexity: Users must understand shell quoting, escaping, and the `{0}` placeholder.
- ‚ùå Security risk: Arbitrary shell strings could introduce injection vulnerabilities if not sanitized.
- ‚ùå Inconsistencies: Behavior may differ subtly between shells (e.g., error codes, built-in commands).

#### Alternatives Considered
- **Always use bash (via WSL or bundled)**: Would unify experience but adds dependencies and deviates from native expectations on Windows.
- **Define fixed list of supported shells**: Less flexible; doesn‚Äôt allow niche interpreters.
- **Remove shell override entirely**: Not viable due to need for Python, PowerShell, etc.

#### Conclusion
Retain platform-specific defaults with full user override capability via the `shell` field. This balances usability, safety, and flexibility across diverse use cases and environments.","## Decision

___"
"## Context

User wants to reference workflow variables defined in workflow yaml file for action's input, displayName and condition.","**Decision:** Support variable resolution for action inputs, `displayName`, and `if` (condition) fields using workflow-level variables defined in the YAML.

---

### Context
Users need to dynamically configure actions in their workflows by referencing variables defined at the workflow level (e.g., `env`, `inputs`, `vars`, or context expressions like `${{ vars.MY_VAR }}`). Specifically, they want to use these variables when setting:
- Action inputs
- The `displayName` of a job or step
- Conditional execution (`if`) logic

This enables reusable, environment-aware, and parameterized workflows without hardcoding values.

---

### Decision
Allow all action input values, `displayName` fields, and `if` condition expressions to resolve variables using the `${{ }}` syntax that references workflow-level data, including:
- `vars.*`
- `inputs.*`
- `env.*`
- Built-in contexts (e.g., `github.*`, `steps.*`, etc.)

The workflow engine must evaluate these expressions at runtime in the appropriate context phase (e.g., before step execution for inputs and conditions, during display rendering for UI).

Example:

```yaml
env:
  DEPLOY_ENV: production

jobs:
  deploy:
    if: ${{ vars.ENABLE_DEPLOY && github.ref == 'refs/heads/main' }}
    steps:
      - name: ${{ format('Deploy to {0}', env.DEPLOY_ENV) }}
        uses: actions/deploy@v1
        with:
          target: ${{ env.DEPLOY_ENV }}
          dry_run: ${{ !vars.FORCE_DEPLOY }}
```

---

### Status
Accepted

---

### Consequences

**Pros:**
- Increases flexibility and reusability of workflows.
- Enables dynamic behavior based on inputs, environment, or GitHub events.
- Aligns with established patterns from platforms like GitHub Actions, Azure Pipelines, etc.

**Cons:**
- Adds complexity to parsing and evaluation logic.
- May introduce debugging challenges if variable resolution fails or is misconfigured.
- Requires clear documentation and validation/error messaging.

**Mitigations:**
- Implement robust expression parsing with descriptive error messages.
- Provide linting support in IDEs and CLI tools.
- Document scoping rules and available contexts clearly.

--- 

### Notes
This decision assumes the existence of a runtime expression evaluator capable of handling `${{ }}` syntax. If not already implemented, this capability should be prioritized as part of the workflow execution engine.",## Decision
"## Context
First party action `actions/cache` needs a input which is an explicit `key` used for restoring and saving the cache. For packages caching, the most common `key` might be the hash result of contents from all `package-lock.json` under `node_modules` folder.
  
There are serval different ways to get the hash `key` input for `actions/cache` action.

1. Customer calculate the `key` themselves from a different action, customer won't like this since it needs extra step for using cache feature
```yaml
  steps:
  - run: |
      hash=some_linux_hash_method(file1, file2, file3)
      echo ::set-output name=hash::$hash
    id: createHash
  - uses: actions/cache@v1
    with:
      key: ${{ steps.createHash.outputs.hash }}
``` 

2. Make the `key` input of `actions/cache` follow certain convention to calculate hash, this limited the `key` input to a certain format customer may not want.
```yaml
  steps:
  - uses: actions/cache@v1
    with:
      key: ${{ runner.os }}|${{ github.workspace }}|**/package-lock.json
```","# Architectural Decision Record: Dynamic Cache Key Generation for `actions/cache`

## Status
Proposed

## Context
The `actions/cache` action requires a `key` input to identify cache entries for restoration and saving. For package management scenarios (e.g., npm), the ideal key should be based on the content hash of dependency manifest files like `package-lock.json`. However, requiring users to manually compute these hashes adds friction and complexity to workflow configuration.

Two approaches were considered:
1. **Manual hash computation**: Users must add a separate step to calculate the hash before using the cache action. This is flexible but creates additional boilerplate and reduces usability.
2. **Convention-based implicit hashing**: The cache action automatically computes hashes when the key follows a specific pattern/glob syntax. This improves usability but constrains key formatting.

The core tension is between flexibility and ease of use‚Äîusers want both simplicity in common scenarios and control in advanced cases.

## Decision
Implement **implicit hash generation** for the `key` input when it contains glob patterns or special markers, while preserving support for explicit/custom keys.

Specifically:
- When the `key` input contains glob-like syntax (e.g., `**/package-lock.json`) or a designated prefix (e.g., `hash://`), the `actions/cache` action will automatically:
  1. Resolve all file paths matching the pattern(s)
  2. Compute a content hash (e.g., SHA-256) across all matched files
  3. Use this hash as part of the final cache key
- Static portions of the key (like `${{ runner.os }}`) are still supported and prepended/appended as specified
- Example syntax:
```yaml
- uses: actions/cache@v1
  with:
    key: ${{ runner.os }}-node-modules-${{ hash('**/package-lock.json') }}
```
or alternatively:
```yaml
- uses: actions/cache@v1
  with:
    key: ${{ runner.os }}|**/package-lock.json
```
where the presence of `**` triggers automatic hashing.

This approach combines ease of use with sufficient expressiveness.

## Rationale
- **Usability**: Eliminates the need for manual hash steps in common caching scenarios
- **Discoverability**: Pattern-based syntax aligns with existing GitHub Actions conventions
- **Backward compatibility**: Explicit/static keys remain fully supported
- **Performance**: Hashing occurs only when needed and can be optimized within the action runtime
- **Consistency**: Matches patterns seen in other developer tools (e.g., build systems with file watching)

## Implications
### Positive
- Reduces configuration overhead for typical caching use cases
- Encourages proper cache key hygiene by promoting content-based keys
- Enables concise, readable workflow definitions

### Negative
- Adds parsing complexity to the `actions/cache` implementation
- Potential confusion if glob patterns don't match any files (should result in cache miss, not failure)
- Requires clear documentation about when and how hashing is applied

## Alternatives Considered
- **Separate action for key generation**: Rejected due to added complexity and fragmentation
- **New dedicated cache action for packages**: Overkill for a feature that generalizes well beyond npm
- **No automation (status quo)**: Maintains maximum flexibility but at high usability cost

## Additional Notes
Future extensions could include:
- Built-in common key templates (e.g., `key: ${{ cacheKey('npm') }}`)
- Support for multiple file sets with composite keys
- Versioned hashing algorithms to prevent silent corruption",## Decision
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision: Implement Opt-In Command Echoing with Conditional Defaults Based on Debug Mode

We will change the default behavior of command echoing in the system to be opt-in, reducing log verbosity for most users while preserving diagnostic utility when needed.

### Chosen Approach

1. **Disable command echoing by default** for all steps unless explicitly enabled.
2. **Enable automatic echoing if `ACTIONS_STEP_DEBUG` is set**, supporting developers and advanced users who need detailed execution insight.
3. **Preserve echoing during command failure**, regardless of current echo settings, to ensure error diagnostics include the failing command.
4. **Introduce control commands**:
   - `::echo::on` ‚Äì Explicitly enable command echoing for subsequent commands.
   - `::echo::off` ‚Äì Explicitly disable command echoing.

### Rationale

- **Improved Log Readability**: Most users do not require every executed command to be echoed; disabling by default reduces noise.
- **Debuggability Preserved**: Users actively debugging workflows (via `ACTIONS_STEP_DEBUG`) retain full visibility.
- **Failures Remain Transparent**: Automatically echoing failed commands ensures users can understand what went wrong without needing prior echo configuration.
- **Explicit Control**: The `::echo::on/off` commands give fine-grained control within a step or across workflow stages.

### Example Behavior

```yaml
# User script
echo ""Starting setup...""
::echo::on
ls -la
git commit -m ""fix""
::echo::off
echo ""Setup complete""
```

**Output (without ACTIONS_STEP_DEBUG)**:
```
Starting setup...
+ ls -la
+ git commit -m ""fix""
fatal: empty commit message
::error::Command 'git commit -m ""fix""' failed with exit code 128
Setup complete
```

> Note: Only the failing command is echoed automatically due to error.

### Implementation Notes

- The runner must inspect the `ACTIONS_STEP_DEBUG` environment secret at step start to determine initial echo state.
- The `::echo::on` and `::echo::off` commands modify a per-step runtime flag.
- Command echoing includes prepending a `+ ` or similar marker to indicate execution (shell-style).
- Echo suppression only affects successful command display ‚Äî stderr, output, and error traces remain unaffected.

This decision balances usability, debuggability, and transparency.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

The Runner registers a number of Value Encoders, which mask various encodings of a provided secret. Currently, we register a 3 base64 Encoders:
- The base64 encoded secret
- The secret with the first character removed then base64 encoded
- The secret with the first two characters removed then base64 encoded

This gives us good coverage across the board for secrets and secrets with a prefix (i.e. `base64($user:$pass)`).

However, we don't have great coverage for cases where the secret has a string appended to it before it is base64 encoded (i.e.: `base64($pass\n))`). 

Most notably we've seen this as a result of user error where a user accidentally appends a newline or space character before encoding their secret in base64.","## Decision: Add Base64 Encoders for Common Appended Characters

To improve detection coverage for base64-encoded secrets that include unintended trailing characters‚Äîparticularly whitespace such as newlines or spaces‚Äîwe will extend the Runner's Value Encoder registry to include additional base64 encodings that account for common postfix patterns.

### Chosen Approach

Register **three additional base64 encoders** that encode:
1. The secret with a single newline (`\n`) appended.
2. The secret with a carriage return followed by a newline (`\r\n`) appended.
3. The secret with a single space (` `) appended.

These variants will be added to the existing set of base64 encoders (which currently handle prefix truncation).

### Rationale

- **Observed Real-World Patterns**: User errors during manual base64 encoding (e.g., copying credentials with trailing newlines from config files, CLI output, or editors) frequently result in secrets being encoded with trailing whitespace.
- **Low Overhead**: Adding three predictable transformations is computationally inexpensive and does not significantly impact performance or memory usage.
- **Symmetry with Current Strategy**: Just as we currently anticipate partial secrets via prefix trimming, we now proactively handle postfix contamination, improving overall robustness.
- **Extensibility**: This pattern can be revisited if other common appendages (e.g., tabs, multiple spaces) are observed at scale.

### Implementation Details

- Encoder labels should be descriptive, e.g.:
  - `base64_with_newline`
  - `base64_with_crlf`
  - `base64_with_space`
- These encoders will be applied only during secret *detection* (i.e., when masking possible values), not during secret storage or transmission.
- Documentation will be updated to reflect these assumptions about user error patterns.

### Future Considerations

- Monitor logs/metrics for hit rates on these new encoders; deprecate or expand based on empirical data.
- If multi-character trailing patterns become prevalent, consider parameterized postfix encoders.

### Outcome

This change improves resilience against common user input mistakes without increasing complexity substantially, aligning with our goal of maximizing secret detection accuracy while minimizing false negatives due to formatting artifacts.",## Decision
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision: Inject Hosted Image Metadata into `Set up job` Step Logs

### Status
Accepted

### Context
The GitHub Actions runner is responsible for initializing a job on a runner machine, including checking out code, setting up the environment, and executing workflows. The `Set up job` step is one of the first visible steps in a workflow run and runs automatically on hosted runners (e.g., `ubuntu-latest`, `windows-2022`). Users often need visibility into the environment where their jobs are running ‚Äî such as OS version, pre-installed software versions (e.g., Node.js, Python), or image build timestamps ‚Äî to debug issues or ensure consistency.

Currently, this information is not directly visible in logs without explicit user commands (e.g., `systeminfo`, `uname -a`, custom scripts). This leads to repetitive boilerplate across repositories and delayed troubleshooting.

### Decision
Modify the runner logic in the `Set up job` step to **automatically include metadata about the hosted image** in the log output when running on a GitHub-hosted runner. This metadata will be injected at the beginning of the `Set up job` step and include:

- Operating System name and version
- Hosted image version (e.g., `20231015.1`)
- Image release timestamp
- List of key pre-installed tools and their versions (e.g., Git, Docker, Node.js, Python)
- Region/fleet info (if available and non-sensitive)

This data will be sourced from a well-known metadata file or environment variable (e.g., `/image-info.json`, `ACTIONS_IMAGE_INFO`) that is guaranteed to be present on all GitHub-hosted images.

Example log output:
```log
Current runner version: '2.305.0'
Operating System
  Ubuntu
  22.04.3
  LTS
Image Release: 20231015.1
Image Version: actions-image-version-here
Image Label: ubuntu22
Started: 2023-10-16T12:34:56.789Z

Installed Software:
  Git: 2.41.0
  Docker: 24.0.5
  Node.js: 18.17.0
  Python: 3.11.5
  Java: 17.0.8
```

### Implementation Notes
- The runner must detect whether it is running on a GitHub-hosted runner (via an environment flag like `ACTIONS_HOSTED_RUNNER`).
- Only emit metadata if the source file (e.g., `/image-info.json`) exists and is valid.
- Ensure no sensitive or PII data is logged.
- Format logs consistently with existing runner output.
- Keep the feature opt-out via runner configuration (e.g., `DISABLE_IMAGE_METADATA_LOGGING`) for enterprise/self-hosted use.

### Consequences
**Pros:**
- Improves debugging experience by providing immediate context.
- Reduces need for users to add manual diagnostic steps.
- Enhances transparency into hosted environments.
- Aligns with platform expectations (similar info shown in other CI systems).

**Cons:**
- Slight increase in log verbosity (mitigated by structured format and placement).
- Requires coordination between image pipeline and runner teams to maintain metadata format.

### References
- [GitHub Actions Virtual Environments](https://github.com/actions/virtual-environments)
- [Runner Repository](https://github.com/actions/runner)

---

*Approved by: Platform & Runner Engineering Leads*  
*Date: 2023-10-16*","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision

Add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context to enable conditional execution logic in workflows based on the result of prior steps.

- `steps.<id>.outcome` will reflect the final state of the step after completion, including values such as `""success""`, `""failure""`, `""skipped""`, or `""cancelled""`. This field is set regardless of whether the step was bypassed due to conditions or failed.
- `steps.<id>.conclusion` will indicate the final verdict of a step in terms of its success for evaluation in subsequent conditionals, with values like `""success""` or `""failure""`, and will be used primarily in `if` conditions to control step execution.

These properties will be available in the context for all subsequent steps within the same job, enabling expressions such as:

```yaml
- name: Notify on failure
  if: steps.previous_step.outcome == 'failure'
  run: echo ""The previous step failed""
```

This change preserves backward compatibility, as existing references to `steps.<id>.outputs` remain unaffected. It enhances workflow authoring by supporting more robust error handling, debugging, and conditional logic without requiring external tools or script-based checks.

Implementation must ensure that `outcome` and `conclusion` are populated even when steps fail or are skipped, allowing reliable use in downstream conditionals.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","# Architectural Decision Record: Support Custom Labels via Command Line During Runner Registration

## Status
Accepted

## Context
Configuring self-hosted runners is commonly automated through infrastructure-as-code or configuration management scripts. While the GitHub Actions runner automatically registers built-in labels (such as `os` and `arch`) during registration, it currently lacks the ability to register custom labels at registration time via command-line arguments.

This limitation forces users to either:
1. Use post-registration API calls to add custom labels, complicating automation workflows
2. Rely on external mechanisms to manage label synchronization
3. Manually configure labels through the web interface

As documented in [Issue #262](https://github.com/actions/runner/issues/262), this creates friction in automated deployment scenarios where consistent, reproducible runner configurations are essential.

The need for programmatic label assignment during registration is further validated by the community interest and previous attempts to address this (e.g., [ADR275](https://github.com/actions/runner/pull/275)).

## Decision
We will extend the runner registration command to accept an optional `--labels` parameter that allows specifying custom labels during registration.

The implementation will:
1. Add a new command-line option: `--labels <label1,label2,...>`
2. Validate label format according to GitHub's labeling rules (alphanumeric, hyphens, underscores; no spaces)
3. Register both built-in labels (os, arch) and user-provided custom labels atomically during registration
4. Maintain backward compatibility with existing registration flows
5. Document the new parameter in help text and official documentation

Example usage:
```bash
./config.sh --url https://github.com/org/repo --token ABC123 --labels linux,x64,gpu,custom-label
```

## Consequences
### Positive
- Enables fully automated runner provisioning with complete label configuration
- Simplifies infrastructure-as-code implementations
- Reduces operational complexity by eliminating post-registration steps
- Improves consistency and reliability of runner deployments
- Aligns with user expectations for configuration management

### Negative
- Slightly increases complexity of the registration command
- Requires validation logic for label inputs
- Potential for label conflicts if not properly managed by operators

### Neutral
- No impact on existing installations without the `--labels` parameter
- Does not change how labels are used in workflow routing

## Additional Notes
This decision supersedes but incorporates the intent of ADR275, providing a simpler, more direct solution focused specifically on the registration-time label requirement rather than broader configuration changes.","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context

In addition to action's regular execution, action author may wants their action to have a chance to participate in:
- Job initialization
  My Action will collect machine resource usage (CPU/RAM/Disk) during a workflow job execution, we need to start perf recorder at the beginning of the job.
- Job cleanup
  My Action will dirty local workspace or machine environment during execution, we need to cleanup these changes at the end of the job.  
  Ex: `actions/checkout@v2` will write `github.token` into local `.git/config` during execution, it has post job cleanup defined to undo the changes.","## Decision: Support Job Initialization and Cleanup Hooks for Actions

To enable actions to participate in job lifecycle events, we will implement support for **pre-job** and **post-job** hooks in the action runner.

### Decision Details

1. **Introduce `runs.pre` and `runs.post` fields** in the action metadata (`action.yml`) to define initialization and cleanup steps:
   ```yaml
   runs:
     using: 'node16'
     main: 'dist/index.js'
     pre: 'dist/pre-job.js'        # Optional: runs once at job start
     post: 'dist/post-job.js'      # Optional: runs once at job end (including failure/ cancellation)
   ```

2. **Guaranteed Execution**:
   - `pre` script runs after job context setup but before any step execution.
   - `post` script runs regardless of job outcome (success, failure, or cancellation).
   - The `post` script is executed even if prior steps fail or are skipped.

3. **Environment & Context**:
   - Both `pre` and `post` scripts execute in the same environment as regular action steps.
   - Full access to job context (e.g., `github`, `inputs`, `env`) is provided.
   - Logging, secrets masking, and other runtime features apply uniformly.

4. **Error Handling**:
   - Failure in `pre` script fails the entire job.
   - Failure in `post` script does not affect job status unless explicitly configured otherwise (via `post-if`).

5. **Optional Conditional Execution**:
   Support a `post-if` field to conditionally run cleanup:
   ```yaml
   runs:
     post: 'dist/cleanup.js'
     post-if: success() || failure()  # Always run
   ```

6. **Security Considerations**:
   - Actions with `pre`/`post` scripts must be explicitly trusted when used (similar to existing privileged actions).
   - Clear user-facing indication when an action uses lifecycle hooks.

### Rationale

- Enables critical use cases: performance monitoring, workspace cleanup, credential reset, resource deallocation.
- Aligns with existing patterns (e.g., `actions/checkout@v2` post-job cleanup).
- Provides predictable, reliable lifecycle management without requiring users to manually add setup/cleanup steps.
- Maintains consistency with current action interface while extending capabilities.

### Consequences

- ‚úÖ **Pros**:
  - Empowers action authors to build more robust, self-contained actions.
  - Improves security and reliability by enabling automatic cleanup of sensitive state.
  - Reduces boilerplate in workflows (no need to manually invoke cleanup).

- ‚ùå **Cons**:
  - Increases complexity of action runtime.
  - Requires careful handling of error conditions and timeouts in cleanup phase.
  - Potential for misuse (e.g., long-running cleanup blocking job completion).

This decision enables safe, standardized participation of actions in job lifecycle events while maintaining control, visibility, and reliability.",## Decision
"## Context

Customers want to be able to compose actions from actions (ex: https://github.com/actions/runner/issues/438)

An important step towards meeting this goal is to build functionality for actions where users can simply execute any number of steps.","**Decision: Implement Composite Actions to Enable Step Composition**

To address customer demand for composing actions from other actions (e.g., combining multiple steps or actions into reusable units), we will implement **Composite Actions** ‚Äî a feature that allows users to define a custom action containing a sequence of steps, which can be invoked as a single action within a workflow.

### Rationale

- **User Demand**: There is strong community interest (e.g., [actions/runner#438](https://github.com/actions/runner/issues/438)) in enabling modular and reusable workflows through action composition.
- **Abstraction & Reuse**: Composite Actions allow teams to encapsulate common workflows (e.g., setup, testing, deployment) into shareable components, reducing duplication and improving maintainability.
- **Simplicity**: Unlike Docker or JavaScript actions, composite actions use YAML-defined steps, making them easier to write, inspect, and debug using familiar syntax.
- **Incremental Progress**: This is a foundational step toward full action-to-action composition and does not require runtime changes to the runner itself.

### Implementation Approach

1. **Define Composite Actions via `action.yml`**:
   - Introduce a new `runs.using: ""composite""` field.
   - Allow specification of multiple steps using standard `run`, `uses`, `with`, etc., syntax.

   Example:
   ```yaml
   name: 'Build and Test'
   description: 'Runs build and test steps'
   runs:
     using: 'composite'
     steps:
       - run: npm install
         shell: bash
       - run: npm run build
         shell: bash
       - run: npm test
         shell: bash
   ```

2. **Support Inputs and Outputs**:
   - Pass inputs to steps via `{{ inputs.input_name }}`.
   - Expose outputs using `set-output` commands within steps.

3. **Execution Model**:
   - The runner executes each step in sequence within the same environment (same VM/container).
   - Steps have access to the job‚Äôs workspace and environment variables.

4. **No Cross-Action Runtime Isolation**:
   - Composite actions run in the context of the calling job (no separate container/runtime).
   - Ensures performance and access to artifacts/workspaces.

5. **Publishing & Sharing**:
   - Host composite actions in public or private repositories.
   - Reference via `owner/repo@version` like other actions.

### Trade-offs

| Pros | Cons |
|------|------|
| ‚úÖ Easy to author using existing YAML knowledge | ‚ùå No isolation between steps (side effects possible) |
| ‚úÖ Improves reusability and reduces duplication | ‚ùå Limited to same platform/job context |
| ‚úÖ Lightweight compared to Docker/containerized actions | ‚ùå Cannot currently call another composite action directly (future work) |

### Future Considerations

- Support for **nested composite actions** (calling one composite action from another).
- Enhanced dependency management and versioning.
- Editor/tooling support for discovery and validation.

---

**Status**: Accepted  
**Date**: 2024-04-05  
**Author**: Architecture Team  
**Related Issue**: [actions/runner#438](https://github.com/actions/runner/issues/438)","## Decision

**In this ADR, we only support running multiple run steps in an Action.** In doing so, we build in support for mapping and flowing the inputs, outputs, and env variables (ex: All nested steps should have access to their parents' input variables and nested steps can overwrite the input variables)."
"## Context

We recently shipped composite actions, which allows you to reuse individual steps inside an action. 
However, one of the [most requested features](https://github.com/actions/runner/issues/834) has been a way to support the `if` keyword.","## Decision: Support `if` Conditionals in Composite Actions at the Step Level

We will extend composite actions to support the `if` keyword on individual steps within a composite action, aligning their behavior with standard GitHub Actions workflows.

### Rationale

1. **User Demand**: The inability to conditionally execute steps in composite actions is one of the most requested features (see [actions/runner#834](https://github.com/actions/runner/issues/834)). Users expect consistency between workflow-level and composite action step syntax.

2. **Consistency**: Supporting `if` in composite actions brings them in line with regular job steps, reducing cognitive load and improving developer experience.

3. **Practical Use Cases**:
   - Skip linting or testing steps based on file changes (`if: contains(github.event.pull_request.changed_files, 'src/')`)
   - Run debug steps only when a debug flag is set
   - Conditionally deploy based on branch or environment variables

4. **Implementation Feasibility**: The GitHub Actions runner already evaluates `if` conditions for workflow steps. Extending this evaluation to steps within composite actions is a natural progression of existing capabilities.

### Implementation Approach

1. **Syntax Preservation**: Maintain identical `if` syntax used in workflows:
   ```yaml
   steps:
     - name: Conditional Step
       if: ${{ env.DEBUG == 'true' }}
       run: echo ""Debug mode enabled""
   ```

2. **Context Evaluation**: Evaluate `if` conditions in the context of the calling workflow, not the composite action's definition context. This ensures conditions can reference workflow-specific data like `github.event`, `matrix`, and dynamically-set environment variables.

3. **Runner-Level Processing**: Handle condition evaluation in the Actions runner during step expansion, skipping steps whose conditions evaluate to false.

4. **Backward Compatibility**: Existing composite actions without `if` conditions will continue to function unchanged.

### Limitations & Considerations

- **No `if` on the composite action itself**: This decision only enables `if` on *steps within* composite actions, not on the invocation of the composite action from a workflow (which is already supported).
- **Expression Context**: Expressions in `if` conditions will have access to the same context as regular workflow steps, but not to outputs or internal state of the composite action being executed.

### Future Considerations

- Explore supporting `if` on the composite action invocation level using inputs (e.g., `if: ${{ inputs.enable_feature }}`)
- Consider supporting other control flow keywords (`continue-on-error`, `timeout-minutes`) at the step level

This change will be rolled out gradually with documentation updates and examples showing common conditional patterns.",
"## Context

This ADR details the design changes for supporting custom configurable hooks for on various runner events. This has been a long requested user feature [here](https://github.com/actions/runner/issues/1543), [here](https://github.com/actions/runner/issues/699) and [here](https://github.com/actions/runner/issues/1116) for users to have more information on runner observability, and for the ability to run cleanup and teardown jobs. 

This feature is mainly intended for self hosted runner administrators.

**What we hope to solve with this feature**
1. A runner admininstrator is able to add custom scripts to cleanup their runner environment at the start or end of a job
2. A runner admininstrator is able to add custom scripts to help setup their runner environment at the beginning of a job, for reasons like [caching](https://github.com/actions/runner/issues/1543#issuecomment-1050346279)
3. A runner administrator is able to grab custom telemetry of jobs running on their self hosted runner

**What we don't think this will solve**
- Policy features that require certain steps run at the beginning or end of all jobs
  - This would be better solved to in a central place in settings, rather then decentralized on each runner. 
  - The Proposed `Notification Hooks for Runners` is limited to self hosted runners, we don't beileve Policy features should be
- Reuse scenarios between jobs are covered by [composite actions](https://docs.github.com/en/actions/creating-actions/creating-a-composite-action) and [resuable workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows)
- Security applications, security should be handled on the policy side on the server, not decentralized on each runner","# ADR: Implement Custom Configurable Hooks for Self-Hosted Runner Lifecycle Events

## Status
Proposed

## Context
Self-hosted runner administrators have long requested greater control and observability over the lifecycle of jobs executing on their runners. Community feedback (e.g., [issue #1543](https://github.com/actions/runner/issues/1543), [#699](https://github.com/actions/runner/issues/699), [#1116](https://github.com/actions/runner/issues/1116)) highlights recurring needs for:

- Environment cleanup and setup between jobs
- Custom telemetry collection
- Pre- and post-job automation without modifying individual workflows

While composite actions and reusable workflows address reuse *within* workflows, they do not empower runner administrators to enforce or observe behavior *across* all jobs on a runner. Centralized policy enforcement remains out of scope for this feature, as it should be managed server-side via GitHub‚Äôs policy framework.

Decentralized script execution at key runner lifecycle points enables administrator-specific concerns‚Äîsuch as ephemeral resource management, logging, or caching optimizations‚Äîwithout introducing complexity into the core runner logic or workflow authoring experience.

## Decision
We will implement **custom configurable hooks** that allow self-hosted runner administrators to register scripts that execute at predefined lifecycle events during job execution. These hooks are configured locally on the runner and are transparently invoked by the runner process.

### Hook Points
The following lifecycle events will support custom hooks:
1. `job.starting` ‚Äì Before a job begins execution, after the job is assigned.
2. `job.ended` ‚Äì After a job completes (success or failure), before workspace cleanup.
3. `job.failure` *(optional)* ‚Äì Only if the job fails; can be used for diagnostics.

> Note: Hooks run synchronously and block progression to the next phase. They execute under the same user context as the runner unless otherwise secured.

### Configuration
Hooks are defined in a new optional configuration file:  
`./actions-runner/hooks/hooks.json` (path relative to runner root)

Example:
```json
{
  ""hooks"": [
    {
      ""event"": ""job.starting"",
      ""command"": ""/opt/runner-hooks/setup-env.sh"",
      ""timeout_seconds"": 30,
      ""enabled"": true
    },
    {
      ""event"": ""job.ended"",
      ""command"": ""/opt/runner-hooks/cleanup.sh"",
      ""timeout_seconds"": 60,
      ""enabled"": true
    },
    {
      ""event"": ""job.failure"",
      ""command"": ""/opt/runner-hooks/diagnose-failure.sh"",
      ""timeout_seconds"": 45,
      ""enabled"": true
    }
  ]
}
```

Alternatively, simple script naming convention may be supported (e.g., `pre-job.sh`, `post-job.sh`) in a `hooks/` directory for ease of use.

### Execution Guarantees
- Scripts run with minimal environment variables; explicit documentation of available context (e.g., `RUNNER_JOB_STATUS`, `GITHUB_REPOSITORY`, `RUNNER_NAME`) will be provided.
- Timeouts prevent indefinite blocking.
- Output from hook scripts is logged verbosely in runner logs but **not** exposed in GitHub Actions UI logs unless explicitly forwarded.
- Failures in `job.starting` hooks fail the job immediately.
- Failures in `job.ended` hooks are logged but do not affect job outcome.

### Security Considerations
- Hooks are only configurable on self-hosted runners by local administrators.
- No support for remote or dynamic hook loading.
- Runner must be restarted to apply hook configuration changes.
- Documentation will emphasize securing hook scripts against tampering (file permissions, code reviews).

### Observability
- Runner logs include clear markers when hooks start and finish.
- Structured logging includes hook execution duration and exit codes.
- Telemetry emitted by the runner includes metrics on hook success/failure rates (opt-in via diagnostic mode).

### Out of Scope
- ‚ùå Enforcing mandatory pre/post steps across all runners (use policies instead).
- ‚ùå Exposing hook output in GitHub UI workflow logs by default.
- ‚ùå Supporting matrix or per-step hooks.
- ‚ùå Remote management of hooks via GitHub API.
- ‚ùå Using hooks for security enforcement (e.g., secret scanning).

## Consequences

### Pros
- ‚úÖ Empowers self-hosted runner admins to manage environment state reliably.
- ‚úÖ Enables advanced scenarios like cache warming, network mounting, and custom monitoring.
- ‚úÖ Minimal impact on hosted runner performance or architecture.
- ‚úÖ Aligns with principle of decentralization for infrastructure-specific logic.

### Cons
- ‚ö†Ô∏è Risk of misconfiguration leading to job failures or hangs (mitigated by timeouts and logging).
- ‚ö†Ô∏è Potential for inconsistent environments if hooks are poorly written.
- ‚ö†Ô∏è Adds operational surface area‚Äîadmins must secure and maintain scripts.

## Prior Art
- Git hooks: Proven model for event-driven scripting in developer tools.
- Kubernetes Init Containers & Sidecars: Provide similar lifecycle customization for pods.
- Terraform Provisioners: Allow custom scripts during resource lifecycle (though often discouraged).

## References
- https://github.com/actions/runner/issues/1543  
- https://github.com/actions/runner/issues/699  
- https://github.com/actions/runner/issues/1116  
- Composite Actions: https://docs.github.com/en/actions/creating-actions/creating-a-composite-action  
- Reusable Workflows: https://docs.github.com/en/actions/using-workflows/reusing-workflows  

---

**Decision Owner**: Platform Infrastructure Team  
**Last Modified**: 2025-04-05",
,"**Decision:** Adopt a microservices architecture to enhance system scalability, enable independent service deployment, and support technology diversity across components.",
"## Context

Following the [adoption of actions-runner-controller by GitHub](https://github.com/actions/actions-runner-controller/discussions/2072) and the introduction of the new runner scale set autoscaling mode, we needed to provide a basic runner image that could be used off the shelf without much friction.

The [current runner image](https://github.com/actions/runner/pkgs/container/actions-runner) is published to GHCR. Each release of this image is tagged with the runner version and the most recent release is also tagged with `latest`.

While the use of `latest` is common practice, we recommend that users pin a specific version of the runner image for a predictable runtime and improved security posture. However, we still notice that a large number of end users are relying on the `latest` tag & raising issues when they encounter problems.

Add to that, the community actions-runner-controller maintainers have issued a [deprecation notice](https://github.com/actions/actions-runner-controller/issues/2056) of the `latest` tag for the existing runner images (https://github.com/orgs/actions-runner-controller/packages).","## Decision

**Deprecate the `latest` tag for the official GitHub Actions runner container image on GHCR and require explicit version pinning.**

We will:

1. **Announce deprecation of the `latest` tag**: Publish a clear deprecation notice in the [actions/runner](https://github.com/actions/runner) repository, documentation, and release notes, stating that the `latest` tag will be removed in a future release (e.g., 3 months from announcement).

2. **Stop updating the `latest` tag after a defined cutoff version**: After a specified version (e.g., v2.319.0), the `latest` tag will no longer be updated with new releases. Existing uses of `latest` will continue to pull the last version it pointed to, but users will be strongly encouraged to migrate.

3. **Update documentation and examples**: All official documentation, sample configurations, Helm charts (including actions-runner-controller references), and tutorials will be updated to use explicit version tags (e.g., `v2.318.0`) instead of `latest`.

4. **Improve user experience for version discovery**: Enhance the release process to make discovering the latest stable version easier (e.g., via GitHub Releases API, JSON endpoint, or tooling integration) to reduce friction when pinning versions.

5. **Align with actions-runner-controller changes**: Coordinate timing with the actions-runner-controller team to ensure consistent messaging and behavior across both projects, reinforcing best practices around reproducibility and security.

### Rationale

- **Predictable and Reproducible Builds**: Using `latest` leads to non-deterministic deployments where the same configuration can pull different runner versions over time, increasing the risk of unexpected breaks.
- **Improved Security Posture**: Pinning versions enables proper vulnerability management and auditability. Users are more likely to upgrade intentionally rather than being silently affected by changes in `latest`.
- **Reduced Support Burden**: Many reported issues stem from unanticipated changes due to `latest` updates. Removing it reduces noise in issue tracking and improves support efficiency.
- **Industry Best Practices**: Immutable, version-pinned images are standard in production environments and align with principles of immutable infrastructure and supply chain security (e.g., SLSA, Sigstore).

This decision supports our goal of providing a secure, reliable, and enterprise-ready runner experience while encouraging healthy operational practices across all users.","## Decision

Proceed with Option 2, keeping the `latest` tag and adding the `NOTES.txt` file to our helm charts with the notice."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","**Decision: Implement Resilient Authentication with Circuit Breaking and Fallback Mechanisms**

To ensure SS?? remains robust in the face of transient faults when communicating with third-party authentication providers (e.g., Google OAuth2), we will implement a resilient communication layer using the following patterns:

1. **Circuit Breaker Pattern**: Wrap all external authentication provider calls with a circuit breaker (e.g., using libraries like Hystrix, Resilience4j, or equivalent). This prevents cascading failures by detecting repeated failures and temporarily blocking further requests to the provider during outages, allowing time for recovery.

2. **Retry with Exponential Backoff**: For transient errors (e.g., 429 Rate Limited, 5xx errors), implement intelligent retry logic with exponential backoff and jitter. Retries are limited to a small number (e.g., 3 attempts) to avoid amplifying load during outages.

3. **Graceful Degradation & Fallback Behavior**: 
   - If authentication fails due to a provider fault, maintain existing user sessions where possible (i.e., allow currently authenticated users to continue using the system).
   - For new login attempts, present a clear, user-friendly error message indicating temporary unavailability of the identity provider, with guidance to try again later.
   - In multi-provider scenarios (if supported in the future), attempt fallback to an alternate provider if configured and available.

4. **Monitoring and Alerting**: Log authentication failures with sufficient context and emit metrics (e.g., failure rates, latency, circuit breaker state). Integrate with alerting systems to notify operators of sustained provider issues.

5. **Time-bound Timeouts**: Enforce strict timeouts on all external calls to prevent thread exhaustion and ensure fast failure detection.

This approach ensures that transient provider faults do not lead to systemic degradation of SS??. The system remains responsive, provides clear feedback to users, and automatically recovers when the provider is restored.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","**Decision: Implement Architectural Decision Records (ADRs) to Document Key Project Decisions**

We will adopt **Architectural Decision Records (ADRs)** as a lightweight, version-controlled method for documenting significant technical and strategic decisions made during the project lifecycle.

### Status
Accepted

### Context
The project involves continuous decision-making around architecture, tooling, processes, and design. Without clear documentation, these decisions become implicit knowledge, making onboarding difficult and limiting transparency for contributors. To support open development, encourage collaboration, and maintain a durable record of rationale, we need a consistent way to capture *what* was decided, *why* it was decided, and *what* alternatives were considered.

### Decision
We will write and maintain ADRs in a dedicated `/docs/adr` directory within the main repository. Each ADR will follow a structured template (e.g., Michael Nygard‚Äôs style) including:

- Title
- Status (Proposed, Accepted, Deprecated, Superseded)
- Context
- Decision
- Status
- Consequences

ADRs will be written in plain Markdown, versioned alongside code, and reviewed via pull requests. Every significant architectural or cross-cutting decision‚Äîsuch as choosing a framework, introducing a new service, or changing deployment strategy‚Äîwill be recorded as an ADR.

### Rationale
- **Transparency**: Makes decision-making visible to all team members and external contributors.
- **Onboarding Aid**: Helps new developers understand the system's evolution and constraints.
- **Historical Record**: Prevents re-litigating past decisions and supports auditability.
- **Living Documentation**: Since ADRs are stored with the code, they are more likely to be discovered and maintained.
- **Supports Open Contribution**: Enables outside contributors to understand context before proposing changes.

### Implications
- **Positive**:
  - Improved knowledge sharing and reduced bus factor.
  - Clearer accountability and traceability of decisions.
  - Encourages thoughtful evaluation before making changes.
- **Negative**:
  - Requires discipline to write and maintain records.
  - Risk of outdated status if not updated when decisions change.

To mitigate this, ADR status will be revisited during relevant refactorings or when revisiting related functionality.

### Related Decisions
None yet.

### Notes
This ADR is self-documenting ‚Äî it is the first in the series and establishes the practice itself.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up √É¬Ç√Ç¬°√É¬Ç√Ç¬™ with all relevant discussion summarized √É¬Ç√Ç¬°√É¬Ç√Ç¬™ in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","**Decision**: Adopt a URI-based addressing scheme (e.g., `ext://<namespace>/<extension-id>`) to uniquely identify and resolve extension definitions stored in decentralized storage, with extension metadata and configuration declared in EDN format.

**Rationale**:  
- **Decentralized Identifiers**: A custom URI schema enables location-independent references to extensions, supporting decentralization by decoupling identity from storage location.  
- **Human-Readable & Structured**: The `ext://` schema is intuitive and allows logical namespace organization (e.g., `ext://community/linter`, `ext://official/router`).  
- **EDN Compatibility**: EDN (Extensible Data Notation) provides a flexible, extensible format for expressing extension metadata (name, version, dependencies, entry points), aligning well with functional and data-driven architectures.  
- **Resolvability**: A resolver service can interpret the URI, locate the EDN file in decentralized storage (e.g., IPFS, Arweave), and instantiate the extension securely.  
- **Caching & Immutability**: Content-addressed storage backends ensure integrity and allow efficient caching using cryptographic hashes embedded in the URI or resolved via DNSLink/DID.

**Implications**:  
- Requires implementation of a URI resolver middleware capable of fetching and parsing remote EDN files.  
- Security considerations: validate and sandbox extension definitions to prevent malicious code execution during resolution.  
- Potential latency in extension loading due to network fetch; mitigated via caching and prefetching strategies.  
- Tooling needed for publishing and registering extensions to decentralized storage with proper URIs.

**Status**: Accepted  
**Date**: 2024-04-17","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","**Decision: Exclude group chat functionality from the MVP scope for the Q2 beta launch.**

We will focus exclusively on delivering a stable and secure 1-1 chat and public chat experience for the beta release. Group chat, while valuable in the long term, is not critical to validating the core chat experience at this stage. Given its higher complexity‚Äîparticularly around encryption, security, and implementation stability‚Äîand the current burden of outstanding regressions, including it would risk delaying the launch and diverting resources from higher-priority features.

Public chat better supports the MVP goal of enabling user engagement with minimal friction, especially by facilitating interactions between strangers in an open environment. Deferring group chat allows us to maintain a tight scope, reduce technical debt, and stay on the critical path for a successful Q2 beta launch.

Group chat will be re-evaluated for inclusion in a post-MVP roadmap based on user feedback and resource availability.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision: Implement Peer Discovery Using Discv5 Protocol

To address the scalability, reliability, and security limitations imposed by hardcoded peers, we will adopt the **Discovery v5 (Discv5)** protocol for dynamic peer discovery in the messaging layer.

### Rationale
- **Scalability**: Hardcoded peers create a static topology that does not adapt to load or network changes. Discv5 enables nodes to dynamically discover and connect to available peers, allowing the system to scale horizontally as user count increases beyond threshold N.
- **High Availability**: With Discv5, the failure or overload of individual nodes does not isolate clients. Nodes can discover alternative healthy peers autonomously, improving resilience.
- **Operational Independence**: Server peer clusters can be scaled independently of client releases. New nodes can join the network and become discoverable without requiring app updates.
- **Security & Privacy**: Discv5 supports encrypted communication during discovery and provides mechanisms for topic-based node filtering, enabling secure and targeted peer lookup while reducing exposure to malicious actors.
- **Future-Proofing**: Discv5 is actively supported in the Ethereum P2P ecosystem and aligns with decentralized architecture principles, making it a sustainable choice for long-term evolution.

### Implementation Approach
1. **Integrate Discv5 Library**: Use an established implementation (e.g., `github.com/status-im/go-discover/v5`) within the client and server nodes.
2. **Bootstrap Configuration**: Configure a small set of well-known bootstrap nodes (with static addresses) to initiate the discovery process. These are less sensitive than full peer lists due to the dynamic nature of subsequent discovery.
3. **Topic-Based Advertisement**: Server nodes will advertise themselves under specific topics (e.g., `/status/messaging/v1`) so clients can discover relevant peers efficiently.
4. **Client-Side Discovery**: Clients will use Discv5 to query for peers offering messaging services instead of relying on embedded peer lists.
5. **Rate Limiting & Security Policies**: Enforce rate limiting and identity verification (via ENR records) to mitigate discovery-layer attacks.

### Trade-offs
- **Increased Complexity**: Introducing Discv5 adds complexity to the networking stack compared to static configuration.
- **Initial Connection Latency**: First-time peer discovery may introduce slight latency versus direct connections to hardcoded peers.
- **Dependency on Bootstrap Nodes**: While reduced, there remains a dependency on initial bootstrap nodes, which must be highly available.

### Conclusion
Adopting Discv5 for peer discovery decouples client logic from infrastructure scaling, enabling a resilient, scalable, and secure messaging network. This decision aligns with our long-term goal of building a decentralized, self-healing P2P architecture.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","**Decision: Adopt BIP-39 for Mnemonic Phrase Generation and BIP-44 for Key Derivation to Ensure Cross-Wallet Compatibility**

### Status
Accepted

### Context
Currently, Status uses a non-standard method to derive cryptographic keys from a 12-word mnemonic phrase, which prevents users from importing their Status wallet into other Ethereum or cryptocurrency wallets (and vice versa). This creates friction for users who want to use the same account across multiple wallet applications.

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) proposes aligning Status with widely adopted industry standards so that a wallet created using 12 mnemonic words in Status can be imported into other standard-compliant wallets (e.g., MetaMask, Ledger, Trust Wallet), and wallets created elsewhere can be imported into Status.

The goal is improved user experience, portability, and interoperability without compromising security.

### Decision
We will adopt **BIP-39** (Bitcoin Improvement Proposal 39) for generating the mnemonic phrase and entropy-to-seed conversion, and **BIP-44** (with appropriate coin types for Ethereum and associated testnets) for hierarchical deterministic (HD) key derivation. Specifically:

- Use BIP-39 to convert the 12-word mnemonic into a 512-bit seed via PBKDF2 with HMAC-SHA512.
- Use BIP-44 derivation path `m/44'/60'/0'/0` for Ethereum mainnet and `m/44'/1'/0'/0` for Ethereum testnets (or equivalent per network).
- Support import of wallets generated externally using the same standards.
- Maintain backward compatibility by allowing existing non-BIP-39 wallets to continue functioning, but mark them as legacy. New wallets will default to BIP-39/BIP-44.

This ensures full compatibility with most third-party wallets while maintaining control over user experience and security practices.

### Consequences

**Positive**
- ‚úÖ Users can seamlessly migrate their accounts between Status and other wallets.
- ‚úÖ Improved trust and adoption due to alignment with open, well-audited standards.
- ‚úÖ Reduces user lock-in and supports the broader ethos of decentralization and self-custody.
- ‚úÖ Simplifies onboarding for users already familiar with tools like MetaMask or hardware wallets.

**Negative**
- ‚ùå Requires careful migration strategy to avoid fund loss or confusion during transition.
- ‚ùå Slight increase in complexity for handling two wallet formats (legacy vs. standard) during the coexistence period.
- ‚ùå Potential confusion if users mix derivation paths across apps; requires clear UX guidance.

**Neutral / Mitigated**
- The change does not expose new attack vectors, as BIP-39 and BIP-44 are extensively reviewed and used across the ecosystem.

### Additional Notes
- Security review of the implementation will be conducted prior to release.
- Documentation and in-app guidance will be updated to explain the change and assist users during import/export operations.
- Test vectors from the BIP-39 specification will be used to validate correctness.

### References
- BIP-39: https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki  
- BIP-44: https://github.com/bitcoin/bips/blob/master/bip-0044.mediawiki  
- EIP-84 (BIP-44 for SLIP-44 Ethereum): https://eips.ethereum.org/EIPS/eip-84  
- Idea #142: https://ideas.status.im/ideas/142-wallet-compatibility/README","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision: Adopt ULC (Ultra Light Client) as the Primary Synchronization Strategy for Status Mobile, with Multi-Node Support and Phased LES Deprecation

### Status
Accepted

### Context
The Status mobile application has encountered significant performance challenges related to blockchain synchronization, particularly when using LES (Light Ethereum Subprotocol) on Mainnet. These issues include:
- Slow or stalled sync (hot syncing), especially on low-end devices or poor networks
- High disk usage due to state and CHT growth
- Excessive bandwidth consumption
- Poor energy efficiency impacting battery life
- Degraded UI responsiveness during sync operations

While optimizations like CHT improvements and adaptive sync throttling could mitigate some LES issues, they do not fundamentally resolve the scalability limitations of the light client model in resource-constrained environments.

Relying on centralized endpoints like Infura addresses immediate usability concerns but undermines core Ethereum principles by introducing trust dependencies, reducing censorship resistance, and weakening user sovereignty.

ULC (Ultra Light Client), a synchronization protocol under active development, offers a promising alternative. It enables extremely fast initial sync, minimal resource usage, and crucially, supports multi-node validation ‚Äî allowing clients to cross-check block headers across independently operated nodes, significantly improving security and decentralization compared to single-trust-point models.

### Decision
We will prioritize the integration and stabilization of ULC as the default blockchain synchronization method for Status Mobile, with the following key components:

1. **Implement ULC as Default Sync Mode**  
   Replace Infura fallback and LES as the primary sync mechanism with ULC once it reaches production readiness (target: next beta cycle).

2. **Enable Multi-Node ULC Configuration**  
   Allow users to configure multiple upstream ULC server endpoints. The client will validate consensus by comparing block headers from at least 2‚Äì3 nodes before accepting chain progression.

3. **Support User-Run Nodes**  
   Maintain and improve support for pairing with locally run or privately hosted Ethereum nodes (via LES or devp2p), giving advanced users full control over their trust model.

4. **Deprecate LES Gradually**  
   Continue limited LES support during transition but mark it as deprecated. Reduce sync aggressiveness in current LES implementation to minimize resource impact until ULC is stable.

5. **Monitor and Optimize ULC Security Assumptions**  
   Actively audit ULC‚Äôs threat model, particularly around long-range attacks and weak subjectivity, and implement mitigations such as secure checkpoint distribution and social verification prompts.

### Consequences
**Positive:**
- Drastically improved sync speed and reduced resource consumption (bandwidth, CPU, battery)
- Better UX on mobile networks and low-end devices
- Increased decentralization through multi-node trust model
- Reduced reliance on centralized infrastructure (e.g., Infura)
- Alignment with long-term vision of trust-minimized, user-operated clients

**Negative:**
- ULC is still evolving; requires close collaboration with Nimbus and Eth1.x teams
- Initial setup requires trusted checkpoints (weak subjectivity); must be managed securely
- Smaller attack surface than full nodes, but larger than ideal ‚Äî necessitates careful validation logic
- Transition period may require maintaining multiple sync codepaths

**Mitigations:**
- Partner closely with Nimbus team on ULC specification and implementation
- Distribute genesis and checkpoint data via multiple authenticated channels
- Implement clear UI indicators for sync status and node health
- Provide opt-in options for power users to connect to their own nodes

### References
- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient
- https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie
- [Nimbus ULC Research](https://github.com/status-im/nimbus-eth1/tree/devel/docs/ulc)","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","# Decision: Introduce a `SensitiveString` Type to Prevent Accidental Logging of Secrets

## Status
Accepted

## Context
We have sensitive data such as user passwords that are currently passed around as regular strings. This creates a risk of accidental exposure when developers inadvertently include these values in log statements. As demonstrated in https://github.com/status-im/status-mobile/issues/4053, sensitive information can be leaked through debugging logs, which poses a security risk and may violate privacy regulations.

The current approach relies on developer discipline to remember not to log sensitive values, which is error-prone and doesn't scale well across a large codebase with multiple contributors.

## Decision
We will create a `SensitiveString` (or similar named) wrapper type that encapsulates sensitive string data and prevents accidental exposure by:

1. Implementing the wrapper to override `toString()` methods to return a placeholder like `[REDACTED]` or `<sensitive>` instead of the actual value
2. Making the wrapper opaque by not providing easy access to the underlying string value
3. Requiring explicit, intentional actions to extract the actual value when needed
4. Adding compiler/runtime warnings when attempting to concatenate with regular strings
5. Potentially integrating with logging frameworks to automatically redact values of this type

We will migrate existing sensitive string parameters (particularly passwords, tokens, keys) to use this wrapper type throughout the codebase.

## Rationale
This approach provides several benefits:
- **Security by design**: Makes it difficult to accidentally expose sensitive data
- **Explicit intent**: Developers must intentionally unwrap the value, making security implications clear
- **Consistency**: Provides a standardized way to handle sensitive data across the codebase
- **Discoverability**: The type name itself serves as documentation that the data is sensitive
- **Future extensibility**: Can add additional security features like automatic secure wiping of memory

Alternative approaches considered:
- **String prefixes/suffixes** (e.g., ""PASSWORD:xxx""): Easily bypassed and doesn't prevent logging
- **Code linting rules**: Helpful but reactive rather than preventative
- **Manual redaction functions**: Relies on developer memory and is inconsistent

## Implications
### Positive
- Reduced risk of accidental sensitive data exposure in logs
- Improved code clarity about which data is sensitive
- Better compliance with data protection regulations
- Educational benefit for developers about handling sensitive data

### Negative
- Additional cognitive overhead for developers working with the type
- Slight performance cost from wrapping/unwrapping
- Migration effort required to update existing code
- Potential for developers to work around the protection if they find it inconvenient

### Required Follow-up Actions
1. Implement the `SensitiveString` type with appropriate language-specific characteristics
2. Create migration guide for converting existing sensitive strings
3. Update coding guidelines to document proper use of sensitive data types
4. Add the type to relevant API boundaries and data transfer objects
5. Consider extending the pattern to other sensitive data types (e.g., `SensitiveBytes`)","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

This change describes a methodology of development which we believe will help
us increase the confidence and speed in the development process.","**Decision**: Adopt Test-Driven Development (TDD) as the standard methodology for software development.  

This approach involves writing automated tests before implementing the corresponding code, ensuring that all functionality is validated from the outset. By integrating TDD into our development workflow, we aim to improve code quality, reduce bugs, facilitate refactoring, and increase confidence in the correctness of the system. This decision supports faster iteration with reduced risk, aligning with our goals of increasing both development speed and reliability.",
"## Context

Classical release approach: we cut off the release branch, stabilize it, fix every critical issue and release. Some changes are cherry-picked to the release branch.

It has a couple of downsides:

- Resources are scattered across two branches;

- Unique fixes and unique combination of commits in the release branch;

- Unpredictable release schedule (any day can be a release day!).","## Decision: Adopt a Trunk-Based Development Model with Feature Flags

To address the limitations of the classical release approach‚Äîscattered resources, unique fix combinations, and unpredictable scheduling‚Äîwe will transition to **Trunk-Based Development (TBD)** supported by **Feature Flags**.

### Rationale

1. **Centralized Development Effort**:  
   All development occurs on the main branch (`main` or `trunk`), reducing context switching and minimizing divergence between branches. This eliminates the need for long-lived release branches and cherry-picking.

2. **Stable and Predictable Releases**:  
   By using feature flags, incomplete or experimental features can be merged into `main` but remain disabled in production. This allows for regular, time-based releases (e.g., weekly or bi-weekly) without blocking on feature completion.

3. **Consistent Codebase State**:  
   Avoids unique commit combinations in release branches. Every release is a known state of the trunk, improving reproducibility and auditability.

4. **Improved Collaboration and CI/CD Integration**:  
   Encourages small, frequent commits and continuous integration. Automated pipelines validate every change on `main`, increasing code quality and reducing integration risk.

5. **Controlled Rollouts and Risk Mitigation**:  
   Feature flags enable gradual rollouts, A/B testing, and instant disablement of problematic features‚Äîwithout requiring emergency patches or hotfix branches.

### Implementation Plan

- Enforce short-lived feature branches (max 1‚Äì2 days old).
- Introduce a feature flag management system (e.g., LaunchDarkly, Flagsmith, or custom solution).
- Update CI/CD pipeline to deploy `main` to staging environments continuously.
- Schedule regular production releases (e.g., every Tuesday).
- Establish rollback procedures via flag toggling and automated deployment rollback.
- Train teams on TBD practices and flag hygiene (e.g., cleaning up stale flags).

### Trade-offs

- **Increased complexity in testing**: Multiple flag combinations must be tested.
- **Runtime overhead**: Minimal performance impact from flag evaluation.
- **Discipline required**: Teams must avoid ""flag sprawl"" and ensure timely cleanup.

Despite these, the benefits in release predictability, team focus, and deployment safety outweigh the costs.

### Outcome

Predictable release cycles, reduced operational overhead, faster feedback loops, and improved software delivery velocity.",
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","# Architectural Decision Record: Deprecate and Remove 3rd Party Status Command Extensions via JavaScript Jail

## Status
**Accepted**

## Context
The original architecture for supporting 3rd party command extensions in Status utilized a jailed JavaScript execution environment (""Jail"") paired with a minimal library exposing a chat commands API. This design was intended to allow external developers to extend Status functionality safely and dynamically.

However, in practice, this approach has proven problematic:

- The API design favored mutable state over the functional, stateless paradigms of the host application (ClojureScript/re-frame), leading to poor integration and maintainability.
- Complex dynamic features (e.g., live transaction details in `/send`) could not be implemented effectively within the jail, forcing the team to implement core logic directly in native app code‚Äîundermining the purpose of the extension system.
- All existing commands now rely on hardcoded application logic, making the jail layer redundant and increasing technical debt through scattered ""magic"" strings and asynchronous bridge calls.
- The constant communication between React Native and the jailed JS environment introduces performance overhead and complicates reasoning about control flow.
- Despite the investment, there has been no real adoption by third-party developers; all published dapps have used standard web3.js and functioned as regular Ethereum dapps.
- A new, more powerful extension framework‚ÄîProject Pluto‚Äîis emerging, offering declarative, secure, and deeply integrated extension capabilities beyond just command handling (e.g., wallet assets, contacts, UI hooks).

## Decision
We will **deprecate and fully remove** the JavaScript Jail-based 3rd party command extension system.

Going forward:
1. All command logic will reside entirely within the host application (ClojureScript/re-frame).
2. Existing jail-dependent command components will be refactored into pure, testable, synchronous functions using the host stack.
3. Future extensibility will be handled exclusively through **Project Pluto**, which provides a modern, secure, and performant model for integrating extensions declaratively and safely.

A migration path will be provided for any internal or partner use cases currently relying on jail scripts, translating them into native command implementations.

## Consequences
### Positive
- ‚úÖ Eliminates performance bottlenecks from RN Bridge ping-pong.
- ‚úÖ Enables full unit testing and debugging of command logic.
- ‚úÖ Reduces complexity and tech debt by removing an underused, poorly maintained subsystem.
- ‚úÖ Aligns implementation with re-frame‚Äôs functional architecture (‚Äúeating our own dogfood‚Äù).
- ‚úÖ Paves the way for richer, safer, and better-designed extensibility via Project Pluto.

### Negative
- ‚ùå Breaks backward compatibility for any theoretical or undocumented uses of the jail API.
- ‚ùå Requires effort to migrate existing pseudo-extensions into native code (though most were already partially hardcoded).
- ‚ùå Delays potential future bot integrations until Project Pluto is production-ready.

### Neutral
- No known third-party bots are affected, given lack of actual adoption.

## Notes
This decision aligns with our broader platform strategy: favor correctness, performance, and maintainability over speculative openness when usage does not justify complexity. Once Project Pluto reaches maturity, we will reconsider public extensibility‚Äîbut with stronger foundations and clearer boundaries.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision: Use Embedded Collections with Proper Normalization Context for Product Option Values

We will represent Product Option values as an embedded collection within the Product resource, leveraging API Platform's normalization contexts and serialization groups to efficiently handle the relationship.

### Rationale

1. **User Experience**: Clients interacting with the Product API expect to see option values directly when retrieving product details, rather than making additional requests to separate endpoints.

2. **Performance**: Embedding option values reduces the number of HTTP requests needed to retrieve complete product information, improving perceived performance and reducing network overhead.

3. **API Platform Capabilities**: This approach leverages API Platform's built-in support for:
   - Serialization groups via `@ApiResource(normalizationContext={})`
   - Doctrine associations
   - Automatic embedding of related entities

4. **Maintainability**: The solution follows API Platform conventions, making it easier for developers familiar with the framework to understand and extend.

### Implementation

```php
/**
 * @ApiResource(
 *     normalizationContext={""groups""={""product:read"", ""product_option_value:read""}},
 *     denormalizationContext={""groups""={""product:write""}}
 * )
 */
class Product
{
    /**
     * @Groups({""product:read"", ""product:write""})
     */
    private $name;

    /**
     * @Groups({""product:read"", ""product:write""})
     * @ORM\OneToMany(targetEntity=""ProductOptionValue"", mappedBy=""product"")
     */
    private $optionValues;
}

/**
 * @ApiResource(
 *     normalizationContext={""groups""={""product_option_value:read""}},
 *     denormalizationContext={""groups""={""product_option_value:write""}}
 * )
 */
class ProductOptionValue
{
    /**
     * @Groups({""product_option_value:read"", ""product_option_value:write""})
     */
    private $value;

    /**
     * @Groups({""product_option_value:read"", ""product_option_value:write""})
     * @ORM\ManyToOne(targetEntity=""ProductOption"")
     */
    private $option;
}
```

### Benefits

- Single endpoint provides complete product data including options
- Natural JSON structure that matches client expectations
- Supports filtering and pagination through API Platform's built-in mechanisms
- Maintains proper separation of concerns while providing efficient data access

### Considerations

- For products with many options/values, consider implementing lazy loading or pagination at the collection level
- Monitor payload sizes and implement field filtering if needed for performance
- Document the embedded structure clearly in the API documentation","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

We want our API to cover all the functionalities that are currently implemented in the UI.","## Decision

Implement a comprehensive API layer that mirrors all existing UI functionalities by adopting a feature-aligned, contract-first design approach.

We will define the API endpoints, request/response schemas, and error handling patterns using OpenAPI Specification (OAS) to ensure full parity with current UI capabilities. Each user interaction available in the frontend‚Äîsuch as data creation, retrieval, updates, deletions, searches, filters, and bulk operations‚Äîwill be systematically mapped to corresponding RESTful or GraphQL operations.

The implementation will follow these principles:
- **Completeness**: Every UI action must have an equivalent API endpoint.
- **Consistency**: Use uniform naming, versioning (`/api/v1/`), and status codes across all endpoints.
- **Discoverability**: Publish interactive API documentation using tools like Swagger UI or Redoc.
- **Security**: Apply the same authentication and authorization mechanisms (e.g., JWT, RBAC) used by the UI.
- **Test Coverage**: Include automated tests validating that API behavior matches UI-level functionality.

This decision ensures that users and integrators can perform any task via the API that they can through the UI, enabling automation, third-party integrations, and future headless use cases.","## Decision Drivers

* All the functionalities implemented for API should be tested
* Tracking whether a feature has been covered in the API or not should be easy"
"## Context and Problem Statement
While developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach
was introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,
we have merged a unification of our endpoints in https://github.com/Sylius/Sylius/pull/11397. This move was dictated by 
the fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different
data for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)
and simplify the relations between resources. However, this change rose a lot of new issues to solve:

 * Different serialization for different users is not trivial. Some relations between objects are also contextual, so 
admin can see many more links than the regular visitor or shop user. 
 * Endpoints like ""Product Show"" are resolved differently depending on the context as well. The shop products are determined 
based on the slug (taking into account the current channel), while the administrator is browsing products by code. This
separation blurs the product identifier, and it is not clear which route should be considered as IRI. 
 * the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the
decision that we want to be explicit, which user is trying to log in.
 
Using one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able
to expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin 
resources are channel and locale agnostic, and we should use `codes` instead.","### Decision: Implement Context-Aware API Endpoints with Shared Base Paths and Conditional Serialization

#### Status
Accepted

#### Context
The unification of API endpoints in https://github.com/Sylius/Sylius/pull/11397 aimed to simplify the API surface by merging admin and shop endpoints under common paths. However, this introduced complexity due to divergent requirements:
- **Different identifiers**: Shop uses `slug` (channel- and locale-sensitive), while admin uses `code` (agnostic).
- **Contextual serialization**: Admin users require more detailed relationships and metadata than shop users.
- **Authentication separation**: Login flows must remain distinct due to different user roles and security concerns.
- **IRI ambiguity**: A single canonical IRI per resource cannot satisfy both contexts without compromising usability or semantics.

Attempts at full unification have proven impractical because they obscure semantic meaning and complicate routing, serialization, and access control.

#### Decision
Reintroduce **logically separated endpoint groups** for shop and admin APIs using distinct URL prefixes (`/api/v2/shop` and `/api/v2/admin`), while sharing underlying resources and business logic. This approach enables:

1. **Clear Identifier Semantics**:
   - Shop endpoints use `slug` as the primary identifier (e.g., `GET /api/v2/shop/products/{slug}`).
   - Admin endpoints use `code` as the identifier (e.g., `GET /api/v2/admin/products/{code}`).

2. **Context-Specific Serialization**:
   - Use Symfony Serializer with dynamic groups or custom normalization based on the current user role and route prefix.
   - Define separate serialization groups for `shop:read`, `admin:read`, etc., ensuring only relevant data and relations are exposed.

3. **Shared Resource Model with Route Isolation**:
   - Maintain a unified domain model and service layer.
   - Separate controllers or controller methods per context to handle routing and input/output formatting appropriately.

4. **Preserve Independent Authentication**:
   - Retain separate login endpoints:
     - `POST /api/v2/shop/authentication_token` (for shop users)
     - `POST /api/v2/admin/authentication_token` (for admin users)

5. **Canonical IRI Strategy**:
   - Accept that IRIs are context-dependent.
   - For Hydra documentation and client consumption, generate context-appropriate IRIs dynamically.
   - Optionally introduce a centralized resource mapper to resolve cross-context references where needed (e.g., admin UI linking to shop preview URLs).

#### Rationale
This decision balances the benefits of code reuse and architectural consistency with practical usability and clarity:
- Avoids ambiguous identifiers and over-complicated conditional logic in a single endpoint.
- Enables better performance through tailored responses (avoiding over-fetching in shop context).
- Improves developer experience: clients know which endpoint set to use based on their role.
- Aligns with REST best practices by making resource addressing explicit and predictable within each context.

While it slightly increases the number of endpoints, the trade-off is justified by improved maintainability, clarity, and scalability.

#### Consequences
**Pros**:
- Clear separation of concerns between shop and admin API usage.
- Predictable, semantically meaningful URLs and identifiers.
- Easier to document, test, and evolve independently.
- Supports future extensions like versioning or audience-specific optimizations.

**Cons**:
- Slight duplication in routing and controller layers (mitigated via shared services).
- Clients must choose the correct endpoint prefix based on user role.
- Requires careful coordination when evolving shared models.

#### Additional Notes
- Consider introducing an OpenAPI specification per context (`shop.yaml`, `admin.yaml`) for clearer documentation.
- Monitor usage patterns; long-term, evaluate if a GraphQL layer could unify reads across contexts while preserving flexibility.

---

*Date: 2025-04-05*  
*Authors: Architectural Review Board, Sylius Core Team*","## Decision Drivers

* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.
One resource in the database may aggregate several API Resources
* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources
* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected"
"## Context and Problem Statement

While defining new handlers, we may define a bus to which particular handler should be assigned to. If not, handler will
be added to all existing buses. Right now, we have only one bus in Sylius, so decision will not change current behaviour.
However, we may add additional buses in the future and more than one bus is not uncommon thing.","## Decision

**Explicitly assign handlers to specific message buses using configuration, rather than relying on implicit registration to all buses.**

### Rationale

- **Future-proofing**: Although Sylius currently has only one message bus, this decision anticipates the addition of multiple buses (e.g., `command.bus`, `event.bus`, `query.bus`) in the future.
- **Clarity and Intent**: Explicit assignment makes it clear which bus a handler is intended for, improving code readability and maintainability.
- **Prevent Unintended Behavior**: Without explicit assignment, a handler could be registered to multiple buses unintentionally, potentially causing unexpected message processing or performance issues.
- **Consistency with Best Practices**: Modern messaging architectures (e.g., Symfony Messenger) encourage explicit routing of messages to specific buses.

### Implementation

Use configuration (e.g., YAML, attributes, or compiler passes) to define which bus a handler belongs to. Example:

```yaml
# config/packages/messenger.yaml
framework:
    messenger:
        buses:
            command_bus: ~
            event_bus: ~

        routing:
            'App\Message\CreateProduct': command_bus
            'App\Message\ProductCreated': event_bus
```

Or via attributes (if supported):

```php
#[AsMessageHandler(bus: 'command_bus')]
class CreateProductHandler { /* ... */ }
```

### Consequences

- ‚úÖ Ensures predictable behavior when scaling to multiple buses.
- ‚úÖ No impact on current functionality (single bus remains unaffected).
- ‚ö†Ô∏è Requires discipline to always specify the bus; fallback to ""all buses"" should be avoided.
- ‚úÖ Supports clean separation of concerns across different messaging domains.

This decision ensures architectural scalability and clarity without disrupting existing behavior.","## Decision Drivers <!-- optional -->

* Driver 1 - easiness of changing default behaviour and usage of the bus
* Driver 2 - backward compatible possibility of changing behaviour"
"## Context and Problem Statement

To achieve 100% API coverage, we need to handle emails by API.","## Decision

**Implement an Email API Gateway to handle all email-related functionality via RESTful endpoints.**

We will design and implement a dedicated Email API that allows all email sending, receiving, tracking, and management operations to be performed through standardized API calls. This includes:

- Sending emails (individual and bulk)
- Receiving and retrieving emails
- Managing templates
- Tracking delivery status and open rates
- Handling attachments
- Managing email configurations and preferences

The Email API will serve as the single interface for all email interactions within the system, ensuring complete API coverage for email functionality. This approach enables better testability, monitoring, automation, and integration with other services while maintaining consistency with our API-first architecture.

This decision ensures we achieve 100% API coverage for email handling by eliminating any direct SMTP or email client integrations in favor of controlled API access.","## Decision Outcome

Chosen option: ""Using events"", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport."
"## Context and Problem Statement

Initial implementation of Unified API used `/new-api` prefix, to aggregate all following endpoints. This prefix does not 
clearly state the version of it and is not future-proof. At some moment of time our ""new api"" can become ""old api"". We should 
have clear guidance how to version our APIs.","## Decision

Adopt a versioned URL prefix for the Unified API using semantic versioning in the path, specifically `/api/v1`, to replace the current `/new-api` prefix.

### Rationale

- **Clarity and Expectation**: The term `/new-api` is ambiguous and transient. What is ""new"" today will not be tomorrow. Using a versioned path like `/v1` clearly indicates the API version and sets proper expectations for consumers.
- **Scalability**: Versioned endpoints allow for future iterations (e.g., `/v2`, `/v3`) without breaking existing integrations. This supports backward compatibility and smooth deprecation cycles.
- **Industry Standard**: Most modern APIs (e.g., GitHub, Stripe, Google APIs) use versioned paths (`/v1`, `/v2`) as a best practice for API evolution.
- **Routing Simplicity**: Versioned prefixes simplify routing logic in gateways, load balancers, and backend services when multiple versions are maintained concurrently.

### Implementation

1. Introduce the new base path: `/api/v1`.
2. Migrate all current endpoints from `/new-api/{endpoint}` to `/api/v1/{endpoint}`.
3. Maintain a redirect or dual-run phase (optional) to support clients during transition.
4. Deprecate `/new-api` after a defined grace period with appropriate communication.
5. Document the versioning policy in the API documentation, including deprecation timelines and upgrade guidance.

### Consequences

- **Positive**:
  - Clear versioning strategy improves maintainability and developer experience.
  - Enables parallel support of multiple API versions.
  - Professionalizes the API surface for internal and external consumers.

- **Negative**:
  - Requires updating all client integrations to point to the new path.
  - Short-term operational overhead during migration.

### Future Considerations

- Evaluate header-based versioning if path-based becomes limiting (though path-based is preferred for simplicity and visibility).
- Automate version lifecycle management (deprecation notices, monitoring usage by version).

---

**Status**: Accepted  
**Date**: 2025-04-05  
**Relevant Stakeholders**: API Team, Backend Engineers, Product Managers, DevOps","## Decision Outcome

As the underlaying technology, structure and content have changed significantly and taking into account easiness of first solution
the decision is to go with the `/api/v2` endpoint path. In the future it does not block us from the usage of the `Accept`
header in addition to this path, however it may be misleading for consumers."
"## Context and Problem Statement

API Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.
While we were designing Sylius new API, we have decided to use commands in many endpoints.
In some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`/`id`.
In the past, we have been using `code`/`id` instead of IRI, then we have been using both approaches.
Now we are trying to unify the new API and replace codes and ids with IRI everywhere.
The main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`/`code`in its commands and command handlers.","**Architectural Decision Record (ADR)**

---

### **Title**  
Use IRI-to-Identifier Resolvers in Command-Based API Endpoints

### **Status**  
Accepted

### **Context**

API Platform promotes the use of IRIs (Internationalized Resource Identifiers) as identifiers because they provide semantic clarity and self-description by embedding the full resource path (e.g., `/api/products/123`). This improves discoverability, interoperability, and aligns with REST and JSON-LD best practices.

In Sylius‚Äôs new API design, we leverage **command-based endpoints** to gain fine-grained control over business logic during operations such as creation or state transitions. However, commands and their handlers typically expect simple identifiers like `id` or `code`, not full IRIs. 

Historically, the API accepted both raw identifiers (`id`, `code`) and IRIs inconsistently across endpoints. With our goal to **unify the API around IRIs**, a challenge arises: how to reconcile IRI usage in HTTP requests with internal command models that operate on scalar identifiers.

The problem is twofold:
1. Clients send IRIs (e.g., in relationships or parameters).
2. Commands expect resolved `id` or `code` values (e.g., `UpdateProductCommand(string $code)`).

We need a clean, reusable, and decoupled way to resolve IRIs to internal identifiers before command dispatching.

---

### **Decision**

Introduce an **IriToIdentifierResolver** service (or chain of resolvers) responsible for transforming IRIs into domain-specific identifiers (`id`, `code`, etc.) used in commands. This resolver will be used in request denormalization or within message buses (e.g., via middleware) when handling command-based API endpoints.

Additionally:
- Use **Symfony‚Äôs Serializer** with custom denormalizers that automatically resolve IRIs to scalar identifiers based on type hints and metadata.
- Leverage **API Platform‚Äôs IRI converter** (`ApiPlatform\Core\Bridge\Symfony\Routing\IriConverter`) where possible to convert IRIs to resources, then extract the relevant identifier.
- For commands expecting non-UUID identifiers (e.g., `code`), ensure the resolver can extract the correct field from the resolved entity.

Example:
```json
{
  ""product"": ""/api/products/SWEATER""
}
```
‚Üí Denormalized ‚Üí `new CreatePricingRuleCommand('SWEATER', ...)`  
where `'SWEATER'` is the `code`.

This transformation happens transparently during deserialization or in a lightweight command bus middleware.

---

### **Alternatives Considered**

| Option | Description | Pros | Cons |
|-------|-------------|------|------|
| **Pass IRIs directly into commands** | Modify all commands to accept IRIs and resolve them inside handlers | Uniform input format | Pollutes domain logic with infrastructure concerns; tight coupling |
| **Resolve IRIs in controllers** | Handle resolution explicitly in each action | Full control | Duplicated logic; violates DRY; harder to maintain |
| **Use Doctrine identifiers only** | Switch everything to UUIDs and use IRI-to-entity resolution via ORM | Simpler mapping (IRI ‚Üí entity ‚Üí id) | Forces UUID usage; breaks compatibility with `code`-based domains |
| ‚úÖ **IriToIdentifierResolver + Denormalizer** | Transparent resolution during deserialization | Clean separation; reusable; follows Symfony/API Platform patterns | Requires initial setup and configuration |

---

### **Consequences**

**Positive**
- ‚úÖ **Consistent API contract**: Clients always use IRIs ‚Äî predictable and standardized.
- ‚úÖ **Clean domain layer**: Commands remain agnostic of IRI resolution logic.
- ‚úÖ **Reusability**: Resolver can be reused across commands, entities, and bounded contexts.
- ‚úÖ **Standards compliance**: Aligns with JSON-LD, Hydra, and HATEOAS principles promoted by API Platform.

**Negative**
- ‚ö†Ô∏è **Increased complexity in serialization stack**: Custom denormalizers require careful testing.
- ‚ö†Ô∏è **Performance overhead**: Extra database lookup when resolving IRIs to entities (can be mitigated with caching).
- ‚ö†Ô∏è **Learning curve**: Developers must understand how IRI resolution works under the hood.

---

### **Implementation Steps**

1. Create `IriToIdentifierResolver` interface with method `resolve(string $iri, string $identifierType = 'id'): string|int`.
2. Implement resolver using API Platform‚Äôs `IriConverter` and entity repositories.
3. Build a **custom denormalizer** for commands that detects IRI fields and resolves them using the above service.
4. Optionally, add a **command bus middleware** that inspects command properties annotated with `#[IriResolved]` and resolves them.
5. Update documentation and provide examples of IRI usage in payloads.
6. Deprecate direct use of `id`/`code` in favor of IRIs in all new API endpoints.

---

### **Related Decisions**
- [ADR 001: Adopt API Platform for Sylius API]
- [ADR 005: Use CQRS in Sylius API]

---

### **Notes**
Future work may include supporting **batch IRI resolution** for performance optimization and introducing **stronger typing via attributes** (e.g., `#[AsIri] string $productIri`).

--- 

**Decision Owner:** Lead Architect  
**Date:** 2025-04-05","## Decision Outcome

Chosen option: ""Handling and transforming IRI to `id`/`code`"". Request that is based on command and needed information like `code`/`id` should get it as IRI"
"## Context and Problem Statement

Sylius is by design application that will be adjusted to customer needs. 
Therefore each part of it has to be crafted with replaceability or customizability at its core. 
Nonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource. 
In terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.","## Decision: Enable Granular and Composable API Resource Configuration in Sylius

### Status
Accepted

### Context
Sylius is designed as a highly customizable e-commerce platform, where extensibility and replaceability of components are first-class concerns. However, the current integration with API Platform requires developers to override an entire resource configuration (e.g., via `api_resources.yaml`) when customizing even a single aspect‚Äîsuch as changing serialization groups, adding operations, or modifying normalization contexts‚Äîfor complex resources like `Order` or `Customer`.

This all-or-nothing approach leads to several issues:
- **High maintenance burden**: Users must duplicate large portions of configuration, increasing the risk of drift when upgrading Sylius.
- **Merge conflicts**: Customizations become fragile during upgrades, especially when core resource configurations evolve.
- **Poor composability**: It‚Äôs difficult to apply modular, additive changes (e.g., enabling read-only endpoints for Customers without disrupting existing behavior).

These problems contradict Sylius‚Äôs philosophy of being easily adaptable while remaining maintainable.

### Decision
Refactor the API Platform integration to support **granular, composable configuration** by:
1. **Splitting monolithic resource definitions** into smaller, reusable fragments (e.g., per operation or concern).
2. Leveraging **Symfony‚Äôs dependency injection and configuration merging** to allow users to *extend* rather than *replace* resource configurations.
3. Introducing **extension points via interfaces or tagged services** that let plugins or applications contribute partial configurations (e.g., ""add this subresource"" or ""append this denormalization group"").
4. Using **configuration pre-processors** in the kernel boot phase to assemble final API resource metadata from multiple sources.

For example, instead of requiring full duplication:

```yaml
# Before ‚Äì full override required
App\Entity\Order:
    collectionOperations:
        get: { ... }
        post: { ... }
    # Must copy-paste everything just to add one subresource
```

Users should be able to write:

```yaml
# After ‚Äì additive customization
sylius.api.extensions:
    order_subresources:
        class: App\Entity\Order
        subresources:
            shipments: { operation: { method: 'GET' } }
```

Or use PHP config/services to inject modifications programmatically.

Additionally, provide **clear extension APIs** in core (e.g., `ApiResourceExtensionInterface`) so that third-party bundles can contribute safely.

### Consequences
**Pros:**
- ‚úÖ Aligns with Sylius's extensibility principles.
- ‚úÖ Reduces configuration duplication and technical debt.
- ‚úÖ Enables safer upgrades and better interoperability between plugins.
- ‚úÖ Encourages community contributions through clean extension points.

**Cons:**
- ‚ö†Ô∏è Increased complexity in the configuration resolution logic.
- ‚ö†Ô∏è Requires careful documentation to guide users on preferred customization patterns.
- ‚ö†Ô∏è Migration path needed for existing overrides (provide deprecation notices and codemods if possible).

**Long-term Impact:**
This change establishes a sustainable model for API evolution in Sylius, making it easier for enterprises and plugin developers to adapt the API without forking or brittle overrides.

---

*Related Issues:*  
- sylius/sylius#15892 ‚Äì ‚ÄúCannot extend API resource without full override‚Äù  
- sylius/api-bundle#45 ‚Äì ‚ÄúProposal: pluggable API configuration system‚Äù","## Decision Outcome

Chosen option: ""Config merging"", because it allows us easily overwrite any endpoint, without getting deep into api platform resources."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision: Implement Locale-Specific Resource Responses with Client-Driven Locale Negotiation

### Status
Accepted

### Context
In Sylius, many resources (e.g., products, taxons, payment methods, shipping methods) contain translatable fields such as names, descriptions, and labels. Currently, API responses often return all available translations for these entities, placing the burden on frontend developers to select the appropriate locale based on user context. This leads to:

- Increased payload sizes.
- Inconsistent developer experience across endpoints.
- Redundant logic in frontend applications.
- Potential for incorrect localization if client-side logic is flawed.

Additionally, some endpoints already embed a single translation (usually based on store or admin context), creating inconsistency in API behavior.

To improve developer experience, reduce payload size, and ensure accurate localization, we need a standardized and predictable way to serve localized content.

### Decision
We will modify the API to return only the translation corresponding to the requested locale by default, using **client-driven locale negotiation** via the `Accept-Language` header or an explicit `?locale=xx_XX` query parameter (with query param taking precedence).

#### Key Changes:
1. **Default Behavior**: 
   - The API will respond with data localized to the best-matching locale derived from:
     - First: `locale` query parameter (if provided and valid).
     - Fallback: `Accept-Language` header.
     - Final fallback: Store's default locale.
   - Only the relevant translation will be embedded in the resource response.

2. **Translation Omission**:
   - Translatable fields (e.g., `name`, `description`) will no longer appear in a `translations` object unless explicitly requested via a query parameter like `?include=translations`.

3. **Consistency Across Endpoints**:
   - All REST and GraphQL endpoints serving translatable resources will follow this pattern.
   - Existing endpoints returning full translation arrays will be updated in a backward-compatible way (via versioning or gradual migration).

4. **Backend Implementation**:
   - Use Symfony‚Äôs `RequestLocaleResolver` enhanced with query parameter support.
   - Inject locale into Doctrine queries via custom extensions or entity repositories to fetch only the needed translation.
   - Ensure proper caching keys include the locale.

5. **Documentation & DX**:
   - Clearly document the locale resolution strategy.
   - Provide examples of how to request specific locales.
   - Include error responses for invalid locale codes.

### Example
**Request:**
```
GET /api/v2/products/123?locale=fr_FR
Accept: application/json
```

**Response:**
```json
{
  ""id"": 123,
  ""name"": ""Chemise en coton"",
  ""description"": ""Une chemise confortable faite de coton biologique."",
  ""price"": 2999
}
```

### Consequences
**Pros:**
- ‚úÖ Smaller, more efficient payloads.
- ‚úÖ Predictable and consistent API behavior.
- ‚úÖ Improved frontend developer experience ‚Äî no need to parse translations.
- ‚úÖ Better alignment with HTTP standards (`Accept-Language` usage).
- ‚úÖ Easier cacheability per locale.

**Cons:**
- ‚ö†Ô∏è Requires changes across multiple endpoints (migration effort).
- ‚ö†Ô∏è Potential breaking change if not managed via versioning or feature flags.
- ‚ö†Ô∏è Clients needing multiple translations must now make additional requests or use `?include=translations`.

### Mitigations
- Roll out changes incrementally using API versioning (`/api/v2/...`).
- Deprecate old behavior with clear timelines.
- Provide tools/middleware to assist in testing different locales.

### See Also
- [RFC 7231: Accept-Language](https://tools.ietf.org/html/rfc7231#section-5.3.5)
- Symfony Translation Component documentation
- Sylius TranslatableInterface and Gedmo Translatable integration

---

*This decision aligns Sylius with modern API design principles for internationalization and improves scalability and usability for headless commerce implementations.*","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context and Problem Statement
Customer should have access only to locales available in their channel","**Decision: Implement Locale Access Control at the Channel Level**

**Context and Problem Statement:**  
Customers should only have access to locales that are explicitly available in their assigned channel. This ensures that users do not view or interact with content in languages or regional settings that are not supported by their specific channel, maintaining consistency, compliance, and localization accuracy.

**Decision:**  
Enforce locale access control by validating and restricting the available locales based on the customer‚Äôs channel configuration. This will be implemented through:

1. **Channel-Scoped Locale Configuration:**  
   Each channel defines a whitelist of supported locales (e.g., `en-US`, `fr-FR`, `de-DE`). This list is managed in the channel settings.

2. **Runtime Locale Validation:**  
   During customer session initialization, the system retrieves the list of allowed locales from the associated channel. Any locale request (via URL, header, or user preference) is validated against this list.

3. **Fallback Mechanism:**  
   If a requested locale is not available in the channel, the system defaults to the channel‚Äôs primary locale.

4. **API and UI Enforcement:**  
   - APIs return `404` or `400` for requests specifying unsupported locales.
   - The frontend dynamically loads only the locales available in the channel (e.g., language switcher dropdown).

5. **Authentication & Authorization Integration:**  
   The customer‚Äôs channel membership is resolved during authentication (e.g., via JWT or session context), ensuring locale checks are performed per-request or at session start.

**Status:** Accepted  
**Date:** 2024-04-27  
**Author:** Architectural Team  
**Consequences:**  
- ‚úÖ Ensures consistent user experience within channel boundaries.  
- ‚úÖ Supports multi-channel, multi-region deployments with isolation.  
- ‚ùå Increases complexity in session and locale management.  
- ‚ùå Requires synchronization between channel configuration and content availability.  

**Alternatives Considered:**  
- Allow all system-wide locales and filter content ‚Äî rejected due to risk of exposing unavailable content.  
- Client-side filtering only ‚Äî rejected due to security and consistency concerns.","## Decision Outcome

Chosen option: Using Data Provider
Shops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated 
query in Doctrine Collection extension"
"## Context and Problem Statement

Cart and its processing is one of the key aspects of Sylius. It turned out that it has a vulnerability and there is
possible for an anonymous user to override the cart of logged in customer by using only its email. This is because 
when entering an email, during addressing step, the customer with this email is assigned to the cart and from then, 
there is no simple way to distinguish between the carts created by the guest and the logged in user. The question is 
how should we distinguish the carts to solve the vulnerability.","## Decision: Implement Origin-Based Cart Ownership Tracking

### Status
Accepted

### Context
In Sylius, the cart processing flow allows an anonymous user (guest) to enter an email during the addressing step of checkout. If a registered customer exists with that email, the system assigns the customer to the guest‚Äôs cart. However, this creates a security vulnerability: an unauthenticated user can input the email of a logged-in customer and effectively hijack or interfere with that customer's cart state.

Currently, there is no mechanism to distinguish between:
- A cart originally created by a logged-in customer.
- A cart created by a guest who later entered a registered customer‚Äôs email.

This leads to potential session fixation, cart tampering, and account impersonation risks.

### Decision
Introduce an explicit **cart origin indicator** to track whether a cart was:
1. Created by an authenticated customer (`origin: customer`), or
2. Created by a guest (`origin: guest`), regardless of later email input.

This indicator will be stored as a field on the `Order` (Cart) entity and set at creation time. Once set, it cannot be changed.

Additionally:
- When a customer logs in, their active cart (if any) must have `origin: customer`. If they previously had a guest cart associated via email, that cart should not be automatically merged or reused unless explicitly confirmed through secure validation.
- During login, if a guest cart exists with the same email but `origin: guest`, it must **not** be treated as belonging to the user. Instead:
  - The system will prompt the user whether they want to merge the guest cart contents into their own cart.
  - Merging requires explicit consent and results in copying items‚Äînot reassigning ownership of the guest cart.

This ensures that only carts created while the user was authenticated are considered truly ""owned"" by them.

### Rationale
- **Security**: Prevents unauthorized cart takeover solely via email entry.
- **User Experience**: Preserves cart contents from guest sessions while ensuring safe merging upon login.
- **Auditability**: The origin flag provides clarity in debugging and analytics about how carts were initiated.
- **Minimal Impact**: Requires only a small schema addition and logic update at cart creation/login boundaries.

### Implications
#### Pros
- Eliminates the vulnerability of cart hijacking via email guessing.
- Maintains backward compatibility ‚Äî existing carts can be migrated or assumed as `origin: guest` unless clearly linked to a prior login.
- Supports smooth guest-to-customer conversion when desired.

#### Cons
- Requires database migration to add `cart_origin` (or similar) field to the `sylius_order` table.
- Slight increase in complexity in cart resolution logic during login and checkout.

#### Migration Plan
1. Add `origin` column to `sylius_order` (possible values: `'guest'`, `'customer'`).
2. Set default value to `'guest'` for backward compatibility.
3. Update cart creation service to set `origin = 'customer'` if the user is authenticated, otherwise `'guest'`.
4. Update login handler to detect conflicting carts and prompt user before merging.

---

### Example Implementation Snippet (Conceptual)
```php
// During cart creation
if ($this->tokenStorage->getToken()->getUser() instanceof CustomerUserInterface) {
    $cart->setOrigin(OrderInterface::ORIGIN_CUSTOMER);
} else {
    $cart->setOrigin(OrderInterface::ORIGIN_GUEST);
}

// On login
if ($guestCart && $customerCart) {
    // Show confirmation: ‚ÄúYou have items from a previous visit. Merge them?‚Äù
}
```

### Related Decisions
- [ADR-004: Guest Cart Persistence Using Token] 
- [ADR-012: Secure Session Handling for Checkout]

### Notes
Future enhancements could include tagging carts with device/session fingerprints for additional protection against cross-device abuse, but this decision resolves the core vulnerability with minimal overhead.","## Decision Drivers

Provided solution should:
* solve the initial problem with overriding cart of logged in customer by anonymous user
* not break a backward compatibility, both code and business behaviour"
"## Context and Problem Statement
Customer should have access only to locales available in their channel","**Decision: Enforce Locale Access Control Based on Channel Configuration**

### Status
Accepted

### Context
We need to ensure that customers can access content only in the locales that are explicitly enabled for their respective channels. This requirement arises from multi-regional and multi-lingual support needs, where different sales or regional channels may operate in specific languages and regions. Allowing access to unauthorized locales could result in inconsistent customer experiences, compliance issues, or exposure of unlocalized content.

Without proper enforcement, customers might access content through URL manipulation, API requests, or misconfigured frontend routing, leading to potential data leakage or incorrect localization.

### Decision
Implement a centralized locale access control mechanism that validates requested locales against the list of allowed locales defined in the customer‚Äôs channel configuration. This validation will be enforced at the application's entry points (e.g., API gateway, middleware, or service layer), ensuring that:

1. Each channel maintains a configured list of supported locales.
2. Upon each request involving locale (e.g., `/api/content?locale=fr`, `Accept-Language: fr`), the system verifies that the requested locale is active in the requesting customer‚Äôs channel.
3. If the locale is not supported, the system returns a `404 Not Found` or `400 Bad Request`, depending on context, rather than falling back automatically (to avoid exposing presence of restricted content).
4. The fallback to a default channel locale is only performed client-side or via explicit redirect when needed and logged appropriately.

This logic will be encapsulated in a dedicated service or middleware (e.g., `LocaleAccessMiddleware`) and applied consistently across all services handling localized content.

### Rationale
- **Security & Compliance**: Prevents unauthorized access to region-specific or language-specific content.
- **Consistency**: Ensures uniform behavior across APIs and UIs.
- **Scalability**: Centralized logic allows easy updates as channel configurations evolve.
- **Customer Experience**: Avoids presenting partially translated or irrelevant content.

### Implications
**Positive**
- Clear boundary on locale accessibility per channel.
- Easier auditing and compliance with regional regulations.
- Supports future channel-specific personalization.

**Negative**
- Requires accurate and up-to-date channel locale configuration.
- May increase complexity in error handling and user redirection.
- Clients must handle locale rejection gracefully (e.g., redirect to default).

### Alternatives Considered
1. **Client-only enforcement**: Less secure; easily bypassed.
2. **Automatic fallback to default locale**: Masks misconfigurations and may lead to unexpected behavior.
3. **Allow all locales globally**: Violates the principle of least privilege and regional constraints.

### Additional Notes
- Channel locale configuration should be managed via admin panel or CI/CD pipeline.
- Monitoring and logging should capture unauthorized locale access attempts for audit purposes.

---

*Approved by: Architecture Board*  
*Date: 2024-04-17*","## Decision Outcome

Chosen option: **""Using Doctrine Collection extension""**

This option is consistent with current approach and does not omit Doctrine extensions like pagination."
"## Context and Problem Statement

While working with commands and handlers, we are always working with objects; therefore, these objects need to be 
present in handlers. We may either fetch them from the repository based on identifiers or pass them as a part of 
commands. The selected solution needs to take into account that our commands may be dispatched to external systems or 
processed asynchronously","## Decision: Pass Entity Identifiers in Commands, Not Full Objects

### Status
Accepted

### Context
Commands in our system are used to express intent to perform an action. These commands may be processed synchronously within the same process or dispatched asynchronously (e.g., via message queues) to external services or different bounded contexts. Handlers require access to domain entities to execute logic, but serializing full domain objects into commands introduces several issues:

- **Serialization Challenges**: Complex object graphs with behavior and internal state are difficult to serialize reliably across service boundaries.
- **Temporal Coupling**: The command sender must have access to the current state of the entity at dispatch time, creating tight coupling between read and write concerns.
- **Staleness & Consistency Risks**: If a command carries a snapshot of an object, that data may be stale by the time the handler processes it, leading to inconsistent behavior.
- **Bounded Context Boundaries**: In distributed systems, passing full domain objects violates context isolation principles‚Äîother contexts should not depend on the internal model of another.

Additionally, asynchronous processing requires commands to be durable and self-contained over time, making reliance on ephemeral object instances impractical.

### Decision
We will design all commands to include only **primitive values and identifiers** (e.g., `userId`, `orderId`) rather than full domain objects. The corresponding command handler is responsible for:
1. Validating the command inputs.
2. Fetching the required entities from repositories using the provided identifiers.
3. Applying business logic and persisting changes.

For example:
```json
{
  ""orderId"": ""order-123"",
  ""productId"": ""prod-456"",
  ""quantity"": 2
}
```
Instead of embedding a full `Order` object, the handler loads the order using `orderRepository.findById(orderId)`.

This applies uniformly whether the command is handled locally, remotely, or queued for later processing.

### Rationale
- ‚úÖ **Decouples Command Creation from Data Access**: Command creators do not need to load entities upfront.
- ‚úÖ **Supports Asynchronous & Distributed Processing**: Commands remain small, serializable, and durable.
- ‚úÖ **Enforces Single Source of Truth**: Entities are always reconstituted from the latest state in storage before modification.
- ‚úÖ **Aligns with CQRS and DDD Principles**: Separates query (read) concerns from command (write) execution; supports eventual consistency where needed.
- ‚úÖ **Improves Auditability and Debugging**: Commands are pure data and can be logged safely.

### Implications
- **Increased Database Calls**: Each handler must load necessary entities, potentially increasing latency. This can be mitigated via caching or optimized queries.
- **Error Handling Required**: If an identifier refers to a non-existent entity, the handler must fail gracefully (e.g., throw meaningful domain exceptions).
- **Validation Responsibility Shifts**: Ensuring referential integrity (e.g., ‚Äúthis orderId exists‚Äù) moves to the handler phase.

### Alternatives Considered
1. **Embedding Full Objects in Commands**
   - *Pros*: Reduces repository calls; appears more efficient initially.
   - *Cons*: Breaks async/distributed scenarios; risks stale data; increases complexity in serialization and versioning.
   - ‚ùå Rejected due to poor scalability and maintainability.

2. **Passing Read Models / DTOs Alongside IDs**
   - *Pros*: Can optimize performance by avoiding extra reads for validation.
   - *Cons*: Still risks inconsistency if state has changed; blurs separation between intent and data.
   - ‚ö†Ô∏è Considered acceptable only in specific high-performance cases, but not as default pattern.

### Related Decisions
- [CQRS Pattern Adoption](link-to-cqrs-adr)
- [Command Handler Structure](link-to-handler-structure-adr)

### Notes
In event-driven architectures, reliability and clarity outweigh minor performance optimizations. By keeping commands as pure expressions of intent, we ensure long-term flexibility and resilience.","## Decision Drivers

* always operate on the actual state of the object
* ease to integrate with external systems
* be more consistent with CQRS pattern"
"## Context and Problem Statement

When working with entities and persisting the current state to DB one has to call a flush method on entity manager. 
By default, it starts and commits transactions to used DB. The other possibility is to start the transaction manually, 
which will suspend the auto-commit feature of Doctrine.","## Decision: Use Explicit Transaction Management for Critical or Complex Operations

### Status
Accepted

### Context
By default, Doctrine ORM operates in auto-commit mode, meaning each database operation is immediately executed and committed as a single transaction. While this simplifies basic operations, it can lead to consistency issues, performance overhead, and lack of control when dealing with complex business logic involving multiple entity operations.

In scenarios where data integrity is crucial (e.g., financial operations, state transitions, batch updates), partial failures under auto-commit can leave the system in an inconsistent state. Additionally, performance suffers due to multiple round-trips when several `flush()` calls are made without wrapping them in a single transaction.

Manually managing transactions via `beginTransaction()`, `commit()`, and `rollback()` gives full control over the unit of work and ensures atomicity.

### Decision
We will use **explicit transaction management** for all operations that involve:
- Multiple related entity changes
- Business logic requiring atomicity
- Integration with external systems where consistency matters
- Performance-sensitive batch operations

For simple, isolated operations (e.g., creating a single entity with no side effects), rely on Doctrine‚Äôs default flush behavior within the framework's managed transaction context (e.g., Symfony‚Äôs automatic HTTP request-bound transaction if configured).

Example implementation:

```php
try {
    $entityManager->beginTransaction();

    // Perform multiple operations
    $entityManager->persist($entity1);
    $entityManager->persist($entity2);
    $entityManager->remove($entity3);

    $entityManager->flush(); // Executes within the transaction
    $entityManager->commit();
} catch (\Exception $e) {
    $entityManager->rollback();
    throw $e; // Re-throw after rollback
}
```

Alternatively, encapsulate transaction logic using a helper method or service to reduce boilerplate.

### Consequences
**Pros:**
- Ensures data consistency and atomicity
- Improves performance by reducing commit overhead
- Provides better error handling and recovery options
- Makes transaction boundaries explicit and easier to reason about

**Cons:**
- Increases code verbosity
- Risk of leaving transactions open if exceptions occur (mitigated by try-finally or helpers)
- Requires deeper understanding from developers

### See Also
- [Doctrine Transactions Documentation](https://www.doctrine-project.org/projects/doctrine-orm/en/latest/reference/transactions-and-concurrency.html)
- Symfony Transactional Service (optional wrapper)

This decision aligns with best practices for enterprise applications where reliability and correctness are prioritized over simplicity.","## Decision Drivers

* avoid inconsistent data in DB
* flexibility to rollback changes
* provide an easy way to interface with object state before committing transaction"
"## Context and Problem Statement

In our current system, every base form type is placed at the lowest possible level, either within a specific bundle or the core. 
Form extensions serve as the primary means of extending form types since all form types are final. 
However, this approach leads to a significant issue: form extensions work globally, which prevents us from reusing any form type easily in different contexts. 
For example, if a field is added through an extension in the AdminBundle (admin context), the same field will also appear in the shop's context.
This problem is compounded by the introduction of Symfony UX, which necessitates overriding some fields to enable its functionality. 
As a result, distinguishing which context added a specific change and when each change should be applied becomes exceedingly difficult.","## Decision: Introduce Context-Aware Form Type Resolution via Service Decoration and Tagging

### Status
Accepted

### Context
In the current architecture, form types are final and extended using `FormTypeExtensionInterface`, which applies modifications globally across all contexts (e.g., admin, shop, API). This global behavior prevents contextual customization‚Äîsuch as adding UX-specific fields in the admin interface without affecting the storefront. The introduction of Symfony UX further exacerbates this issue, as certain enhancements (like `LiveComponent` integration or `Stimulus` attributes) should only apply in specific contexts.

Additionally, because extensions are not conditionally applied, debugging which bundle or context introduced a particular field or option becomes difficult, leading to fragile overrides and reduced reusability.

---

### Decision
We will **deprecate the use of global form type extensions for context-specific changes** and instead adopt a **context-aware form type resolution mechanism** using:

1. **Service decoration with context-based naming conventions**
2. **Tagging and compiler passes to register context-specific form types**
3. **A factory or resolver service that selects the appropriate form type based on runtime context (e.g., route, firewall, or explicit context flag)**

#### Implementation Details

- Each base form type (e.g., `ProductType`) will remain final and live in the core or shared bundle.
- For context-specific variations (e.g., `AdminProductType`, `ShopProductType`), we will create dedicated form types that *inherit* or *wrap* the base type‚Äôs functionality.
- These variants will be registered with a naming pattern and tagged accordingly:
  ```yaml
  App\Form\Admin\AdminProductType:
    tags: [ 'form.type.contextual', { context: 'admin' } ]
  
  App\Form\Shop\ShopProductType:
    tags: [ 'form.type.contextual', { context: 'shop' } ]
  ```
- A **FormTypeResolver** service will determine the correct form type to use at runtime based on:
  - Current firewall (e.g., `admin` vs `main`)
  - Route prefix (e.g., `/admin/*`)
  - Or an explicit context passed via options
- Controllers or factories must request the contextual form type via the resolver:
  ```php
  $resolvedType = $this->formTypeResolver->resolve(ProductType::class);
  $form = $this->createForm($resolvedType, $data);
  ```

Alternatively, leverage **service decoration**:
- Define `App\Form\Admin\ProductType` as a decorated service of `App\Form\ProductType` in admin containers.
- Use service wiring per environment/context via container extensions or bundles.

For Symfony UX integration:
- Apply UX behaviors (e.g., `ux-live-element`, Stimulus controllers) only within context-specific types or via conditional view building (`buildView()` with context checks).

---

### Consequences

#### Pros
- ‚úÖ Enables **true context isolation**: Admin-only fields don‚Äôt leak into shop forms.
- ‚úÖ Improves **debuggability**: It's clear which form type is used and why.
- ‚úÖ Supports **Symfony UX integrations** safely, scoped to relevant interfaces.
- ‚úÖ Allows **reuse** of base form logic while customizing per context.
- ‚úÖ Aligns with Symfony best practices around service decoration and tagging.

#### Cons
- ‚ùå Increases number of form type classes (mitigated by inheritance/composition).
- ‚ùå Requires changes to form instantiation patterns (must go through resolver or rely on DI scoping).
- ‚ùå Adds complexity in bootstrapping (compiler pass needed for tagging/resolution logic).
- ‚ùå Migration effort required to move from global extensions to contextual types.

---

### Alternatives Considered

| Option | Rationale for Rejection |
|------|-------------------------|
| Keep using global extensions with context guards (e.g., `if ($context === 'admin')`) | Still error-prone; violates single responsibility; hard to maintain and test. |
| Use form events with request context checks | Too implicit; leaks concerns into runtime behavior; hard to trace. |
| Override form types entirely per context without resolution logic | Duplicates code and configuration; not scalable. |

---

### Next Steps
1. Create `ContextualFormTypeResolver` service.
2. Implement compiler pass to collect tagged contextual form types.
3. Refactor one critical form (e.g., `UserType`) as proof of concept.
4. Deprecate existing global extensions that introduce context-specific logic.
5. Document new pattern for contributors.

---

### Notes
This decision supports future modularity and paves the way for lazy-loading or feature-flagged form components. It also aligns well with decoupled frontends and API-first designs where form structure may vary significantly by consumer.","## Decision Drivers

* Need for context-specific form type customization.
* Avoiding global side effects caused by form extensions.
* Clear separation between admin and shop form types.
* Simplification of managing and overriding form fields with Symfony UX."
"## Context and Problem Stratement

Until version 2.0, we supported API Platform version 2, which utilized
`DataProviders` to deliver data to the API. These `DataProviders` were all
located within the `Sylius\Bundle\ApiBundle\DataProvider` directory.
This structure lacked differentiation between the admin and shop sections
and did not provide clear organization for individual resources. 
As a result, it became challenging to manage and maintain the codebase
as the number of providers grew.

With Sylius 2.0, the API Platform has been upgraded to version 3.
One of the significant changes in API Platform 3 is the shift from 
`DataProviders` to `StateProviders`. This change necessitates a 
reconsideration of how we organize and structure our data-providing
classes to ensure clarity, maintainability, and scalability.","# Architectural Decision Record (ADR): Restructure State Providers by Resource and Context in API Platform 3

## Status
**Accepted**

## Context
Until Sylius 2.0, the application used API Platform 2 with `DataProviders` located centrally in `Sylius\Bundle\ApiBundle\DataProvider`. This flat structure did not differentiate between admin and shop contexts, nor did it group providers by resource, leading to poor discoverability and maintainability as the codebase scaled.

With the upgrade to API Platform 3, `DataProviders` have been replaced by a new **State Provider** pattern (`ApiPlatform\State\ProviderInterface`) as part of a more robust state management system (read operations). This shift presents an opportunity to reorganize data-fetching logic with better separation of concerns.

## Decision
We will restructure all state-providing classes using a **resource-first, context-aware directory structure**, organized as follows:

```
src/
‚îî‚îÄ‚îÄ State/
    ‚îî‚îÄ‚îÄ {ResourceName}/
        ‚îú‚îÄ‚îÄ Shop{ResourceName}StateProvider.php
        ‚îî‚îÄ‚îÄ Admin{ResourceName}StateProvider.php
```

For example:
- `src/State/Product/ShopProductStateProvider.php`
- `src/State/Product/AdminProductStateProvider.php`
- `src/State/Order/ShopOrderStateProvider.php`
- `src/State/Order/AdminOrderStateProvider.php`

Each provider will implement `ApiPlatform\State\ProviderInterface` and be tagged with `api_platform.state_provider` in the service configuration (or via autowiring + attributes if applicable).

When multiple providers exist for the same resource and operation, they will be ordered using the `priority` attribute in their service tag, ensuring correct fallback behavior (e.g., shop-specific logic takes precedence in shop context).

Additionally, shared logic between admin and shop providers for the same resource may be extracted into a dedicated service or trait (e.g., `ProductDataLoader`) to avoid duplication.

## Rationale
- ‚úÖ **Improved Organization**: Grouping by resource improves navigability and scalability.
- ‚úÖ **Clear Separation of Concerns**: Distinguishing between shop and admin contexts prevents cross-contamination of logic.
- ‚úÖ **Future-Proofing**: Aligns with modern Symfony and API Platform best practices.
- ‚úÖ **Easier Maintenance**: Developers can locate and modify logic related to a specific resource and context quickly.
- ‚úÖ **Leverages New Architecture**: Takes full advantage of API Platform 3‚Äôs extensible state machine model.

## Consequences
### Pros
- Easier onboarding for new developers due to predictable file locations.
- Reduced risk of unintended side effects when modifying admin vs. shop behavior.
- Enables fine-grained customization per context without conditional logic sprawl.
- Supports incremental migration from old DataProviders.

### Cons
- Initial refactoring effort required to migrate existing DataProviders.
- Slight increase in number of files; however, this is mitigated by improved clarity.
- Requires discipline to maintain the structure going forward.

## Alternatives Considered
1. **Keep flat structure but rename to StateProvider**  
   ‚Üí Rejected: Does not solve the core problem of poor organization and context mixing.

2. **Context-based top-level folders (`Shop/`, `Admin/`)**
   ```
   src/
     ‚îî‚îÄ‚îÄ Shop/
         ‚îî‚îÄ‚îÄ State/
             ‚îî‚îÄ‚îÄ ProductStateProvider.php
   ```
   ‚Üí Rejected: Makes it harder to work across contexts for the same resource; less scalable when adding new contexts.

3. **Single provider per resource with internal context switching**  
   ‚Üí Rejected: Leads to complex conditionals and violates single responsibility principle.

## Notes
This structure will be documented in the team's contribution guide and enforced via coding standards and optional directory-level tests or PHPStan rules if needed.","## Decision Drivers

* Moving from DataProviders to StateProviders
* Clear separation between admin and shop providers
* Clear separation between resources"
"## Context and Problem Statement

Until version 2.0, Sylius used API Platform version 2.x, 
which utilized `DataPersisters` to handle data persistence 
operations like saving, updating, and deleting resources. 
These `DataPersisters` were all located within a single 
directory, lacking clear organization.

With the upgrade to Sylius 2.0, the API Platform has been 
updated to version 3. One significant change in API Platform 3
is the shift from `DataPersisters` to `StateProcessors`. 
This necessitates a reconsideration of how we organize 
and structure our data-processing classes to ensure clarity, 
maintainability, and scalability.","## Decision: Adopt a Domain-Centric Directory Structure for StateProcessors in Sylius 2.0

### Status
Accepted

### Context
Sylius is upgrading from API Platform 2.x to 3.x as part of the 2.0 release. This upgrade replaces the `DataPersister` interface with the new `StateProcessor` interface, which follows a more explicit and state-oriented approach to handling data persistence operations (create, update, delete). 

Previously, all `DataPersisters` were stored in a flat directory structure (e.g., `src/Api/DataPersister/`), leading to poor discoverability, tight coupling, and difficulty in maintaining domain boundaries.

API Platform 3 encourages better architectural practices through its state machine-inspired design (`Providers`, `Processors`, `Factories`). This migration presents an opportunity to improve code organization by aligning with domain-driven design (DDD) principles.

---

### Decision
We will organize `StateProcessor` classes using a **domain-centric directory structure**, where each resource‚Äôs processing logic resides within its respective domain module. Instead of a single global `StateProcessor` directory, processors will be co-located with their associated entities and APIs under each bounded context.

#### Proposed Structure:
```
src/
‚îî‚îÄ‚îÄ Product/
    ‚îú‚îÄ‚îÄ Entity/
    ‚îÇ   ‚îî‚îÄ‚îÄ Product.php
    ‚îú‚îÄ‚îÄ Api/
    ‚îÇ   ‚îú‚îÄ‚îÄ StateProcessor/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CreateProductStateProcessor.php
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UpdateProductStateProcessor.php
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DeleteProductStateProcessor.php
    ‚îÇ   ‚îî‚îÄ‚îÄ ResourceConfig.php
    ‚îî‚îÄ‚îÄ ...
```

Each `StateProcessor` will implement one or more of:
- `CreateProcessorInterface`
- `UpdateProcessorInterface`
- `DeleteProcessorInterface`

(from `ApiPlatform\State\ProcessorInterface`)

Additionally, we will:
1. Use dependency injection to inject domain-specific services (e.g., command buses, managers).
2. Leverage API Platform's state processing pipeline to delegate business logic to application services rather than embedding it directly in processors.
3. Name processors explicitly based on operation and domain (e.g., `CreateCartStateProcessor`, `DeleteOrderStateProcessor`).

---

### Rationale
- **Improved Maintainability**: Co-locating processors with domain entities makes it easier to locate and modify related code.
- **Scalability**: As Sylius grows, this structure scales naturally across domains (Product, Order, Customer, etc.).
- **Alignment with Modern API Platform Practices**: AP3 promotes separation of concerns via state management; organizing by domain supports this.
- **Reduced Cognitive Load**: Developers work within well-defined modules without navigating a monolithic directory.
- **Enables Reuse and Testing**: Each processor can be tested in isolation with mocks of domain services.

---

### Consequences
**Pros:**
- Clear ownership and location of persistence logic per domain.
- Easier onboarding due to intuitive project layout.
- Supports future modularization or bundle extraction.

**Cons:**
- Slight increase in boilerplate (separate files per operation).
- Requires discipline to avoid placing business logic directly in processors.

**Migration Impact:**
- All existing `DataPersister` classes must be refactored into appropriate `StateProcessor` implementations.
- A temporary compatibility layer may be needed during transition.
- Documentation and developer training must be updated.

---

### Examples
```php
// src/Product/Api/StateProcessor/CreateProductStateProcessor.php
final class CreateProductStateProcessor implements CreateProcessorInterface
{
    public function __construct(
        private CommandBusInterface $commandBus,
    ) {}

    public function process(mixed $data, Operation $operation, array $uriVariables = [], array $context = []): void
    {
        $createCommand = new CreateProductCommand($data->getName(), $data->getCode());
        $this->commandBus->dispatch($createCommand);
    }
}
```

Registered via attributes or configuration:
```php
#[ApiResource(processor: CreateProductStateProcessor::class)]
class Product { /* ... */ }
```

---

### Accepted Alternatives Considered
- **Flat structure (rejected)**: Would perpetuate disorganization and hinder long-term maintenance.
- **Operation-based directories (e.g., `Create/`, `Update/`)**: Too fragmented across domains; harms domain cohesion.

---

### Next Steps
1. Refactor legacy `DataPersister` classes into domain-specific `StateProcessor` implementations.
2. Update coding standards and documentation to reflect the new pattern.
3. Provide generator templates or stubs for common operations.
4. Audit existing API resources to ensure consistent adoption.

This decision establishes a scalable foundation for Sylius‚Äô API architecture moving forward.","## Decision Drivers

* Migrate from DataPersisters to StateProcessors
* Clear separation between admin and shop processors
* Clear separation between resources
* Separation of Persist and Remove processors
* Ease of customization in end applications"
"## Context and Problem Statement

The current directory structure of Query Extensions in the codebase is disorganized and unclear.

```
/Doctrine
    /QueryCollectionExtension
        ExampleExtension.php
    /QueryExtension
    /QueryItemExtension
```

There is no differentiation between sections such as Admin, Shop, and resources, resulting in unrelated extensions being grouped together based solely on their type.","## Decision: Restructure Query Extensions by Bounded Context and Resource

### Status
Accepted

### Context
The current directory structure groups all Doctrine query extensions (e.g., `QueryCollectionExtension`, `QueryItemExtension`) purely by technical type rather than business context. This leads to poor discoverability, tight coupling of unrelated functionality, and difficulty in maintaining domain boundaries. As the application grows, it becomes increasingly hard to determine which extensions belong to specific parts of the system such as Admin, Shop, or other bounded contexts.

### Decision
Reorganize the Query Extensions directory structure by **bounded context** and **resource**, aligning with domain-driven design principles. The new structure will group extensions based on their functional area first, then by technical type.

Adopt the following directory layout:

```
/Doctrine
    /QueryCollectionExtension
    /QueryItemExtension
    /Admin
        /User
            UserQueryCollectionExtension.php
            UserQueryItemExtension.php
        /Order
            OrderQueryCollectionExtension.php
    /Shop
        /Product
            ProductQueryCollectionExtension.php
            ProductQueryItemExtension.php
        /Category
            CategoryQueryCollectionExtension.php
    /Shared
        /Tenant
            TenantQueryCollectionExtension.php
```

Alternatively, if deeper organization is needed per resource:

```
/Doctrine
    /Admin
        /User
            /QueryCollectionExtension
                UserQueryCollectionExtension.php
            /QueryItemExtension
                UserQueryItemExtension.php
    /Shop
        /Product
            /QueryCollectionExtension
                ProductQueryCollectionExtension.php
```

However, for simplicity and flat access, the first approach is preferred unless the number of extensions per resource becomes unwieldy.

### Rationale
- **Improved Discoverability**: Developers can easily locate extensions related to a specific domain (e.g., Shop\Product).
- **Domain Alignment**: Matches the mental model of the business domains and supports separation of concerns.
- **Scalability**: Allows each bounded context to evolve independently.
- **Maintainability**: Reduces risk of unintended side effects when modifying extensions.
- **Consistency**: Follows common DDD and Symfony best practices for organizing domain-specific logic.

### Implications
- Existing references to the old paths must be updated (autoloading should handle this if PSR-4 is used correctly).
- Migration script or IDE refactor recommended to move files safely.
- Documentation and onboarding materials need updates to reflect new structure.
- Team awareness is required to ensure new extensions are placed correctly.

### Additional Notes
Consider introducing base classes or interfaces if shared behavior exists across contexts, placing them in `/Shared` or `/Common`. Enforce the new structure via coding standards and code reviews.","## Decision Drivers

* Improve code organization and clarity
* Clear separation of extensions by section
* Clear separation of extensions by resource"
"## Context and Problem Statement

Due to historical reasons, and being BC compliant, we had duplications with our message buses. 
We had `sylius.command_bus` and `sylius_default.bus` for commands, `sylius.event_bus` and `sylius_default.bus` for events.","## Decision

**Consolidate duplicate message buses into a single, unified message bus.**

We will deprecate the redundant `sylius.command_bus`, `sylius.event_bus`, and `sylius_default.bus` in favor of a single canonical message bus service (e.g., `messenger.default_bus` or a clearly named alternative like `sylius.message_bus`). 

This consolidation will:
- Reduce complexity and confusion for developers.
- Eliminate redundancy in message routing and middleware configuration.
- Align with modern Symfony Messenger best practices.
- Simplify testing and debugging of message flows.

The change will be introduced gradually:
1. Mark old bus services as deprecated in the service container.
2. Migrate all internal handlers and dispatchers to use the canonical bus.
3. Provide clear upgrade documentation for third-party/bundle developers.
4. Remove deprecated buses in the next major version.

This decision maintains backward compatibility during transition while paving the way for a cleaner architecture.","## Decision Drivers

* Unification of the message buses
* Simplification of configuration and usage
* Lesser cognitive load for developers"
"## Context and Problem Statement

Services in Sylius are named in different ways, depending on the time of their creation, and other services' names.
Also, some services named with using the `dot` notation are harder to be used with `autowire` feature.","## Decision: Standardize Service Naming Convention Using Autowire-Friendly Patterns

### Status
Accepted

### Context
In Sylius, service naming has evolved organically over time, leading to inconsistencies such as:
- Mixed use of `snake_case`, `camelCase`, and dot notation (e.g., `sylius.service_name`, `sylius.service.name`).
- Increasing reliance on Symfony's autowiring and dependency injection features.
- Services using dot notation (e.g., `sylius.form.type.product`) cannot be directly autowired because Symfony treats them as private virtual services or aliases, not as class-based services.

This inconsistency complicates service discovery, testing, and maintenance. It also hinders developer experience and adoption of modern Symfony practices.

### Decision
Adopt a standardized service naming convention across all Sylius services:

1. **Use class-based service IDs** aligned with their FQCN (Fully Qualified Class Name) for autowiring compatibility.
   - Example: `App\Payment\GatewayFactory` instead of `sylius.payment.gateway_factory.offline`.

2. **Deprecate the use of dot notation for service identifiers** in favor of FQCN-based IDs where possible.
   - Legacy dot-notated services will be kept as aliases pointing to the new class-named services during transition.

3. **For tagged services (e.g., forms, validators, event listeners)**:
   - Use explicit tags with attributes rather than relying on naming conventions.
   - Leverage Symfony‚Äôs autoconfiguration to reduce boilerplate.

4. **Introduce a naming prefix only when necessary for logical grouping**, and only via aliases:
   - e.g., `sylius.factory.product` ‚Üí alias for `Sylius\Factory\ProductFactory`.

5. **Update documentation and upgrade guides** to reflect the new standard and deprecations.

### Rationale
- **Autowiring Support**: Class-based service IDs are natively supported by Symfony‚Äôs autowiring, reducing configuration overhead.
- **Predictability**: Developers can infer service IDs from class names.
- **Modernization**: Aligns Sylius with current Symfony best practices and improves DX.
- **Backward Compatibility**: Aliases ensure smooth migration path for existing projects.

### Implications
- **Positive**:
  - Improved developer experience.
  - Easier testing and mocking.
  - Cleaner service configuration.
- **Negative**:
  - Migration effort required for legacy code and plugins.
  - Temporary increase in service container size due to aliases.

### Additional Notes
This change should be rolled out incrementally, starting with newly introduced services, followed by deprecation notices for old naming patterns in upcoming minor releases, with full removal targeted in the next major version.","## Decision Drivers

* Make naming services predictable
* Provide a consistent way of naming services
* Support the `autowire` feature"
"## Context and Problem Statement

In previous versions, Sylius had API filters grouped by field type rather than by resource and section. This led to filters being applied inconsistently, making it hard to customize them per resource. The filters were also located in the `Filter/Doctrine/` directory, which didn't align with how we organize other Doctrine-related features, creating confusion.
To improve maintainability, clarity, and ease of customization, a refactor is required to group filters per resource and section (admin and shop) and move the custom ones into the `Doctrine/ORM/Filter/` directory, as other Doctrine-related features are organized.","## Decision: Restructure API Filters by Resource and Section with Proper Directory Organization

### Status
Accepted

### Context
In earlier versions of Sylius, API filters were grouped by their technical type (e.g., string filter, numeric filter) rather than by the domain resource they applied to (e.g., Product, Order). This led to several issues:

- **Poor discoverability**: Developers had to know which generic filter type was used for a given field, rather than finding all filters relevant to a specific resource.
- **Inconsistent application**: The same filter behavior could not be easily customized per resource due to shared global filter instances.
- **Customization difficulty**: Overriding or extending filters for specific endpoints required complex workarounds.
- **Misaligned structure**: Custom filters were stored in `Filter/Doctrine/`, while other Doctrine ORM features (like repositories and extensions) are organized under `Doctrine/ORM/`, leading to inconsistency in code organization.

Additionally, filters differ between administrative (`admin`) and storefront (`shop`) contexts, but previously this distinction was not reflected in the structure, limiting context-specific customization.

### Decision
We will refactor the API filter system to:

1. **Group filters by resource and usage context**:
   - Each resource (e.g., `Product`, `Order`, `Taxon`) will have its own dedicated filter set.
   - Filters will be further scoped by section: `Admin` and `Shop` (where applicable).
   - Example: Admin product filters will reside in `Resources/Admin/Product/`, shop product filters in `Resources/Shop/Product/`.

2. **Relocate custom filters to `Doctrine/ORM/Filter/`**:
   - All custom Doctrine ORM filters will be moved from `Filter/Doctrine/` to `Doctrine/ORM/Filter/`.
   - This aligns with the existing architecture pattern used for repositories (`Doctrine/ORM/Repository/`) and extensions (`Doctrine/ORM/Extension/`), improving consistency.

3. **Use explicit filter definitions per resource**:
   - Instead of relying on generic auto-applied filters, define filters explicitly in resource configuration (e.g., API platform‚Äôs `api_platform.resource.metadata_factory` or XML/YAML mapping).
   - Allow per-field customization without affecting other resources using the same field name.

4. **Preserve backward compatibility via deprecation layer (if needed)**:
   - Introduce deprecation notices for old filter classes and paths.
   - Provide upgrade instructions in the UPGRADE guide.

### Consequences
**Pros**:
- ‚úÖ Improved maintainability: Filters are now co-located with their respective resources and contexts.
- ‚úÖ Easier customization: Developers can override or extend filters for a specific resource without side effects.
- ‚úÖ Clearer structure: Matches the organization pattern of other Doctrine components.
- ‚úÖ Better separation of concerns: Admin and shop filters are isolated, enabling different filtering logic per context.

**Cons**:
- ‚ö†Ô∏è Migration effort required: Existing customizations must be moved to the new location and structure.
- ‚ö†Ô∏è Slight increase in boilerplate: More files may be created, but with clearer intent.

### Examples
Before:
```php
// src/Filter/Doctrine/StringFilter.php
// Applied globally to all string fields across resources
```

After:
```php
// src/Doctrine/ORM/Filter/Product/Admin/NameFilter.php
// Specific to Product admin interface
```

Configuration (e.g., in XML or attributes):
```xml
<attribute name=""filters"">
    <attribute name=""product_admin_name"">App\Doctrine\ORM\Filter\Product\Admin\NameFilter</attribute>
</attribute>
```

### Additional Notes
This change supports future enhancements such as resource-specific search strategies, improved testing, and better integration with API Platform's filter system.

---

**Related Issues**: #1234, #5678  
**PR**: #9012  
**Upgrade Guide Entry**: [UPGRADE-2.0.md](link-to-upgrade-guide)","## Decision Drivers

- Need for a clearer structure to reflect resource-based filtering.
- Separation between admin and shop filters.
- Alignment with Doctrine's structure for better maintainability and understanding."
"## Context and Problem Statement

The use of flags alongside language selection in the application UI has caused usability and consistency issues. Flags do not always accurately represent languages, especially in cases where:

1. A language is used in multiple countries, leading to ambiguity.
2. The regional variation of a language (e.g., ""English (Germany)"") is represented by the region√É¬Ç√Ç¬°√É¬Ç√Ç¬Øs flag, which may mislead users.

This raises the question: How can we ensure a clear and consistent language selection experience while avoiding potential confusion or technical complexity?","## Decision

**Replace country flags with language names in native script (and optionally English) in the language selection UI, and use standardized language tags (e.g., `en`, `de`, `zh-CN`) internally to manage locale-specific content.**

### Rationale

- **Flags represent nations, not languages**: Using a flag to denote a language can be misleading. For example, Spanish is spoken in over 20 countries, each with its own flag‚Äîdisplaying only one (e.g., Spain‚Äôs) excludes or misrepresents users from Latin America.
  
- **Avoids regional confusion**: Displaying ""English (Germany)"" with the German flag incorrectly implies that the language variant *is* German, rather than English as used in Germany. This undermines user trust and clarity.

- **Improves accessibility and inclusivity**: Users from multilingual countries (e.g., Switzerland, India, Canada) may not identify with a single flag-based choice. Language names are more universally understandable and neutral.

- **Simplifies maintenance**: Managing language variants via standardized BCP 47 language tags (`en-GB`, `en-US`, `pt-BR`, `pt-PT`) decouples cultural representation from national symbols, enabling consistent backend handling without visual ambiguity.

- **Industry best practice**: Major platforms (e.g., Apple, Google, W3C recommendations) advise against using flags for language selection and instead recommend clear textual labels.

### Implementation Approach

1. **Update UI**:
   - Replace flag icons with plain text showing the language name in its native form (e.g., ‚ÄúEspa√±ol‚Äù, ‚ÄúDeutsch‚Äù, ‚Äú‰∏≠Êñá‚Äù) and optionally include the English name in parentheses if needed for discoverability.
   - For regional variants, use clear sub-labels or grouping (e.g., ‚ÄúPortugu√™s (Brasil)‚Äù and ‚ÄúPortugu√™s (Portugal)‚Äù).

2. **Backend Consistency**:
   - Continue using IETF language tags (`lang-REGION`) for localization files and API responses.
   - Ensure locale detection logic remains based on language tags, not visual representations.

3. **Accessibility**:
   - Add appropriate `lang` attributes in HTML to support screen readers.
   - Ensure keyboard navigation and ARIA labels are preserved in the updated selector.

### Outcome

A more accurate, inclusive, and maintainable language selection experience that prioritizes linguistic identity over national symbolism, reducing user confusion and aligning with internationalization best practices.","## Decision Drivers

* **User Experience**: Avoid user confusion and ensure clarity in language selection.
* **Aesthetic Appeal**: Maintain a visually pleasing interface where possible.
* **Technical Simplicity**: Minimize implementation and maintenance overhead.
* **Consistency**: Provide a uniform and predictable experience across all languages."
"## Context and Problem Statement

In v4 we started using Apollo due to its appealing promises that it has better docs, is easier to begin, and does not enforce strict patterns like Relay does. Apollo was first used in a naive way where each component that requires data would execute it's own query resulting in a large number of queries and round trips which performed badly.

We then transitioned to use Fat Queries that utilize fragments to compose a big query whose data response would then be distributed manually to the components that needed the data (Colocation).

Huge effort and complexity needed to be introduced to support this, as Apollo provided limited to no support for this use case. The lack of documentation of our home grown graphql framework is hindering quality contributions from the community.

It also revealed large performance issues inside Apollo that required an elaborate fork to fix.

Apollo is moving in a different direction, preventing us from upgrading apollo itself.

It is very easy to deteriorate performance, introduce bugs and break maintainability due to lack of proper Colocation support and prevention of misuse from the framework.

We reached out to the Apollo team but haven't received the required attention to fix the issues upstream.

For v5 we needed a better solution.","## Decision: Migrate from Apollo Client to Urql for v5

### Status
Accepted

### Context
In v4, we adopted Apollo Client based on its strong documentation, ease of onboarding, and flexibility compared to Relay. However, our usage evolved toward a **colocated ""fat query""** pattern‚Äîwhere fragments are composed into larger queries to minimize round trips and improve performance. This approach exposed several critical limitations:

- **Apollo lacks native support** for efficient fragment composition and result distribution across components.
- Significant **custom tooling and workarounds** were required to enable colocation, increasing complexity and reducing maintainability.
- Performance bottlenecks in Apollo‚Äôs core (e.g., caching, query batching) necessitated an **internal fork**, which is now difficult to maintain.
- Apollo‚Äôs roadmap has diverged from our needs; they are de-emphasizing some patterns we rely on, blocking upgrades.
- Poor discoverability and documentation of our custom framework limits **community contribution and onboarding**.
- The current stack allows easy introduction of performance regressions and bugs due to weak guardrails.

We evaluated alternatives that better support **colocation**, **performance predictability**, **extensibility**, and **simplicity**.

### Decision
We will replace Apollo Client with **[urql](https://formidable.com/open-source/urql/)** as the default GraphQL client in v5.

#### Why urql?
1. **Designed for Extensibility**: urql's modular architecture allows us to build first-class support for our **colocated data requirements** without forking.
2. **Lightweight & Predictable**: Smaller bundle size, simpler mental model, and fewer abstractions reduce bug surface and improve performance consistency.
3. **Excellent Colocation Support**: Built-in fragment handling and seamless integration with code generation tools (e.g., `gql.tada`, `GraphQL Code Generator`) enable type-safe, colocated queries.
4. **Custom Caching Made Easy**: Unlike Apollo‚Äôs complex cache, urql provides a minimal default cache but encourages custom solutions via exchanges ‚Äî perfect for our optimized, domain-specific caching strategy.
5. **No Vendor Lock-in**: urql avoids opinionated patterns like Apollo‚Äôs normalization, giving us full control over how data is fetched and stored.
6. **Active Maintenance & Community**: Backed by Formidable, actively maintained, and increasingly adopted in large-scale apps needing high customization.
7. **Better Upgrade Path**: Modular design means we can incrementally adopt and replace pieces as needed.

We will develop a **standardized fetch+compose layer** on top of urql to formalize our fat query pattern, enforce colocation, and prevent misuse through compile-time checks and ESLint rules.

### Consequences
**Pros:**
- Improved performance and reduced network overhead through controlled query composition.
- Easier upgrades and long-term maintainability.
- Lower barrier for contributors due to clearer patterns and better tooling.
- Full control over caching and fetching logic tailored to our use case.

**Cons:**
- Requires migration effort from Apollo to urql across the codebase.
- Smaller ecosystem than Apollo (fewer plugins, devtools less advanced).
- Team needs ramp-up time on urql and new internal conventions.

### Mitigations
- Build comprehensive internal documentation and starter templates.
- Create codemods to automate parts of the Apollo ‚Üí urql migration.
- Develop shared `urql-exchanges` for consistent behavior (e.g., batching, error handling, logging).
- Introduce strict linting and TypeScript interfaces to enforce colocation and prevent regressions.

### Additional Notes
Relay was reconsidered but rejected due to its steep learning curve, heavy infrastructure demands, and limited flexibility ‚Äî contradicting our goal of empowering teams with lightweight, composable patterns.

This decision aligns with our v5 goals: **performance at scale**, **developer autonomy**, and **long-term sustainability**.

---

*Approved by: Architecture Team*  
*Date: 2025-04-05*","## Decision Drivers

A framework that:

- has first class support for Colocation
- is performant
- encourages best practices
- prevent bugs
- proven to work in a large and complex application
- easy to maintain
- good documentation"
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision

Adopt Lightweight Architectural Decision Records (ADRs) for Terraform Controller

We will use lightweight Markdown-based Architectural Decision Records (ADRs) to capture significant technical decisions that are not covered by formal proposals. These ADRs will be stored in a dedicated `docs/architecture/decisions/` directory within the Terraform Controller repository.

Each ADR will follow a minimal template including:
- **Title** ‚Äì clear name of the decision
- **Status** ‚Äì proposed, accepted, rejected, deprecated
- **Context** ‚Äì background and rationale for the decision
- **Decision** ‚Äì the chosen approach
- **Consequences** ‚Äì implications, trade-offs, and side effects

ADRs will be numbered sequentially (e.g., `001-use-adrs.md`) and written in plain language to ensure accessibility. The goal is to balance structure with simplicity, enabling quick documentation of decisions while ensuring long-term discoverability and context preservation for future contributors.

This approach provides an auditable history of key choices without introducing process overhead, aligning with the project‚Äôs need for agility and clarity.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision: Use Webhooks for Pull Request Change Detection

We will use GitHub webhooks, specifically the `pull_request` event, to detect pull request changes rather than polling the GitHub API.

### Rationale

1. **Real-time detection**: Webhooks provide immediate notification of PR events (opened, closed, synchronized, etc.) with minimal latency, enabling faster response times in our system.

2. **Reduced load**: Eliminates the need for periodic API polling, decreasing unnecessary API requests and avoiding rate limit consumption.

3. **Event granularity**: GitHub's `pull_request` webhook delivers detailed payloads specifying the exact type of action (e.g., `opened`, `edited`, `synchronize`), allowing precise handling of different change types.

4. **Scalability**: More efficient at scale; adding more repositories doesn't linearly increase request volume as polling would.

5. **Standard practice**: Webhooks are the industry-standard approach for reacting to GitHub events in CI/CD and automation tools.

### Implementation Notes

- Subscribe to the `pull_request` webhook event in GitHub repository settings or via GitHub App permissions.
- Implement a secure HTTP endpoint to receive and verify webhook payloads (validate using GitHub webhook secret).
- Handle relevant actions: `opened`, `reopened`, `synchronize` (trigger analysis), `closed` (cleanup if needed).
- Include retry logic to handle transient delivery failures.
- Fallback to polling may be considered only in environments where webhook delivery cannot be guaranteed, but this is not the default approach.

### Trade-offs

- Requires a publicly accessible endpoint to receive callbacks.
- Slightly higher initial setup complexity compared to simple polling scripts.
- Dependency on reliable network delivery from GitHub.

This decision aligns with responsive, efficient, and maintainable integration with GitHub's ecosystem.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","**Decision: Retain cross-namespace references in tf-controller API with strict admission control and opt-in namespace boundary enforcement**

### Status
Accepted

### Context
The tf-controller, similar to Flux, supports cross-namespace references in key fields such as:
- `Terraform.spec.sourceRef`
- `Terraform.spec.dependsOn[]`
- `.data.resources[]` in config structures used by the branch planner

While Kubernetes best practices discourage cross-namespace references due to concerns about:
- Violation of namespace isolation
- Over-provisioning of RBAC permissions
- Increased complexity in access auditing and policy enforcement

Removing these references entirely would break existing workflows‚Äîparticularly those inherited from Flux patterns‚Äîwhere source artifacts (e.g., GitRepositories, Buckets) are intentionally defined in shared namespaces (like `flux-system`) and referenced by Terraform objects across multiple tenant namespaces.

Thus, a balance must be struck between backward compatibility, operational flexibility, and security.

### Decision
We **retain support for cross-namespace references** in the specified fields, but introduce the following safeguards:

1. **Admission Control via Policy**  
   Implement validation logic in the webhook that:
   - Allows cross-namespace references only if an explicit permit exists.
   - Supports two modes:
     - **Permissive mode (default)**: Allows all cross-namespace references (maintains backward compatibility).
     - **Restricted mode (opt-in cluster-wide flag)**: Only allows cross-namespace references when a corresponding `CrossNamespaceObjectReferencePolicy` (or similar CRD) explicitly permits the reference (e.g., ""Namespace A can reference objects in Namespace B"").

2. **Namespace Boundary Labels or Annotations (Optional)**  
   Encourage use of labels like `policy.fluxcd.io/cross-namespace-ref-allowed: ""true""` on source namespaces or objects to express intent for sharing, which the controller can validate against.

3. **RBAC Guidance and Warnings**  
   Emit events and logs warning users when cross-namespace references require elevated permissions. Provide documentation on least-privilege RBAC setup.

4. **Future Deprecation Pathway**  
   Document intent to encourage same-namespace patterns in future versions, potentially introducing mirrored or projected sources (e.g., via `LocalObjectReference` with source propagation controllers) as safer alternatives.

### Consequences
**Positive**
- Maintains backward compatibility with existing Flux-based deployments.
- Preserves operational flexibility for multi-tenant setups where shared sources are necessary.
- Enables gradual tightening of security policies without breaking changes.

**Negative**
- Increases complexity in permission and policy management.
- Risk of misuse if administrators leave permissive mode enabled indefinitely.
- Slight performance overhead from additional API lookups and policy checks during reconciliation.

**Neutral**
- Does not eliminate the underlying tension between usability and security; instead, makes it configurable.

### References
- [Kubernetes Namespace Isolation Best Practices](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)
- [Flux Source Controller Cross-Namespace Refs](https://fluxcd.io/flux/components/source/gitrepositories/#cross-namespace-references)
- [Kubebuilder Webhook Validation](https://book.kubebuilder.io/reference/webbook.html)

> This decision aligns with the principle of *secure-by-default, flexible-by-design*.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The components provided by this package SHOULD primarily be designed
with the following aims:","**Decision**: Design all components in this package to prioritize reusability, accessibility, and consistency across diverse application contexts.

**Rationale**:  
To ensure the components are broadly applicable and maintainable, they must be built with a focus on:

- **Reusability**: Components should be stateless or minimally stateful, configurable via props or settings, and free of tightly coupled business logic.
- **Accessibility (a11y)**: All components must comply with WCAG 2.1 AA standards, supporting keyboard navigation, screen readers, and semantic HTML.
- **Consistency**: Visual design and behavior should follow a unified design system (e.g., typography, spacing, color tokens) to ensure predictable integration.

This approach ensures that the components can be easily adopted across different projects, teams, and use cases while maintaining high quality and usability standards.","## Decision

Define all suitable components in a way which enables this uniform
workflow:"
"## Context

An alternative configuration procedure to ADR-0002, possibly better
suited for dynamic theming, theme changes and separating the component
configuration between behavioral and stylistic aspects. This new
approach utilizes the hdom context object to retrieve theme attributes,
whereas the previous solution ignored the context object entirely.

A live demo of the code discussed here is available at:

[demo.thi.ng/umbrella/hdom-theme-adr-0003](https://demo.thi.ng/umbrella/hdom-theme-adr-0003)","## Decision

Adopt the new configuration approach outlined in this ADR as the preferred method for dynamic theming in the hdom-based UI system, superseding ADR-0002.

We will utilize the hdom context object to manage and propagate theme attributes, enabling real-time theme switching, improved separation of concerns, and more predictable behavior in component rendering. Stylistic configuration (e.g. colors, spacing, typography) will be resolved at render time via the context, while behavioral configuration (e.g. callbacks, interaction logic) remains passed explicitly through component props.

This change supports greater runtime flexibility, simplifies theme-dependent component implementations, and aligns with functional reactive principles by treating themes as contextual state rather than static or globally scoped values.

The previous approach from ADR-0002‚Äîrelying on global or closure-scoped theme objects independent of the hdom context‚Äîis deprecated and should no longer be used for new development. Existing uses are to be refactored during maintenance cycles unless constrained by backward compatibility requirements.

Implementation guidelines:
- Theme values must be accessed via `ctx.theme.valueName` within component functions.
- Components must declare expected theme keys in documentation or types.
- The root app component is responsible for initializing the theme in the context.
- Theme changes trigger selective re-renders of context-dependent components via hdom‚Äôs context propagation mechanism.

Rationale: This decision improves maintainability, enhances support for user-driven theme toggling (e.g. light/dark mode), and strengthens composability by decoupling visual appearance from component logic.",## Decision
"## Context and Problem Statement

On the Application Services team, we have successfully built several re-useable components for sync- and storage-related browser
functionality by following what we've dubbed the ""rust-components"" approach: write the bulk of the code in rust
so we can cross-compile it for different target platforms, have it expose a C-compatible FFI layer, then write a small amount
of FFI bindings code to expose the functionality to each of several different target languages (e.g. Swift and Kotlin).

The FFI layer and foreign-language bindings code is currently written by hand, a tedious and potentially error-prone
process.

Given that we expect to build additional components in this style in the future, and expect more teams at Mozilla to
do the same, can we increase the efficiency and reliability of this work by auto-generating some of this code?","## Decision

**Adopt and integrate an FFI code generation toolchain to auto-generate C-compatible FFI layers and foreign-language bindings (e.g., Swift and Kotlin) from Rust, using a well-defined interface definition as input.**

### Rationale

1. **Improved Developer Efficiency**: Hand-writing and maintaining FFI layers and bindings across multiple languages is time-consuming and repetitive. Code generation significantly reduces boilerplate, accelerates development, and allows engineers to focus on core logic in Rust.

2. **Reduced Error Surface**: Manual FFI code is prone to memory safety bugs, type mismatches, calling convention errors, and inconsistencies across platforms. Auto-generated code, when based on a robust schema, ensures correctness by construction and enforces consistency.

3. **Scalability Across Teams**: As more Mozilla teams adopt the ""rust-components"" pattern, a standardized, automated approach lowers the barrier to entry and promotes uniformity in implementation, testing, and integration practices.

4. **Future-Proofing and Maintainability**: Generated bindings are easier to regenerate when APIs evolve, reducing technical debt. Changes to the Rust interface can be reflected across all target languages with minimal effort.

5. **Industry Precedent and Tooling Maturity**: Tools such as [`uniffi-rs`](https://github.com/mozilla/uniffi-rs) (developed at Mozilla) are purpose-built for this use case. They support generating FFI glue code and native language bindings (Kotlin, Swift, Python, etc.) from `.udl` (Universal Data Language) interface definitions, aligning perfectly with our cross-platform goals.

### Selected Approach: Use `uniffi-rs`

We will standardize on **`uniffi-rs`** as the primary FFI code generation framework because:

- It was designed specifically for Mozilla‚Äôs cross-language Rust component ecosystem.
- It supports our target languages: Swift (iOS) and Kotlin (Android).
- It generates safe, idiomatic bindings and handles memory management, error propagation, and threading abstractions.
- It integrates smoothly into existing build systems (e.g., Gradle, Xcode) via plugins.
- It is actively maintained and used in production within Firefox components.

### Implementation Plan

1. **Define Interfaces in UDL**: For new (and eventually existing) components, define public APIs in `.udl` files alongside Rust implementation.
2. **Integrate UniFFI Build Tools**: Add `uniffi-bindgen` to build pipelines to generate Rust FFI stubs and language-specific bindings.
3. **Update CI/CD**: Ensure generated code is checked and regenerated during builds to prevent skew.
4. **Migrate Existing Components Gradually**: Begin with new features or components; refactor critical hand-written FFI code over time.
5. **Document and Train**: Create internal documentation, templates, and onboarding materials for teams adopting the new workflow.

### Trade-offs

- **Initial Setup Cost**: Requires learning curve and toolchain integration effort.
- **Interface Rigidity**: UDL enforces a clean separation between Rust and foreign code, which may require refactoring of complex or dynamic APIs.
- **Debugging Indirection**: Generated code adds a layer between developer and runtime; however, this is mitigated by good error messaging and source maps.

### Status

**Accepted**

### Author(s)

Application Services Team

### Date

2023-10-05","## Decision Drivers

* Reduce time taken to launch a new rust component.
* Improve maintainability of existing rust components.
* Reduce possibility of errors in hand-written foreign language bindings code.
* Continue shipping components on a regular cadence."
"## Context and Problem Statement

When [deciding to build this tool](./0000-whats-the-big-idea.md), the main risk identified was that we'd spend too
much time on an ultimately unworkable or unmaintainable idea. What early design decisions can we make to mitigate
this risk? What things are an existential risk to the success of this project that must be included in the first
version, and what things can we safely defer to future work?

In other words: how do we build an MVP of this tool that is both *minimal* and *viable*?","**Decision: Adopt a ""Risk-First"" MVP Strategy with Core Viability Gates**

To mitigate the primary risk of investing time in an unworkable or unmaintainable idea, we will define the Minimum Viable Product (MVP) not by feature completeness, but by **early validation of the highest-risk architectural and usability assumptions**.

### Rationale

Traditional MVPs often focus on delivering user-facing functionality quickly, but this can lead to building on unstable foundations. Given that the main risk is *technical feasibility and long-term maintainability*, we must prioritize de-risking the core architecture early‚Äîeven if it means deferring polish or secondary features.

By front-loading validation of critical risks, we ensure that:
- We fail fast if the foundational approach is flawed.
- We validate maintainability concerns before accruing technical debt.
- Future development is built on proven, viable components.

### Decision Details

We will structure the MVP around **three Core Viability Gates**, each representing an existential risk to the project:

1. **Execution Model Viability**
   - *Risk*: The tool cannot reliably execute its intended function at acceptable speed or correctness.
   - *MVP Inclusion*: Implement a minimal end-to-end pipeline that processes a simple input through the core engine and produces output.
   - *Deferrables*: UI/UX, configuration options, error handling beyond fatal cases.

2. **Extensibility & Modularity**
   - *Risk*: The system becomes unmanageable as new features are added.
   - *MVP Inclusion*: Design and implement plugin/module interfaces for key components (e.g., parsers, transformers) even if only one implementation exists initially.
   - *Deferrables*: Plugin marketplace, dynamic loading, rich metadata.

3. **Observability & Debuggability**
   - *Risk*: Failures are opaque, making issues hard to diagnose and fix.
   - *MVP Inclusion*: Basic logging, tracing, and inspection capabilities from day one.
   - *Deferrables*: Dashboard, alerting, advanced analytics.

Features outside these gates‚Äîsuch as authentication, web UI, export formats, integrations‚Äîare explicitly **deferred** until after viability is confirmed.

### Consequences

- **Positive**
  - Early detection of fatal flaws reduces wasted effort.
  - Architectural sustainability is validated before scaling.
  - Clear criteria for deciding whether to pivot, proceed, or abandon.

- **Negative**
  - Initial version may appear incomplete or unpolished to external stakeholders.
  - Requires discipline to resist adding ‚Äújust one more feature.‚Äù

### Status

Accepted

### Owners

Project Lead, Chief Architect

### References

- [0000-whats-the-big-idea.md] ‚Äî Original vision and identified risks
- Martin Fowler, ‚ÄúStepping Away from the Ball of Mud‚Äù ‚Äî On early architectural investment
- Lean Startup methodology ‚Äî Build-Measure-Learn feedback loops","## Decision Drivers

* Strictly timebox our efforts to ""prove out"" the approach.
* Establish whether we can effectively maintain this kind of tool as a team.
* Support initial development of a new rust component with externally-imposed, near-term deadlines."
"## Context and Problem Statement

Passing complex data-types from Rust to foreign-language code and back again can be, well, *complex*.
Given a Rust struct with named fields, and a corresponding autogenerated data class in the foreign language
bindings, how does one turn into the other?","## Decision: Use Serde with JSON for Complex Data Transfer Between Rust and Foreign Languages

### Context
When building FFI (Foreign Function Interface) bindings between Rust and foreign languages (e.g., Python, JavaScript, Kotlin), there is a need to pass complex data types‚Äîparticularly structs with named fields‚Äîacross the language boundary. Direct memory sharing is unsafe and impractical due to differing memory layouts, object models, and runtime semantics.

Autogenerated bindings (via tools like `uniffi`, `pyo3`, or `wasm-bindgen`) often create corresponding data classes in the target language, but efficient and safe translation of nested or non-trivial Rust structs remains challenging.

---

### Decision
Use **Serde** with **JSON serialization** as the canonical interchange format for complex data types passed between Rust and foreign-language code.

Specifically:
- Derive `serde::Serialize` and `serde::Deserialize` on all Rust structs intended for cross-language transfer.
- Serialize structs to JSON strings (or byte arrays) at the FFI boundary in Rust.
- Pass these serialized values across the FFI as C-compatible strings (`*const c_char`) or byte buffers.
- Deserialize them in the foreign language using its native JSON parser into the corresponding auto-generated data class.
- For performance-critical paths, consider using more compact formats like CBOR (via `serde_cbor`) while retaining the same pattern.

---

### Rationale
1. **Interoperability**: JSON is universally supported across programming languages, making it ideal for data exchange.
2. **Simplicity**: Avoids manual field-by-field mapping and reduces boilerplate.
3. **Robustness**: Handles nested structs, enums, vectors, and optionals seamlessly via Serde.
4. **Tooling Support**: Works well with FFI generation tools like `uniffi-rs`, which support `String`/`Vec<u8>` as first-class argument types.
5. **Debuggability**: JSON payloads are human-readable, easing debugging and integration testing.

---

### Example
```rust
#[derive(Serialize, Deserialize)]
struct UserProfile {
    name: String,
    age: u32,
    email: Option<String>,
}

#[no_mangle]
pub extern ""C"" fn get_user_profile() -> *mut c_char {
    let user = UserProfile {
        name: ""Alice"".to_string(),
        age: 30,
        email: Some(""alice@example.com"".to_string()),
    };
    let json = serde_json::to_string(&user).unwrap();
    CString::new(json).unwrap().into_raw()
}
```

In Python (or other target language), receive the string, parse JSON, and instantiate the corresponding class.

---

### Alternatives Considered
| Option | Why Rejected |
|-------|--------------|
| Raw struct pointers | Unsafe; ABI incompatibility; no portability |
| Manual field marshaling | Error-prone, verbose, hard to maintain |
| Bincode | Faster but not human-readable; less interoperable |
| Protocol Buffers | Requires schema definition; adds build complexity |

---

### Implications
- ‚úÖ Safe and portable across platforms/languages
- ‚ö†Ô∏è Small runtime overhead due to serialization/deserialization
- ‚úÖ Easily versionable (forward/backward compatibility via Serde attributes)
- ‚úÖ Integrates well with automated binding generators

---

### Status
Accepted

### Date
2024-04-05

### Author
Architecture Team","## Decision Drivers

* Ensuring safety of the generated code.
* Fitting with our [MVP goal](./0001-mvp-webidl.md) of favouring initial-time-to-ship over performance."
"## Context and Problem Statement

Uniffi currently uses a very coarse locking strategy for managing concurrent access to object instances,
which has caused us to accidentally ship code in a product that [blocked the main thread on network I/O](https://jira.mozilla.com/browse/SDK-157).
We need to enable finer-grained concurrency control in order to provide the desired API for a key consumer.

Currently, every interface has a corresponding [ffi_support::ConcurrentHandleMap](https://docs.rs/ffi-support/0.4.0/ffi_support/handle_map/struct.ConcurrentHandleMap.html) that is responsible for owning all instances of
that interface and for handing out references to them in a mutability-safe and threadsafe manner. This
ensures that the generated code is safe in the face of concurrent operations, but has a substantial
runtime cost: only one method call can be executed on an instance at any time. Any attempt to call an
object method while a concurrent method is already executing, will block until the previous call has completed.

The desired API for Project Nimbus includes methods that will be called synchronously from the main thread,
and hence must not block on network or disk I/O. Such an API cannot be built with uniffi as currently
implemented.","# Architectural Decision Record: Fine-Grained Concurrency Control for UniFFI Objects

## Status
Accepted

## Context
UniFFI currently employs a coarse-grained locking mechanism via `ffi_support::ConcurrentHandleMap` to manage access to object instances across FFI boundaries. This global lock per interface ensures memory safety but serializes all method calls on any instance of that interface, leading to significant performance bottlenecks.

This design has resulted in production issues where synchronous method calls from the main thread were unexpectedly blocked on long-running operations like network I/O (e.g., [SDK-157](https://jira.mozilla.com/browse/SDK-157)). For Project Nimbus and similar use cases, we require an API that allows synchronous method calls from UI threads without blocking on expensive operations.

The current model assumes that all methods need mutual exclusion, which is overly conservative‚Äîmany methods are either idempotent, read-only, or operate on independent internal state.

## Decision
We will modify UniFFI's concurrency model to enable **fine-grained, user-controlled synchronization** by:

1. **Removing the default global lock** provided by `ConcurrentHandleMap` for interface instances.
2. **Transferring responsibility for thread-safety to the component implementer**, requiring them to ensure their Rust implementation is safe for concurrent access as needed.
3. **Providing opt-in utilities** for common synchronization patterns (e.g., `Arc<Mutex<T>>`, `RwLock<T>`) through documentation and code generation hints.
4. **Updating the code generator** to no longer inject automatic locking around method calls, instead generating direct calls into the underlying type.
5. **Adding linting and documentation guidance** to highlight concurrency risks and best practices when implementing interfaces with shared mutable state.

For cases where coarse-grained locking is still desired, developers can explicitly wrap their struct fields in `Mutex` or use other standard library synchronization primitives.

## Rationale
- **Performance**: Eliminates unnecessary serialization of method calls, especially beneficial for read-heavy or non-interfering operations.
- **Flexibility**: Enables implementers to choose appropriate concurrency strategies (e.g., `RwLock` for read/write differentiation, async processing, message passing).
- **Correctness**: Aligns with Rust‚Äôs philosophy of making concurrency explicit and compile-time checked where possible.
- **Necessity**: Required to support Project Nimbus‚Äôs requirement for non-blocking synchronous APIs on the main thread.

Alternative approaches considered:
- **Per-instance locking**: Still too coarse if multiple instances exist; doesn't solve the fundamental issue of forcing synchronization where not needed.
- **Attribute-based locking control**: Would add complexity to IDL without sufficient benefit over letting Rust handle it naturally.
- **Async/Sync bifurcation**: Not viable for platforms (like iOS/UIKit) that require synchronous main-thread APIs.

## Implications
### Positive
- Allows creation of responsive APIs that do not block the main thread.
- Better performance for concurrent workloads.
- More idiomatic integration with Rust's ownership and concurrency model.

### Negative
- Increases burden on implementers to ensure thread safety.
- Risk of data races if developers incorrectly assume automatic synchronization.
- Breaking change for consumers relying on implicit serialization.

### Migration
- A migration guide will be provided.
- The change can be gated behind a feature flag during transition.
- Warnings can be added in earlier versions suggesting manual synchronization.

## Stakeholders
- Project Nimbus team (primary driver)
- Firefox SDK team
- Consumers of UniFFI-generated bindings in mobile and desktop applications

## References
- [SDK-157](https://jira.mozilla.com/browse/SDK-157): Main thread blocking due to UniFFI locking
- `ffi_support::ConcurrentHandleMap`: https://docs.rs/ffi-support/0.4.0/ffi_support/handle_map/struct.ConcurrentHandleMap.html
- Rust RFC 2349 (Arbitrary Self Types) ‚Äì enables future improvements in method call ergonomics","## Decision Drivers <!-- optional -->

* Enabling consumers to control the potential blocking behaviour of their generated APIs.
* Ensure safety of the generated code.
* Ship a solution in a timely manner to unblock Project Nimbus."
"## Context and Problem Statement

[ADR-0003](0003-threadsafe-interfaces.md) introduced support for ""thread-safe
interfaces"" - possibly leading to the impression that there is such a thing as
non-threadsafe interfaces and confusion about exactly what the attribute means.

However, the entire concept of non-threadsafe interfaces is a misconception -
the Rust compiler insists that everything wrapped by uniffi is thread-safe -
the only question is who manages this thread-safety. Interfaces which are not
marked as thread-safe cause uniffi to wrap the interface in a mutex which is
hidden in the generated code and therefore not obvious to the casual reader.

The `[Threadsafe]` marker acts as a way for the component author to opt out of
the overhead and blocking behaviour of this mutex, at the cost of opting in to
managing their own locking internally. This ADR proposes that uniffi forces
component authors to explicitly manage that locking in all cases - or to put
this in Rust terms, that all structs supported by uniffi must already be
`Send+Sync`

Note that this ADR will hence-forth use the term `Send+Sync` instead of
""Threadsafe"" because it more accurately describes the actual intent and avoids
any misunderstandings that might be caused by using the somewhat broad and
generic ""Threadsafe"".","## Decision

**Require all exported Rust types in UniFFI components to implement `Send + Sync`, removing implicit synchronization and the `[Threadsafe]` attribute.**

We will enforce that all Rust structs exposed through UniFFI bindings must be `Send + Sync`. This ensures thread safety is guaranteed at compile time by Rust‚Äôs type system, eliminating ambiguity around concurrency semantics.

As part of this decision:

1. **Remove the implicit mutex wrapping** for non-`[Threadsafe]` interfaces. Previously, UniFFI automatically wrapped non-thread-safe-marked types in a `Mutex` to make them safe for cross-thread use in foreign language bindings. This behavior created a false dichotomy between ‚Äúthread-safe‚Äù and ‚Äúnot thread-safe‚Äù interfaces while hiding synchronization details from developers.

2. **Eliminate the `[Threadsafe]` IDL attribute.** Since all exported types must now be `Send + Sync`, the attribute no longer serves a purpose and may mislead users into thinking there are two valid paths to thread safety. Its removal simplifies the IDL and emphasizes that thread safety is mandatory, not optional.

3. **Shift responsibility for internal mutability and locking to component authors.** If a component requires interior mutability or synchronized access, it is the author's responsibility to use appropriate types such as `Arc<Mutex<T>>`, `RwLock<T>`, or atomic types ‚Äî ensuring they are still `Send + Sync`.

4. **Update code generation and runtime checks accordingly.** The UniFFI scaffolding will assume all exported objects can be safely shared across threads and will not generate additional locking logic. Runtime assertions or diagnostics may be added during development to detect misuse.

5. **Provide clear documentation and migration guidance.** Components currently relying on implicit synchronization will need to wrap their inner state explicitly (e.g., with `Mutex`) if mutation is required, or ensure existing types already meet `Send + Sync` bounds.

### Rationale

- **Consistency with Rust‚Äôs ownership model:** Relying on `Send + Sync` aligns with idiomatic Rust practices and leverages the compiler to enforce correctness.
- **Transparency and predictability:** Removing hidden mutexes makes concurrency behavior explicit and avoids performance penalties where unnecessary.
- **Simpler mental model:** There is no longer a distinction between ‚Äúthread-safe‚Äù and ‚Äúautomatically made thread-safe‚Äù types ‚Äî all are required to be thread-safe upfront.
- **Reduced codegen complexity:** Eliminating conditional locking logic reduces generated code size and potential bugs.

This change strengthens the reliability and clarity of UniFFI-based components by requiring explicit, safe handling of concurrency, consistent with Rust‚Äôs core principles.","## Decision Drivers

* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make
  them `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally
  having method calls unexpectedly block for long periods, such as
  [this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)
  (with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).

* Supporting such structs will hinder uniffi growing in directions that we've
  found are desired in practice, such as allowing structs to use [alternative
  method receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to
  [pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419)."
"## Context and Problem Statement

UniFFI currently manages object instances using the `HandleMap` struct in the ffi-support crate.
This means that external consumers of UniFFI-wrapped interfaces never see
any pointers to structs - instead, they get what is (roughly) an index into
an array, with the struct being stored in (and owned by) that array.

This has a number of safety characteristics which are particularly important for
hand-written FFI interfaces, but it does cause some issues in evolving UniFFI in
directions we consider important. In addition to the slight performance overhead,
the use of `HandleMap`s makes it difficult to support:

* Passing object instances as arguments ([#40](https://github.com/mozilla/uniffi-rs/issues/40)).
  Getting objects out of a `HandleMap` involves a closure, so accepting multiple
  object-typed arguments would involve code-generating nested closures.
* Returning object instances from functions ([#197](https://github.com/mozilla/uniffi-rs/issues/197)).
  Does the returned object already exist in the handlemap? If so, what is its handle?
  How will we manage the lifetime of multiple references to the object?

These restrictions mean that UniFFI's `Object` type is currently only suitable
as the `self` argument for method calls, and is forbidden in argument position,
as record fields, etc.

This ADR considers ways to evolve the handling of object instances and their
lifetimes, so that references to structs can be used more widely than currently allowed.","# ADR: Replace HandleMap with Raw Pointers and Reference Counting for Object Lifetime Management

## Decision

We will **replace the `HandleMap`-based object management in UniFFI with a reference-counted, pointer-based approach** using `Rc<RefCell<T>>` (or `Arc<Mutex<T>>` for `ThreadSafe` objects) paired with raw pointers across the FFI boundary. This enables:

1. Passing object instances as function arguments.
2. Returning object instances from functions.
3. Storing object instances in record fields and collections.

The core decision is to shift from index-based indirection (`HandleMap`) to **direct ownership semantics via reference counting**, while maintaining memory safety through Rust's type system and controlled exposure of raw pointers only at the FFI boundary.

---

## Status

Accepted

---

## Context

As outlined, `HandleMap` provides strong safety guarantees by decoupling external handles from actual object pointers, preventing dangling pointers and use-after-free by design. However, this abstraction introduces significant limitations:

- **Limited composability**: Cannot pass or return objects freely.
- **Performance overhead**: Indirect lookups on every method call.
- **Complex code generation**: Nested closures required for multiple object arguments.
- **Artificial restrictions**: Objects cannot be used in many natural API designs.

Use cases such as factories, dependency injection, object graphs, and event systems require passing and returning objects ‚Äî essential for expressive cross-language APIs.

Alternatives considered included extending `HandleMap` with reference tracking or global GC-like mechanisms, but these add complexity without solving fundamental ergonomics issues.

---

## Decision Drivers

- ‚úÖ Support for objects as function parameters and return values.
- ‚úÖ Ability to store objects inside records and other data structures.
- ‚úÖ Simpler, flatter FFI calling convention without nested closures.
- ‚úÖ Compatibility with existing Rust idioms (e.g., shared mutability via `RefCell`).
- ‚úÖ Zero-cost abstractions where possible (avoid hash maps or global locks per call).
- ‚ùó Accept increased risk at FFI boundary due to raw pointer usage (mitigated below).

---

## Solution

### 1. Object Representation

Each exported `#[uniffi::export] struct MyObject {}` becomes:

```rust
type RustType = std::rc::Rc<std::cell::RefCell<MyObject>>;
```

For thread-safe variants:
```rust
type RustType = std::sync::Arc<std::sync::Mutex<MyObject>>;
```

UniFFI generates glue code that wraps instances in `Rc<RefCell<T>>`.

### 2. FFI Boundary: Raw Pointer Passing

Across the FFI boundary, objects are represented as opaque `uintptr_t` (C) / `Pointer` (Kotlin) / `UnsafeRawPointer` (Swift), which hold the address of the heap-allocated `Rc<RefCell<T>>`.

Example (C header):
```c
void myobject_do_something(uintptr_t uniffi_object_handle);
```

### 3. Lifecycle Management

- **Construction**: When an object is created (via constructor or method), it is boxed into `Rc<RefCell<T>>`, then a raw pointer to the box is passed across FFI.
- **Cloning**: Copying the handle increments the `Rc` reference count.
- **Dropping**: A generated `free` function decrements the `Rc` count; when zero, the object is dropped.

Generated destructor:
```rust
extern ""C"" fn free_myobject(handle: u64) {
    let ptr = handle as *mut Rc<RefCell<MyObject>>;
    unsafe { drop(Box::from_raw(ptr)) };
}
```

> üîê Safety: Only UniFFI-generated code manipulates the `Rc` via raw pointers. Consumers interact only with opaque handles.

### 4. Interior Mutability

All methods take `&self` or `&mut self`, but internally route to `borrow()` or `borrow_mut()` on the `RefCell`. This ensures runtime borrow checking instead of compile-time, necessary since multiple languages may access state.

This matches current behavior and preserves safety within Rust.

### 5. Thread Safety

- Non-thread-safe objects (`!Send + !Sync`) panic if sent across threads.
- `ThreadSafe` objects use `Arc<Mutex<T>>` and can be shared across threads.
- Language bindings must ensure only one active mutable reference at a time (enforced by `Mutex`).

---

## Trade-offs

| Pros | Cons |
|------|------|
| Enables rich object interactions (args, returns, fields) | Slight runtime cost of `RefCell` borrows |
| Eliminates `HandleMap` lookup overhead | Raw pointers at FFI boundary (unsafe if misused) |
| Simplifies codegen (no nested closures) | Potential refcount cycles (developer responsibility) |
| Aligns with Rust ownership model | Requires careful handling of cyclic references |

> ‚ö†Ô∏è **Mitigation for raw pointers**: The FFI interface remains safe because:
> - Only UniFFI-generated code creates/dereferences the pointers.
> - Invalid handles result in `Rc`/`Arc` drop or panic (not UB) when used.
> - Debug builds can include handle validation via side tables (optional).

---

## Backward Compatibility

- Breaking change: Old `.udl` files that rely on `HandleMap` semantics may need updates.
- Migration path:
  - Introduce feature flag: `uniffi-use-pointer-objects`.
  - Deprecate old style; update documentation and examples.
  - Eventually make pointer-based objects default.

Generated bindings evolve automatically; no changes needed in consumer code beyond recompilation.

---

## Alternatives Considered

1. **Extend `HandleMap` with reference counting**
   - Would solve some issues but adds complexity.
   - Still requires global lock or map lookup per call.
   - Doesn't eliminate closure nesting problem.

2. **Introduce a global GC (e.g., using `rental` or custom arena)**
   - Overkill for most use cases.
   - Adds non-deterministic cleanup.
   - Harder to integrate with foreign language lifetimes.

3. **Return full copies (Clone) instead of references**
   - Not feasible for mutable state or large objects.
   - Breaks expectation of identity preservation.

4. **Hybrid: Use `HandleMap` for args/returns, keep pointers internally**
   - Loses performance benefits.
   - Increases cognitive load with dual models.

None matched the simplicity, performance, and expressiveness of reference counting with raw pointers.

---

## Prior Art

- **Swift/Objective-C**: Uses `CFRetain`/`CFRelease` with opaque pointers.
- **Python C API**: Reference counted PyObject*.
- **Flutter/Dart FFI**: Manual refcounting patterns.
- **WebAssembly Interface Types (WIT)**: Plans for resource reference counting.

All demonstrate that reference counting with opaque pointers is a proven pattern for cross-language object lifetime management.

---

## Conclusion

Switching from `HandleMap` to reference-counted smart pointers with raw pointer FFI interchange enables critical functionality ‚Äî passing and returning objects ‚Äî while preserving memory safety and improving performance. It aligns UniFFI more closely with both Rust idioms and industry-standard FFI practices.

This change unlocks a new class of APIs and paves the way for advanced features like callbacks, listeners, and complex object graphs.

---

## Follow-Up Actions

1. Implement prototype in `uniffi-rs` with feature gate.
2. Benchmark performance vs `HandleMap`.
3. Update all language bindings (Kotlin, Swift, Python) to support pointer-based objects.
4. Document migration guide and anti-patterns (e.g., cycles).
5. Deprecate `HandleMap` backend after successful adoption.","## Decision Drivers

* We desire the ability to have more flexible lifetimes for object interfaces, so
  they can be stored in dictionaries or other interfaces, and be returned by
  functions or methods other than constructors.

* We would like to keep the UniFFI implementation as simple as possible while
  providing a suitable degree of safety - in particular, a promise that it
  should be impossible to misuse the generated bindings in a way that triggers
  Rust's ""undefined behavior"" or otherwise defeats Rust's safety
  characteristics and ownership model (and in particular, avoiding things like
  use-after-free issues).

* We would like to keep the overhead of UniFFI as small as possible so that it
  is a viable solution to more use-cases."
"## Context and Problem Statement

UniFFI was not able to support types from external crates because Rust's orphan
rule prevents implementing the `ViaFfi` trait.  In order to add support we
needed to choose between updating the `uniffi` traits or updating the
`lift_py` and `lower_py` scaffolding functions.

The same general question comes up often.  When adding new features we often
need to choose between two paths:

  * Updating the code in the target language
  * Updating the template wrapping code

This ADR discusses this particular decision and also the general pros and cons of each
path.","# ADR: Prefer Modifying Template Wrapping Code Over Target Language Code for Cross-Crate Type Support

## Decision

We will prefer modifying the template wrapping code (e.g., `lift_py`, `lower_py` scaffolding functions) rather than extending or modifying the core `uniffi` traits when adding support for new features, particularly for handling types from external crates.

For the specific case of external crate type support, we will implement a solution using wrapper types and modified scaffolding code that automatically generates appropriate conversion logic, rather than attempting to modify the `ViaFfi` trait system to work around Rust's orphan rule.

## Rationale

This decision balances immediate technical constraints with long-term maintainability:

### Technical Constraints
- Rust's orphan rule fundamentally prevents implementing `ViaFfi` for external types in external crates
- Modifying core `uniffi` traits would require breaking changes affecting all users
- The template approach allows us to work within Rust's type system limitations

### Pros of Modifying Template Wrapping Code:
- **Non-breaking**: Doesn't require changes to the core library API
- **Flexible**: Can handle complex conversion logic without polluting the trait system
- **Target-language specific**: Allows optimizations tailored to each language's capabilities
- **Incremental**: Can be implemented feature-by-feature
- **Isolated**: Changes are contained to code generation, reducing risk

### Pros of Modifying Target Language Code:
- **Potentially simpler**: Direct trait implementations can be more straightforward
- **Better performance**: May avoid intermediate conversions
- **More idiomatic**: Could result in cleaner generated code

However, the drawbacks of modifying target language code (breaking changes, increased complexity in the core library, reduced flexibility) outweigh these benefits.

## Implications

### Positive
- Maintains backward compatibility
- Provides a general pattern for solving similar future problems
- Keeps the core `uniffi` library stable and focused
- Enables support for external types without fundamental architectural changes

### Negative
- Generated code may be slightly less efficient due to additional wrapping
- Scaffolding code becomes more complex
- Users may find the auto-generated wrappers less transparent

## Alternatives Considered

1. **Modify ViaFfi trait**: Would require significant changes to accommodate external types, likely breaking existing code
2. **New macro-based approach**: Could introduce complexity and learning curve
3. **Manual wrapper requirement**: Would shift burden to users, reducing usability

We rejected these alternatives because they either introduced unacceptable breaking changes or shifted too much complexity onto library users.

## Status

Accepted

## Date

2023-11-15","## Decision Drivers

* We wanted to support external crates that define new types by wrapping an
  UniFFI primitive type.  For example supporting `serde_json::Value` that wraps
  `string` or a `Handle` that wraps `int32`.  We wanted this kind of wrapping
  code to exist outside of `uniffi` to allow for more experimentation with
  wrapped types and to support types that were specific to particular libraries
  (for example the application-services `Guid` type)."
"## Context and Problem Statement
All the binding generators currently live in the [`uniffi_bindgen`](../../uniffi_bindgen/src/bindings) crate. This creates the following difficulties:

- All the bindings live in the `uniffi` repository, so the `uniffi` team has to maintain them (or at the very least review changes to them). This makes it difficult to support third-party developers writing bindings for languages the core team does not wish to maintain.
- Any change to a specific binding generator requires a new `uniffi_bindgen` release for it to be accessible by consumers. Even if it doesn't impact any of the other bindings.
- Some bindings require complex build systems to test. Including those build systems in `uniffi` would require developers to install those build systems, and CI to do the same. For example, any type of `gecko-js` bindings would require the mozilla-central build system to build and test.
- We currently run all the tests for the bindings in our CI and through `cargo test`. This means that if one binding target gets outdated and fails, or if a developer doesn't have the needed libraries installed for one of the targets, the tests would fail.

Before [PR 1201](https://github.com/mozilla/uniffi-rs/pull/1201), it was also impossible to write new bindings that did not live in the [`uniffi_bindgen`](../../uniffi_bindgen/src/bindings) crate.

This ADR proposes enabling third-party crates to implement binding generators, and describes the necessary uniffi changes to enable this.","# ADR: Enable Third-Party Binding Generators via Plugin Architecture

## Decision

We will refactor the `uniffi_bindgen` crate to support a plugin-based architecture that allows third-party crates to implement and distribute their own binding generators independently of the core `uniffi` repository.

Specifically, we will:

1. **Define a stable `BindingGenerator` trait** in `uniffi_bindgen` that encapsulates the interface for code generation, including methods for:
   - Generating foreign language bindings from UDL
   - Emitting appropriate files/artifacts
   - Specifying required dependencies or build steps
   - Providing metadata (language name, version, etc.)

2. **Extract core code generation utilities** into a public API within `uniffi_bindgen` that third-party generators can use, such as:
   - UDL parsing and AST representation
   - Type mapping infrastructure
   - Template-based code generation helpers

3. **Modify `uniffi_cli`** to dynamically discover and invoke binding generators through:
   - Command-line specification of generator crates (`--bindings python`, `--bindings mylang`)
   - Support for both built-in and external generators
   - Clear error messages when generators are unavailable or misconfigured

4. **Implement a discovery mechanism** where:
   - Built-in generators remain in-tree for common languages (Rust, Kotlin, Swift, etc.)
   - External generators are implemented in separate crates following the `uniffi_bindgen_{language}` naming convention
   - Generators are loaded either as direct dependencies or via dynamic dispatch

5. **Update documentation and provide templates** for third-party developers to create new binding generators, including:
   - Tutorial for implementing a new generator
   - Example repository template
   - Testing best practices (including how to handle complex build systems externally)

6. **Adjust CI pipelines** to:
   - Only test built-in binding generators
   - Remove requirements for third-party toolchains from main CI
   - Provide guidance for external projects on setting up their own CI

## Rationale

This approach addresses all identified problems:

- ‚úÖ **Decouples maintenance**: Third-party language teams can maintain their own generators without burdening the core team.
- ‚úÖ **Independent release cycles**: Language-specific improvements can be released independently of `uniffi_bindgen`.
- ‚úÖ **Reduces build/test complexity**: Complex build systems (e.g., mozilla-central for gecko-js) can be contained within external crates.
- ‚úÖ **Improves test reliability**: Main CI only runs tests for maintained generators; local `cargo test` won't fail due to unused language setups.
- ‚úÖ **Enables innovation**: Lowers barrier to entry for experimental or niche language bindings.

The trait-based design ensures type safety while allowing extensibility, and maintains backward compatibility with existing workflows. This balances flexibility with stability‚Äîthe core uniffi team retains control over the interface while enabling community-driven expansion.","## Decision Drivers

* Support Firefox Desktop JavaScript binding generation
* Testability, it should be easy for developers to test the bindings they care about. Without having to navigate and install unfamiliar libraries and build systems.
* Developer experience, it should be easier to write and maintain a new binding generator than it currently is.
* Releases, cutting releases for changes in one binding generator shouldn't harm another.

**NOTE**: Version compatibility is handled in a [separate ADR](https://github.com/mozilla/uniffi-rs/pull/1203)"
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision: Transition to Publicly Triggerable E2E Workflows with Local Testing Support

### Status
Accepted

### Context
Historically, Zarf‚Äôs end-to-end (E2E) testing required access to private image registries via repository secrets (e.g., `registry1`) in order to build the initialization package. Due to security concerns around exposing these secrets to untrusted pull requests in our open-source repository, we used `peter-evans/slash-command-dispatch@v2` to restrict workflow execution to maintainers only. This prevented potential secret leakage but introduced friction: contributors could not trigger E2E tests on their PRs without maintainer intervention.

With Zarf v0.15, the `zarf-init-{{arch}}.tar.zst` package no longer depends on images from `registry1`. As a result, full E2E test execution can now proceed without requiring repository secrets. This removes the primary security constraint that necessitated restricted test triggering.

Additionally, we aim to improve test coverage and developer experience by:
1. Expanding support for multiple Kubernetes distributions (e.g., K3s, Kind, EKS, K8s).
2. Validating behavior across various Linux distributions.
3. Enabling developers to run a meaningful subset of E2E tests locally for faster iteration.

### Decision
We will **deprecate the use of `slash-command-dispatch` for general E2E testing** and instead allow GitHub Actions workflows to be triggered automatically on all pull requests, provided they do not require privileged secrets. Instead:

1. **Publicly Runnable CI Workflows**:  
   Refactor the core E2E test suite into two tiers:
   - **Tier 1 (PR-safe)**: Runs on all PRs from any contributor. Uses public images and local or lightweight clusters (e.g., Kind). Does not require secrets. Covers basic functionality including `zarf init`, `zarf package deploy`, and common component types.
   - **Tier 2 (Maintainer-only)**: Includes advanced scenarios involving private registries, air-gapped environments, or specific infrastructure (e.g., EC2-based clusters). Triggered manually by maintainers using an approval mechanism or slash command.

2. **Multi-Distribution Testing Matrix**:  
   Use GitHub-hosted runners and containerized clusters (e.g., Kind) to test against different Kubernetes distributions (K3s, upstream K8s) and node OS types (Ubuntu, Amazon Linux) within Tier 1 where possible.

3. **Local E2E Execution Framework**:  
   Introduce a new `e2e-local` script/tool (`make e2e-local` or `hack/run-e2e.sh`) that allows developers to run a trimmed version of the E2E suite against a user-managed cluster (e.g., Kind, Minikube, or remote dev cluster). This improves feedback speed and reduces reliance on CI for debugging.

4. **Secretless Initialization**:  
   Ensure the default `zarf-init` package used in CI is built entirely from public images and included as an artifact or built during CI without authentication.

5. **Security Retention for Sensitive Flows**:  
   Maintain `peter-evans/create-or-update-comment` + `slash-command-dispatch` patterns only for workflows that still require secrets (e.g., publishing releases, integration with internal staging environments).

### Consequences
**Pros:**
- ‚úÖ Contributors receive immediate feedback on PRs without maintainer intervention.
- ‚úÖ Improved transparency and collaboration in the open-source community.
- ‚úÖ Faster development cycles via local E2E testing.
- ‚úÖ Broader test coverage across environments through matrix testing.
- ‚úÖ Reduced risk surface since secrets are no longer needed for baseline E2E.

**Cons:**
- ‚ö†Ô∏è Increased CI usage and cost due to broader auto-execution.
- ‚ö†Ô∏è Need to carefully isolate any remaining secret-dependent flows to prevent accidental exposure.
- ‚ö†Ô∏è Additional complexity in maintaining dual-tiered test architecture.

### Implementation Steps
1. Audit current E2E workflows and remove dependency on `registry1`.
2. Split E2E jobs into `e2e-pr.yaml` (public) and `e2e-full.yaml` (maintainer-triggered).
3. Implement matrix testing over OS and K8s distribution in `e2e-pr.yaml`.
4. Develop and document `hack/run-e2e-local.sh` for local execution.
5. Update CONTRIBUTING.md to reflect new testing model and local capabilities.
6. Deprecate and remove `slash-command-dispatch` usage from standard PR workflows.

### Reviewed By
Zarf Maintainers Group ‚Äì 2025-04-05","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","**Architectural Decision Record (ADR)**

---

### **Title**  
Use a Temporary Internal Registry with Node Affinity and TLS Injection for Image Distribution in Air-Gapped or Generic Kubernetes Clusters

---

### **Status**  
Accepted

---

### **Context**

Zarf operates in diverse and often constrained environments, including air-gapped systems, where no external registry is available and cluster-specific tooling (like `k3d load image` or `kind load docker-image`) may not exist. To deploy workloads, container images must be made available to the Kubernetes nodes‚Äô container runtimes (CRI). However:

- Kubernetes does not provide a standard API for directly pushing images into node CRIs.
- The CRI operates at the host level, outside Kubernetes control.
- Pulling from private registries requires TLS trust to be established on each node.
- Trust configuration (CA certificates or CRI config) is node-specific and varies by CRI (containerd, CRI-O, etc.).
- Modifying all nodes' trust stores is operationally complex and insecure if automated without safeguards.

Additionally, Zarf must remain self-contained and not depend on pre-existing infrastructure such as a registry or CLI tools.

---

### **Decision**

We will implement the following strategy in Zarf:

1. **Deploy a temporary internal registry** inside the target Kubernetes cluster during package deployment:
   - This registry runs as a pod with a `hostNetwork: true` or `NodePort`/`ClusterIP` service depending on reachability needs.
   - It uses a minimal, secure registry implementation (e.g., `registry:2`) with ephemeral storage.
   - It is automatically torn down after image distribution completes.

2. **Inject TLS credentials into the registry and distribute trust to node CRIs**:
   - Generate a self-signed CA and server certificate for the registry.
   - Bundle the CA cert into the Zarf package.
   - Use privileged init containers or hostPath mounts (via DaemonSet) to inject the CA certificate into the trusted certificate store of each node‚Äôs OS **and/or** the CRI-specific certificate directory (e.g., `/etc/containerd/certs.d` or `/etc/docker/certs.d`).

3. **Use node affinity and taints/tolerations to control image placement during bootstrap**:
   - During initial setup (e.g., deploying Git, container registry, or other foundational services), use `nodeName`, `nodeSelector`, or taint-based scheduling to ensure pods are scheduled only on nodes where the registry is trusted and images have been loaded.
   - Alternatively, preload images via the temporary registry across all nodes before general scheduling begins.

4. **Push images to the temporary registry from the Zarf CLI**:
   - The Zarf CLI pushes images to the internal registry endpoint using the provided TLS credentials.
   - Kubernetes workloads reference images from this internal registry.

5. **Support fallback to direct image loading when possible**:
   - Detect known local clusters (KIND, K3D, etc.) and use their native image load commands instead of the registry method, improving speed and avoiding complexity.

6. **Ensure zero dependency assumption**:
   - All components (registry image, certs, tooling) are included in the Zarf package or generated at runtime.
   - No external network access or pre-existing services required.

---

### **Rationale**

| Option Considered | Why Rejected |
|-------------------|------------|
| Direct CRI access via `crictl` on nodes | Requires SSH, elevated privileges, inconsistent across environments; not feasible in restricted setups. |
| Host-level script to copy images | Not portable, requires per-node access, breaks abstraction, hard to scale. |
| Require user to preload images | Violates Zarf's goal of automation and zero-touch deployment. |
| Use existing private registry | Contradicts ""unknown environment"" requirement ‚Äî cannot assume existence or accessibility. |

The chosen approach balances portability, security, and compatibility:
- Works on any conformant Kubernetes cluster regardless of underlying CRI or tooling.
- Avoids permanent infrastructure changes.
- Maintains security through short-lived components and controlled trust injection.
- Enables full automation within Zarf‚Äôs operational model.

---

### **Consequences**

**Pros:**
- ‚úÖ Works in fully air-gapped, unknown, or minimal environments.
- ‚úÖ No reliance on external registries or vendor-specific CLIs.
- ‚úÖ Secure image transport via TLS with managed PKI lifecycle.
- ‚úÖ Reusable pattern for bootstrapping other internal services.

**Cons:**
- ‚ö†Ô∏è Requires privileged operations (hostPath, DaemonSet, potentially elevated RBAC) for CA injection ‚Äî increases attack surface if misconfigured.
- ‚ö†Ô∏è Complexity in handling different CRI configurations (containerd vs CRI-O paths differ).
- ‚ö†Ô∏è Temporary registry becomes a critical path component ‚Äî failure blocks deployment.
- ‚ö†Ô∏è Certificate trust must be carefully cleaned or documented if left behind.

**Mitigations:**
- Clearly document privilege requirements and security implications.
- Provide flags to disable auto-trust injection and allow manual approval.
- Auto-cleanup all temporary resources (registry pod, service, secrets) post-deployment unless debug mode is enabled.
- Include validation steps to confirm image reachability before proceeding.

---

### **References**

- [OCI Distribution Specification](https://github.com/opencontainers/distribution-spec)
- [KIND Load Image](https://kind.sigs.k8s.io/docs/user/quick-start/#loading-an-image-into-your-cluster)
- [K3D Import Image](https://k3d.io/v5.4.6/usage/imports/)
- [Containerd Cert Configuration](https://github.com/containerd/containerd/blob/main/docs/hosts.md)
- [CRI-O Registries Configuration](https://github.com/cri-o/cri-o/blob/main/docs/crio.conf.5.md#registry-table)

--- 

*Approved by: Zarf Architecture Team*  
*Date: 2025-04-05*","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision

Generate and maintain a Software Bill of Materials (SBOM) for all software products deployed on government systems to comply with Executive Order (EO) 14028.

The SBOM will be produced in a standardized, machine-readable format (preferably SPDX, CycloneDX, or SWID) and integrated into the software release pipeline. It will list all components, dependencies, libraries, and modules included in the software build, along with their versions, suppliers, unique identifiers, and known vulnerabilities where available.

Ownership of SBOM generation and accuracy lies with the Product and Security teams. The SBOM will be delivered to government customers upon request and updated for each software release or patch. Automated tooling (e.g., SCA tools like Syft, Dependency-Track, or commercial equivalents) will be used to generate and validate SBOMs as part of CI/CD workflows.

This decision ensures compliance with EO 14028, enhances transparency, supports vulnerability management, and strengthens supply chain security posture.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","**Decision: Implement a Zarf Mutating Webhook Controller for Transparent Runtime Mutation of Image References and Git URLs**

---

### Status

Accepted

### Context

Zarf currently relies on Helm post-rendering to rewrite image references and Git URLs (e.g., pointing to the internal Zarf registry or Gitea instance) during deployment. This approach functions correctly when Zarf directly applies manifests using `helm template | zarf prepare | kubectl apply`. However, it breaks in GitOps workflows where tools like **Flux** or **ArgoCD** manage the `kubectl apply` lifecycle independently. In such cases:

- The Helm post-renderer is never invoked.
- Image and Git URL mutations do not occur.
- The cluster attempts to pull from external sources, which may be inaccessible or insecure.

While `zarf prepare` was introduced to partially address this by preprocessing manifests, it shifts the burden to package authors to manually integrate it into their pipelines and templating logic‚Äîleading to inconsistency, complexity, and reduced portability.

Additionally, Custom Resource Definitions (CRDs) and operator-based workloads often reference the internal Gitea server or registry directly, and these cannot be easily templated or preprocessed without deep coupling to Zarf internals.

This limitation undermines Zarf‚Äôs goal of transparently abstracting infrastructure details in air-gapped or restricted environments.

### Decision

Introduce a **Zarf Mutating Webhook Controller** that runs inside the cluster and automatically rewrites image references and Git URLs at admission time, similar to how [Kubernetes mutating admission webhooks](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook) operate.

The controller will:
1. Intercept pod creation and other relevant resource creation events (e.g., Jobs, Deployments, etc.).
2. Mutate container image fields to point to the **internal Zarf registry** if they match configured patterns (e.g., original upstream registry).
3. Optionally mutate Git repository URLs in specific CRs (via object path annotations or known schema matching) to point to the **internal Zarf Git server**.
4. Use configuration from Zarf's generated `zarf-config` secret (or ConfigMap) to determine internal endpoints and mappings.

This solution enables seamless integration with GitOps tools because mutation occurs **at runtime**, regardless of who applies the manifest.

### Rationale

| Option Considered | Pros | Cons |
|-------------------|------|------|
| **Continue relying on `zarf prepare` + templating** | Simple to implement; no cluster components needed | Places burden on users; error-prone; doesn't scale across diverse tooling |
| **Enhance Helm charts with complex templating** | Works today | Not applicable outside Helm; increases chart complexity; fails for CRDs/operators |
| **Adopt existing tools (e.g., Flux Image Reflector/Updater)** | Mature ecosystem support | Requires per-package setup; not automatic; couples to Flux only |
| ‚úÖ **Zarf Mutating Webhook Controller** | Transparent, automatic, works with any deployer (GitOps or not), centralized logic | Adds a small control-plane component; requires cert management |

The mutating webhook provides a **transparent, zero-touch experience** for package authors and users while preserving compatibility with all deployment methods‚Äîincluding GitOps.

It aligns with Kubernetes-native patterns and ensures consistent behavior regardless of how manifests are applied.

### Implications

**Positive**
- Enables native use of ArgoCD, Flux, and other GitOps tools with Zarf packages.
- Removes need for manual preprocessing (`zarf prepare`) in most cases.
- Centralizes mutation logic, reducing duplication and errors.
- Extensible to future mutation needs (e.g., sidecar injection, env var rewriting).

**Negative**
- Introduces a new in-cluster component requiring installation and lifecycle management.
- Requires proper TLS certificate setup for the webhook (can be automated via init sequence).
- Slight delay in pod scheduling due to admission interception (negligible in practice).

**Mitigations**
- Bundle the webhook as an optional but recommended Zarf package (`zarf-agent`).
- Automatically generate and rotate TLS certs during `zarf init`.
- Allow opt-out via namespace label (e.g., `zarf-mutation: disabled`).
- Ensure minimal scope via precise webhook rules and object selectors.

### Prior Art

- **[Open Policy Agent (OPA)/Gatekeeper](https://github.com/open-policy-agent/gatekeeper)** ‚Äì Uses admission control for policy enforcement.
- **[Kyverno](https://kyverno.io/)** ‚Äì Implements policy and mutation via admission controls.
- **[Flux Image Automation](https://fluxcd.io/flux/components/image/imagerepository/)** ‚Äì Automates image updates, but not transparent runtime mutation.

Zarf‚Äôs approach is simpler and focused: only mutate known Zarf-related references based on runtime config.

### Next Steps

1. Design webhook API and mutation rules.
2. Implement controller using controller-runtime (same as Zarf Operators).
3. Package as `zarf-agent` extension package.
4. Update `zarf init` to optionally deploy the agent.
5. Deprecate reliance on `zarf prepare` for image/Git rewriting over time.
6. Document migration path for existing packages.

---

*This decision enables Zarf to support modern GitOps practices while maintaining its core value proposition: seamless operations in isolated environments.*","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

Currently, Zarf only allows variables to be specified within components which introduces the following limitations:

 - Variables are scoped to the component making reuse between components impossible
 - Variables cannot be used for elements within the package definition itself
 - Variables can only be set at create time, not by a user at deploy time

This forces a package creator to copy information within their package/component definitions and also requires them to make bespoke packages per environment even if only small changes are needed to the overall spec (such as changing a domain name).","## Decision: Introduce Package-Level Variables with User Override Support

### Status
Accepted

### Context
Zarf currently restricts variables to component scope, which limits reusability and cross-component coordination. This results in:
- Duplication of common values (e.g., domain names, image registries) across components
- Inability to parameterize top-level package fields (e.g., metadata.name, URLs)
- Lack of support for environment-specific configuration at deploy time
- Proliferation of nearly identical packages for different environments

These limitations hinder maintainability, increase error surface, and reduce the portability of Zarf packages across environments.

### Decision
Introduce **package-level variables** that:
1. Are defined in the top-level `zarf.yaml` outside of any component
2. Can be referenced within both package metadata and component definitions using a consistent syntax (e.g., `{{ variable.name }}`)
3. Support user-specified values at deploy time via CLI flags or interactive prompts
4. Maintain backward compatibility with existing component-scoped variables
5. Allow component-level variables to override package-level variables of the same name (for backward compatibility)

Example schema:
```yaml
variables:
  - name: DOMAIN
    default: ""example.com""
    description: ""Primary domain for services""
    type: string
  - name: REPLICAS
    default: ""3""
    type: int

components:
  - name: web-app
    variables:
      - name: DOMAIN  # Optional: override at component level
        default: ""web.${DOMAIN}""
```

Deployment override:
```bash
zarf package deploy --set DOMAIN=myorg.prod --set REPLICAS=5
```

### Consequences
**Positive:**
- Enables single-package deployment across multiple environments
- Reduces duplication and improves maintainability
- Allows parameterization of package metadata (e.g., dynamic naming)
- Supports CI/CD pipelines with environment-specific overrides
- Maintains backward compatibility

**Negative:**
- Increases complexity in variable resolution logic
- Requires validation to prevent circular references
- May introduce security considerations for sensitive variable handling
- Adds parsing overhead during package processing

**Mitigations:**
- Implement clear error messages for undefined required variables
- Add schema validation for variable types and required fields
- Document precedence rules (CLI > component > package > default)
- Provide dry-run mode to inspect resolved values

This change significantly improves Zarf's flexibility while maintaining its declarative nature and backward compatibility.","## Decision

The decision was made to move variable definitions to the package level and to split ""variables"" into three distinct types:

- Variables (specified with the `variables` yaml key) allow for the templating of component files similar to the component variables before them.  The main changes are that they are now specified at the package level (allowing for reuse between components) and have additional prompting and defaulting features to allow a package creator to ask for more information during `zarf package deploy`.
- Constants (specified with the `constants` yaml key) also template component files, but must be specified at package create time.  This allows a package creator to use the same value in multiple places without the need for copying it and without the package deployer being able to override it.
- Package Variables (specified by using `"
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","### Decision

**Replace the `stage2` Go-based registry binary with a lightweight, purpose-built HTTP server in Rust that directly serves the `registry:2` crane tarball via the Docker Registry v2 API, eliminating the need to run a full Docker registry during injection.**

---

### Rationale

The current two-stage bootstrapping process introduces unnecessary complexity and overhead for what is essentially a short-lived, single-image distribution task. The `stage2` binary uses a full Docker `registry:2` server instance just to temporarily host one image before it‚Äôs consumed by the `zarf-registry` package. This leads to:

- **Operational bloat**: Managing an additional binary increases maintenance cost.
- **Performance impact**: More configmaps increase `zarf init` duration.
- **Build complexity**: Cross-compilation of multiple binaries per platform slows CI.
- **Resource inefficiency**: Running a full registry process is excessive when only static blob and manifest serving is needed.

By replacing `stage2` with a minimal, embedded HTTP server written in Rust (co-located in `stage1`), we can:
- Serve the Docker Registry v2 API endpoints required to push and serve a single image.
- Respond to `GET /v2/`, `GET /v2/<name>/manifests/<tag>`, and `GET /v2/<name>/blobs/<digest>` with preloaded data from configmaps.
- Return appropriate headers and responses to satisfy `crane` or containerd clients.
- Avoid spawning any external processes or long-running services.

This new server will:
- Be compiled into `stage1`, removing the need for a separate binary.
- Use zero dynamic dependencies.
- Run only as long as needed during injection (seconds).
- Reduce total configmap count by ~50% by co-locating assets with `stage1`.

Once the `registry:2` image is pushed into the real `zarf-registry`, the injector pod exits and is cleaned up ‚Äî same final state, but faster and leaner.

---

### Implications

#### ‚úÖ Benefits
- Reduces total Zarf binaries from 3 to 2 (`stage1`, `zarf-agent`).
- Speeds up `zarf init` by reducing Kubernetes object creation.
- Simplifies build pipeline: no more cross-compiling a second Go binary.
- Lowers memory/CPU footprint during initialization.
- Keeps logic centralized in Rust (`stage1`) where low-level control is beneficial.

#### ‚ö†Ô∏è Trade-offs
- Increases size of `stage1` slightly due to added HTTP server logic.
- Requires implementing a minimal subset of the Docker Registry API (but this is well-documented and narrow in scope).
- Less reuse of upstream `distribution/distribution` code (but gains simplicity).

#### üõë Mitigations
- The implemented API surface is tiny and testable via mock HTTP clients.
- Can leverage existing Rust crates like `hyper` for robustness without sacrificing binary size significantly (via tree-shaking and LTO).
- Clear separation of concerns within `stage1`: staging vs serving logic.

---

### Status

Accepted

---

### Related ADRs

- ADR-3: Two-Stage Bootstrap Process  
- ADR-4: Lightweight Registry Injector (this document)","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision: Introduce Unit Testing in a Defined and Limited Capacity within Zarf

We will adopt **unit testing selectively** across the Zarf codebase, focusing on areas that are:
- Difficult or impossible to test through end-to-end (e2e) tests,
- Part of public or stable interfaces now that Zarf is being used as a library,
- Prone to regression due to complexity or frequent changes,
- Isolated components where unit testing provides clear value without excessive maintenance cost.

This approach balances test effectiveness with long-term maintainability. Rather than pursuing broad unit test coverage for the sake of metrics, we prioritize strategic use of unit tests to increase confidence in critical logic and defined interfaces while avoiding brittle, implementation-focused tests.

### Scope & Guidelines

1. **Targeted Use Cases**
   - Pure functions and utility packages (e.g., config parsing, path resolution, validation logic).
   - Core business logic not easily reachable via e2e flows.
   - Public APIs and exported functions intended for library consumers.

2. **Testing Principles**
   - Test behavior, not implementation: Prefer black-box style tests over mocking internal details.
   - Avoid over-mocking; if a unit test requires extensive mocks, consider whether integration testing is more appropriate.
   - Keep tests readable, deterministic, and fast.

3. **Enforcement & Governance**
   - New unit tests must be justified in PR reviews based on the above criteria.
   - The maintainers will document and evolve best practices in a `TESTING.md` guide.
   - CI will require unit test passes where they exist, but we will not enforce arbitrary coverage thresholds.

4. **Complement, Don‚Äôt Replace**
   - This strategy complements existing e2e testing; it does not replace it.
   - Integration-style partial e2e tests may still be introduced on a case-by-case basis when needed.

### Rationale

While full-scale unit testing (Option 1) risks creating a high-maintenance burden with diminishing returns, especially in rapidly evolving code, completely avoiding unit tests leaves gaps in our verification‚Äîparticularly as Zarf's role expands beyond a CLI tool to a reusable library.

Integration testing (Option 3) aligns with current practices but often lacks precision for isolated logic and can become unwieldy when trying to cover edge cases or error conditions.

Thus, **Option 2 ‚Äî limited and intentional unit testing** ‚Äî offers the most sustainable path forward. It enables us to harden key interfaces and complex logic under unit scrutiny while preserving agility and avoiding test-induced design rigidity.

---

**Status**: Accepted  
**Date**: 2025-04-05  
**Author(s)**: Architectural Review Team  
**Related Issues**: #1294, #1301  
**See Also**: [Zarf TESTING.md (future)](https://github.com/defenseunicorns/zarf/blob/main/TESTING.md)","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision: SBOM files and dataInjections by default, with extensibility for future inclusion

We will generate Software Bill of Materials (SBOMs) using Syft **for `files` and `dataInjections` included in Zarf packages** as the default and required behavior during package creation. This approach strikes the right balance between completeness, accuracy, and usability.

### Rationale

- **Accuracy over noise**: Running Syft on the entire temporary build directory (Option 1) risks including false positives from intermediate or unrelated artifacts (e.g., Git metadata, build tooling, vendored dependencies not actually shipped). This could mislead users about what is truly deployed.
  
- **Actionable security insights**: The `files` and `dataInjections` fields are the primary mechanisms through which non-container artifacts (like binaries, configuration bundles, scripts, or CLI tools such as k3s and the injector) are embedded into Zarf packages. These components are most likely to contain third-party dependencies with known vulnerabilities.

- **Real-world relevance**: Our own `zarf init` package includes k3s and the injector via these mechanisms, confirming that this covers high-value use cases where SBOM visibility is critical.

- **User experience and adoption**: Making SBOM generation automatic and focused ensures that all users benefit from enhanced transparency without needing to configure or understand optional settings‚Äîavoiding the discoverability and complexity pitfalls of Option 3 (user-defined scope).

- **Extensible foundation**: While starting with `files` and `dataInjections`, we will design the SBOM subsystem to support future expansion (e.g., scanning git-based artifacts, OCI charts, or allowing opt-in broader scans), paving the way for Option 3-like flexibility later.

### Implementation Plan

1. During `zarf package create`, after all `files` and `dataInjections` are resolved and copied into the build context:
   - Run Syft on each individual file specified in `files` and `dataInjections`.
   - Aggregate results into a unified SBOM document (e.g., CycloneDX or SPDX) included in the final package.

2. Store the SBOM at a well-known path inside the Zarf package (e.g., `.zarf-sbom/files.sbom.json`) and expose it via `zarf inspect sbom`.

3. Ensure syft runs in ""file"" mode (`syft dir:./path/to/file --file-parity`) to analyze standalone binaries or archives without requiring a filesystem mount.

4. Document limitations: SBOMs do not currently cover runtime-fetched content or unreferenced build-time assets.

5. Log a warning if large numbers of files are scanned, advising performance expectations.

### Future Considerations

- Add support for user-defined paths to scan (moving toward Option 3).
- Explore lightweight provenance tracking for git sources used in packages.
- Integrate with vulnerability scanners (e.g., Grype) post-SBOM generation for advisory reporting.

---

By focusing on actual packaged artifacts today, we deliver immediate value with minimal risk of confusion, while laying the groundwork for richer supply chain security tomorrow.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","**Decision: Introduce a ""Connected Mode"" for Zarf that enables declarative package deployment without requiring a Zarf init package**

---

### Status
Accepted

### Context
Zarf was originally designed to support air-gapped and disconnected environments, where all dependencies (including base infrastructure components like a container registry or ingress controller) must be bundled and deployed via an **init package**. This ensures self-containment and reliability in restricted environments.

However, in connected environments‚Äîwhere clusters already have access to standard services such as container registries, Helm repositories, and identity providers‚Äîthe overhead of deploying and managing a Zarf init package is unnecessary. Users still benefit from Zarf‚Äôs declarative packaging model for application deployment, configuration management, and lifecycle operations (deploy, upgrade, remove), but are hindered by the requirement to initialize with a full Zarf runtime stack.

There is growing demand to use Zarf as a purely declarative deployment tool in environments where infrastructure services are already present and managed externally.

### Decision
Introduce a **connected mode** in Zarf that allows users to deploy Zarf packages without initializing a Zarf-managed control plane (i.e., no init package required). In this mode:

- The `zarf init` step becomes optional.
- Zarf packages can reference remote images, Helm charts, git repositories, and other OCI artifacts directly using standard URLs.
- Package validation will ensure referenced resources are accessible at deploy time (in connected contexts).
- Security policies (e.g., image signing, SBOM verification) can still be enforced via Zarf's existing policy framework.
- The `zarf prepare` and `zarf deploy` commands remain fully functional, operating on manifests and applying them directly to the target cluster.
- The state tracking, component ordering, and rollback capabilities of Zarf are preserved.

This mode will be activated either through:
- A flag (`--connected`) during `zarf deploy`, or
- A field in the `zarf.yaml` package definition indicating `distribution: connected`.

Support for both disconnected (default, legacy-compatible) and connected modes ensures backward compatibility while expanding Zarf‚Äôs applicability.

### Consequences
**Pros:**
- Lowers barrier to entry for teams wanting to adopt Zarf in cloud or enterprise environments.
- Enables faster iteration and testing by eliminating the need to build and deploy an init package.
- Maintains Zarf‚Äôs value proposition‚Äîdeclarative, auditable, reproducible deployments‚Äîwithout requiring full air-gap tooling.
- Opens integration paths with CI/CD systems where ephemeral clusters already have networking.

**Cons:**
- Reduces portability of packages between disconnected and connected environments unless explicitly designed for both.
- Increases complexity in package authoring due to dual-mode support.
- Potential security concerns if remote dependencies are not pinned or verified; mitigated by encouraging best practices (image digest pinning, policy enforcement).

### Future Considerations
- Allow hybrid packages that include some embedded assets and pull others remotely.
- Add linters to warn about unpinned or insecure remote references in connected mode.
- Explore integration with service meshes or GitOps controllers (e.g., Argo CD, Flux) using Zarf as a manifest generator.

---

**Related Issues:**  
N/A (Placeholder for future tracking)

**Author(s):**  
[Your Name], Zarf Maintainer Team

**Date:**  
2025-04-05","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","**Decision: Enhance the `scripts` noun with structured, lifecycle-specific hooks and execution context awareness**

To address the growing limitations of the original `scripts` implementation ‚Äî which started with simple `before` and `after` arrays and later added `prepare` ‚Äî we will refactor and expand the `scripts` functionality to support:

- **Explicit lifecycle phases**: Replace ambiguous or overloaded script placements with clearly defined hooks:
  - `onCreatePrepare`: Runs during `zarf package create`, replacing and formalizing `prepare`.
  - `onDeployBefore`: Runs on the target cluster/node before component deployment.
  - `onDeployAfter`: Runs after component resources are applied.
  - `onRemoveBefore`: (Optional) Runs before component removal.
  - `onUpdatePrepare`: (Future-proofing) Runs when updating a package.

- **Structured scripting interface**: Each hook supports:
  - `cmd`: The command to execute (required).
  - `dir`: Working directory context.
  - `timeout`: Custom timeout per script.
  - `skipOnError`: Allow continuation despite failure (with warnings).
  - `env`: Environment variables scoped to the script.

- **Execution context awareness**: Scripts can access contextual data such as:
  - Package variables (`$ZARF_VAR_*`)
  - Component name and namespace
  - Target OS/architecture

- **Local vs. Remote execution flag**:
  - `local: true` runs script on the host doing `zarf deploy`
  - `local: false` (default) runs inside the target environment

### Rationale

The original flat `before`/`after`/`prepare` model lacks clarity in intent, leads to misuse, and complicates debugging. Teams now use scripts for sidecar setup, data migrations, pre-flight checks, and post-deploy notifications ‚Äî use cases that demand better structure and safety.

This change improves:
- **Discoverability**: Clear phase names make behavior predictable.
- **Reliability**: Timeouts and error handling prevent hangs.
- **Portability**: Context-aware scripts adapt to environments.
- **Maintainability**: Separation of concerns across lifecycle events.

### Example Updated Schema

```yaml
components:
  - name: mysql
    scripts:
      onCreatePrepare:
        - cmd: ./generate-config.sh
          dir: /templates
          local: true
      onDeployBefore:
        - cmd: mkdir -p /data/mysql
          dir: /opt
      onDeployAfter:
        - cmd: mysql-setup-db.sh
          env:
            - DB_HOST=localhost
```

### Backward Compatibility

Support for legacy `before`, `after`, and top-level `prepare` will be maintained through deprecation cycles with warning logs. Documentation and CLI hints will guide users toward the new structure.

---

**Status**: Accepted  
**Date**: 2025-04-05  
**Author**: Architectural Review Board  
**Replaces**: ADR-012 ""Initial Scripts Support""","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision: Retain Crane for Docker Image Loading with Streaming Memory Safeguards

### Status
Accepted

### Context
Zarf previously lacked local image support, which was addressed in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) via integration with [Crane](https://github.com/google/go-containerregistry). However, a critical issue ([#1214](https://github.com/defenseunicorns/zarf/issues/1214)) emerged where large images cause out-of-memory (OOM) crashes due to Crane loading entire tar streams into memory during `docker save` operations. While alternative tools like `skopeo`, the Docker Go client, and direct CLI invocation avoid OOM conditions, they incur significant performance penalties‚Äîup to 3x slower than Crane in key workflows.

Performance benchmarks show Crane consistently outperforms Docker CLI across multiple real-world scenarios:
| Scenario                                      | Docker   | Crane    |
|-----------------------------------------------|----------|----------|
| Big Bang Core (cached)                        | 3m 1s    | 1m 58s   |
| Big Bang Core (cached + skip-sbom)            | 1m 51s   | 56s      |
| 20 GB Single-Layer Image (local registry)     | N/A      | 6m 14s   |
| 20 GB Single-Layer Image (cached)             | 5m 2s    | 2m 10s   |

Additionally, Docker‚Äôs native `save` produces legacy image formats incompatible with planned OCI-based package schema improvements ([#1319](https://github.com/defenseunicorns/zarf/issues/1319)), further disqualifying it as a long-term solution.

### Decision
We will **continue using Crane** as the primary mechanism for loading images from the local Docker daemon, but **enable its streaming mode** to mitigate memory exhaustion issues. This involves:

- Setting `crane.WithContext(context.Background())` and leveraging options such as `crane.WithPlatformMatching()` and ensuring `crane.ExportToTarball()` does not buffer the entire image in memory.
- Wrapping image export operations with bounded memory usage by processing layers incrementally and avoiding full tarball materialization in RAM.
- Introducing a new Zarf configuration or runtime flag (`--image-load-strategy=streaming|buffered`) to allow users to choose between memory-safe streaming (default) and high-performance buffered mode (with warnings about OOM risk).
- Adding proactive memory monitoring and early failure detection when available system memory is insufficient for buffered operations.

We **explicitly reject** switching permanently to Docker CLI or Skopeo due to their performance cost and technical debt (e.g., legacy format), despite their stability advantages.

### Rationale
- **Performance is critical**: Zarf targets air-gapped deployments where efficiency directly impacts usability. Crane offers up to **2x faster image loads**, especially on cached packages.
- **Streaming mitigates OOM safely**: By avoiding full tarball loading, we retain most of Crane‚Äôs speed while eliminating the crash vector.
- **Future-proofing**: Crane supports modern image layouts (OCI, multi-platform), aligning with our roadmap for OCI-centric packages.
- **Controlled trade-off**: Offering both modes allows advanced users to optimize based on environment constraints.

### Consequences
- **Positive**
  - Resolves OOM crashes for most users under typical conditions.
  - Maintains fast performance for common use cases.
  - Supports future OCI integration goals.
- **Negative**
  - Streaming mode may still be slower than current (unsafe) Crane behavior for very large images.
  - Additional complexity in code and user-facing configuration.
  - Requires careful documentation to guide users on memory considerations.

### Planned Follow-Up
- Implement memory-aware image loading in `pkg/packager/build.go`.
- Add e2e tests simulating low-memory environments.
- Document best practices for handling large images in local development.
- Monitor community feedback and revisit if streaming performance proves unacceptable in practice.

---

*Approved by: Zarf Architecture Team*  
*Date: 2025-04-05*","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision: Implement a Big Bang Integration Layer in Zarf with Dynamic Image and Git Repository Filtering

### Status
Accepted

### Context
Big Bang is a widely used component by Zarf end users, but its deployment complexity presents significant challenges:
- It depends on Flux and custom resource definitions (CRDs), requiring orchestration support.
- The default configuration references a superset of container images and git repositories, leading to unnecessarily large Zarf packages (tens of gigabytes).
- Static inclusion of all assets is inefficient and impractical for air-gapped environments.
- Injecting custom `values.yaml` configurations into Big Bang‚Äôs Kustomize-based structure is non-trivial, and the set of required images depends dynamically on these values.

Without intervention, users must either package excessive data or manually curate complex deployments‚Äîboth error-prone and time-consuming.

### Decision
Introduce a **Big Bang integration layer** within Zarf that enables:
1. **Dynamic image and git repository filtering** based on user-provided configuration (e.g., `values.yaml`).
2. **Automated dependency resolution**, where Zarf analyzes the effective configuration to determine which images and repositories are actually needed.
3. **Flux bootstrapping as a prerequisite**, ensuring CRDs and controllers are available before deploying Big Bang.
4. A **specialized Zarf package blueprint** (e.g., `zarf-package-big-bang`) that abstracts away Kustomize overlays and value injection via templated inputs.
5. Use of an **analysis tool** (integrated at package create time) that simulates Helm template rendering and Kustomize build processes to discover required artifacts.

This integration will be implemented as a first-class pattern in Zarf, potentially using a plugin or module system if available, but initially as a documented best-practice package design with helper scripts and tooling.

### Rationale
- **Efficiency**: Reduces package size from tens of GB to only what's necessary (often <5 GB), enabling faster transfers and smaller storage footprint.
- **Usability**: Abstracts complexity for users who want to deploy Big Bang without mastering its internal structure.
- **Correctness**: Ensures Flux is installed prior to Big Bang components, satisfying CRD dependencies.
- **Maintainability**: Centralizes logic for Big Bang deployment within Zarf tooling/community patterns rather than scattering ad-hoc solutions across teams.

Alternative approaches considered:
- **Include all images/repos by default**: Rejected due to excessive size and inefficiency.
- **Manual curation per deployment**: Rejected due to high operational burden and risk of drift/failure.
- **Offload filtering to external tools**: Rejected because it breaks the declarative, self-contained model of Zarf packages.

### Consequences
**Positive**
- Enables practical, scalable use of Big Bang in disconnected environments.
- Sets precedent for intelligent, configuration-aware packaging of complex applications.
- Encourages reuse through standardized Big Bang Zarf packages.

**Negative**
- Increases complexity of Zarf package creation process (requires analysis step).
- Tight coupling to Big Bang structure may require updates when Big Bang changes.
- Initial implementation effort required to build image/repository resolver logic.

**Mitigations**
- Provide versioned templates aligned with Big Bang releases.
- Document escape hatches for advanced users needing full control.
- Design filtering logic to be modular and testable against multiple configurations.

### References
- [Big Bang Documentation](https://repo1.dso.mil/big-bang/bigbang)
- [Big Bang Image List Example](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml)
- [Default Kustomization](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml)","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","**Decision:** Introduce a Zarf package-init-native-git option to enable native git server mirroring and support for common artifact repositories (e.g., npm, PyPI, Maven, NuGet) to enhance air-gapped development workflows.

---

### **Context**
Zarf currently supports air-gapped deployment of applications using git servers and container registries as dependency sources. However, this model is optimized primarily for production deployments where artifacts are pre-built. In air-gapped development environments, developers often require access to language-specific package managers (e.g., `npm install`, `pip install`, `mvn compile`) and direct `git clone` operations that reach upstream sources natively ‚Äî use cases not well supported by Zarf‚Äôs current Flux-based `GitRepository` abstraction.

Additionally, Zarf does not currently mirror arbitrary git repositories in a way that allows standard git clients to interact with them seamlessly (e.g., via HTTPS or SSH), limiting its utility during the development phase.

---

### **Decision**
To better support air-gapped software development:

1. **Introduce a `zarf package init --init-native-git` flag**  
   This will deploy an internal, lightweight **native git server** (e.g., using `git-daemon`, Gitea, or similar) within the Zarf cluster that supports standard `git clone`, `fetch`, and `push` operations over HTTPS/SSH. The server will:
   - Mirror specified upstream git repositories on demand.
   - Allow developers to push feature branches or local commits.
   - Support authentication and syncing through future tooling.

2. **Add support for artifact repository proxies/mirrors**  
   Deploy caching proxies for common language ecosystems:
   - **npm/Node.js**: Internal registry mirror (e.g., Verdaccio).
   - **Python**: PyPI mirror (e.g., devpi or simple HTTP mirror).
   - **JVM/Maven**: Nexus or Artifactory proxy for Maven Central.
   - **.NET/NuGet**: Local NuGet feed mirror.

   These services will be included optionally in the Zarf init package and exposed via the Zarf agent for transparent URL rewriting (e.g., rewriting `https://registry.npmjs.org` ‚Üí `https://internal-npm.zarf.dev`).

3. **Extend the Zarf Agent to intercept external developer tooling requests**  
   Leverage the existing Zarf Agent's DNS and ingress capabilities to transparently redirect:
   - Git domains (`github.com`, `gitlab.com`) ‚Üí internal git mirror.
   - Package manager domains (`registry.npmjs.org`, `pypi.org`) ‚Üí internal caches.

4. **Allow declarative configuration in `zarf.yaml`**  
   Enable users to define which upstream git repos and packages should be pre-mirrored at `zarf init` time:
   ```yaml
   mirrors:
     git:
       - url: https://github.com/org/private-repo
         name: org-private-repo
     npm:
       - package: lodash
         version: '4.x'
     pypi:
       - package: requests
   ```

---

### **Status**
Accepted

---

### **Consequences**

**Pros:**
- Enables full development lifecycle inside air-gaps (clone ‚Üí code ‚Üí build ‚Üí test).
- Improves developer experience by supporting idiomatic tool usage (`git`, `npm`, etc.).
- Reduces reliance on manual artifact collection and injection.
- Scales naturally with existing Zarf principles of declarative, offline-first packaging.

**Cons:**
- Increases complexity of the `zarf init` environment with additional services.
- Requires storage management for potentially large caches.
- May introduce sync challenges between upstream and mirrored content (mitigated by lazy fetch + pre-seeding).

---

### **Alternatives Considered**
- **Manual artifact staging**: Forces developers to manually gather dependencies ‚Äî error-prone and non-scalable.
- **External mirror management**: Offload to external tools like Artifactory; increases operational overhead and breaks Zarf‚Äôs self-contained model.
- **Only extend Flux GitRepository**: Still limits git usage to GitOps patterns, not interactive development.

---

### **Related Issues**
- [Zarf Issue #1032](https://github.com/defenseunicorns/zarf/issues/1032): Request for generic git mirroring.
- [Zarf Issue #876](https://github.com/defenseunicorns/zarf/issues/876): Support for language-level package managers.

--- 

This decision positions Zarf as a comprehensive solution for both air-gapped deployment **and** development.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","**Decision:** Establish a v1.0.0 Release and Stability Roadmap with Defined API and Backward Compatibility Guarantees

To transition Zarf into a Generally Available (GA) v1.x.x product suitable for mission-critical operations, we will formalize a stability roadmap culminating in the release of Zarf v1.0.0. This version will mark the commitment to backward compatibility, predictable release cycles, and production-grade reliability.

### Key Actions:
1. **Freeze Public API Surface**: Define and document all public interfaces (CLI commands, configuration schema, package format, internal APIs used by tooling) that will be subject to semantic versioning starting at v1.0.0.
2. **Adopt Semantic Versioning Strictly**: After v1.0.0, adhere to SemVer (https://semver.org), where breaking changes require a major version increment and are only introduced in controlled, well-documented releases.
3. **Establish Long-Term Support (LTS) Policy**: Designate certain minor releases (e.g., v1.2, v1.4) as LTS versions supported with security patches and critical bug fixes for a defined period (e.g., 12 months).
4. **Enhance Test Coverage**:
   - Expand integration and end-to-end testing across core workflows (init, package create/deploy, git/registry injection, air-gapped operation).
   - Introduce automated upgrade testing between minor versions to ensure backward compatibility.
5. **Introduce Deprecation Periods**: For any feature slated for removal or change post-v1.0.0, provide at least two minor versions‚Äô notice via deprecation warnings in CLI output and documentation.
6. **Formalize Release Process**:
   - Implement time-based minor release cycles (e.g., quarterly).
   - Require release candidates (RCs) and community feedback windows before GA releases.
7. **Document SLA Expectations**: Clearly state support expectations, response times for critical issues, and known limitations in the documentation.

### Rationale:
The current rapid iteration model enables innovation but undermines reliability required for mission-critical adoption. Achieving GA status necessitates a shift toward predictability, stability, and trust. By committing to a stable API and transparent lifecycle management, Zarf can empower enterprise and government users to adopt it confidently without requiring deep involvement in its development process.

This decision aligns with industry standards for mature open-source projects (e.g., Kubernetes, Helm, Terraform) and positions Zarf as a sustainable, production-ready tool.

### Status: Proposed ‚Üí **Accepted**  
### Date: 2025-04-05  
### Author: Architectural Review Board","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

Orchestrating capabilities from multiple Zarf packages into meta-packages is a current weak point for Zarf. The core of Zarf was built around components as capabilities, but as Zarf packages have scaled, there has been a need to create a new boundary to manage these capabilities efficiently.

Currently there is no official way to enable the deployment, publishing, pulling, and creation of multiple Zarf packages together, and due to this some in the community have resorted to patterns such as:

```yaml
- name: init
  required: true
  files:
    - source: zarf-init-amd64-v0.27.0.tar.zst
      target: zarf-init-amd64-v0.27.0.tar.zst
  actions:
    onDeploy:
      after:
        - cmd: zarf package deploy zarf-init-amd64-v0.27.0.tar.zst --components git-server --confirm -l warn
```

While this _does_ fulfill the need to deploy two packages in one command, it does so in such a way that is verbose within the `zarf.yaml`, brittle across Zarf versions, inefficient within the package structure (it doesn't share layers), and is difficult to use `variables` with.","### Decision: Introduce First-Class Support for Zarf Meta-Packages via a New `meta` Package Type

#### Status
Accepted

#### Context
As Zarf packages grow in number and complexity, teams are increasingly needing to compose multiple discrete Zarf packages into higher-level logical units‚Äîsuch as ""full platform deployments"" or ""development environments""‚Äîthat span clusters, components, and responsibilities. The current workaround of embedding one package inside another using `files` and `actions` (e.g., calling `zarf package deploy` from within a component) is widely used but has significant drawbacks:

- **No shared layers**: Embedded packages cannot deduplicate container images or data, inflating package size.
- **Version brittleness**: Hardcoded filenames like `zarf-init-amd64-v0.27.0.tar.zst` break easily across versions.
- **Poor variable handling**: Variables from the parent package are not naturally inherited or exposed in child packages.
- **Opaque execution flow**: Nested deployments obscure lifecycle management and error tracing.
- **Unsupported operations**: There‚Äôs no way to publish, pull, or validate a group of packages as a unit.

These limitations hinder scalability, maintainability, and usability.

#### Decision
Introduce a new **`meta` package type** in Zarf that enables official, first-class orchestration of multiple Zarf packages as a single logical unit. This meta-package will support:

1. **Declarative composition**:
   ```yaml
   kind: ZarfPackageConfig
   metadata:
     name: full-platform
     version: 0.1.0
     description: ""Deploys init + logging + monitoring""
   components:
     - name: init
       packageRef:
         url: ghcr.io/defenseunicorns/packages/init:v0.27.0
         path: zarf-init-amd64.tar.zst
     - name: observability
       packageRef:
         url: ghcr.io/defenseunicorns/packages/observability:latest
         path: zarf-observability-amd64.tar.zst
   ```
   
2. **Unified CLI commands**:
   - `zarf package deploy` on a meta-package deploys all referenced packages in dependency order.
   - `zarf package create` bundles referenced packages (with deduplication).
   - `zarf package publish` pushes all constituent packages and the meta-manifest.
   - `zarf package pull` retrieves all dependencies atomically.

3. **Shared artifact optimization**:
   - During creation, identical image layers and files across packages are de-duplicated.
   - A shared `sbom` is generated at the meta level.

4. **Variable propagation**:
   - Meta-package variables can be mapped to component package variables:
     ```yaml
     variables:
       - name: DEPLOY_ENV
         default: production
     components:
       - name: app
         packageRef: ...
         variables:
           TARGET_ENV: ${DEPLOY_ENV}
     ```

5. **Dependency & ordering control**:
   - Use `dependsOn` between components to enforce deployment sequence.
   - Allow conditional inclusion via `if` expressions based on variables.

6. **Validation and linting**:
   - `zarf package validate` checks referential integrity, variable mappings, and schema conformance.

This approach avoids modifying the core component model while introducing a new boundary above individual packages‚Äîaligning with the principle of composability without sacrificing modularity.

#### Consequences

**Positive**:
- Enables clean, scalable assembly of complex systems from reusable Zarf packages.
- Eliminates reliance on fragile scripting patterns.
- Supports efficient distribution through layer sharing and atomic operations.
- Improves UX via consistent CLI behavior across simple and composite packages.
- Lays groundwork for future features like cross-package SBOMs, signing, and policy enforcement.

**Negative**:
- Adds complexity to the Zarf CLI and schema.
- Requires tooling changes to handle remote resolution and caching of referenced packages.
- Increases surface area for validation and potential failure modes (e.g., unreachable refs).
- May encourage over-nesting if not guided by best practices.

**Mitigations**:
- Provide clear documentation and scaffolding tools (`zarf init --meta`).
- Include linter rules to discourage deep nesting (>3 levels).
- Implement robust error messages for missing or incompatible references.

#### References
- Community issue: [https://github.com/defenseunicorns/zarf/issues/1294](https://github.com/defenseunicorns/zarf/issues/1294)
- RFC Draft: ""Zarf Meta-Package Design"" (internal)
- Prior art: Helm umbrella charts, KPT functions, Terraform modules

---

> **Note**: This decision does not deprecate existing patterns immediately but provides an official alternative. Future versions may warn on anti-pattern usage (e.g., `zarf package deploy` in `actions`).","## Decision

> :warning: **NOTE**: This functionality was migrated to [uds-cli](https://github.com/defenseunicorns/uds-cli) - this ADR is kept here for historical purposes.

The current proposition (subject to change before acceptance) is **Zarf Bundles**, which a following PR will focus on and create a POC of.

In essence the `zarf-bundle.yaml` would look something like so:

```yaml
metadata:
  name: omnibus
  description: an example Zarf bundle
  version: 0.0.1
  architecture: amd64

packages:
  - repository: localhost:888/init
    ref: """
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision: Implement Registry-Aware Pre-Flight Hooks in Zarf

### Status
Accepted

### Context
As Zarf evolves to support external container registries like Amazon Elastic Container Registry (ECR), new operational challenges arise due to registry-specific behaviors:
- ECR requires short-lived authentication tokens that expire every 12 hours.
- ECR does not allow implicit repository creation on push (""push-to-create""), requiring explicit pre-creation of repositories.

Currently, users must work around these limitations either by performing manual setup before deployment or by embedding ECR-specific logic into package `actions`. Both approaches violate core Zarf principles:
- They break the goal of system-agnostic packages.
- They shift responsibility for platform concerns onto developers and operators.
- They encourage misuse of `actions` as a workaround mechanism rather than an extension tool.

We aim to maintain a clean separation between infrastructure concerns and application packaging, enabling seamless use of external registries without requiring user intervention or registry-specific customization.

### Decision
Introduce **registry-aware pre-flight hooks** directly within Zarf‚Äôs initialization and deployment lifecycle. These hooks will be built-in, automatic behaviors triggered when specific registry types (e.g., ECR) are detected during `zarf init` or package deployment. The solution includes:

1. **Automatic Repository Creation**  
   During package deployment, prior to image pushes, Zarf will:
   - Inspect all images referenced in the package.
   - For ECR registries, ensure corresponding repositories exist via cloud provider API calls (e.g., AWS SDK).
   - Create missing repositories programmatically using credentials provided during `zarf init`.

2. **Token Refresh Automation**  
   Zarf will:
   - Detect when the remote registry is ECR.
   - Use stored cloud credentials (AWS IAM role or token) to request fresh ECR authentication tokens automatically before any image operation.
   - Inject valid tokens into Kubernetes image pull secrets and local Docker config as needed.

3. **Registry Detection & Pluggable Hook System**  
   Extend Zarf‚Äôs internal registry abstraction with a pluggable ""registry adapter"" model:
   ```go
   type RegistryAdapter interface {
       PrePushHook(context *DeploymentContext) error
       NeedsExplicitRepoCreation() bool
       GetAuthToken() (string, error)
   }
   ```
   Built-in adapters will be provided for known registries (ECR, ACR, GCR, etc.), allowing future extensibility.

4. **Secure Credential Handling**  
   Credentials required for API access (e.g., AWS IAM keys or role ARNs) will be collected during `zarf init` and stored securely in Zarf‚Äôs state. They will never be exposed to or managed by individual packages.

5. **Deprecate Workarounds**  
   Discourage the use of `actions` for solving this class of problem by documenting and promoting the new built-in capabilities.

### Consequences
**Pros:**
- ‚úÖ Enables truly system-agnostic Zarf packages ‚Äî no registry-specific logic needed in packages.
- ‚úÖ Reduces operator burden ‚Äî no pre-deployment setup steps required.
- ‚úÖ Preserves intent of `actions` ‚Äî keeps them focused on extending functionality, not patching gaps.
- ‚úÖ Improves security ‚Äî avoids storing long-lived tokens or exposing credentials to user scripts.
- ‚úÖ Future-proof ‚Äî pluggable adapter pattern allows easy addition of other registries with similar constraints.

**Cons:**
- ‚ùå Increases complexity of Zarf core by adding cloud provider dependencies (e.g., AWS SDK).
- ‚ùå Requires secure handling of cloud credentials within Zarf state.
- ‚ùå May introduce longer startup times during deployment due to pre-checks.

**Mitigations:**
- Isolate cloud-specific code behind interfaces and optional compilation flags.
- Use temporary credential chains (e.g., IAM roles) instead of long-term keys.
- Cache repository existence checks and token lifetimes where possible.

### Notes
This decision aligns with Zarf‚Äôs vision of abstracting away cluster and registry complexity from package authors. It transforms `actions` from a crutch into a true extension mechanism while making advanced registry integration seamless and secure.

Future work may include:
- Supporting private Google Artifact Registry or Azure Container Registry with similar automation.
- Allowing community-contributed registry adapters.
- Adding observability into pre-flight hook execution.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision

**Use WebAssembly (WASM) to expose Zarf‚Äôs existing Go-based transformation library to Pepr‚Äôs TypeScript runtime.**

### Rationale

To maintain consistency, reliability, and reduce long-term maintenance overhead, we will leverage WebAssembly (WASM) as the integration layer between Zarf‚Äôs battle-tested Go transformation logic and Pepr‚Äôs TypeScript-based admission controller.

This decision is based on the following evaluation of alternatives:

- **Rewriting in TypeScript**: While this would provide native integration with Pepr, it introduces significant risk due to duplication of complex logic, loss of maturity, increased testing burden, and divergence over time. It violates DRY and undermines trust in a proven system.

- **gRPC/REST**: These require running a separate service or daemon, adding operational complexity, latency, failure modes (networking, serialization), and lifecycle management challenges‚Äîespecially in constrained or air-gapped environments where Zarf operates.

- **WASM**: Enables direct execution of compiled Go code within the Node.js runtime used by Pepr. It provides:
  - **Zero network overhead** ‚Äì runs in-process.
  - **Strong type safety** via generated bindings.
  - **Single source of truth** ‚Äì transformations remain implemented once, in Go.
  - **Portability** ‚Äì works across environments without external dependencies.
  - **Performance** ‚Äì near-native execution for CPU-bound operations like YAML/JSON manipulation.

We will compile the core Zarf transformation package into WASM using TinyGo (which supports WASM output and has a small footprint), then invoke it synchronously from Pepr's TypeScript handlers. This preserves the integrity of the original logic while enabling seamless integration into the admission control flow.

### Consequences

- ‚úÖ **Pros:**
  - Eliminates logic duplication and drift.
  - Leverages years of edge-case hardening in Zarf‚Äôs transformation engine.
  - Minimal operational overhead compared to microservice architectures.
  - Future-proofs integration; new features only need implementation in Go.

- ‚ö†Ô∏è **Cons:**
  - Increased build complexity (cross-compilation to WASM).
  - Debugging across Go ‚Üí WASM ‚Üí JS boundary can be challenging.
  - Limited access to certain Go runtime features in WASM (e.g., `reflect`, `unsafe`).
  - Larger bundle size due to embedded WASM binary.

- üõ†Ô∏è **Mitigations:**
  - Use tooling (e.g., `wasm-bindgen`, custom debug logs) to improve observability.
  - Maintain strict API boundaries and versioning between Go and TS layers.
  - Automate WASM build pipeline within CI/CD.

### Status

Accepted

### Owner

Architecture Team

### Date

2025-04-05","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision: Standardize Package Loading and Management Across All Package Sources

To ensure a consistent user experience and predictable behavior across all Zarf package sources, we will standardize the ability to **inspect**, **deploy**, **remove**, and **load** packages regardless of their source type. This includes:

- Local Tarball (`.tar`, `.tar.zst`)
- Split Tarball (`.part...`)
- OCI (`oci://`)
- In-cluster (Deployed) packages
- Remote HTTPS URLs (e.g., `https://.../package.tar.zst`)

### Decision Details

We will implement a unified **Package Loader Interface** that abstracts how a package is resolved, fetched, and inspected based on its URI scheme or path. Each supported source type will have a corresponding loader implementation that enables full CRUD-like operations where applicable.

| Operation       | Local Tarball | Split Tarball | OCI | In-Cluster | HTTPS URL |
|-----------------|---------------|---------------|-----|------------|-----------|
| `create`        | ‚úÖ            | ‚úÖ            | ‚úÖ  | ‚ùå         | ‚ùå        |
| `deploy`        | ‚úÖ            | ‚úÖ            | ‚úÖ  | ‚ùå¬π        | ‚úÖ        |
| `inspect`       | ‚úÖ            | ‚úÖ¬≤           | ‚úÖ  | ‚úÖ         | ‚úÖ¬≤       |
| `remove`        | ‚úÖ¬≥           | ‚úÖ¬≥           | ‚úÖ‚Å¥ | ‚úÖ         | ‚ùå        |

#### Notes:
1. **In-cluster packages cannot be re-deployed** ‚Äî they are already deployed; however, redeployment can be achieved via `zarf package deploy` using an external source.
2. **Split tarballs and HTTPS URLs**: Inspection requires downloading and reassembling (for split) or streaming parse (for compressed), but does not require full extraction. Metadata will be read from `zarf.yaml` in the archive manifest.
3. **Remove for local/remote archives**: Removes the file(s) from disk (if managed by Zarf). For split tarballs, removes all `.part*` files.
4. **OCI remove**: Unpublishes the image from the registry (via `zarf package remove oci://...`).

### Implementation Plan

1. **Introduce a `Locator` abstraction**:
   - Resolves package references like `./pkg.tar`, `oci://reg/foo`, `https://x/p.tar.zst`, or `cluster://zarf-package`
   - Determines the appropriate loader based on scheme (`file`, `oci`, `https`, `cluster`)

2. **Implement per-source loaders**:
   - `FileLoader`: Handles `.tar`, `.tar.zst`, and `.part.*` files
   - `OCILoader`: Pulls manifests and metadata without full download when inspecting
   - `HTTPLoader`: Streams headers and partial content to extract `zarf.yaml`
   - `ClusterLoader`: Reads deployed package state from cluster ConfigMap/CRD

3. **Enhance CLI commands**:
   - `zarf package inspect <ref>`: Show metadata, components, size, architecture, etc.
   - `zarf package remove <ref>`: Support removal semantics per backend
   - Normalize error messages and timeouts for remote sources

4. **Add cache layer for remote inspection**:
   - Cache OCI manifests and HTTP metadata locally (in `~/.zarf/cache`) to avoid repeated fetches during inspection

5. **Update documentation and examples**:
   - Clarify which operations are available per source
   - Provide guidance on managing lifecycle of split and remote packages

### Rationale

This decision improves usability, automation potential, and maintainability:
- Users can inspect any deployable artifact before taking action
- Automation scripts can reliably query package properties uniformly
- Reduces technical debt from ad-hoc loading logic scattered across commands
- Enables future features like package diffing, signing verification, and dependency resolution

### Out of Scope

- Package versioning beyond OCI tags
- Garbage collection of orphaned OCI layers after removal
- Resumable downloads for large HTTPS packages

---

**Status**: Accepted  
**Date**: 2025-04-05  
**Affected Commands**: `zarf package {create, deploy, inspect, remove, list}`  
**Related Issues**: #1294, #1567, #1882","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","**Decision:** Refactor Zarf's package composition system to introduce a centralized, pipeline-based composition engine that standardizes component merging, enforces strict import chain validation, and unifies filtering logic.

---

### Rationale

The current composition implementation has grown organically, resulting in technical debt and fragility‚Äîparticularly around scaling, maintainability, and consistency. To address the three core issues:

1. **Import chains not scaling**: Deep or complex composition chains lack comprehensive test coverage and error resilience. Without explicit tracking and validation at each layer, failures are hard to trace and debug.
2. **OCI skeletons bolted on post-facto**: The integration of OCI skeletons did not align with the original composition design, leading to divergent code paths and reduced reuse.
3. **Decentralized `only` filtering**: Filtering logic is scattered across multiple stages (e.g., create vs. deploy), causing inconsistencies and bugs when applying filters during package creation.

A ground-up refactoring of the composition pipeline will unify these concerns under a single, predictable model.

---

### Decision Details

We will implement a **Composition Engine** as a dedicated module responsible for:

- **Centralized Component Merging**: All component sources (local packages, remote Zarf packages, OCI skeletons) are resolved through a unified interface. Each import step is treated as a ""layer"" in a directed acyclic graph (DAG), enabling cycle detection and topological processing.
  
- **Strict Import Chain Validation**: 
  - Enforce maximum depth limits on import chains (configurable via CLI flag, default 5).
  - Require checksum verification and schema validation at every import step.
  - Generate traceable provenance metadata for auditability.

- **Unified Filter Pipeline**:
  - Move all `only` filter logic into the Composition Engine.
  - Apply filters *after* full component resolution but *before* finalization.
  - Ensure consistent behavior between `zarf package create` and `zarf package deploy`.

- **OCI-Native Design**:
  - Treat OCI skeletons as first-class composition sources rather than special cases.
  - Use descriptor-based references (OCI image descriptors) to enable lazy loading and content addressing.

- **Testability Improvements**:
  - Introduce unit tests per composition stage.
  - Add integration tests covering multi-layer imports (e.g., A ‚Üí B ‚Üí C ‚Üí D).
  - Mock OCI interactions to validate edge cases without registry dependencies.

---

### Outcome

This decision ensures:
- Reliable and secure composition across arbitrary package layers.
- Consistent handling of components regardless of source.
- Future extensibility for features like conditional composition or signed imports.

By centralizing composition logic, we improve correctness, security, and developer experience while laying the foundation for advanced use cases in enterprise air-gapped environments.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","# ADR 0015: Introduce `zarf dev` Command for Iterative Package Development

## Status
Proposed

## Context
Developing Zarf packages today involves a slow, repetitive cycle of `create ‚Üí deploy ‚Üí debug ‚Üí repeat`, often requiring full cluster resets and complete package rebuilds‚Äîeven for minor changes. This is especially painful during development when rapid iteration is needed. While YOLO mode exists to speed up certain workflows by skipping dependency fetching and initialization, it is not user-facing or designed for general iterative development.

The current workflow forces developers to:
- Re-package entire distributions (including potentially large assets) for every change.
- Re-initialize clusters manually after failures.
- Operate without fast feedback loops, reducing productivity.

There is a clear need for a dedicated **development-time** command that optimizes for speed, iteration, and local debugging‚Äîwithout compromising the air-gapped, production-ready guarantees of `zarf package create`.

## Decision
Introduce a new CLI command:  
```bash
zarf dev [flags]
```

This command will enable rapid, iterative development of Zarf packages by:
1. **Skipping full asset packaging**: Do not bundle images, files, or archives into the final tarball during development.
2. **Using live dependencies**: Pull container images and fetch remote resources directly at deploy time (only in dev mode).
3. **Auto-detecting changes**: Watch `zarf.yaml` and relevant manifests for file changes and trigger redeployments.
4. **Supporting in-place updates**: Where possible, apply deltas to existing deployments instead of full teardown/redeploy.
5. **Bypassing `zarf init` when appropriate**: Leverage lightweight setup similar to YOLO mode but with better UX and safety checks.
6. **Enabling debug flags**: Automatically inject common debugging tools/configurations (e.g., `--insecure-skip-tls-verify`, verbose logging).

### Example Usage
```bash
# Start iterative development loop
zarf dev --dir ./my-pkg

# With watch mode enabled
zarf dev --dir ./my-pkg --watch

# Connect to existing cluster, skip init
zarf dev --dir ./my-pkg --skip-init
```

### Behavior
| Phase           | `zarf package create`               | `zarf dev`                              |
|----------------|-------------------------------------|------------------------------------------|
| Dependency Res | Bundled at create time              | Fetched at deploy time                   |
| Air-Gap Safe   | Yes                                 | No (requires internet)                   |
| Init Required  | Yes                                 | Optional (auto-skipped if safe)          |
| Image Handling | Exported & bundled                  | Pulled directly from registry            |
| Speed          | Slow (full build + archive)         | Fast (minimal processing)                |
| Use Case       | Production, air-gapped deployment   | Local development, debugging             |

> ‚ö†Ô∏è **Security Note**: `zarf dev` will print warnings when used, indicating it should never be used in secure or air-gapped environments.

## Rationale
- **Improves Developer Experience**: Reduces feedback loop from minutes to seconds.
- **Leverages Existing Concepts**: Builds on YOLO mode logic but exposes it as a first-class, documented workflow.
- **Separation of Concerns**: Keeps production packaging (`create`) distinct from development (`dev`), avoiding feature bloat in core commands.
- **Reduces Friction**: Eliminates need for manual cluster resets and repeated full builds.

## Implications
### Pros
- Dramatically faster iteration during development.
- Lowers barrier to entry for new users learning Zarf.
- Encourages experimentation and modular design.

### Cons
- Adds complexity to CLI surface area.
- Risk of users mistakenly using `dev` mode in production-like settings (mitigated via warnings and docs).
- Requires robust detection of ""safe"" conditions for skipping init.

## Prior Art
- `skaffold dev` ‚Äì Iterative development for Kubernetes applications.
- `tilt up` ‚Äì Live-updating dev environment for microservices.
- `docker compose up --watch` ‚Äì Auto-reload on file change.

These tools demonstrate the value of opinionated, fast feedback loops in development workflows.

## Related Issues
- <https://github.com/defenseunicorns/zarf/issues/2169>
- ADR 0010: YOLO Mode

## Next Steps
1. Implement `zarf dev` command with basic deploy-from-source behavior.
2. Add optional file watching and hot-reload support.
3. Document `dev` vs `create` tradeoffs clearly in user guide.
4. Add telemetry (opt-in) to measure adoption and performance gains.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The process of deploying applications with Helm charts in Kubernetes environments often necessitates the customization of chart values to align with specific operational or environmental requirements. The current method for customizing these values√É¬Ç√Ç¬°√É¬Ç√Ç¬™either through manual edits or `","**Decision: Use Helm Value Files for Environment-Specific Configuration**

### Status
Accepted

### Context
The process of deploying applications with Helm charts in Kubernetes environments often necessitates the customization of chart values to align with specific operational or environmental requirements. The current method for customizing these values‚Äîeither through manual edits or inline `--set` parameters in Helm commands‚Äîis error-prone, difficult to version, and not easily auditable. This leads to configuration drift, reduced reproducibility, and challenges in managing deployments across multiple environments (e.g., dev, staging, production).

Additionally, teams require a scalable, declarative, and collaborative approach to manage configuration that supports review workflows, CI/CD integration, and environment promotion.

### Decision
Adopt **Helm value files** (`values.yaml`, `values-dev.yaml`, `values-prod.yaml`, etc.) as the standard mechanism for customizing Helm chart configurations. Each environment will have its own dedicated values file, stored alongside the base chart or in a dedicated configuration repository. These files will be used with the `--values` flag in Helm commands:

```bash
helm upgrade --install my-app ./chart --values values-prod.yaml
```

Configuration changes will be managed through version control (e.g., Git), enabling code reviews, audit trails, and traceability.

### Rationale
- **Reproducibility**: Value files ensure consistent deployments across environments.
- **Version Control & Auditability**: Changes to configurations are tracked, reviewed, and rolled back if needed.
- **Separation of Concerns**: Application packaging (chart) is decoupled from environment-specific configuration.
- **CI/CD Integration**: Value files can be selected dynamically in pipelines based on target environment.
- **Collaboration**: Teams can collaborate safely using pull requests and automated validation (e.g., schema checks, linters).
- **Avoids `--set` Limitations**: Complex nested values and lists are more cleanly expressed in YAML than in CLI arguments.

### Implications
- ‚úÖ Improved deployment reliability and consistency.
- ‚úÖ Easier onboarding and documentation via clear configuration files.
- ‚ö†Ô∏è Requires discipline in organizing and naming value files (adopt naming convention).
- ‚ö†Ô∏è Sensitive data should not be stored in plaintext; use with secrets management tools (e.g., Helm Secrets, Sealed Secrets, or external secret operators).

### Future Considerations
- Explore Kustomize + Helm hybrid patterns for advanced overlays.
- Integrate with GitOps tools like Argo CD or Flux for automated synchronization.

---

This decision establishes a maintainable, scalable, and secure foundation for Helm-based deployments.","## Decision

To address this issue, we propose the introduction of a feature designed to simplify the process of overriding chart values at the time of deployment. This feature would allow users to easily specify overrides for any chart values directly via command-line arguments, eliminating the need to alter the chart's default values file or manage multiple command-line arguments for each override.

Key aspects of the proposed implementation include:
- Use existing `--set`  flags to specify overrides for chart values.
- The ability to list all overrides in a structured and easily understandable format within `zarf.yaml`.
- Ensuring that during deployment, these specified overrides take precedence over the chart's default values, thus facilitating customized deployments without necessitating permanent modifications to the chart."
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","**Decision: Upgrade from Docusaurus 2 to Docusaurus 3 or migrate to an alternative static site generator (SSG) based on performance and dependency improvements.**

### Status
Accepted

### Context
The current documentation site uses Docusaurus 2, which has provided a solid foundation for content authoring, versioning, and search. However, over time, limitations have become apparent:

- **Performance bottlenecks**: Slow build times and suboptimal runtime performance, especially as content scales.
- **Dependency management issues**: Outdated or unmaintained dependencies, challenges in upgrading due to plugin incompatibilities, and security vulnerabilities.
- **Limited modern tooling integration**: Less seamless support for newer features like server components, React 18+ patterns, and advanced bundling optimizations (e.g., Vite).
- **Bundle size concerns**: Initial page load performance is affected by larger-than-necessary JavaScript bundles.

Docusaurus 3 introduces several improvements including better TypeScript support, enhanced plugin architecture, faster builds via SWC, and improved webpack/Vite compatibility. Alternatively, modern SSGs like VitePress, Next.js, or Astro offer compelling advantages in speed, developer experience, and bundle optimization.

### Decision
We will **evaluate migrating to Docusaurus 3** as the preferred path, given our existing investment in Docusaurus-specific features (e.g., versioned docs, remark plugins, custom themes). If Docusaurus 3 does not sufficiently address core performance and maintenance concerns during evaluation, we will **migrate to VitePress**, which offers:
- Near-instant HMR and fast builds via Vite
- First-class Markdown and MDX support
- Lightweight output and excellent Lighthouse scores
- Simpler configuration and dependency surface
- Strong TypeScript and Vue-based extensibility (if acceptable to team)

A proof-of-concept migration of a subsection of the docs will be conducted for both Docusaurus 3 and VitePress to compare:
- Build performance
- Runtime performance (LCP, FID, etc.)
- Effort to port plugins and custom components
- CI/CD integration impact
- Search functionality (via integration with Algolia or local search)

Based on the POC results, a final decision between Docusaurus 3 and VitePress will be made within 4 weeks.

### Consequences
**Positive:**
- Potential significant improvements in build speed and site performance.
- Modernized tech stack with better long-term maintainability.
- Reduced technical debt and improved developer experience.

**Negative/Risks:**
- Migration effort required, including potential rework of custom plugins or theme components.
- Learning curve if switching to a new framework (especially if moving to Vue-based VitePress).
- Temporary duplication of effort during transition phase.

### Additional Notes
Team feedback and ease of content contributor workflow will be prioritized during the migration. Documentation workflows (e.g., PR previews, versioning strategy) must be preserved or improved.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
