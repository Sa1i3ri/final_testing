context,decision,predicted_decision
"## Context

* Currently, the images/thumbnails inside the Storefront are not making use of any lazy loading mechanism. 
* Without a third-party extension which includes something like ""lazysizes"" it is not possible to get lazy loading images.
* Native lazy loading of images is available in current browsers, see: https://caniuse.com/?search=loading","## Decision  
We will implement native browser lazy loading for images and thumbnails in the Storefront by utilizing the standard HTML `loading=""lazy""` attribute on `<img>` tags.

This decision is based on the following rationale:

- **Simplicity and Maintainability**: Native lazy loading requires no additional JavaScript libraries or third-party dependencies, reducing bundle size, complexity, and potential points of failure.
- **Browser Support**: Modern browsers (including Chrome, Firefox, Edge, and Safari) support the `loading` attribute on `<img>` and `<iframe>` elements. According to caniuse.com, support covers over 95% of global users, which is sufficient for our current target audience.
- **Performance Gains**: Lazy loading defers the loading of offscreen images, improving initial page load time, reducing memory usage, and minimizing bandwidth consumption—especially beneficial on mobile devices and slow connections.
- **Progressive Enhancement**: For older browsers that do not support `loading=""lazy""`, the images will still load normally without any degradation in functionality (graceful degradation).
- **Alignment with Web Standards**: Using native features aligns with our goal of relying on standardized, future-proof web platform capabilities rather than external tools when feasible.

### Implementation Guidelines:
- All product images, thumbnails, and other offscreen or below-the-fold images in the Storefront must include `loading=""lazy""` unless there is a specific reason to load them immediately (e.g., hero images or images critical to above-the-fold content).
- Critical above-the-fold images should use `loading=""eager""` explicitly or omit the attribute to ensure timely loading.
- The implementation will be applied consistently across all relevant templates and components.

If future requirements demand more granular control (e.g., custom placeholders, intersection tracking), we may revisit this decision and consider lightweight enhancements—but only if justified by measurable user impact. For now, native lazy loading meets our needs effectively and efficiently.","## Decision

* We want to make use of native lazy loading for images in the Storefront without adding additional JavaScript logic."
"## Context

When using the `OneToOneAssociationField` & `ManyToOneAssociationField` associations it is possible to specify the parameter `autoload` as `true`. This causes the association to be loaded with every query, regardless of whether the data is used or not.

This is a performance issue due to unnecessary data transfer, it slows down SQL queries with extra joins, slows the application down with hydrating and data processing and finally, API payloads are larger than they need to be.","## Decision  
We will deprecate the use of `autoload: true` in `OneToOneAssociationField` and `ManyToOneAssociationField` associations by default, and discourage its usage in new code.

Rationale:
- **Performance Impact**: Enabling `autoload: true` results in implicit joins on every query, increasing database load, query latency, and memory usage.
- **Unintended Data Transfer**: Entities are eagerly loaded even when not needed, leading to unnecessary hydration and serialization overhead.
- **Poor API Efficiency**: Automatically including associated data inflates API response sizes, harming client performance and bandwidth usage.
- **Lack of Explicitness**: Autoloading hides data-fetching behavior from developers, reducing transparency and making it harder to reason about performance.

Instead, we adopt the following patterns:
1. **Explicit Loading**: Associations should be explicitly fetched only when required, using dedicated query methods or eager loading mechanisms (e.g., via query hints or explicit join declarations).
2. **Use of DTOs or Projections**: For API responses, use tailored data transfer objects or repository projections to select only the fields needed.
3. **Lazy Loading Where Appropriate**: Where supported and performant, prefer lazy loading with proper fetch planning to avoid overfetching.

Migration Plan:
- Log a deprecation warning when `autoload: true` is used.
- Update documentation to reflect recommended alternatives.
- Provide automated migration scripts or linting rules to detect and flag existing uses.
- Target removal of `autoload` in the next major version.

This decision supports our broader goals of performance, transparency, and intentional design.","## Decision

We will deprecate the usage of autoload === true in the core codebase in version 6.5. All usages in entity definitions should be removed by the time 6.6 is released.

We have introduced a new PHPStan rule which will check for any usages of autoload === true and fail if it finds any. The failures are currently ignored in the `phpstan.neon.dist` file until they can be fixed by the respective teams."
"## Context
Currently, our ADRs are grouped into folders according to different areas. We have added this structure to help people find their way around when they are looking for specific ADRs.

However, there are different approaches when searching for ADRs:
1) I am looking for an ADR and I know the headline.
2) I would like to read the latest ADRs
3) I would like to see ADRs for a specific area.
4) ...

The structuring by areas is good for case 3, but not for the other two. In the first case, the user can easily use the search via the directory to use the appropriate ADR, but will also get duplicate matches. Depending on the keyword the result can be quite long.

In the second case, I would like to read the latest ADRs, the user would have to go to git history, which is only possible via shell or IDE.","## Decision  
We will maintain the current folder-based organization of ADRs by area, as it supports domain-specific navigation and provides valuable context for contributors exploring decisions within a particular subsystem.

However, to improve discoverability and support alternative access patterns, we will introduce a centralized `README.md` file at the root of the ADR directory that serves as a dynamic index. This index will be manually curated (and updated with every new ADR) to include:

1. **A chronological list of all ADRs**, ordered by creation date (newest first), including:
   - Title
   - Date created
   - Status (proposed, accepted, rejected)
   - Area/Module
   - Link to the ADR file

2. **A searchable table of contents** organized by area, mirroring the folder structure but providing direct links.

3. **A section for recently accepted ADRs** (e.g., last 5) to highlight recent changes.

Additionally, we will adopt a consistent naming convention for ADR files:  
`<area>-<YYYYMMDD>-<short-slug>.md` (e.g., `core-20250405-routing-middleware-execution-order.md`)

This ensures:
- Easy identification from git history or filesystem listing
- Chronological sorting via filename
- Clear scoping to a module/area
- Improved searchability without duplication issues

We will also add frontmatter (YAML or equivalent) to each ADR containing metadata such as:
```yaml
status: accepted
area: core
created: 2025-04-05
modified: 2025-04-07
authors: [Alice Dev, Bob Architect]
related: [core-20250315-http-pipeline-design.md]
```

This metadata enables potential tooling in the future for generating reports, dashboards, or changelogs.

The combination of structured folders, a human-readable index, standardized naming, and embedded metadata satisfies all primary use cases:
- Known-title lookup → fast via search or index
- Browse by area → supported by folders and TOC
- See latest ADRs → visible in chronological list and ""recently accepted"" section
- Automation-friendly access → enabled by naming and metadata

All ADRs must be registered in the central index upon creation. Failure to do so will result in the PR being blocked.","## Decision

We will remove the area structure on directory level and introduce front matter in our ADRs as we already do in our changelog files. This makes it possible to extract metadata from content. Additionally, it can be used as groundwork, if we want to release our ADRs in different front-ends (handbook, dev-docs) and use the front-end's search functionalities.
With front matter and the flat file structure, you can solve all three problems described in this ADR. An example of this specific ADR would be:

```markdown
---
title: Unstructured ADRs
date: 2023-02-23
area: Product Operation #I added prod-ops because this is a workflow topic
tags: [ADR, file structure, workflow]
---
```

We also hope that this will make it easier for people to submit ADRs without being confused about which folder to put the file in."
"## Context
Our current plugin extension system for the administration is based on our Component Factory. This factory generates native Vue components at runtime based on a base Shopware component which can be extended or overwritten by plugins. This approach offers flexibility to plugin developers to modify every component. For the template part, we use Twig.JS and compile the Vue template in the client at runtime.

However, this approach has its drawbacks. We cannot use the full power of Vue and its ecosystem because everything related to Vue is wrapped with our factories and generated at runtime. We cannot use Vue tools without custom modifications, like linting template files, providing good static analysis, and more. The other downside is that upgrading Vue is challenging, and the performance suffers because we cannot precompile components.

To address these issues, we aimed to test a new idea with Vue 3, which includes all the features of the existing plugin approach, but developed using only native Vue tools. This would offer several benefits, like using the full power of Vue, better performance, better static analysis, and more.

Our main idea for the template part was to use native Vue components named sw-block that can replace the Twig.JS template part. Plugin developers can extend or overwrite the sw-block component as needed.

```html
<sw-block name=""sw-hello-world"">
<div>Hello World</div>
</sw-block>
```

For the script and logic part, we aimed to use the Composition API. Before returning all the component data and methods, we would provide a hook point for plugins so they could modify or inject everything they want.

```js
// The original component
<script setup lang=""ts"">
// Hook for providing extensibility
useExtensibility();
</script>

// The plugin component
<script lang=""ts"">
    import SwOrignal from './sw-original.vue';
    // use our extensibility helpers
    import { useComponentExtender, getPropsFromComponent } from 'sw-vue-extensbiles';
    
    const myCustomData = ref('test');
    
    export default {
        name: 'sw-extended-component',
        props: {
            ...getPropsFromComponent(SwOrignal)
        },
        setup(props, context) {
            return {
                ...useComponentExtender(SwOrignal, props, context),
                myCustomData
            }   
        }
}
</script>
```

However, during the evaluation and testing, we realized that this approach was not feasible due to several reasons. The most critical challenges were that it was hard to merge data because the Vue compiler optimizes many parts of the component, and it was not easy to pass all the data of the component to the block system to give plugin developers full access to the original component's data.

To solve these problems, we would need to use internal Vue logic, which is not update-safe and could break with every Vue update.","## Decision
We will retain the current Component Factory-based plugin extension system for the administration, despite its limitations, and instead focus on incremental improvements to mitigate its drawbacks.

The native Vue-only approach using `sw-block` and Composition API hooks was found to be infeasible due to fundamental technical constraints:
- The Vue 3 compiler's optimizations make it difficult to reliably extract and merge component internals (e.g., setup state, computed properties, lifecycle hooks).
- Achieving full extensibility would require reliance on Vue’s internal APIs, which are not part of the public contract and subject to breaking changes with any update—violating our need for stability and upgrade safety.

Instead, we will pursue the following path forward:

1. **Improve Tooling Around Runtime Generation**: Invest in enhanced developer tooling that provides better static analysis, linting, and type checking for factory-generated components. This includes developing custom ESLint plugins and IDE extensions tailored to our runtime component model.

2. **Precompile Where Possible**: Introduce build-time precompilation of component templates wherever feasible, reducing client-side compilation overhead while maintaining compatibility with dynamic plugin overrides.

3. **Stabilize and Document Extension Contracts**: Define clear, versioned extension interfaces (slots, props, events) for core components to reduce the need for deep component overriding and improve plugin compatibility across versions.

4. **Explore Hybrid Approaches for Future Versions**: Continue researching alternative architectures—such as module federation or compile-time macro systems—that could enable true native Vue component extension without sacrificing safety or performance, targeting potential adoption in future major releases.

This decision prioritizes long-term maintainability, framework upgrade resilience, and ecosystem compatibility over idealized developer experience trade-offs that carry unacceptable technical risk.","## Decision
After considering the challenges we faced with the proposed solution, we concluded that we couldn't find solutions without using internal Vue logic. Given this situation, we did not see any significant benefits to adopting a native Vue solution over our current plugin system, which, despite its challenges, has proven to work well for our use case.

Therefore, we decided to stick with our current plugin system for the administration until new possibilities arise in Vue that solve our problems."
"## Context
The current workflow for writing technical changelog is not very suited for a trunk based development and branching process. There are some major issues with the current workflow of using one file with a fixed version number:

*  Unnecessary merge conflicts in the changelog file
*  Uncertainty about the correct release version of the change
*  Differences between the issue state in Jira and the code state

To tackle these issues the technique for writing technical changelog and upgrade information, should be changed to a single solution, which is easy to understand and maintain by developers.","## Decision  
We will adopt a **fragment-based changelog** workflow to address the limitations of maintaining a single monolithic changelog file under trunk-based development.

Instead of editing a central `CHANGELOG.md` directly, contributors will create small, atomic changelog fragments for each change they make. These fragments will be stored in a dedicated directory (e.g., `changelog.d/`) and named using the convention: `<issue-id>.<change-type>.md`, where:

- `<issue-id>` corresponds to the tracking identifier (e.g., Jira ticket ID)
- `<change-type>` is one of:
  - `added`
  - `changed`
  - `deprecated`
  - `removed`
  - `fixed`
  - `security`

Each fragment should contain a concise, user-focused description of the change written in Markdown, suitable for inclusion in a release note.

At release time, a tool will automatically aggregate these fragments into a structured `CHANGELOG.md`, grouped by version and change type, and remove consumed fragments (or archive them) after processing.

This approach provides the following benefits:
- Eliminates merge conflicts on the main changelog file
- Decouples writing change descriptions from knowing the final release version
- Ensures every change is documented with a traceable link to its issue tracker
- Supports automation of changelog generation and cleanup
- Maintains clarity and consistency for both developers and users

To support this workflow, we will provide:
- A template for new fragments
- CLI tooling or build integration to validate, collect, and render fragments
- Clear contribution guidelines explaining when and how to add fragments

This decision applies to all Arachne modules and core projects.","## Decision
The changelog and upgrade files for a release should be autogenerated in the build process of the release. Developers should write technical changelog and upgrade information in separate markdown files."
"## Context

The actions in Flow Builder are listening for Business Events. We want to implement the flow DelayAction in Flow Builder,
that means the actions can be delayed, and will be executed after  a set amount of time.
But we have some problems after the action was delayed:
* Events may contain old data, that data may be updated during the delay, and currently we don't have any way to restore
    the data.
* After a delay the rules have to be re-evaluated, but the data in the rules could be outdated or changed, so the rules
    have to be reloaded as well. Or the rules do not exist anymore.","## Decision  
We will implement the `DelayAction` in Flow Builder by decoupling the execution context from the original event data and instead capturing a snapshot of relevant data and rules at the time of delay initiation. This ensures that delayed actions operate on consistent, versioned information rather than potentially stale or missing runtime state.

The solution consists of the following components:

1. **Execution Context Snapshotting**:  
   When a `DelayAction` is triggered, the system will create a serialized snapshot of:
   - The relevant portion of the event data required by downstream actions.
   - The specific business rules and conditions associated with the flow at that moment.
   - Metadata including schema versions, rule identifiers, timestamps, and TTL for the snapshot.

   This snapshot will be stored externally in a durable, transient storage layer (e.g., a time-to-live-enabled store like Redis or a persistent queue with retention).

2. **Rule and Data Versioning**:  
   Business rules must be versioned immutably. Each rule evaluation reference within the snapshot will include a rule version identifier. This prevents failures due to deleted or modified rules during the delay period.

3. **Rehydration at Execution Time**:  
   Upon expiration of the delay:
   - The system retrieves the snapshot.
   - It validates the integrity and freshness of the data and rules against current schemas (using version compatibility checks).
   - If validation passes, the flow resumes using the snapshot’s data and rules.
   - If validation fails (e.g., incompatible schema changes), the action is not executed and an alert is raised for manual review or automated fallback handling.

4. **Eventual Consistency & Observability**:  
   A monitoring mechanism will track delayed actions, their snapshots, and final dispatch status. Operators can inspect, retry, or cancel pending delayed executions.

5. **Developer Transparency**:  
   Flow designers must explicitly opt into data inclusion for delayed actions, promoting awareness of what data is being captured and for how long—supporting both performance and privacy considerations.

This approach maintains consistency, avoids reliance on volatile runtime state, and aligns with principles of predictable, auditable workflow execution. Future extensions may allow dynamic data refresh policies (e.g., ""merge latest customer data before executing""), but the default behavior will prioritize deterministic repeatability over live data.","## Decision

We would need to detach the Event System and the Flow System from each other, thus removing the dependency on the runtime objects within an event.
Meaning the Actions must not have access to the original Events.

We would create a class `StorableFlow`, that can store the data in the original event as scalar values, and restore the original data based on this stored data.
```php
class StorableFlow
{
    // contains the scalar values based on the original events
    // $store can be serialized and used to restore the object data
    protected array $store = [];
    
    // contains the restored object data like the data we defined in the `availableData` in original events 
    // $data can not be serialized, but can be restored from $store
    protected array $data = [];
      
    public function __construct(array $store = [], array $data = [])
    {
        $this->store = $store;
        $this->data = $data;
    }
    
    // This method will be called in each `Storer` to store the representation of data
    public function setStore(string $key, $value) {
        $this->store[$key] = $value;
    }
    
    public function getStore(string $key) {
        return $this->store[$key];
    }
    
    // After we restored the data in `Storer`, we can set the data, we'll use `$this->data` instead getter data on original event
    public function setData(string $key, $value) {
        $this->data[$key] = $value;
    }
    
    public function getData(string $key) {
        return $this->data[$key];
    }
}
```

The `StorableFlow` class will be use on Flow Builder:

Before:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // Currently, dispatch on Flow Builder use the original event to execute the Flow 
        $this->callFlowExecutor($event);
        ...
    }
}
```

After:

```php
class FlowDispatcher 
{
    public function dispatch(Event $event) {
        ...
        // The `FlowFactory` will create/restore the `StorableFlow` from original event
        $flow = $this->flowFactory->create($event);
        // use the `StorableFlow` to execute the flow builder actions instead of the original events
        $this->execute($flow);
        ...
    }
}
```

* Flow Builder actions may no longer access the original event.
* Each Aware Interface gets its own `Storer` class to restore the data of Aware, so we have many `Storer` like `OrderStorer`, `MailStorer`, `CustomerStorer` ...
* The main task of a `Storer` is to restore the data from a scalar storage.
* The `Storer` provides a store function, in order to store itself the data, in order restore the object
* The `Storer` provides a restore function to restore the object using the store data.

```php
interface FlowStorer {}
```
Example for `OrderStorer`:

```php
class OrderStorer implements FlowStorer
{
    // This function to check the original event is the instanceof Aware interface, and store the representation.
    public function store(FlowEventAware $event, array $storedData): array 
    {
        if ($event instanceof OrderAware) {
            $storedData['orderId'] = $event->getOrderId();
        }
        
        return $storedData;
    }
    
    // This function is restore the data based on representation in `storedData`
    public function restore(StorableFlow $flow): void
    {
        if ($flow->hasStore('orderId')) {
            // allows to provide a closure for lazy data loading
            // this opens the opportunity to have lazy loading for big data
            // When we load the entity, we need to add the necessary associations for each entity
            $flow->lazy('order', [$this, 'load']);    
        }
        ...
    }
}
```

About the additional data defined in `availableData` in original events, that aren't defined in any Aware Interfaces and we can't restore that data in the `Storer`.
To cover the additional data from original events, we will have another `store` `AdditionalStorer` to store those data.
```php
class AdditionalStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $storedData)
    {
        ...
        // based on the `getAvailableData` in the original event to get the type of additional data
        $additionalDataTypes = $event::getAvailableData()->toArray();
        
        foreach ($additionalDataTypes as $key => $eventData) {
            // Check if the type of data is Entity or EntityCollection
            // in the $storedData, we only store the presentation like ['id' => id, 'entity' => entity], we'll restore the data in `AdditionalStorer::restore`
            if ($eventData['type'] === 'Entity' || 'EntityCollection') {
                $storedData[$key] = [
                    'id' => $event->getId(),
                    'entity' => Entity                 
                ];
            }
            
            // Check if the type of data is ScalarValueType
            if ($eventData['type'] === ScalarValueType) {
                $storedData[$key] = value
            }
            
            // start to implement /Serializable for ObjectType
            if ($eventData['type'] === ObjectType) {
                $storedData[$key] = value->serialize()
            }
            
            ...
        }
        
        ... 
        
        return $storedData;
    }
      
    // this function  make sure we can restore the additional data from original data are not covered in `Storer`
    // The additional data can be other entity, because the entities we defined in Aware interface like `order`, `customer` ... covered be `Storer`
    public function restore(StorableFlow $flow): void
    {
        if (type === entity) {
            // About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this
            $flow->setData($key, $this->load());
        } else {
            $flow->setData($key, $flow->getStore($key));
        }
        ...
    }
}
```

About the associations for entity data, mostly the additional entity data is the base entity, we don't need to add associations for this.
About the `ObjectType` data, we enforce all values used in ObjectType implement /Serializable, and serialize the object before store to `$storedData`.

* Flow Builder actions only work with the `StorableFlow` instead of the `FlowEvent`. The `StorableFlow` will restore the data from original events via `Storer`,
  and the Actions can get the data via `getData($key)` from `StorableFlow` instead of `getAvailableData` from original events.

Before, in the flow actions still dependency Aware interfaces:

```php
    public function handle(StorableFlow $event) {
        ...
        $baseEvent = $event->getEvent();
    
        if ($baseEvent instanceof CustomerAware) {
            $customerId= $baseEvent->getCustomerId();
        }
        ...
    }
```
After in the flow actions:
```php
    public function handle(StorableFlow $event) {
        ...
        if ($event->hasStore('customerId') {
            $customerId= $event->getStore('customerId');
        }
        ...
    }
```

* `getAvailableData` must NOT be responsible for the access of the data.
* To create new or restore the `StorableFlow` by on the existing stored data, we need to provider the `FlowFactory`.
```php
class FlowFactory
{    
    ...
    public function create(FlowEventAware $event)
    {
        $storedData = [];
        foreach ($this->storer as $storer) {
            // Storer are responsible to move the corresponding 
            // data from the original event 
            $storer->store($event, $storedData);
        }
        
        return $this->restore($storedData);
    }
  
    public function restore(array $stored = [], array $data = [])
    {
        $flow = new StorableFlow($stored, $data);
      
        foreach ($this->storer as $storer) {
            $storer->restore($flow);
        }
  
        return $flow;
    }
    ...
}
```
But when executing a delayed actions, we won't have a `StorableFlow`, we just have the `$stored` from the previously stored `StorableFlow`,
and based on the `$stored`, we can restore a new `StorableFlow`.

Example in Delay Actions:

```php
// In handler delay actions -> put the actions to `queue`
$stored = json_encode($flow->stored());

$connection->executeStatement('INSERT INTO `swag_delay_action` (store) VALUES (:stored)...', ['stored' => $stored]);
```

```php
// In handler execute delay actions
$stored = 'SELECT store FROM `swag_delay_action` .... ';

$flow = $this->flowFactory->restore(json_decode($stored));
```"
"## Context

We need a different extension mechanism besides the well known plugins, that works in cloud and on-prem environments in the same way.
That's why we envisioned the app system, currently publicly available as a [plugin](https://github.com/shopware/app-system).
We were able to quickly develop the app system, gather feedback and iterate over ideas without being bound to the strict release workflow of the shopware platform.
That was the reason why we started working on the app system as a plugin in the first place.

Now as the app system matured, it is time to integrate the app system back into the platform.
This has the following benefits:
* It sends the signal to partners and potential app manufacturers that the app system is stable
* Partners / app manufacturers can rely on the shopware 6 release cycle and the according upgrade process
* Users don't need to install an extra plugin in order to use apps","## Decision 
We will integrate the app system directly into the Shopware 6 platform core, deprecating its previous distribution as a standalone plugin.

This integration ensures that the app system is universally available across all deployment environments—cloud and on-premises—without requiring additional installation steps. By embedding the app system into the core platform, we reinforce its status as a first-class, stable extension mechanism and align it with the standard release, upgrade, and support cycles of Shopware 6.

The migration path will include:
1. Full incorporation of the app system’s codebase into the main platform repository under a dedicated module namespace.
2. Deprecation of the standalone app-system plugin, with clear end-of-life communication to existing users.
3. Backward-compatible interfaces during transition, ensuring existing apps continue to function without modification.
4. Documentation updates to reflect the app system as a native platform feature, including developer guides and onboarding materials.

This decision supports long-term maintainability, improves developer and user experience, and strengthens ecosystem confidence in the app system as the primary extensibility model for Shopware.","## Decision

We will migrate the existing app system from the [plugin](https://github.com/shopware/app-system) into the platform. The app system will be part of the core bundle, analogous to the plugin system.
It will be moved to the `Shopware\Core\Framework\App` namespace."
"## Context
At the moment we have a bunch of different `FlowStorer` implementations. Most of them are used to store scalar values without any restore logic. Each of the Storer class has an own interface which is used to identify if the data of the event should be stored. This leads to much boilerplate code when adding new storer implementations or when plugins want to bypass some for events.","## Decision  
We will consolidate all `FlowStorer` implementations into a single, unified interface with a pluggable strategy pattern for storage and restoration behavior. Instead of requiring a unique interface per storer type, we will define one core `FlowStorer` interface that supports:

- Storing scalar values and structured data
- Optional restore logic (with a default no-op implementation)
- Metadata-driven registration so plugins can conditionally apply or bypass storers based on event type, context, or configuration

The new design will include:
1. A central registry of storer strategies, each identified by a keyword or type tag.
2. A generic `store` method that dispatches based on the registered strategy and associated predicates.
3. Default lightweight wrappers for simple scalar storage (eliminating boilerplate for common cases).
4. Extension points allowing plugins to register new strategies or modify application rules without inheritance or interface proliferation.

This reduces redundancy, improves extensibility, and enables declarative control over storage behavior through configuration rather than code. New storers can be added without changing core interfaces, and plugins gain fine-grained control over which events are handled by which storers.","## Decision

We introduce a generic `ScalarValuesAware` interface which can be used to store simple values which should be simply stored and restored one to one:

```php
interface ScalarValuesAware
{
    public const STORE_VALUES = 'scalar_values';
    
    /** @return array<string, scalar|null|array> */
    public function getValues(): array;
}
```

This event can be used in different events which needs a simple storage logic:

```php
class SomeFlowAwareEvent extends Event implements ScalarStoreAware, FlowEventAware
{
    public function __construct(private readonly string $url) { }

    public function getValues(): array
    {
        return ['url' => $this->url];
    }
}
```

To store and restore this data, we provide a simple `FlowStorer` implementation:

```php

class ScalarValuesStorer extends FlowStorer
{
    public function store(FlowEventAware $event, array $stored): array
    {
        if (!$event instanceof ScalarValuesAware) return $stored

        $stored[ScalarValuesAware::STORE_VALUES] = $event->getValues();

        return $stored;
    }

    public function restore(StorableFlow $storable): void
    {
        $values = $storable->getStore(ScalarValuesAware::STORE_VALUES);
        foreach ($values as $key => $value) {
            $storable->setData($key, $value);
        }
    }
}
```"
"## Context
The current sw-text-editor in the administration has numerous low-level bugs in basic WYSIWYG features. It is built in a
way that is difficult to maintain and understand. Therefore, we require a new text editor that is easy to maintain, has
a good feature set, is flexible to extend and more stable.","## Decision  
We will replace the current `sw-text-editor` in the administration interface with a modern, well-maintained WYSIWYG editor framework that is built on a solid architectural foundation, actively supported, and designed for extensibility and stability.

After evaluating several candidates (including **Slate.js**, **ProseMirror**, **Tiptap**, and **Quill**), we have chosen **Tiptap** as the basis for the new text editor. Tiptap offers the following advantages:

- **Modular architecture**: Built on ProseMirror, it provides a reliable and semantically correct document model while exposing a plugin-based system that allows us to compose functionality cleanly.
- **Excellent maintainability**: The codebase is well-documented, widely adopted, and actively maintained by a dedicated team.
- **Extensibility**: Custom nodes, marks, and commands can be implemented in a declarative, composable way—aligning well with functional programming principles.
- **Stability**: It avoids direct DOM manipulation, instead using a structured document model which reduces bugs related to contenteditable inconsistencies.
- **Developer experience**: Strong TypeScript support, clear API, and extensive documentation make it easier for new contributors to understand and extend.
- **Integration flexibility**: Runs in standard browser environments without requiring special build steps or non-standard runtimes, making it compatible with our existing frontend stack.

The new editor will:
- Be implemented as a modular component, allowing reuse across different parts of the administration UI.
- Represent editable content and configuration as structured data where possible, enabling introspection and alignment with Arachne’s “everything is data” philosophy.
- Avoid tight coupling to global state or Clojure runtime constructs; instead, it will communicate via well-defined interfaces and data payloads.

During implementation, an Architecture Decision Record (ADR) will be created to formalize this decision, document alternatives considered, and record the rationale. This ADR will be proposed, reviewed by the steering group, and accepted before any major implementation work begins.

Additionally, we will design an abstraction layer between the ClojureScript application logic and the Tiptap instance to minimize direct JavaScript interop and allow for future replacement if needed. This maintains transparency, supports hot reloading, and prevents entanglement with the Clojure runtime.","## Decision
Building a new text editor from scratch is not a viable option. Therefore, we have evaluated various existing text 
editors and narrowed down our options to the following:

    - CKEditor 5
    - TinyMCE
    - QuillJS
    - Prosemirror
    - TipTap V2
    - Lexical

We have decided to skip CKEditor 5 and TinyMCE because they require a license for our use case and are not 100% open
source. Prosemirror was also ruled out because it provides only a low-level API and requires much more implementation 
time than the other editors. Additionally, Lexical is not a valid option for us since it is specialized for React 
environments and has no official support for VueJS.

The remaining two editors are TipTap V2 and QuillJS. Both have similar feature sets and APIs, but we have found some 
major differences between them. The first difference is that TipTap is a headless editor, which means that it only
provides the editor logic and requires a UI implementation. QuillJS, on the other hand, is a fully featured editor that
provides a UI out of the box. In our case, it is better to use a headless editor like TipTap because we can implement 
the UI to fit our needs, especially for several edge cases that are hard to implement in QuillJS.

The second major difference is in extensibility. TipTap's API is more flexible, allowing us to implement our own
features using the existing TipTap and ProseMirror plugins or build our own plugins. In most cases, the powerful main
TipTap API can already solve most of our use cases. QuillJS, on the other hand, is not as flexible as TipTap, and its
extension system is more complicated to use.

The third big difference is in stability. We found that TipTap is more stable than QuillJS, probably because TipTap is 
built on top of ProseMirror, which is a very stable and well-tested editor. Although QuillJS is also generally stable,
other developers have reported some issues with it in the past.

Our main decision driver was the extensibility of the editor. We want a text editor that is easy to extend and allows us
to implement our own features. TipTap provides a powerful extension system that is easy to use, and we can implement our
design without overwriting an existing design of other editors. Therefore, we have decided to use TipTap as the base of
our new text editor."
"## Context
The current implementation of the salutation in Shopware 6 needs to handle cases where the salutation is not specified by the customer or administrator. To address this requirement and promote inclusivity, we have updated the default salutation to ""not_specified"" for unspecified salutations in our Shopware 6 platform.","## Decision
We will update the default salutation in Shopware 6 to ""not_specified"" to explicitly represent cases where the customer or administrator does not provide a salutation. This change supports inclusivity by avoiding assumptions about gender, title, or form of address, and ensures data integrity by clearly distinguishing unspecified salutations from other values.

The ""not_specified"" value will be treated as a first-class salutation option throughout the system, including in:
- Customer registration and profile forms
- Admin backend user interfaces
- Email templating and communication systems
- API responses and payloads
- Database schema and migrations

Existing implementations must gracefully handle this new default, particularly in frontend templates and third-party integrations. Documentation and developer guides will be updated to reflect this change, and translation files will include appropriate localized labels for ""not_specified"" in all supported languages.

Custom themes and plugins are expected to support this value through conditional logic or generic addressing (e.g., using the customer's first name only when salutation is ""not_specified"").","## Decision
We have modified the existing salutation handling in Shopware 6 to update the default value to ""not_specified"" when the salutation is null. This decision was made based on the following considerations:

* Inclusivity: By offering a default salutation of ""not_specified"" for null values, we promote inclusivity and ensure that all customers are appropriately addressed, even when salutation data is missing.
* Customer Experience: Providing a default salutation ensures consistency in customer communications and prevents any confusion or misinterpretation when a salutation is not explicitly specified.
* Non-Deletable Default Salutation: It has been decided that the ""not_specified"" salutation, being the default value for unspecified salutations, should not be deletable by the shop owner. This ensures that there is always a fallback option available, guaranteeing a consistent experience for customers."
"## Context

The storefront CSS, compiled by `scssphp/scssphp`, is currently automatically prefixed with vendor prefixes using `padaliyajay/php-autoprefixer`.
However, the generated prefixes no longer add much value since the browser support has been updated with the Bootstrap v5 update.
Most of the prefixes are unnecessary for the supported browsers in .browserslist.
Unfortunately, the `theme:compile` process experiences a significant decrease in performance due to the auto-prefixer, which is particularly problematic in our SaaS solution.
Moreover, the `padaliyajay/php-autoprefixer` package does not work dynamically and fails to consider `.browserslist` while applying the appropriate vendor-prefixes.
This package hard-codes the CSS properties that require prefixing, making it an unsustainable long-term solution.
To demonstrate this, a table containing all automatic prefixes generated by `padaliyajay/php-autoprefixer` is provided below.","## Decision
We will remove `padaliyajay/php-autoprefixer` from the `theme:compile` build process and replace it with a modern, standards-compliant alternative that respects the project’s `.browserslistrc` configuration and applies vendor prefixes only where necessary for targeted browsers.

Specifically:
- We will adopt **[spatie/css-prefixer](https://github.com/spatie/css-prefixer)** or implement direct integration with **[PostCSS](https://postcss.org/)** using **[autoprefixer](https://github.com/postcss/autoprefixer)** in the SCSS compilation pipeline.
- The new solution will read from the existing `.browserslistrc` file to determine required browser support and apply minimal, accurate vendor prefixes accordingly.
- This change will be applied to the `theme:compile` command, resulting in faster compile times and CSS output that is leaner and more maintainable.

Rationale:
- Most vendor prefixes currently generated are redundant given our updated browser support baseline via Bootstrap v5 (which drops IE11 and older browsers).
- `padaliyajay/php-autoprefixer` is static, unmaintained, and not aligned with current tooling practices. It cannot adapt to evolving CSS standards or dynamic browser targets.
- PostCSS + autoprefixer is an industry-standard, well-maintained solution used across modern front-end tooling, ensuring long-term sustainability.
- Removing unnecessary processing will significantly improve performance of `theme:compile`, especially important in multi-tenant SaaS environments where build efficiency impacts scalability.

Migration Plan:
1. Introduce PostCSS + autoprefixer into the theme compilation pipeline alongside the existing SCSS compilation step.
2. Configure autoprefixer to use the same `.browserslistrc` as the rest of the application.
3. Validate output CSS for correctness and browser compatibility.
4. Remove `padaliyajay/php-autoprefixer` dependency after successful testing.
5. Document the updated workflow for theme developers.

This decision improves performance, maintainability, and alignment with modern web development practices while preserving visual consistency across supported browsers.","## Decision

* Due to the above points we have decided to disable the CSS auto-prefixing by default.
* In case it is still needed, to support older browsers or some special CSS property from the table above, it can still be activated via the config key `storefront.theme.auto_prefix_css` in `Storefront/Resources/config/packages/storefront.yaml`. However, we recommend to do a manual prefix inside the SCSS instead.
* We will deprecate the auto-prefixing for v6.6.0 and only use SCSS compiling
* We do not consider the deactivation of the auto-prefixing as a hard breaking change because:
    * Most prefixes are not needed due to current browser support.
    * Some prefixes are for CSS properties which are no longer implemented and developed by browsers.
    * Prefixes which are still valid are primarily cosmetic/appearance properties which are very unlikely to affect the Storefronts functionality."
"## Context
A pre-release package version is a version followed by a hyphen and an alphanumeric string.

Imagine the following scenario:
* An imaginary package is marked as insecure with version 1.8.7
* The issue is fixed with 2.0.0
* We use version `1.9.0-alpha1`
* Any pre-release package version like `1.9.0-alpha1` is interpreted as `<0.0.0` by npm

Why is this problematic?

The insecurity introduced with version `1.8.7` would never get reported to us by npm, unless we switch to a none pre-release version.","## Decision  
We will avoid using pre-release version identifiers (e.g., `1.9.0-alpha1`) when publishing security-sensitive fixes or updates in Arachne packages.

Instead, we will use standard semantic versioning (SemVer) with full release versions (e.g., `1.9.0`) for any version that resolves a security issue or is intended to be recognized by package managers as a valid upgrade target over a known-insecure version.

This decision is based on the behavior of npm (and some other package managers), which treat pre-release versions as *lower precedence* than all release versions — including older ones — per the SemVer specification. As a result:
- A version like `1.9.0-alpha1` is considered *older than* `1.8.7`
- Therefore, it will not satisfy dependency ranges aiming to avoid `1.8.7`, nor will it be flagged as a secure alternative by vulnerability scanners or automated tooling

To ensure that security fixes are properly detected and reported:
- All patches addressing security vulnerabilities must be released as non-prerelease versions
- We will increment the version number appropriately (patch, minor, or major) based on the nature of the change
- Pre-release versions may still be used for experimental features or internal testing, but never for security-critical updates

This guarantees that users relying on automated security advisories (e.g., via `npm audit`) will receive accurate recommendations and can safely upgrade away from vulnerable versions.","## Decision
Using pre-release package versions is prohibited.
This will be checked via a npm `preinstall` script."
"## Context

The existing SQL snippet to delete the outdated cart entries doesn't use any database index to narrow down entries that can be deleted. 
On high traffic shops this leads to SQL query times larger than 30 seconds to find and remove these database entries.

Running 
```
EXPLAIN DELETE FROM cart
WHERE (updated_at IS NULL AND created_at <= '2023-02-01')
   OR (updated_at IS NOT NULL AND updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the original sql query doesn't use an index (`possible_keys` = `NULL`)","## Decision
We will optimize the deletion of outdated cart entries by creating a database index on the `updated_at` column to significantly improve query performance. Given that the current DELETE query relies heavily on filtering based on `updated_at` (and falls back to `created_at` when `updated_at` is null), and since no index is currently used (`possible_keys = NULL`), this operation performs a full table scan which is prohibitively slow for large datasets.

To resolve this:

1. We will create a composite index on `(updated_at, created_at)` to support both conditions in the WHERE clause efficiently.
2. Alternatively, if the query pattern strongly favors `updated_at`, we may instead create a simple index on `updated_at` and include `created_at` as needed via covering strategies or adjust the query logic accordingly.
3. The index will allow the query planner to quickly locate rows matching the timestamp condition, reducing execution time from over 30 seconds to a fraction of a second even under high load.
4. We will update the cleanup job to use an indexed column explicitly and consider using batched deletes with incremental key tracking to avoid long-running transactions.

Additionally:
- We will analyze table statistics after index creation to ensure the query planner uses the new index.
- We will monitor query performance post-deployment and adjust indexing strategy if needed (e.g., partial/index-only scans).
- We recognize potential trade-offs in write performance due to index maintenance, but given that cart deletions are periodic and reads/writes on `updated_at` are already frequent, the overall impact is expected to be positive.

This decision aligns with our operational goals of maintaining responsive database performance at scale while ensuring background cleanup tasks do not degrade user-facing performance.","## Decision

Reorder the query parameters so that the relevant cart entries can be narrowed down by an indexed field.

Testing the new SQL snippet by running 
```
EXPLAIN DELETE FROM cart
        WHERE created_at <= '2023-02-01'
          AND (updated_at IS NULL OR updated_at <= '2023-02-01') LIMIT 1000;
```
shows that the new query uses an index (`possible_keys` = `idx.cart.created_at`)."
"## Context

Currently, it is hard to publish features in an early state to gather feedback regarding those features. If they are useful, what needs to be improved etc.
One major reason is that everything we publish (that is not marked as internal) is part of our backwards compatibility promise, thus changing foundational parts of features is quite hard after first release.
That leads to features being developed over quite some time without getting actual feedback from users or being able to release them, as they need to be implemented to a pretty final state in order to confidently release them in a stable manner, where we will keep backwards compatibility.

This at the same time also means that the current approach is not beneficial to our ecosystem, whom the whole backwards compatibility promise should benefit, because the features are built behind closed curtains they can't chime in with ideas and use cases regarding extendability, etc.

Examples of features that could benefit from an earlier experimental release:
* B2B:
  * We could release the multi account B2B feature with a ""simple"" employee management system first, and then add the more complex budget management or access control on top of that later.
* Advanced Search:
  * We could release a first version of the new advanced search feature, without all configuration options and customizability that we might envision.

In both cases, releasing the first increments of the features without an ""experimental""-flag would mean that we would have to keep backwards compatibility for the whole feature, even if we later decide to change the implementation of the feature, thus making the development of the feature harder.
Also in getting feedback from the customers what additional functionalities are needed after we released the first foundational increment of the feature, we can base our further prioritization on real customer feedback.
Thus, we can ship business value sooner to our customers and lower the risk of building the wrong thing.","## Decision  
We will introduce an **experimental** release mechanism to safely publish early-stage features and gather user feedback without compromising our backwards compatibility commitments.

An **experimental** feature is one that:
- Is fully functional for its documented use cases
- Is available in production environments
- Is clearly marked as `experimental` in documentation, UI (if applicable), and API metadata
- Is not covered by our backwards compatibility guarantees — we may change or remove the feature’s API or behavior in future releases without a major version bump

The process will be:
1. Developers can mark new modules, APIs, endpoints, or functionality as `experimental` during implementation.
2. Experimental features must be clearly documented as such, including potential risks and instability.
3. Feedback from users of experimental features will be actively collected and used to guide refinement.
4. Once a feature is deemed stable, complete, and validated by real-world usage, a follow-up ADR will be created to propose promoting it to **stable** status.
5. Promotion to stable includes finalizing the API, adding full backwards compatibility guarantees, comprehensive documentation, and appropriate testing.

This approach enables us to:
- Ship foundational parts of large features earlier (e.g., basic B2B account management, core advanced search)
- Validate assumptions and gather real-world use cases before locking down designs
- Reduce time-to-value for customers
- Maintain trust in our stable APIs while fostering innovation

We will communicate this policy clearly to our ecosystem so users understand how to adopt experimental features at appropriate levels of risk.","## Decision

To ship features earlier, we add the concept of ""experimental"" features, thus giving early access to meaningful increments of features that are still in active development.
That means in particular that there is no backwards compatibility promise for experimental features, thus we can change the implementation as is needed, without having to worry about breaking changes.
We mark the code for those features with a new `experimental` annotation, to make it clear on code level that the API is **not yet** stable.
For code where already expect that it should never become part of the public API we will use the `@internal` annotation directly, to make sure that even if the feature is stable we will continue to tread those parts of the code as internal and not keep backwards compatible.
Everything that is marked with `@experimental` is designed to be part of the public API, when the feature is stable.

At the same time, it offers a way for the ecosystem to give early feedback on the feature, as well as to test it in their own projects. Especially, extension developers can check how they might want to integrate and extend the feature being built, and thus suggest the needed extension points during the development process.
To make this possible that means that there also will be documentation (API docs, dev docs and user docs) for experimental features

All experimental features are developed with a specific target version, beginning with that version, the feature is considered stable, and the APIs will be kept backwards compatible.
This means that `experimental` annotation/attribute have to be removed, before the version can be released. Because it is hard to estimate exactly with which release a feature may be stable (as it also depends on the feedback we get) it makes sense to mark them as being stable with the next major version.
That does not mean that the feature won't be finished and stable earlier (we can remove the experimental status with any minor version), it only means that at the latest with that version it is considered stable, this prevents a situation where a lot of features stay in the experimental state for a long time."
"## Context

The stock handling in Shopware 6 is currently not very flexible and does not support many common use cases. 

* It's not possible to easily replace the loading of stocks with a custom implementation, for example one that communicates with an ERP.
* It's not possible to easily modify how stock is increased/decreased throughout the order lifecycle.
* Available stock calculation is very slow on large catalogs.
* Stock is stored as two distinct values: stock and available stock. This is due to the fact that stock is not reduced until an order is set as completed. Therefore, the available stock is calculated as the stock minus all open orders. This is unnecessarily complex.","## Decision  
We will refactor the stock management system in Shopware 6 to adopt a more flexible, event-driven, and extensible architecture based on inventory movements.

Instead of storing only `stock` and deriving `available stock` via runtime calculation over open orders, we will model all changes to inventory as explicit **inventory movement events** (e.g., ""reserved"", ""sold"", ""restocked"", ""returned"", ""adjusted""). The current stock level will be derived by replaying these movements, enabling full auditability and traceability.

The new architecture will include:

1. **Inventory Movement Registry**: A pluggable service that allows core and plugins to register types of inventory movements. Each movement type defines how it affects stock and available stock.
   
2. **Event Hooks at Key Order Lifecycle Points**: Clear extension points (via subscriber or hooks) at order creation, payment confirmation, cancellation, shipment, and return. This enables custom logic (e.g., ERP integration, warehouse-specific rules) to react to or replace default behavior.

3. **Single Source of Truth**: Stock levels will be calculated from the event log of movements. Caching layers may optimize performance for large catalogs, but the event log remains authoritative.

4. **Replaceable Inventory Providers**: Introduce an interface (`InventoryProviderInterface`) that allows developers to completely replace the stock calculation logic—e.g., with real-time ERP-linked implementations—while maintaining compatibility with the rest of the system.

5. **Simplified Data Model**: Remove the distinction between “stock” and “available stock” as persisted fields. Instead, expose both values as derived from the movement log, making the model easier to understand and extend.

This approach improves:
- **Extensibility**: Custom inventory logic can be implemented without overriding core classes.
- **Transparency**: Every change to stock is logged and auditable.
- **Performance**: Aggregated stock views can be cached and invalidated efficiently, avoiding expensive on-the-fly calculations across large datasets.
- **Correctness**: Reduces race conditions and inconsistencies by centralizing logic around atomic movement events.

Plugins and integrations are encouraged to use the movement-based API rather than directly manipulating stock fields, ensuring long-term compatibility and consistency.","## Decision

We have only one field `stock` in the product definition which always has a real time calculated value.

The `stock` value should be correctly updated as an order and its line items transition through the various states. Eg, stock is decremented when an order is placed. If it is cancelled, the stock is increased, and so on.

We have a clear API for manipulating stock which can be extended and supports arbitrary data, which could, for example, support features such as multi warehouse inventory.

We have a way to disable the stock handling behavior of Shopware."
"## Context
By default, every exception that is thrown in the PHP stack and not caught will be logged by the `symfony/monolog-bridge` on `error` level.
But there are some cases where the exception is caused by clients accessing the API wrong (missing fields etc.) and throwing an `ShopwareHttpException` with an HTTP-Status-Code of 40x is our way of handling such situations and returning a correct HTTP-Status-Code to the client.
So those cases are in fact no ""errors"" that need to be analyzed, but are expected given a malformed API request.
Logging those cases as ""errors"" produces a lot of noise, which makes it harder to actually find errors in the logs.

For our cloud product, we already used a configuration list that configures that some Exception classes should only be logged as notices.","## Decision  
We will introduce a configurable exception logging level mechanism that allows specific exception types, such as `ShopwareHttpException` and other client-facing exceptions with 4xx HTTP status codes, to be logged at a lower severity level (e.g., `notice` or `info`) instead of the default `error` level.

This mechanism will:
- Allow developers and operators to define a list of exception classes that should not be treated as errors in the logs.
- Be configured centrally via application configuration (e.g., YAML or environment-based), making it easy to manage across environments.
- Integrate with the existing `symfony/monolog-bridge` logging stack through a custom monolog processor or a decorated logger that inspects the exception type before determining the log level.
- Preserve full context and stack traces when needed for debugging, even when logged at a lower level.
- Include clear documentation and naming to indicate that this is a *logging behavior override* and does not suppress error handling or response generation.

Rationale:  
Client-induced errors (e.g., malformed requests, missing fields) are expected operational conditions, not application defects. Logging them as `error` creates noise that obscures genuine server-side problems. By downgrading the log level for known 4xx-type exceptions, we improve signal-to-noise ratio in logs—especially critical in cloud and production environments—while retaining visibility into client behavior.

This approach aligns with best practices in observability, where log levels reflect the locus and nature of faults:  
- `error` and above = unexpected system/server failures (bugs, outages, internal misconfigurations)  
- `warning`/`notice` = expected edge cases, invalid inputs, client misuse

The solution is also consistent with prior precedent in our cloud product, where similar logic has proven effective in reducing log bloat without losing diagnostic value.","## Decision

We add a configuration to the platform that degrades the error level of specific exceptions to notices. This way, external hosters can also profit from our classification.
We use [symfony's `exceptions` configuration](https://symfony.com/doc/current/reference/configuration/framework.html#exceptions) for this.

This has the benefit that for specific projects this configuration could be adjusted, for example, for cases where you also control all clients that access the shop, you may want to log also every client error, just to help debugging the client.

Another solution could be to do the configuration of the log level directly in the exception class either by attributes or a separate method specifying the log level, but that would make overwriting it for a specific project harder, so we stick to the default symfony configuration."
"## Context

Using UUIDs as primary keys eases the integration of several different data sources,
but it also brings some performance issues.

Currently, we're using UUIDv4, which is a random UUID the completely random prefix means
that the B-tree indexes of the database are not very efficient.

UUIDv7 time-based prefix is less spread than that of UUIDv4, this helps the database to keep the index more compact.
It allows the Index to allocate fewer new pages and to keep the index smaller.","## Decision  
We will migrate from UUIDv4 to **UUIDv7** for all new primary key generation in Arachne-managed data models.

### Rationale  
While UUIDs support modular, decentralized data integration across services and modules—aligning with Arachne’s goal of modularity and composability—the use of UUIDv4 has demonstrated suboptimal performance in database indexing due to its fully random nature. This randomness leads to poor index locality, increased page splits, and higher I/O overhead in B-tree–based storage engines.

UUIDv7 addresses these concerns by including a time-ordered prefix, which ensures that newly generated IDs are clustered temporally. This improves:
- Index insertion efficiency
- Cache locality
- Range query performance
- Overall index compactness

These gains come without sacrificing the global uniqueness guarantees essential to Arachne's distributed, composable architecture.

### Impact on Arachne Principles  
- **Modularity**: Preserved — UUIDv7 maintains the ability to generate unique identifiers without coordination across modules or services.
- **Transparency & Introspection**: Enhanced — time-ordered UUIDs allow developers to infer creation order from IDs (within clock precision), aiding debugging and auditability.
- **Configurability**: Supported — systems requiring alternative ID strategies (e.g., integers, custom formats) can still override this default via configuration, per Arachne’s principle of strong defaults with full override capability.
- **Separation from Runtime State**: Maintained — UUID generation remains decoupled from Clojure runtime state, avoiding entanglement with vars or namespaces.

### Migration Plan  
1. New entities created by Arachne core and official modules will use UUIDv7 by default.
2. Existing systems using UUIDv4 will not be automatically converted; migration is opt-in and must be handled at the application level.
3. A new ADR will be created if backward compatibility or interop concerns arise during implementation.

This decision aligns with our architectural emphasis on performance-conscious defaults that do not compromise long-term flexibility or composability.","## Decision

Considering there is little risk to using UUIDv7, as v4 and v7 share the same
length and are indistinguishable for shopware, we can switch to v7 without any risk
of breaking anything.

The effort is also very low as we only need to change the
implementation of the `Uuid` class. As using UUIDv7 will improve the speed of
bulk product inserts by about 8 %, we think the effort is worth the measurable and
theoretical gain."
"## Context
Currently, apps can not extend the list of available events in the flow builder.","## Decision  
We will enable Arachne applications and modules to extend the list of available events in the flow builder by defining new, custom events via the application configuration data.

Custom events will be declared as first-class entities within the central configuration value, following the ""everything is data"" principle. Each event will be defined with:
- A namespaced keyword identifier (to avoid naming conflicts)
- A description of when the event is emitted
- Optional metadata such as expected payload structure or associated lifecycle phase

Applications and modules may define new events by contributing entity definitions under a standard schema for `arachne.event` (or similar). These definitions will be merged into the global schema at application initialization time, making them visible and usable in flow builder DSLs and tooling.

The flow builder will be updated to validate flow steps against the full set of known events (including those contributed by loaded modules and the application itself), enabling autocomplete, static analysis, and documentation generation based on the extended schema.

This approach maintains transparency and modularity:
- All available events are discoverable through introspection of the configuration/schema
- No runtime monkey-patching or dynamic var manipulation is required
- Event definitions are immutable and part of the declarative config
- Third-party modules can provide domain-specific events (e.g., `:mybank/payment-processed`) without coupling to core Arachne components

A future ADR may address dynamic registration of event handlers, but this decision focuses solely on extending the *definition* and *availability* of events in the flow system.","## Decision
We update the flow builder so the apps can expand the list of available trigger events in the flow builder UI."
"## Context

Right now only Shopware Backend is able to communicate with the App Server in a secure way.
This is because the App Server is able to verify the origin of the request by checking request signature, 
and this signature is generated by the Shopware 6 backend application with a shop to app server secret.

When an app wants to communicate directly with the App Server,
the App Server is not able to verify the origin of the request and has to trust the client.
This works fine for public available data, but not when you reliably need the logged-in customer and other information.","## Decision  
We will implement a **signed JWT-based authentication and authorization mechanism** for direct communication between apps and the App Server, ensuring secure, verifiable identity of the requesting app and its context (e.g. associated shop and customer session).

The workflow will be:

1. When a Shopware backend initiates a session with an app (e.g., during app installation or user login), it generates a **request-scoped signed JWT** containing:
   - Shop identifier
   - App identifier
   - Optional: customer ID (if available and authorized)
   - Timestamps (issuance and expiration)
   - A unique nonce or session reference
   This token is signed using the established **shop-to-app-server secret**, known only to the Shopware backend and the App Server.

2. The app receives this JWT via a secure redirect or embedded payload and includes it in subsequent requests to the App Server.

3. The App Server validates:
   - The JWT’s cryptographic signature using the known shop-specific secret
   - Token expiration and replay (via nonce/session tracking)
   - Scope and permissions encoded in the claims

4. Upon successful validation, the App Server processes the request in the appropriate security context (e.g., as acting on behalf of a specific shop and optionally a specific customer).

5. For customer-sensitive operations, the presence of a valid customer ID in the token is required and must have been explicitly granted by the Shopware backend (e.g., after successful storefront authentication).

This approach ensures:
- **Origin verification**: Only tokens signed by the Shopware backend (which holds the secret) can be validated.
- **Context integrity**: Critical session data like shop and customer identities are tamper-proof.
- **No shared runtime state**: All necessary information is self-contained within the token and verified without relying on server-side sessions.
- **Scalability and decentralization**: The App Server remains stateless with respect to authentication, enabling horizontal scaling.

Additionally, we will:
- Define a clear schema for the JWT payload to ensure interoperability across apps and services.
- Require all sensitive endpoints on the App Server to demand a valid JWT.
- Provide SDK support in app templates to simplify token handling and propagation.

This decision aligns with our goals of **security**, **transparency**, and **modularity**, while preserving compatibility with existing Shopware authentication flows and avoiding coupling to the Clojure runtime or mutable state.","## Decision

We provide a new endpoint `/store-api/app-system/{appName}/generate-token` (`/app-system/{appName}/generate-token` in Storefront) which generates a JWT token for the given app.
This endpoint requires that the customer is logged-in and the app is installed and active. 

The JWT token contains claims like:
- `iat` - issued at
- `exp` - expiration time
- `shopId` - the shop id of the current shop
- `salesChannelId` - the sales channel id of the current sales channel
- `customerId` - the customer id of the logged-in customer
- `cartToken` - the cart token of the current cart

The additional claims are bound to app permissions when an app does not have permissions to read a customer,
it does not get the `customerId` claim.

The JWT token is signed with the shop to app server secret key which we use already for the signature.
So the App Server needs to be able to verify the JWT key and use the claims in a secure way.
The request body of the client is still untrusted and has to be properly validated by the app server.

The request cycle would look like this:

```mermaid
sequenceDiagram
    participant Client
    participant Shopware Backend
    participant App Server
    Client->>Shopware Backend: GET /store-api/app-system/TestApp/generate-token
    Shopware Backend->>Client: Responds with Signed JWT Token
    Client->>App Server: Post /product-review/submit containing JWT in header
```

The JWT token is valid for 15 minutes and can be used multiple times until expired.
The client should save it in the [session storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)
and request it only on expiry again.
Additionally, the API route should be rate limited to not generate too often an expensive JWT key."
"## Context
In the past merchants had to deal with issues where their custom-built flow did not
behave how they intended it to do. An concrete example: We've had a merchant that contacted
us (shopware) that their shop did not sent out mails. Debugging the flow turned out to
be harder than we thought and honestly harder than it should be. The flow builder should
empower users to build reliable flows and not spend their precious time trying to
figure out what went wrong.

To improve the experience when using the Flow Builder were taking measures. First,
Flow Builder Preview and second Flow Builder Logging. This ADR only discusses the former one,
there are plans internally for the latter one, but it won't get much attention for now, as
it also addresses different issues (what does my flow do vs what went wrong in the past).

Users should be able to preview a flow and get further understanding on how a flow executes.
This preview only displays the steps and decisions happening inside a flow but doesn't
execute / simulate the real flow","## Decision  
We will implement a **Flow Builder Preview** feature that provides users with a visual and structural representation of how a flow will execute, without actually running or simulating the underlying actions.

The preview will:
- Display the sequential path of execution through the flow, including all steps, conditions, and branching logic.
- Highlight entry points and trace possible execution routes based on condition outcomes (e.g., ""if X then Y else Z"").
- Clearly show the order of operations, nested structures, and any loops or parallel paths where applicable.
- Use static analysis of the flow configuration to determine reachable nodes, dead branches, and potential logical gaps (e.g., unreachable steps due to always-false conditions).
- Not perform any side effects, external calls, or data mutations — it is purely a read-only, declarative view of the flow’s structure.

This approach ensures that users can:
- Validate the correctness of their flow's design before activation.
- Understand the intended behavior at a glance.
- Identify structural issues such as missing branches, circular logic, or incorrect step ordering.
- Reduce time spent debugging post-execution by catching design flaws early.

The Flow Builder Preview will be built as a standalone component decoupled from the runtime execution engine, leveraging the same configuration model used to define flows, ensuring consistency between what is previewed and what is executed.

Future improvements may include interactive simulation with sample data, but for now, the focus remains on delivering a reliable, transparent, and fast feedback loop for flow design validation.","## Decision
The whole scope of the Flow Builder Preview is to help merchants *creating* new flows and
*update* their existing flows.

Will do:
* Evaluate path of flow by executing rules
* Validating the input data of the actions
* Leave it up to the action how much ÃÂÃÂ¡ÃÂÃÂ°realÃÂÃÂ¡ÃÂÃÂ± code is actually executed

Won't do:
* Executing Flow Builder Actions i.e. calling the `handleFlow` method of an action
* Execute one huge transaction to the database and rollback later
    * Mail actions e.g. could render the mail template and send it to a given mail address
    * CRUD actions e.g. could decide, if they only want to check the input data or
      open a transaction to roll it back"
"## Context
In the current media system it is possible to configure different `Shopware\Core\Content\Media\Pathname\PathnameStrategy\PathnameStrategyInterface`.

These strategies are used to store files, which are uploaded for media entity, under a certain path.

The configured strategy is then also used to generate the URL that is used in the frontend/store API to embed the file.

The generation of this URL is currently triggered in an event subscriber which is registered to the `media.loaded` event.

For generating the URL an implementation of the `UrlGeneratorInterface` is used.
```php
interface UrlGeneratorInterface
{
    public function getAbsoluteMediaUrl(MediaEntity $media): string;

    public function getRelativeMediaUrl(MediaEntity $media): string;

    public function getAbsoluteThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;

    public function getRelativeThumbnailUrl(MediaEntity $media, MediaThumbnailEntity $thumbnail): string;
}

interface PathnameStrategyInterface
{
    public function getName(): string;

    /**
     * Generate a hash, missing from url if omitted
     */
    public function generatePathHash(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the cache buster part of the path, missing from url if omitted
     */
    public function generatePathCacheBuster(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): ?string;

    /**
     * Generate the filename
     */
    public function generatePhysicalFilename(MediaEntity $media, ?MediaThumbnailEntity $thumbnail = null): string;
}
```","## Decision  
We will decouple the media URL generation logic from the `media.loaded` event and instead make it a pure, deterministic function of the configured `PathnameStrategyInterface` and `UrlGeneratorInterface`. This change ensures consistency, improves performance, and supports future extensibility.

The key aspects of this decision are:

1. **URL Generation is Derived from Strategy Configuration**:  
   The absolute and relative URLs for a media entity (including thumbnails) must be computed based on the output of the active `PathnameStrategyInterface` — specifically its `generatePathCacheBuster`, `generatePathHash`, and `generatePhysicalFilename` methods — combined with base URL configuration (e.g., domain, CDN settings). No additional state or side effects should be involved.

2. **UrlGeneratorInterface Implementation Must Be Stateless and Pure**:  
   Implementations of `UrlGeneratorInterface` will not maintain internal state related to individual media entities. Instead, they will accept a `MediaEntity` and optional `MediaThumbnailEntity`, invoke the configured `PathnameStrategy`, and construct the URL using a consistent pattern:
   ```
   {base-url}/{cache-buster}/{hash}/{filename}
   ```
   Parts that are omitted by the strategy (e.g. null from `generatePathHash`) are excluded from the path.

3. **Eliminate Event-Based URL Assignment in `media.loaded`**:  
   The current practice of populating URL fields during the `media.loaded` event leads to inconsistent behavior, especially when entities are loaded outside of standard contexts (e.g., in CLI commands, APIs, or async processes). Instead, URL resolution should occur on-demand via service calls or through explicit hydration in API responses.

4. **Introduce a Centralized MediaUrlProvider Service**:  
   A new service, `MediaUrlProvider`, will be introduced as the single entry point for generating media URLs. It will depend on:
   - The currently configured `PathnameStrategyInterface`
   - An implementation of `UrlGeneratorInterface`
   - Base URL/CDN configuration

   This service can be injected wherever URL generation is needed and enables testability and flexibility across environments.

5. **Ensure Frontend and Store-API Consistency**:  
   All endpoints that return media objects (e.g., product detail, media API) must use the `MediaUrlProvider` to generate URLs at response time, ensuring alignment between storage paths and public URLs regardless of strategy in use.

6. **Support Runtime Strategy Switching (with Caveats)**:  
   While the system allows switching strategies via configuration, doing so may result in broken links for previously uploaded files unless the new strategy produces compatible paths. A migration tool or compatibility layer should be provided if backward compatibility is required.

7. **Documentation and Developer Guidance**:  
   Clear documentation will be provided outlining how custom `PathnameStrategy` implementations affect URL structure and how to safely extend or replace them without breaking existing assets.

This approach preserves modularity, avoids runtime entanglement, and ensures transparency in how media URLs are formed — aligning with principles of predictability, configurability, and introspection.","## Decision
To address the issues listed above, we will make the following changes to the system:

- The file path will be saved directly to the media entity (and thumbnail) when the file is uploaded.
    - This way we don't have to access the strategy when generating the URL, which might have changed in the meantime.
- We allow to change the file path via API and write it directly when creating the entity
    - This way files can be synchronized directly with an external storage and the path only has to be changed in the entity or given during import.
- To generate the strategy we use new location structs, which can be easily created via a service.
    - So we remove the dependency to the DAL to generate the file path.
- For generating the URL, we implement a new service, which can be operated without entities.
    - The URL generation is more resource efficient and can be done without fully loaded entities.
- The new URL generator uses a new ""cache busting"" system, which writes the updated at timestamp of the media entity as query parameter into the URL.
    - This way the file path can remain the same even if the file is updated. Changes to the entity's meta data will also result in a new URL."
"## Context
We often need a way between the different Shopware versions to provide a one-time update for data. This is currently done on the way to extend an indexer to this logic and then trigger this via a migration. 
This is of course a possible way to enable certain migrations of data, but this migration is also executed again and again when the indexer is executed. 
With certain data this is critical and can lead to system errors. For example, the one-time migration of media path information.","## Decision  
We will introduce a dedicated, idempotent data migration system for Shopware that separates one-time data migrations from regular indexing logic.

This system will:
- Define data migrations as explicit, versioned, and irreversible operations tied to specific Shopware versions.
- Store migration state in a persistent registry (e.g., a database table) to track which migrations have been applied.
- Ensure each migration runs exactly once per environment, regardless of indexer or deployment re-runs.
- Provide a CLI command to execute pending migrations, enabling controlled rollout during upgrades.
- Allow migrations to be written in PHP and operate on domain or persistence layers directly, with full access to services via dependency injection.

By decoupling one-time data transformations from ongoing indexing processes, we eliminate the risk of accidental re-execution and side effects (e.g., corrupted media paths), improve clarity of intent, and enable safer, more predictable upgrades across Shopware versions.","## Decision

We implement a new `PostUpdateIndexer`. This is an extension of the `EntityIndexer` and the system can be adapted 1:1. Also, the indexing registration via database migration can be adapted 1:1. 
However, the indexer is not triggered via the `IndexerRegistry` when a full re-index or an entity written event is triggered.
These indexers are only included after the update process.
In addition to the one-time update of the data, we then often also provide a command that can be used to trigger the migration of the data again."
"## Context
Our current development process uses ['Experimental features'](./2023-05-10-experimental-features.md) to publish features in an early state to gather feedback regarding those features.
During the implementation, developers may encounter challenges related to the effective management of extensive code scattered throughout the platform, particularly in connection with specific experimental features. This codebase fragmentation presents impediments to the tracking, maintenance, and comprehensive understanding of each feature's scope, thereby hindering our development progress.
 
Potential problems:
* Update `stableVersion` property for Prolonged Experiments
    * When a decision is made to extend an experiment, locating all relevant sections of code for updating the property `stableVersion` in `@experimental` annotation becomes a cumbersome task.
* Deprecation of Killed Features
    * Identifying and marking as deprecated the components associated with a deprecated experimental feature is problematic, particularly when multiple experimental features coexist simultaneously within the platform.
    * The ['Experimental features'](./2023-05-10-experimental-features.md) stipulates the ""Killing Feature"" rule, which mandates that a feature must remain within the platform's codebase until the next major version and be appropriately marked as deprecated. However, it is hardly possible to check with current annotation.

In all the above case main problem is detection to which feature belongs experimental code.","## Decision  
We will introduce **feature-scoped experimental code organization** to improve traceability, maintainability, and lifecycle management of experimental features. To address the challenges of fragmented experimental code, we adopt the following practices:

1. **Feature Directory Co-location**:  
   All code related to a specific experimental feature (including implementation, tests, documentation, configuration, and annotations) must be co-located within a dedicated directory named after the feature under a top-level `experiments/` directory (e.g., `experiments/user-auth-refresh/`). This ensures that all artifacts for a given experiment are physically grouped and easily discoverable.

2. **Centralized Feature Manifest**:  
   Each experimental feature must have a manifest file (`feature.json` or `feature.edn`) inside its directory that includes:
   - Feature name
   - Owner(s)
   - Start date
   - Current status (`proposed`, `active`, `extended`, `killed`)
   - Target stable version (`stableVersion`)
   - List of affected modules/components
   - Link to ADR or design doc

   This manifest serves as the single source of truth for feature metadata and enables tooling support.

3. **Tooling for Automation**:  
   We will develop lightweight tooling (scripts or CLI commands) to:
   - Scan and list all active experimental features
   - Update `stableVersion` across all relevant files in a feature scope
   - Identify and flag components belonging to a killed feature for deprecation
   - Validate compliance with the ""Killing Feature"" rule prior to major releases

4. **Annotation Enforcement via Linting**:  
   The `@experimental` annotation must include a reference to the feature name (e.g., `@experimental user-auth-refresh`).  
   A custom linter will enforce that every `@experimental` usage references a valid, existing feature in the `experiments/` directory, ensuring traceability.

5. **Deprecation Workflow Integration**:  
   When a feature is killed:
   - Its status in the manifest is updated to `killed`
   - All associated symbols must be marked with `@deprecated`, referencing the feature name and removal timeline
   - The linter will flag any use of non-deprecated symbols from killed features

6. **Review Gate in CI**:  
   Pull requests modifying experimental code must specify the affected feature. CI will validate that changes are consistent with the feature’s current status and manifest.

This decision enhances transparency, reduces technical debt accumulation in experimental code, and supports disciplined evolution of features toward stabilization or removal. It aligns with Arachne’s principles of modularity, introspection, and shared understanding by making the lifecycle of experimental functionality explicit and machine-auditable.","## Decision
To address the existing challenges, we propose implementing a refined approach to the use of the `@experimental` annotation. 

The key modifications are as follows:
* Mandatory `feature` property: 
  * Every `@experimental` annotation will now require a mandatory `feature` property. This property is a string that must contain the name of the associated feature.
* Uniform feature Naming: 
  * To enhance code organization and traceability, all sections of code related to a particular feature must use the same feature name in the `feature` property of the `@experimental` annotation.
  * Feature names should follow the conventions.
    * Feature names cannot contain spaces
    * Feature names should be written in `ALL_CAPS`."
"## Context
In the current implementation, there exists a challenge for extension developers in uniquely identifying payment and shipping methods using identifiers.
This issue is particularly significant for app servers, as it necessitates calls to the Shopware Admin API for the identification of payment and shipping methods based on their respective IDs.","## Decision  
We will adopt a namespaced identifier system for payment and shipping methods to ensure global uniqueness and avoid identifier collisions across extensions and integrations.

Each payment or shipping method must be identified by a composite key consisting of:
- A namespace (e.g., the extension or app name, such as `com.example.my-payment-gateway`)
- A local identifier (e.g., `stripe-card`, `flat-rate-shipping`)

The full identifier will take the form: `{namespace}/{local-id}` (e.g., `com.example.my-payment-gateway/stripe-card`).

This approach ensures that:
- Extension developers retain control over their own identifier space.
- App servers can resolve methods unambiguously without requiring runtime lookups via the Shopware Admin API.
- Identifiers are predictable, stable, and resolvable at configuration or deployment time.
- The system remains extensible and modular, allowing third-party apps to integrate seamlessly.

These identifiers will be declared in the application’s configuration data structure (as defined in prior ADRs) and will be validated against the schema at initialization time. The schema will enforce structure and uniqueness constraints on method identifiers.

Additionally, we will provide tooling and validators to help developers generate and manage these identifiers consistently, and documentation outlining best practices for namespace selection (e.g., using reverse domain notation).","## Decision
We will introduce a new property called `technicalName` to both the `payment_method` and `shipping_method` entities.
This `technicalName` property will serve as a unique identifier for payment and shipping methods, significantly simplifying the identification process.

While the `technicalName` field will be optional within the database and API to ensure backward compatibility, it will be made mandatory in the Administration.
This ensures that merchants will update their payment and shipping methods accordingly for the upcoming requirement.
An unique index will ensure uniqueness.
Starting from version 6.7.0.0, this `technicalName` field will also become required within the database and the API.

As part of the database migration process, the `technicalName` field will be automatically generated for the default payment and shipping methods provided by Shopware, as illustrated below:

| Type     | Name             | Technical Name          |
|----------|------------------|-------------------------|
| Payment  | Debit            | payment_debitpayment    |
| Payment  | Invoice          | payment_invoicepayment  |
| Payment  | Cash on Delivery | payment_cashpayment     |
| Payment  | Pre Payment      | payment_prepayment      |
| Shipping | Standard         | shipping_standard       |
| Shipping | Express          | shipping_express        |

Furthermore, all payment and shipping methods provided by apps will also benefit from the automatic generation of their `technicalName`.
This generation will be based on the app's name and the `identifier` defined for the payment method in the manifest:

| App Name | Identifier         | Technical Name                    |
|----------|--------------------|-----------------------------------|
| MyApp    | my_payment_method  | payment_MyApp_my_payment_method   |
| MyApp    | my_shipping_method | shipping_MyApp_my_shipping_method |"
"## Context

At the moment the Storefront is implementing a lot of custom SCSS inside `app/storefront/src/scss`.
Some custom SCSS is not necessary, and it already exists within Bootstrap as a utility.
For example default spacing with a custom selector:

```scss
.register-login-collapse-toogle {
    margin-bottom: $spacer;
}
```

This can be replaced with a Bootstrap [spacing utility](https://getbootstrap.com/docs/5.2/utilities/spacing/) in the HTML and the SCSS can be completely removed:

```diff
- <div class=""register-login-collapse-toogle""><div>
+ <div class=""register-login-collapse-toogle mb-3""><div>
```

A class like `register-login-collapse-toogle` should stay in the HTML in case a developer wants to style this specific element.
But there is no need to introduce a new CSS rule using a custom selector to apply a default spacing.

If you implement new UI using mostly utility classes, please consider to still add CSS classes that offer the possibility for themes to add individual styling. For example:

```html
<!-- Classes ""shipping-modal-actions"", ""shipping-abort"" and ""shipping-submit"" are added for better semantics and CSS extensibility, but ship no default CSS. -->
<div class=""border p-3 mb-3 shipping-modal-actions"">
    <button class=""btn btn-light shipping-abort"">Abort</button>
    <button class=""btn btn-primary shipping-submit"">Submit</button>
</div>
```

This principle cannot be applied everywhere. 
For more complex layouts it can still be valid to use custom SCSS because it is not possible to build with default components and utilities, or it would produce a messy template with too many generic utility classes. 
However, for simpler stuff like ""add a border here, add some spacing there"" it's not necessary to implement additional custom styling.","## Decision  
We will minimize the use of custom SCSS in the Storefront by leveraging Bootstrap’s utility classes for styling related to spacing, borders, typography, and other presentational attributes. Custom SCSS should only be introduced when achieving a design requirement that cannot be reasonably met using Bootstrap’s existing utilities or components.

Utility-first styling promotes faster development, reduces CSS bloat, and improves consistency across the application. However, semantic class names must still be added to key elements to support extensibility and theming. These classes (e.g., `shipping-modal-actions`, `shipping-submit`) do not carry styles by default but serve as anchor points for future customization or theme overrides.

### Guidelines:
1. **Prefer Bootstrap Utilities**: Use utility classes (e.g., `mb-3`, `p-2`, `border`, `text-center`) directly in templates for layout and basic styling.
2. **Preserve Semantic Class Names**: Retain meaningful, descriptive class names on elements to enable targeted styling by themes or extensions, even if no custom CSS is currently applied.
3. **Limit Custom SCSS**: Only write custom SCSS when:
   - Creating complex, reusable components not covered by Bootstrap.
   - Achieving visual designs that would result in excessively cluttered or unreadable markup if built solely with utilities.
   - Implementing theme-specific overrides or responsive behaviors beyond utility capabilities.
4. **Remove Redundant Rules**: Refactor existing SCSS to eliminate rules that merely replicate utility functionality (e.g., `.my-class { margin-bottom: $spacer; }` → use `mb-*` instead).

This approach ensures a lean, maintainable stylesheet architecture while preserving flexibility for future design evolution and third-party theming.","## Decision

We want to make more use of Bootstrap utilities and get rid of custom SCSS that is not needed.

In order to do so we want to remove unneeded SCSS and add a utility class to the HTML instead (e.g. `mb-3`).
However, this can break styling overrides of themes/apps because most Bootstrap utility classes apply `!important` by default.

Let's stick to the example of `.register-login-collapse-toogle`.
* The core Storefront adds a bottom margin of `$spacer` which equals `1rem`.
* Then `CustomTheme` overrides this selector with a margin bottom of `80px`:

```diff
/* CustomTheme */
.register-login-collapse-toogle {
+    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

If the core Storefront would migrate to the utility class it would suddenly overrule the styling of `CustomTheme` because of the `!important` property:

```diff
/* Utility class from HTML overrules CustomTheme */
.mb-3 {
+    margin-bottom: 1rem !important;
}

/* CustomTheme */
.register-login-collapse-toogle {
-    margin-bottom: 80px;
}

/* Storefront */
.register-login-collapse-toogle {
-    margin-bottom: 1rem;
}
```

The theme developer would have no other choice other than using `!important` as well, or modifying the Twig template.

Because of this, we have decided to remove the `!important` from Bootstrap utility classes by changing the [Importance](https://getbootstrap.com/docs/5.2/utilities/api/#importance) variable `$enable-important-utilities` to `false`.
By doing this, we can use more utilities while at the same time allowing themes to override the same CSS property without using `!important` or editing the template.

Since it is currently expected that Bootstrap utilities add `!important` and overrule almost everything, we do not want to change this right away but from `v6.6.0` onwards."
"## Context
For our ambitions to consequently reduce manual testing in favour for test automation the current E2E test suite, based on Cypress, is not sufficient anymore. It has several flaws that have become a big road-blocker. The test suite is tightly coupled to the state of the test environment, which leads to tests that are not very deterministic and tend to be slow and flaky. Tests are often created in different ways and don't follow a specific strategy. In addition, the test suite is currently not able to test against our cloud environment.

Our goal is to create a test suite that fulfills the following requirements:

*  Deterministic tests that can run against any environment.
*  Fast and reliable tests that follow a certain test strategy.
*  Tests that are derived from real product requirements and validate behaviour.
*  A test framework that is easy to learn and using a readable syntax that also non-tech people can comprehend.","## Decision  
We will replace the current Cypress-based E2E test suite with a **contract-based, API-first testing strategy** using **Playwright** for browser automation and **Pact** (or equivalent contract testing tools) for service-level validation, supported by a clear **testing pyramid implementation**.

The new test architecture will follow these principles:

1. **Shift Left with Contract Testing**:  
   We will introduce consumer-driven contract tests using Pact to validate interactions between our frontend and backend services. This decouples test execution from environment state, ensures API reliability, and enables testing against production-like contracts without requiring a fully synchronized backend.

2. **Adopt Playwright for E2E Tests**:  
   Replace Cypress with Playwright due to its superior resilience, parallel execution capabilities, cross-browser support, and better handling of modern web patterns. Playwright’s auto-waiting and trace recording features improve determinism and debugging.

3. **Environment-Agnostic Test Execution**:  
   All tests will be designed to run against any environment (local, staging, cloud) by externalizing configuration (e.g., base URLs, credentials, feature flags) via environment variables. Test data will be seeded programmatically or via API calls to ensure consistency.

4. **Implement a Clear Test Strategy Based on the Testing Pyramid**:  
   - **Unit & Component Tests** (largest layer): Enforce behaviour at the code level.  
   - **Integration/Contract Tests** (middle layer): Validate service interactions using Pact or REST/SOAP mocks.  
   - **End-to-End Tests** (smallest layer): Focus only on critical user journeys (e.g., login, checkout, signup), written in a **Gherkin-style syntax** (Given-When-Then) using Cucumber or similar BDD framework to enhance readability for non-technical stakeholders.

5. **Derive Tests from Requirements**:  
   Establish a traceability mechanism where each automated test maps directly to a specific product requirement or user story in our issue tracking system (e.g., Jira). This ensures coverage of real business behaviour and supports auditability.

6. **Standardize Test Design and Syntax**:  
   Introduce a standardized page object or screenplay pattern across all automated tests. Use domain-specific language (DSL) abstractions to make test steps expressive and maintainable (e.g., `Given.IAmLoggedInAs(""admin"")`, `When.IClickOn(""Submit Button"")`).

7. **Enable Cloud Environment Testing**:  
   Extend CI/CD pipelines to execute selected test suites against the cloud environment during designated stages (e.g., pre-production), using ephemeral test data and isolated test accounts.

This approach will result in a faster, more reliable, and maintainable test suite that aligns with real product needs, reduces flakiness, and supports collaboration across technical and non-technical team members.","## Decision
In order to achieve this goal we decided to not further invest time in the existing test suite, but start from scratch. This offers the opportunity to also switch to another test framework that better fulfills our needs. 

After some evaluation we chose Playwright as the new test framework. First benchmarks have shown that it is faster and more stable than Cypress. Additionally, it is easy to learn, has very good documentation and uses a syntax that is easily to understand. It also offers the functionality of creating reusable traces, which come in handy when debugging failing tests from pipelines."
"## Context
1. If flow actions want to interact with the database in a transactional manner, they need to handle it themselves by starting and committing transactions.

2. When there is a problem committing the transaction, the error will be caught and ignored by the flow dispatcher. A vague error message will be logged, but the flows will continue to execute.
This is problematic if a transaction was already started before the flow is executed. If the connection is configured without save points (which is the default with Shopware), when a nested commit fails (eg during a flow action) the connection will be marked as rollback only.
When the outer transaction attempts to commit, eg the calling code, it will be unaware of the previous inner commit failure and thus will also fail.","## Decision
Flow actions that require transactional database interactions must not manage transactions independently. Instead, transaction management will be centralized and handled by the flow execution framework.

To resolve the issues outlined in the context:

1. The flow dispatcher will be updated to detect and propagate transaction-related errors instead of catching and ignoring them.
2. Transaction control (begin, commit, rollback) will be moved to the outermost flow execution layer, ensuring a single transaction boundary per flow execution unless explicitly scoped otherwise.
3. Flow actions needing database access will participate in the ambient transaction but will not initiate or commit their own.
4. In cases where fine-grained transaction control is required (e.g., partial failure tolerance), explicit transaction-scoped sub-flows or actions with savepoint support may be introduced—only when underlying databases support such features (e.g., PostgreSQL, MySQL with proper configuration).

This change ensures predictable transaction behavior, prevents silent failures from invalidating parent transactions unknowingly, and aligns with best practices for nested transaction handling in distributed control environments. A new ADR will be created if savepoints or sub-transaction capabilities are later deemed necessary.","## Decision

We introduce a new marker interface `\Shopware\Core\Content\Flow\Dispatching\TransactionalAction` which flow actions can implement.

The flow executor will wrap any action in a database transaction which implements the interface.

Before:

```php
class SetOrderStateAction extends FlowAction implements DelayableAction
{
    public function handleFlow(StorableFlow $flow): void
    {
        $this->connection->beginTransaction();
        
        //do stuff
        
        try {
            $this->connection->commit();
        } catch (\Throwable $e) {
                
        }
    }
}
```

After:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        //do stuff - will be wrapped in a transaction
    }
}
```

You can also force the flow executor to rollback the transaction by throwing an instance of `\Shopware\Core\Content\Flow\Dispatching\TransactionFailedException`. You can use the static `because` method to create the exception from another one. Eg:

```php

class SetOrderStateAction extends FlowAction implements DelayableAction, TransactionalAction
{
    public function handleFlow(StorableFlow $flow): void
    {        
        try {
            //search for some record
            $entity = $this->repo->find(...);
        } catch (NotFoundException $e) {
            throw TransactionFailedException::because($e);
        }
    }
}
```

The transaction will be rollback if either of the following are true:

1. If Doctrine throws an instance of `Doctrine\DBAL\Exception` during commit.
2. If the action throws an instance of `TransactionFailedException` during execution.
3. If another non-handled exception is thrown during the action execution. This is to aid debugging.

If the transaction fails, then the error will be logged. Also, if the transaction has been performed inside a nested transaction without save points enabled, the exception will be rethrown.
So that the calling code knows something went wrong and is able to handle it correctly, by rolling back instead of committing. As, in this instance, the connection will be marked as rollback only."
"## Context

Vue 3 introduced a new compatibility mode to ease the migration from Vue 2 to Vue 3. This mode is enabled by default
and allows the use of most Vue 2 features in a Vue 3 application. This mode is only recommended for the transition period
and should be turned off as soon as possible.

We have kept the compatibility mode enabled in the administration because it makes it easier for plugins to migrate 
and results in fewer breaking changes during the major release. This splits the work of migrating the administration
and the plugins into two separate majors instead of one.","## Decision  
We will disable the Vue 3 compatibility mode in the core administration application once all built-in features and officially maintained plugins have been fully migrated to Vue 3's modern API patterns (Composition API, new lifecycle hooks, v-model changes, etc).

Until that time, the compatibility mode will remain enabled by default to:
- Minimize disruption for third-party plugin developers
- Allow staggered migration across the ecosystem
- Reduce the risk and complexity of a simultaneous framework and API upgrade during the major release

The following steps will be taken to ensure a smooth transition:
1. Maintain a public tracker of remaining Vue 2 API usage in core and official plugins.
2. Provide detailed migration guides and codemods for common patterns.
3. Emit runtime deprecation warnings for internal use of deprecated APIs to aid debugging.
4. Set a clear timeline for disabling compatibility mode, communicated to all stakeholders.

Once compatibility mode is disabled:
- New code must use Vue 3 idioms exclusively.
- A separate, opt-in compatibility build may be provided for legacy plugins if needed.
- Documentation and tooling will reflect the post-migration standard.

A new Architecture Decision Record will be created when the time comes to formally deprecate and remove any temporary escape hatches or shims introduced during the transition.","## Decision

Migrating all components in one by disabling the compatibility mode for the whole administration is a huge task and
would make it hard to keep the administration stable during the migration. We decided to disable the compatibility mode
per component level. This allows us to migrate components one by one and keep the administration stable during the migration.

This gives all teams and plugin developers the possibility to migrate their components to Vue 3 without waiting for the
whole administration to be migrated and for the global removal of the compatibility mode.

To activate the new mode, the `DISABLE_VUE_COMPAT` feature flag must be enabled. Then, it is possible to disable the
compatibility mode on a per-component level by setting the `compatConfig` option in the component to our custom configuration.
This custom configuration is exposed in `Shopware.compatConfig` and has all compatibility features disabled if the 
feature flag is activated."
"## Context
It was brought to our attention that the latest version of Vuex `4.1.0` contains a bug that destroys getter reactivity under specific circumstances. The proposed fix was to downgrade to `4.0.2`. However, downgrading was not possible as `4.0.2` contains other bugs that caused modules to fail.","## Decision  
We will upgrade to Vuex `4.1.1` once the release is published, as it includes a confirmed fix for the getter reactivity issue without reintroducing the bugs present in `4.0.2`.  

In the interim, we will apply a local workaround in our codebase to preserve reactivity by restructuring affected getters to avoid the specific conditions that trigger the bug in `4.1.0` (namely, deeply nested getter dependencies accessed during component initialization). The workaround will be clearly documented and flagged with a TODO referencing this ADR and the target Vuex version for removal.

We will also implement an automated test suite specifically designed to detect regressions in getter reactivity, ensuring future upgrades can be validated quickly and safely.

This approach allows us to maintain forward momentum while upholding our commitment to stability, transparency, and avoiding regression risks from downgrading. Once `4.1.1` is available, we will upgrade immediately and remove the workaround.","## Decision
Pinia is the new documented standard with Vue 3; therefore, we will switch to Pinia."
"## Context

In our current architecture, we rely heavily on PHP decoration, Adapter, and Factory patterns to allow for extensions and customizations by third-party developers. While these patterns are effective, they present significant challenges:

1. **Backward and Forward Compatibility:**
    - Maintaining backward and forward compatibility with these patterns is complex and labor-intensive. Each change or update can potentially break existing extensions or require extensive rework to ensure compatibility.

2. **Process Extension Limitations:**
    - These patterns do not inherently allow for the extension of subprocesses unless these subprocesses are extracted into separate classes and interfaces. This extraction often results in a proliferation of interfaces, abstract classes, and their implementations.

3. **Proliferation of Code:**
    - The need to extract subprocesses into separate entities leads to an overwhelming number of interfaces and abstract classes. This proliferation makes the codebase more difficult to understand and maintain, and increases the cognitive load on developers.","## Decision  
We will transition from a class-based extension model (relying on PHP's decoration, Adapter, and Factory patterns) to a **composition-over-inheritance** and **plugin-driven architecture** centered around **explicit extension points** and **data-driven configuration**.

Instead of requiring third-party developers to extend abstract classes or implement interfaces to customize behavior, we will:

1. **Define explicit, versioned extension points** in the core system where custom logic can be plugged in via simple, well-documented function contracts or message handlers.
2. **Use a plugin registration system** that allows extensions to declare their contributions declaratively through configuration or manifest files, reducing reliance on class hierarchy manipulation.
3. **Replace deep inheritance chains with composable units of behavior**, implemented as pure functions or lightweight service objects, which are registered into the system at runtime.
4. **Leverage an evented or middleware-style pipeline** where applicable, allowing plugins to participate in processing flows without subclassing or decorating core components.
5. **Maintain backward compatibility via adapter layers only at extension point boundaries**, rather than throughout the class hierarchy, making upgrades safer and more predictable.

This approach reduces the fragility of inheritance-based customization, minimizes interface proliferation, improves long-term maintainability, and supports both forward and backward compatibility by decoupling core evolution from extension implementation.

Existing uses of Adapter and Factory patterns will be refactored incrementally, with new functionality developed using the new model. Documentation and tooling will be provided to help external developers migrate to the new paradigm.","## Decision

To address these challenges, we have decided to transition to an event-based extension system. This new approach will replace the existing decoration, Adapter, and Factory patterns as the primary method for extending and customizing our system."
"## Context

Currently, our component tests in Jest are running with enabled compat mode. To remove the compat mode for each
component, we need to add a new Jest runner with disabled compat mode to make sure that the tests are running without compat mode.","## Decision  
We will introduce a dedicated Jest test runner configuration that explicitly disables React's compatibility (compat) mode for component tests. This new runner will be used to ensure all new and existing component tests are executed in strict, forward-compatible conditions.

The implementation will follow this plan:

1. Create a new Jest runner configuration file (e.g., `jest.runner.strict.js`) that extends the base Jest config but sets React-specific options to disable compat mode.
2. Update the test setup so that component tests are run using this strict configuration either by default or via a flag.
3. Migrate component tests incrementally by running them under the strict runner and fixing any failures caused by deprecated patterns or invalid assumptions exposed when compat mode is disabled.
4. Once all component tests pass under the strict runner, deprecate and eventually remove usage of the compat-mode-enabled runner.
5. Add documentation outlining the requirements for writing tests in strict mode, including proper cleanup, use of modern React testing utilities, and avoidance of legacy lifecycle behaviors.

This change supports our goal of maintaining high code quality and ensures our components are tested under conditions that reflect production usage without deprecated features. It also aligns with React’s long-term direction and improves the reliability and maintainability of our test suite.","## Decision

I added a new runner command in the NPM scripts to run the Jest tests without compat mode. The new runner command is
`unit:disabled-compat` and `unit-watch:disabled-compat`. Also, the composer commands are added to run the tests. These commands are `admin:unit:disabled-compat` and `admin:unit-watch:disabled-compat`. These commands are using the environment variable `DISABLE_JEST_COMPAT_MODE` to disable the compat mode.

For the pipeline, I added a new stage to run the Jest tests without compat mode. The stage is `Jest (Administration with disabled compat mode)`.

To mark a test file working without the compat mode you need to add a comment with the `@group` tag. The tag is `@group disabledCompat`.

Example:
```javascript
/**
 * @package admin
 * @group disabledCompat
 */

import { mount } from '@vue/test-utils';

async function createWrapper() {
...
```

With this tag, the test file is running without compat mode. To make a component working for both modes, you can use the
compatUtils helper function from Vue compat:
```javascript
// Example
import { compatUtils } from '@vue/compat';

...

if (compatUtils.isCompatEnabled('INSTANCE_LISTENERS')) {
    return this.$listeners;
}

return {};

...
```


Important: the test still runs also with compat mode activated in parallel."
"## Context

In some instances, a system rollout was completed where an error in certain system functionalities was not detected until the system was live.

A software system is made up of many components that work together to provide a service. The software system can still be healthy even if some of its components are not fully in a healthy state.  System health checks are a way to monitor the health of a system and detect failures early.","## Decision  
We will implement a hierarchical, composable health checking system for Arachne that enables both fine-grained component-level monitoring and holistic system-level status reporting.

Each modular component (e.g., database connection pools, caches, external service clients, HTTP servers) may expose a health check function that returns structured data indicating its status (`:healthy`, `:degraded`, or `:unhealthy`), along with contextual metadata such as latency, error rates, or timestamps.

These individual health checks are declared *as data* within the Arachne configuration model, allowing them to be registered, composed, and managed without tight coupling to runtime lifecycle phases or global state.

The overall system health is derived by aggregating component statuses according to configurable policies (e.g., ""if any critical component is unhealthy, mark system as unhealthy; if non-critical components are degraded, mark system as degraded""). This aggregation is also defined in configuration, enabling operators to tailor behavior per environment (development, staging, production).

Health endpoints (e.g., `/health`) will be automatically generated based on the configuration and exposed via standard HTTP routes. These return machine-readable responses suitable for integration with orchestration platforms (e.g., Kubernetes, Consul), while also providing human-readable explanations when possible.

To prevent undetected failures post-rollout:
- Health checks must be implemented for all externally dependent services and critical internal subsystems.
- Deployment pipelines should include a ""warm-up"" phase that queries the health endpoint before routing traffic.
- Degraded states do not block startup but must emit clear telemetry and alerts.

By modeling health as a first-class concept in the configuration and making it transparent and inspectable at all levels, we align with Arachne’s goals of modularity, introspection, and operational clarity.","## Decision

We will implement system health checks in Shopware to monitor certain parts of the system with the aim to detect failures and issues early.
This system should be extensible and allow for custom health checks to be added with ease."
"## Context

The `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber` only allows
the `Shopware\Tests\Unit` namespace to be considered when enabling the major feature in the unit tests suite.","## Decision  
We will modify the `TestPreparationStartedSubscriber` to allow configurable test namespaces for feature flag enabling, rather than hardcoding enforcement to only `Shopware\Tests\Unit`.

The current restriction limits flexibility and creates unnecessary friction when organizing or expanding unit tests outside the predefined namespace (e.g., for component-specific or integration-style unit tests that still belong in a broader ""unit"" category).

To maintain control while improving extensibility:
1. The list of allowed namespaces for major feature flag activation during test preparation will be configurable via a central testing configuration.
2. A default configuration will include `Shopware\Tests\Unit` to preserve existing behavior.
3. Additional namespaces can be added per project or suite needs without modifying core extension code.

This change supports better separation of concerns, improves test organization freedom, and aligns with principles of configurability and modularity — allowing teams to structure tests appropriately while retaining the safety and intent of feature-flag-aware testing.","## Decision

To be able to unit test the upcoming major feature in other plugins we will enable the possibility to add other
namespaces to
the `Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\Subscriber\TestPreparationStartedSubscriber`.

We'll add a static method called `addNamespace()` method to the
`Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension` and by this we are able to add other namespaces
to the allowlist of the namespaces to be considered when enabling the major flags in the unit test suite.

This can be useful for plugins that wants to enable the major flags in their unit tests suite.

Therefore, add the extension to the extension list in the `phpunit.xml`:

```xml

<extensions>
    ...
    <bootstrap class=""Shopware\Core\Test\PHPUnit\Extension\FeatureFlag\FeatureFlagExtension""/>
</extensions>
```

And register your test namespace in your test bootstrap file:

```php
FeatureFlagExtension::addTestNamespace('Your\\Unit\\Tests\\Namespace\\');
```

For example, in the Commercial plugin, we added the following code to the `tests/TestBootstrap.php` file:

```php
FeatureFlagExtension::addTestNamespace('Shopware\\Commercial\\Tests\\Unit\\');
```"
"## Context

Our Vue.js application currently uses the Options API, which is the traditional way of writing Vue components in Vue 2. With the release of Vue 3, the Composition API was introduced, offering improved code organization, better TypeScript support, and enhanced reusability of component logic. For more detailed information about the reasons for migrating to the Composition API, see the [documentation entry](https://developer.shopware.com/docs/guides/plugins/plugins/administration/system-updates/vue-native.html).

To modernize our codebase and take advantage of these benefits, we need to migrate our existing Vue 2 Options API components to use the Vue 3 Composition API. Manual conversion of numerous components would be time-consuming and error-prone. Therefore, we need an automated solution to assist in this migration process.","## Decision  
We will adopt an automated migration strategy using a combination of codemods and linting rules to convert our Vue.js application from the Options API to the Composition API.

The workflow will be as follows:

1. **Use Official Vue 3 Migration Tools**: We will leverage [`@vue/codemod`](https://github.com/vuejs/core/tree/main/packages/codemod), the official Vue 3 codemod tool, to automatically transform existing Options API syntax into equivalent Composition API patterns where possible. This includes:
   - Converting `data`, `methods`, `computed`, and `watch` options into `ref`, `reactive`, `computed`, and `watch` usages within `setup()`.
   - Migrating lifecycle hooks (e.g., `mounted`, `created`) to their Composition API equivalents (e.g., `onMounted`, `onCreated`).
   - Handling props, emits, and injections via explicit declarations in `setup`.

2. **Adopt `<script setup>` Syntax**: As part of the migration, we will transition converted components to use the `<script setup>` syntax (SFC compile-time syntactic sugar), which reduces boilerplate and improves readability. Components will be updated to use top-level `await`, reactive declarations, and auto-exposed bindings.

3. **Run Incrementally with Testing**: The codemod will be applied incrementally, module by module, with each change accompanied by unit and integration tests to ensure behavioral consistency. Any breaking changes or edge cases missed by automation will be addressed manually.

4. **Enforce with Linting and Code Standards**: We will update our ESLint configuration with [`@vue/vue3-recommended`](https://eslint.vuejs.org/rules/) rules to enforce Composition API best practices and prevent regression to deprecated patterns. A custom rule may be added to disallow new Options API usage.

5. **Provide Developer Documentation and Training**: We will document the new standards and provide internal training materials to onboard developers on the Composition API, including how to write reusable logic using composables.

6. **Retain Manual Oversight**: While automation handles bulk conversion, all transformed components must undergo code review to verify correctness, especially around complex reactivity logic, this-context reliance, or plugin-specific behavior.

By automating the bulk of the transformation while maintaining human oversight, we ensure a scalable, consistent, and safe migration path to the Composition API—aligning our codebase with modern Vue development practices, improving maintainability, and enabling better type inference with TypeScript.","## Decision

We have decided to implement a Codemod in the form of an ESLint rule to automatically convert Vue 2 Options API components to Vue 3 Composition API. This Codemod will:

1. Identify Vue component definitions in the codebase.
2. Convert the following Options API features to their Composition API equivalents:
    - Convert `data` properties to `ref` or `reactive`.
    - Convert `computed` properties to `computed()` functions.
    - Convert `methods` to regular functions within the `setup()` function.
    - Convert lifecycle hooks to their Composition API equivalents (e.g., `mounted` to `onMounted`).
    - Convert Vue 2 specific lifecycle hooks to their Vue 3 equivalents.
    - Convert `watch` properties to `watch()` functions.
    - Handle `props` and `inject` conversions.
    - Replace `this` references with direct references to reactive variables.
    - Convert writable computed properties.
    - Handle reactive object reassignments using `Object.assign`
    - Handle correct usage of `ref` and replace the access to the value with `.value`.

3. Generate a `setup()` function containing the converted code.
4. Add necessary imports for Composition API functions (e.g., `ref`, `reactive`, `computed`, `watch`).

The Codemod will be implemented as an ESLint rule to leverage the existing ESLint ecosystem and allow for easy integration into our development workflow."
"## Context
The current block system relies on TwigJs for creating reusable and extendable component template.
This system allows plugin developers to override or extend content in predefined blocks using TwigJsÃÂÃÂ¡ÃÂÃÂ¯s inheritance mechanisms. Vue.js offers a more dynamic and reactive component-based architecture which is better suited for modern frontend development.

To replace TwigJs with a native Vue.js approach we need to introduce two new component `sw-block` and `sw-block-parent`.

The new `sw-block` and `sw-block-parent` components in Vue.js aim to replicate and improve on the TwigJs block system by providing a component-based approach. These components offer enhanced flexibility by allowing dynamic content overriding and extension directly in VueÃÂÃÂ¡ÃÂÃÂ¯s templating and reactivity model.","## Decision  
We will deprecate the use of TwigJs for block-level template inheritance and replace it with a native Vue.js component system using `sw-block` and `sw-block-parent`.

The new component-based architecture provides several advantages over the current TwigJs implementation:

- **Improved reactivity**: By leveraging Vue.js’s reactivity system, block content can respond dynamically to state changes, enabling richer interactive experiences.
- **Better developer experience**: Plugin developers can now work entirely within the Vue.js ecosystem, reducing context switching and allowing use of modern tooling (e.g. single-file components, devtools, TypeScript support).
- **Enhanced composability**: The `sw-block` and `sw-block-parent` components enable hierarchical content overriding and extension through standard Vue templating, scoped slots, and component injection patterns—offering greater flexibility than static template inheritance.
- **Reduced complexity**: Removing the dependency on TwigJs simplifies the build chain, reduces bundle size, and eliminates edge cases related to server-side/client-side rendering mismatches.

### Migration Plan:
1. Implement `sw-block` and `sw-block-parent` with feature parity to current TwigJs block behaviors:
   - Support for defining named blocks
   - Allow plugins or child components to override or append content
   - Maintain backward compatibility during transition via adapter layers if necessary
2. Mark all TwigJs-based block definitions as deprecated in documentation and console warnings.
3. Provide codemods and documentation to assist plugin developers in migrating their overrides to the new Vue component model.
4. Set a target release for removal of TwigJs block processing in a future major version.

### Long-term Implications:
- Future UI extensibility will be based exclusively on Vue components and composition APIs.
- We expect increased consistency across plugins and faster development cycles due to unified frontend architecture.

This decision aligns with our goal of modernizing the frontend stack while preserving (and enhancing) the extensibility that the block system was designed to provide.","## Decision
We have decided to migrate from the TwigJs-based block system to the Vue.js-based `sw-block` system. This decision aligns with the following goals:

1. **Uniform Frontend Stack**: Vue.js is already used for the frontend in Shopware Administration. Moving blocks from TwigJs to Vue.js creates consistency in the technology stack, simplifying development and reducing the cognitive load on developers who previously had to switch between TwigJs and Vue.js.

2. **Improved Flexibility**: With Vue.js blocks we have control over the provided access to the componentÃÂÃÂ¡ÃÂÃÂ¯s internals, allowing for more granular control over block behavior and content. This allows us to create a public API for any component, making it easier to extend and modify internal behavior, without breaking the public API."
"## Context

Currently, all product prices that are displayed in the default Storefront have an asterisk `*` next to them, for example: `ÃÂÃÂ 50,00 *`
This asterisk refers to the tax and shipping costs information in the page footer `* All prices incl. VAT plus shipping costs and possible delivery charges, if not stated otherwise.`

Using the asterisk `*` next to every price has several downsides that we want to address:","## Decision  
We will remove the asterisk `*` from individual product prices in the default Storefront and instead display tax and shipping information through a centralized, persistent UI element that is clearly visible but less intrusive.

### Rationale  
The current use of an asterisk next to every price creates visual clutter and degrades the user experience by drawing excessive attention to disclaimers rather than the core pricing information. It also assumes users will scroll to the footer to understand the meaning of the symbol, which harms accessibility and transparency.

Alternatives considered:
- **Inline tooltip on hover**: Adds interactivity but fails for touch devices and doesn’t guarantee visibility.
- **Small footnote text beneath each price**: Repetitive and increases layout complexity across product listings.
- **Persistent banner or info bar at top/bottom of page**: Provides consistent context without repetition.

We choose the **persistent info bar** approach, placed directly below the main navigation or above the footer (depending on viewport), with concise, readable text:  
*""All prices include VAT. Shipping costs are calculated at checkout.""*

### Implementation Guidelines  
1. Remove all `*` symbols adjacent to prices in product listings, detail pages, cart, and checkout previews.
2. Introduce a non-intrusive, responsive info bar that remains visible across browsing sessions unless dismissed (optional persistence via cookie).
3. Ensure the message is semantically marked up (`<small>`, `aria-live` if dynamic) for accessibility compliance.
4. Allow theme and module overrides so merchants can customize wording or re-enable asterisks if legally required in specific jurisdictions.

### Exceptions  
Legal requirements in certain regions may necessitate per-price disclaimers. In such cases, a configurable flag will allow re-enabling the asterisk per store configuration, accompanied by automated compliance checks during deployment.

This change improves clarity, reduces visual noise, and aligns with best practices in e-commerce UX while preserving transparency.","## Decision

The asterisk `*` next to every price will be removed because of the reasons mentioned above.
In most areas of the Storefront, the information that the asterisk `*` refers to is already given, and it is therefore redundant.
In areas where the asterisk `*` was actually needed, it will be replaced by the actual text ""Prices incl. VAT plus shipping costs"" instead to resolve the accessibility issues."
"## Context

To comply with Web Content Accessibility Guidelines (WCAG), we aim to make Shopware 6's document solution accessible (A11y-compliant). This ensures that our solution remains usable while meeting legal requirements and making documents accessible to customers with disabilities. 

Currently, our PDF generation library, DomPDF, does not meet accessibility standards, posing a significant challenge.","## Decision  
We will replace DomPDF with an accessible PDF generation library that supports WCAG-compliant output, or extend our current solution with post-processing tools and techniques to ensure accessibility conformance.

Given the importance of A11y compliance—both for legal adherence and inclusive user experience—we must ensure that all generated documents are perceivable, operable, understandable, and robust (POUR principles). This includes:

- Proper tagging of PDF elements (headings, lists, tables, images with alt text)
- Logical reading order and document structure
- Support for screen readers and assistive technologies
- High color contrast and scalable text

The chosen approach will be one of the following:

1. **Migrate to a WCAG-compliant PDF engine** such as `TCPDF`, `mPDF`, or a headless browser-based solution like Puppeteer/Playwright (which can generate tagged PDFs from semantic HTML), provided they meet our technical and performance requirements.
2. **Enhance DomPDF with accessibility post-processing**, if feasible, using external tooling (e.g., PDF/UA validators, tag injectors) — though this is considered a temporary workaround due to maintenance burden and limited reliability.

A proof of concept will be conducted to evaluate candidate libraries against:
- Output accessibility (validated via tools like PAC or Adobe Acrobat Pro’s Accessibility Checker)
- Fidelity of design rendering
- Performance at scale
- Integration effort within Shopware 6’s architecture

An Architecture Decision Record (ADR) will be created to document the evaluation process, alternatives considered, and final selection. The decision will be reviewed by the steering group to ensure alignment with long-term maintainability and accessibility goals.

Until a compliant solution is implemented, we will flag the current PDF generation as non-compliant in internal documentation and provide a roadmap for remediation.","## Decision

We have decided to make HTML documents available in addition to PDF documents, as these are more accessible.

- **Better for Accessibility**: HTML is naturally organized, making it easier for accessibility tools to read and present content to people with disabilities.

- **Lack of Support**: As our current PDFs lack support for accessibility. Few tools, especially in PHP, can create accessible tagged PDFs, making it difficult to maintain PDF accessibility.

- **Industry Trends**: Many organizations are already moving from PDFs to HTML for accessibility. For example, government websites have been required to meet accessibility standards since the early 2000s. Most of them now use HTML for most of their content because it meets these standards better.

Providing HTML documents aligns with these trends and ensures we are using best practices for accessibility."
"## Context
The existing rule system is flexible but complex, making it difficult to evolve and maintain. Allowing unrestricted extensions of rule classes slows down improvements and increases the complexity of the system.

See RFC: https://github.com/shopware/shopware/discussions/5785","## Decision
We will simplify and stabilize the rule system by restricting direct inheritance of rule classes. Instead, we will introduce a plugin-based extension mechanism that allows customization through configuration and composition rather than subclassing.

This change will:
- Improve maintainability by reducing the surface area of extensibility
- Enable safer refactoring and evolution of core rule logic
- Provide clear, documented extension points via a defined plugin interface
- Encourage reuse and composability of rule behaviors

Existing custom rule implementations will be migrated to the new extension model during a phased deprecation period. Documentation and migration tools will be provided to support developers in adapting their code.

The core rule classes will be marked as final, and future extensions must use the approved plugin mechanism. This ensures long-term stability while still supporting the necessary flexibility for business logic customization.","## Decision
We will mark existing rule classes as internal, limiting direct usage by third parties. Developers should create new rule classes instead of modifying existing ones. 

Nearly all rule classes will be marked as internal, with a few exceptions: 

```
LineItemOfTypeRule
LineItemProductStatesRule
PromotionCodeOfTypeRule
ZipCodeRule
BillingZipCodeRule
ShippingZipCodeRule
```

These classes will remain public for now, because they rely on configuration which is reasonably expected to be extended by third-party developers."
"## Context

The Storefront JavaScript currently offers a helper class `src/helper/iterator.helper.js` that can iterate over different objects like Maps, FormData, plain arrays etc.
Using this approach has several downsides we want to address.

* It creates an unnecessary abstraction over the native alternative. Very often the `Iterator.iterate()` directly uses js plain `forEach` loop without any additional logic. Using the abstraction is actually creating more complexity.
* It prevents the developer from using the appropriate loop for the given data type and instead passes it to an iterator that does it with arbitrary data types.
* The iterator is a special shopware syntax that needs to be understood and documented. It is way easier to use web standards that everyone knows and that is also officially documented already.
* The usage throughout the codebase is inconsistent and the iterator helper is used alongside the native loops which can create confusion.
* It creates the impression that the data that is being iterated is some special custom object that needs a special iterator helper. In reality, it is a simple `NodeList` in 90% of the time.
* It creates an additional dependency/import in every file that is using it.","## Decision
We will deprecate and remove the custom `Iterator` helper class in favor of native JavaScript iteration methods and language constructs.

Developers must use standard, idiomatic JavaScript approaches for iteration, including:
- `for...of` loops for iterable objects (Arrays, Maps, NodeLists, Sets, etc.)
- `Object.keys()`, `Object.values()`, `Object.entries()` for plain objects
- Native array methods such as `forEach`, `map`, `filter`, and `reduce`
- `for...in` loops (with caution) when iterating object properties

This decision aligns with our goals of:
- Reducing unnecessary abstractions that obscure intent
- Promoting use of well-documented, standardized language features
- Improving code readability and maintainability
- Eliminating inconsistencies in iteration patterns across the codebase
- Reducing dependency overhead and import complexity

A migration path will be provided:
1. Mark the `Iterator` class as deprecated in documentation and via JSDoc
2. Gradually replace all existing uses of `Iterator.iterate()` with appropriate native alternatives
3. Remove the `src/helper/iterator.helper.js` file once all usages are eliminated

New code must not use the Iterator helper. Code reviews will enforce the use of native iteration constructs.","## Decision

For all the above reasons we have decided to deprecate `src/helper/iterator.helper.js`, stop using it, and use native alternatives instead.
We want to have a more lean and simple Storefront JavaScript that needs less special things to learn and understand."
"## Context

Currently, flows are executed during the business process. A business
process is any event that is triggered by the user, like a checkout or
a product update. Flows are used to react to these events and execute user defined actions.

There are several downsides to this approach. The most straightforward is the possibility of a fatal error during
the execution of any flow. This will inevitably cancel the in progress business process. While there are remedies in place
to avoid any trivial errors in flow execution disrupting the business process, certain errors can not be avoided.

Further, even if all flows execute without error, they noticeably impact business process performance. Flows are often 
employed to, for example, send mails, an expensive operation that is not necessary to complete the business process.

In addition to these concrete disadvantages, there are also some more abstract ones. For one, currently flows are executed directly
from a decorator of Symfony's `EventDispatcher`, leading to cumbersome debugging and bloated stack traces. This would
be improved if flows are executed by a dedicated event listener. Additionally moving flows to their own execution
environment would not only simplify debugging, but also potentially make expanding their capabilities easier.

> *The following is all experimental and only takes effect if the associated feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS` is enabled*","## Decision
We will decouple flow execution from the main business process by introducing an asynchronous execution model, where flows are executed *after* the completion of the business process, rather than during it.

This change will be gated behind the feature flag `FLOW_EXECUTION_AFTER_BUSINESS_PROCESS`, ensuring a controlled rollout and backward compatibility during testing and evaluation.

The new workflow will be:
1. The business process executes to completion, including all core domain logic and persistence operations.
2. Upon successful completion, relevant events are published to a dedicated event bus or message queue.
3. A separate, dedicated flow executor (implemented as a Symfony console command or message consumer) listens for these events and triggers the corresponding flows asynchronously.
4. Flow execution occurs in isolation from the main request lifecycle, using its own service container context and error handling.

Benefits of this approach include:
- Improved reliability: failures in flow execution no longer jeopardize the business process.
- Better performance: expensive operations (e.g., sending emails, external API calls) do not block user-facing requests.
- Simplified debugging: flow execution is separated into a distinct runtime context with cleaner stack traces.
- Enhanced observability: flow executions can be monitored, retried, and audited independently.
- Future extensibility: paves the way for advanced capabilities such as scheduling, prioritization, batching, and distributed execution.

During the transition period, both execution models will coexist. Once sufficient confidence is gained via monitoring and testing, we will plan to deprecate and eventually remove the synchronous execution path.

Any flow requiring immediate side effects visible to the user must be explicitly refactored into part of the core business logic and cannot rely on the flow system.","## Decision


We will move the flow execution after the business process. As outlined above we believe that this will simplify the
development around flows, while also improving their performance and safeguarding the main execution environment.

To ensure that flows are executed as close to the business process that triggered them as possible, we will
'queue' the flow execution. This means that flows will be executed after the business process has finished.
Flows are stored in memory and executed as soon as the execution environment signals, that it has
finished a unit of work. The associated events are as follows:

1. After a controller action has been executed (Web) => `KernelEvents::TERMINATE`
2. After a queued message has been processed (Queue) => `WorkerMessageHandledEvent`
3. After a command has been executed (CLI) => `ConsoleEvents::TERMINATE`

Another option would be to handle flow executions as queue messages. This would entirely remove
flow executions from the runtime of the business process. While this would be a simpler solution,
it would both make debugging more complex and introduce an unpredictable delay between the business
process and the flow execution. While this delay could be mitigated by using a high priority queue,
it could not reliably be kept under a certain threshold. To entirely avoid this delay, we decided
that the flow execution should be handled as close to the business process as possible."
"## Context and Problem Statement

We want to record architectural decisions made in this project.
Which format and structure should these records follow?","## Decision  
We will adopt the Architecture Decision Record (ADR) format as formalized by Michael Nygard, with a lightweight structure based on Mark Seemann's approach.

Each ADR will be a Markdown file with the following sections:

- **Status**: One of `proposed`, `accepted`, `rejected`, or `deprecated`
- **Deciders**: Names or roles of individuals involved in making the decision
- **Date**: Date when the decision was proposed
- **Context**: Description of the problem or issue requiring a decision. This should clearly explain the forces at play, including technical constraints, user needs, and trade-offs.
- **Decision**: The chosen resolution to the problem.
- **Consequences**: Positive and negative implications of the decision, including impacts on modularity, performance, maintainability, and future work.

These records will be stored in a dedicated `docs/architecture/decisions/` directory in the main Arachne repository (or in module-specific documentation directories for module-scoped decisions), and named sequentially (e.g., `001-use-adrs.md`, `002-application-config-model.md`) to support chronological tracking.

This format is simple, human-readable, version-control-friendly, and aligns with our goals of transparency, collaboration, and long-term maintainability. It enables easy review, linking, and retrieval while remaining accessible to contributors across time and space.","## Decision Outcome

Chosen option: ""MADR 2.1.2"", because

* Implicit assumptions should be made explicit.
  Design documentation is important to enable people understanding the decisions
  later on.
* The MADR structure is comprehensible and facilitates usage & maintenance."
"## Context and Problem Statement

The Metadata API provides a modern Python API for accessing individual pieces
of metadata. It does not provide any wider context help to someone looking to
implement a TUF repository.

The legacy python-tuf implementation offers tools for this but suffers from
some issues (as do many other implementations):
* There is a _very_ large amount of code to maintain: repo.py,
  repository_tool.py and repository_lib.py alone are almost 7000 lines of code.
* The ""library like"" parts of the implementation do not form a good coherent
  API: methods routinely have a large number of arguments, code still depends
  on globals in a major way and application (repo.py) still implements a lot of
  ""repository code"" itself
* The ""library like"" parts of the implementation make decisions that look like
  application decisions. As an example, repository_tool loads _every_ metadata
  file in the repository: this is fine for CLI that operates on a small
  repository but is unlikely to be a good choice for a large scale server.","## Decision 
We will create a new high-level Repository API that is separate from the low-level Metadata API. This Repository API will provide opinionated, composable tools for building and managing TUF repositories, while avoiding the pitfalls of previous implementations.

The design will follow these principles:

1. **Separation of concerns**: The Repository API will be strictly layered on top of the Metadata API and storage abstractions. It will not duplicate low-level metadata operations but will orchestrate them.

2. **Composability over monolithic design**: Instead of providing a single ""do everything"" class, we will offer small, focused components (e.g., SnapshotBuilder, TimestampUpdater, TargetPublisher) that can be combined as needed by applications.

3. **Explicit configuration**: All behavior will be configured explicitly through parameters rather than relying on global state or implicit defaults. This includes storage locations, signing policies, expiration times, etc.

4. **Lazy operations**: Operations will be lazy by default—metadata will only be loaded when specifically requested, allowing efficient operation on large repositories.

5. **Storage abstraction**: All file system interactions will go through a pluggable storage interface, enabling support for different backends (local disk, S3, etc.) without changing core logic.

6. **No automatic side effects**: The API will avoid automatically writing files or modifying state unless explicitly instructed, making it predictable and testable.

This approach allows us to provide helpful higher-level abstractions for repository management while maintaining flexibility, performance, and maintainability—addressing the complexity and architectural issues of prior implementations.","## Decision Drivers

* There is a consensus on removing the legacy code from python-tuf due to
  maintainability issues
* Metadata API makes modifying metadata far easier than legacy code base: this
  makes significantly different designs possible
* Not providing a ""repository library"" (and leaving implementers on their own)
  may be a short term solution because of the previous point, but to make
  adoption easier and to help adopters create safe implementations the project
  would benefit from some shared repository code and a shared repository design 
* Maintainability of new library code must be a top concern
* Allowing a wide range of repository implementations (from CLI tools to
  minimal in-memory implementations to large scale application servers)
  would be good: unfortunately these can have wildly differing requirements"
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision  
We will migrate the Primer React codebase from JavaScript to TypeScript. This will ensure type definitions are co-located with implementation code, enabling automatic synchronization between runtime behavior and type contracts.

The migration will follow these principles:

1. **Incremental Adoption**: We will convert files one at a time, starting with low-dependency, high-impact packages (e.g. `@primer/react-core`). Each converted file will have a `.ts` or `.tsx` extension and include explicit types for props, state, and return values.
2. **Strict Type Checking**: All new and migrated files will be covered under `strict: true` in `tsconfig.json`. This includes strict null checks, no implicit `any`, and exhaustive property checking.
3. **Public API Consistency**: The external API surface of components must remain stable during and after migration. Breaking changes require a major version bump and must be documented.
4. **Tooling Integration**: Our build pipeline (e.g. Rollup, Babel, ESLint) will be updated to support TypeScript compilation and type checking. We will use `tsc --noEmit` in CI to validate types independently of transpilation.
5. **Testing Coverage**: All component tests will also be migrated to TypeScript (`*.test.tsx`) to ensure type safety in usage examples and test assertions.
6. **Documentation Sync**: Storybook and other documentation tools will be verified to correctly extract and display TypeScript types in component docs.

By embedding types directly in the source, we eliminate the maintenance burden and inaccuracies of ambient declarations, improve developer experience, reduce bugs, and future-proof the library for long-term scalability.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂÃÂ¡ÃÂÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision  
We will standardize on using **Box with the `as` prop** as the primary pattern for building components in Primer React, and deprecate the use of `styled-components` for new component implementations.

### Rationale

- **Consistency**: Using Box provides a uniform API across all components, reducing cognitive load for developers and contributors.
- **Theming & Styling**: Box is tightly integrated with Primer’s theme system and supports the `sx` prop, enabling full access to design tokens (e.g., spacing, colors, radii) in a type-safe way.
- **Flexibility**: The `as` prop allows rendering any element or component, supporting dynamic tag assignment without requiring multiple styled factories.
- **Reduced Dependencies**: Migrating away from `styled-components` reduces the number of styling abstractions and dependencies, simplifying the stack and improving bundle size.
- **Easier Composition**: Components built with Box are simpler to extend via composition rather than inheritance or template literals, aligning better with React best practices.
- **Avoids Runtime Overhead**: Unlike `styled-components`, which generates unique class names at runtime, Box + `sx` leverages a constrained style system that can be more easily optimized.

### Migration Plan

1. All new components must be implemented using the Box pattern.
2. Existing `styled-components` usages should be migrated incrementally during maintenance or feature work.
3. Provide codemods where feasible to automate the transformation of common `styled` patterns to equivalent Box-based implementations.
4. Document the new pattern in the contribution guide and conduct internal workshops to align the team.

This decision reinforces Primer React’s goals of consistency, maintainability, and alignment with modern React and CSS-in-JS best practices.","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision
We will continue to use the `drafts` directory as a staging area for both experimental components and new versions of existing components within `@primer/react`. This approach provides clear benefits:

- **Isolation**: Components in `drafts` are explicitly separated from stable APIs, signaling that they are subject to change and not yet suitable for unmonitored production use.
- **Parallel Development**: Multiple versions of a component (e.g., v1 and v2) can coexist, enabling gradual migration and side-by-side testing.
- **Collaborative Feedback**: Requiring imports from `/drafts` creates a natural gating mechanism, encouraging teams to engage with the design system maintainers before adopting unstable features.

However, to ensure clarity and prevent misuse, we establish the following guidelines:
1. No component should be released directly into the root of the package without first being evaluated in `drafts`, unless it is a trivial utility or clearly non-breaking addition.
2. All components in `drafts` must be accompanied by documentation indicating their experimental status, intended roadmap, and instructions for providing feedback.
3. A process must be defined for graduating components from `drafts` to the main export, including required test coverage, design review, accessibility audit, and API stability assessment.
4. Deprecated or abandoned drafts should be clearly marked and removed on a regular cadence to avoid accumulation of technical debt.

The `drafts` mechanism will remain an official part of our release strategy, serving as a controlled channel for innovation while preserving the stability and integrity of the core library.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision  
We will adopt snapshot testing judiciously, with clear guidelines to avoid its pitfalls while leveraging its strengths.

Snapshot tests are appropriate for:
- **Component rendering output**, particularly in UI frameworks (e.g. React), where verifying the structural integrity of markup is valuable.
- **Serializations and transformations** that produce complex but stable data structures (e.g. ASTs, JSON:API output, GraphQL resolvers).
- **Configuration or code generation outputs**, where the exact form matters and changes should be explicit and intentional.

However, we will enforce the following constraints:

1. **Snapshots must be meaningful and focused.** Avoid snapshotting large, deeply nested trees unless absolutely necessary. Prefer extracting and testing smaller subcomponents or specific fields of interest.
2. **Every snapshot test must include a descriptive name** that clarifies intent (e.g., `""renders login button when user is unauthenticated""` rather than `""works""`).
3. **Critical behavior must not rely solely on snapshots.** Unit tests must accompany snapshot tests to verify functional correctness (e.g., event handlers fire, navigation occurs, etc). Snapshots are *not* a substitute for behavioral assertions.
4. **Snapshots must be version-controlled and reviewed.** Any change to a snapshot must be an explicit, human-reviewed decision during code review. CI will reject unapproved snapshot updates.
5. **Avoid snapshotting dynamic or non-deterministic values** (e.g. timestamps, IDs, functions). These must be mocked or stripped before snapshotting.
6. **Large or opaque snapshots must be accompanied by inline comments** explaining their purpose and what aspects are significant vs incidental.

Additionally:
- We will use tools (like `pretty-format` plugins or custom serializers) to sanitize and reduce noise in snapshots (e.g., omitting function references like `[Function]` when possible).
- We will periodically audit and prune outdated or redundant snapshots as part of technical hygiene.

This approach ensures that snapshot tests remain a useful tool for detecting unintended changes, while preserving clarity, maintainability, and developer confidence.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision  
We will use a combination of compile-time dead code elimination and environment-specific conditional logic to ensure that development-only warnings are stripped from production builds.

Specifically:

1. All developer-facing warnings (e.g., deprecation notices, misuse alerts) must be wrapped in a utility function or macro that conditionally invokes `console.warn()` **only** when `process.env.NODE_ENV !== 'production'`.

2. We will create and standardize on a shared utility, e.g., `devWarning(message)`, which is implemented as:
   ```ts
   export function devWarning(message: string) {
     if (process.env.NODE_ENV !== 'production') {
       console.warn(message);
     }
   }
   ```
   This pattern ensures that the call is gated behind an environment check.

3. We will configure our build toolchain (e.g., Webpack, Vite, Rollup) to replace `process.env.NODE_ENV` with a literal string during bundling. This enables tree-shaking tools like Terser to eliminate the entire `if` block and any associated messages in production builds via dead code elimination.

4. Any additional runtime checks or message formatting used solely for development warnings must also be contained within the conditional block or functions only called within such blocks, ensuring they too are removed in production.

5. Documentation and linting rules will be updated to require the use of `devWarning()` (or equivalent) instead of direct `console.warn()` calls for all non-user-facing, development-only diagnostics.

This approach guarantees that:
- Warnings are visible during development and testing.
- No warning logic or strings are included in production bundles.
- Bundle size is not impacted by development-only code.
- The pattern is consistent, auditable, and enforceable.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

There are several components in Primer React that make use of live regions.
However, the library does not have a central pattern for making accessible live
region announcements. This ADR documents the decision to use a central pattern
for live region announcements that can be used across Primer and GitHub.

ARIA Live regions fall into several buckets that we may want to use as component
authors:

- [ARIA live region roles](https://www.w3.org/TR/wai-aria-1.2/#live_region_roles), such as `alert`, `log`, or `status`
- [The `aria-live` attribute](https://www.w3.org/TR/wai-aria-1.2/#aria-live) to turn an element into a live region

In components, we see the following scenarios in Primer React:

- Announce the contents of an element when it is rendered or on page load, like when a spinner is displayed or a form is submitted
- Announce the changes to the content of an element, like when the count inside of a
  button is incremented
- Announce a message programmatically, such as the number of results for a query

Currently, contributors may reach for roles such as `alert` or `status` to
achieve these scenarios. They may also add `aria-live=""assertive""` or `aria-live=""polite""` with an an `aria-atomic=""true""` to an element explicitly.
However, both of these approaches do not announce consistently across screen
readers. This could be due to live regions being injected dynamically into the document (this includes loading content into the document via React), dynamically changing the visibility of a live region, or some other technique causing an announcement to not be announced.

For more information about the ways in which live regions may not work as
expected, visit: [Why are my live regions not working?](https://tetralogical.com/blog/2024/05/01/why-are-my-live-regions-not-working/)","## Decision  
We will implement a centralized, reusable `LiveRegion` component in Primer React to standardize accessible live region announcements across the library and GitHub. This component will encapsulate best practices for reliable screen reader announcements and address inconsistencies observed with ad hoc implementations.

The `LiveRegion` component will:

1. **Be mounted persistently in the DOM** — To ensure consistent announcement behavior, the live region element will be rendered early (e.g., at application root or via a portal) and remain in the document for the lifetime of the app. Dynamic insertion/removal of live regions will be avoided.

2. **Support multiple announcement types** — The component will support distinct modes corresponding to ARIA live region roles and politeness levels:
   - `assertive` (equivalent to `role=""alert""`)
   - `polite` (equivalent to `aria-live=""polite""` with `role=""status""`)

3. **Ensure atomic updates** — It will set `aria-atomic=""true""` by default so that the entire content is announced, even if only part has changed.

4. **Provide a programmatic API** — Expose a function or hook (e.g., `useLiveRegion()`) that allows developers to trigger announcements imperatively, such as announcing search result counts or form submission status.

5. **Avoid content flicker or premature announcements** — Delay rendering of content into the live region until it is ready to be announced, and clear content after a short delay (if needed) to allow repeated messages to be announced.

6. **Be composable and extensible** — Allow wrapper components (e.g., `<ScreenReaderAnnouncement />`) to be built on top for common use cases while ensuring all announcements route through the same underlying mechanism.

This pattern ensures reliability across screen readers and prevents contributors from using fragile, inconsistent patterns like dynamically injecting `aria-live` elements or toggling visibility of live content. All future components requiring live region functionality must use this central pattern. Documentation and lint rules will guide contributors toward correct usage.","## Decision

In order to have a common interop point for live region announcements, Primer React will
make use of a `live-region` custom element from `@primer/live-region-element`.
This package will be included and published from the `primer/react` repo.

The custom element exposes a way to make announcements that can be used
across frameworks. This makes it useful not only for Primer but GitHub as a
whole. The `@primer/live-region-element` exports two helpers to use for making
announcements: `announce()` and `announceFromElement`. Both helpers can be used
when working in Primer and by teams at GitHub.

In addition, `@primer/react` will leverage and export the following helpers for
use within Primer React and GitHub:

- The `AriaStatus` component to correspond with `role=""status""`
- The `AriaAlert` component to correspond with `role=""alert""`

Within `@primer/react`, we should lint against usage of `aria-live` and the
corresponding roles (if possible) and suggest using these alternatives instead.

> [!NOTE]
> Both `AriaStatus` and `AriaAlert` will trigger an announcement when the component is
> rendered. As a result, they should only be used for dynamically rendered
> content. Otherwise, they will trigger announcements on page load. In cases
> where they should always be present, then the first message passed to the
> component should be an empty string. Changes to the content of the component
> will trigger subsequent announcements."
## Context,"Please provide the context for which you'd like me to generate an Architecture Decision Record (ADR). The section header ""## Context"" was included without any actual content. Once you supply the specific goals, constraints, or considerations, I’ll produce a properly structured ADR decision following the established format and Arachne’s principles.","## Decision

1. npm package: ""Experimental"" components, as defined by the component lifecycle, should not be part of semantically versioned npm package for `@primer/react`.

2. npm package: Each experimental component should be independently published and versioned with semver. Breaking changes should be tagged as major versions. Example: `@github/experimental-react-commentbox`

3. The code for experimental components should live in a new repository, example: [github/react-experimental](https://github.com/github/react-experimental) and published as individual packages as suggested in point 2.

This will help in sharing experimental components between the monolith and projects outside of monolith. The ownership and responsibility of maintenance will be shared by multiple teams (including primer)."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision  
We will adopt **Playwright** as our primary testing framework for integration, end-to-end, and complex interactive scenarios, replacing the current ad hoc use of Jest for these purposes. We will continue to use Jest only for true unit tests (i.e., pure functions, utilities, non-DOM logic), and Chromatic for automated visual regression testing.

This decision addresses the core issues in the context:

1. **Eliminate duplication between Storybook and Jest**: Instead of manually recreating interaction scenarios from Storybook in Jest, we will write Playwright tests that target existing Storybook stories directly. Playwright can load any Storybook story in a real browser, interact with it programmatically, and assert on behavior and state. This allows us to treat Storybook not just as a documentation tool, but as a *test fixture library*.

2. **Improve debuggability and visibility**: Playwright provides powerful debugging tools — including live browser inspection, step-through execution, video recording, screenshots on failure, and detailed trace logs. This makes failing tests far easier to understand and fix, especially for contributors who are less familiar with CLI-only debugging. These capabilities dramatically increase test transparency and accessibility.

The new workflow will be:
- Developers create or update a Storybook story to represent a UI state or interaction flow.
- For complex interactions or integration logic, they write a Playwright test that loads that specific story (via its URL) and drives the scenario.
- CI runs Playwright in headless mode with automatic capture of videos and traces on failure.
- Chromatic continues to run in parallel for visual regressions, now focused purely on appearance rather than interaction logic.

Additionally, we will deprecate the use of Jest for DOM interaction or component-level testing. Any such tests will be migrated to Playwright over time.

This shift consolidates our testing surface, reduces duplication, leverages existing investment in Storybook, and creates a more sustainable, observable, and developer-friendly testing experience.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision  
We will adopt a monorepo management tool to unify dependency management, streamline workspace coordination, and provide a standardized structure for scripts across the project. Specifically, we will use **pnpm** with its built-in workspace support as the primary package manager and orchestration tool.

### Rationale  
The current workflow requires manual, repetitive steps to set up workspaces and does not scale well with inter-package dependencies or growing script complexity. Relative paths, file dependencies, and `npm link` are error-prone, platform-specific, and obscure dependency relationships. Additionally, scripts living in `script/` lack dependency isolation and versioning, making them brittle as they grow.

Using **pnpm workspaces** provides:
- **Unified `node_modules` management**: A single, flat `node_modules` (via symlinking) that respects `package.json` specs and avoids duplication.
- **Cross-workspace dependency resolution**: Workspaces can depend on each other using their published package names (e.g., `@primer/react`) during development, resolved via symbolic links—no need for `npm link` or relative paths.
- **Top-level installation and bootstrapping**: Running `pnpm install` at the root installs dependencies for all workspaces and links local packages automatically.
- **Script organization and reuse**: Scripts that require external dependencies can be defined as standalone packages under `tools/*` or `scripts/*`, each with its own `package.json`, enabling isolated dependency management and version pinning.
- **Performance and disk efficiency**: pnpm’s hard-linking strategy reduces disk usage and speeds up installations.

### Implementation Plan  
1. Introduce a `pnpm-workspace.yaml` at the root to define workspace packages:
   ```yaml
   packages:
     - 'packages/*'
     - 'examples/*'
     - 'docs'
     - 'scripts/**'  # For complex scripts needing deps
     - 'tools/**'
   ```
2. Replace `npm install` calls in individual directories with a top-level `pnpm install`.
3. Update the `setup` script in the root `package.json` to run `pnpm install`.
4. Convert high-complexity scripts under `script/` into dedicated workspace packages under `tools/` if they require non-trivial dependencies or testing.
5. Express inter-dependencies (e.g., `docs` → `@primer/react`) using standard semver syntax; pnpm will resolve to the local package during development.
6. Document the new setup process and update contribution guidelines.

### Outcome  
This decision enables:
- Consistent, reproducible project setup.
- Clear, declarative dependency graphs across workspaces.
- Scalable script management with proper dependency scoping.
- Reduced contributor friction and fewer environment-specific issues.

Future architectural decisions around CI/CD, publishing, and change detection (e.g., via `changesets`) will build upon this foundation.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision
We will restructure the `src` directory to clearly separate public and private modules. A new `src/internal/` directory will be created to house all implementation details not intended for public consumption. Only files and directories under `src/`—excluding `src/internal/`—will be included in the package's public API via the `'./lib-esm/*'` export condition.

All existing internal code currently exposed unintentionally will be moved into `src/internal/`. Any re-exports of internal modules in public entrypoints must be removed unless explicitly approved as part of a stable public API.

This change ensures that:
- The public API surface is intentional and well-defined.
- Internal implementation details can evolve without fear of breaking consumers.
- Consumers are guided toward supported usage patterns and away from unstable or private APIs.

Additionally, we will update documentation and contribution guidelines to clarify the distinction between public and internal modules, and establish a review process for any future additions to the public API.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

Reasons for change: Performance, utilise css variables

The main reason for changing our styling architecture is to remove performance issues for applications:

Relevant parts from [primer/discussions#1928](https://github.com/github/primer/discussions/1928#reasons)(Hubbers only link):

1. Initial page loads take longer

   Because styles are injected during runtime, we can not parallelize this work. The page is un-styled till the javascript bundle is
   downloaded and processed. The processing time is longer on slower devices.

   Benchmarks:

   i. Lab benchmarks: rendering a page with 1000 components
   runtime injection of css: 242ms
   static .css files: 96ms (60% faster)

   ii. Application benchmarks: render times for repo directory (>1000 files, [gh/gh#241090](https://github.com/github/github/pull/241090))
   with primer/react Box: 3800ms
   with custom css: 3076ms (20% faster)

2. Slow server side rendering

   Some applications are able to make up for slower initial render times by collecting styles on the server first. In the correct stack, collecting styles requires rendering the app twice before we can ship any html to the user.

   benchmarks: in the issues-app experiment, collecting styles on the server contributed to + 20% of rendering time (450ms on top of 1900ms)

3. Style updates are expensive

   The slowest part of the current stack is updating styles based on props or state. Big changes take linearly longer, worse on slower devices.

   Benchmarks:

   i. Lab benchmarks: updating a page with 1000 components, static runtime css vs. dynamic runtime css (thanks @mattcosta7 for these!)

   re-rendering components with fixed styles on runtime: 242ms
   re-rendering components with dynamic styles on runtime: 441ms (55% slower)

   ii. Application benchmarks: opting out of IconButton for PR diff view (>500 files, [gh/gh#243269](https://github.com/github/github/pull/243269))

   dynamic styles with sx prop: 400ms
   dynamic styles with .css files: 165ms (60% faster)

The other reasons for change are to utilise css variables for theming and improve the guardrails of consuming primer/primitives","## Decision  
We will transition from runtime CSS-in-JS style injection to a **static extraction model** using **CSS variables for dynamic styling**, aligning with performance best practices and enabling robust theming.

### Rationale
The current runtime-based styling approach introduces significant performance bottlenecks:
- **Slower initial render**: Critical styles are not available until JavaScript executes, delaying visual completeness.
- **Increased SSR cost**: Double rendering is required to extract styles server-side, increasing time-to-first-byte.
- **Expensive updates**: Dynamic style recalculations on the client lead to poor runtime performance, especially on low-end devices.

By shifting to static `.css` file generation with CSS variables, we achieve:
- **Faster page loads**: Stylesheets can be loaded in parallel with JavaScript, enabling early browser rendering.
- **Efficient SSR**: Eliminates the need for style collection passes; HTML and CSS can be streamed independently.
- **Improved update performance**: Style changes via CSS variables are handled natively by the browser’s CSS engine, avoiding JavaScript overhead.

### New Architecture

1. **Static Extraction**
   - All base component styles will be precompiled into static `.css` files during build time.
   - These files will be included as standard `<link rel=""stylesheet"">` assets in the document head.

2. **Dynamic Styling via CSS Variables**
   - Prop-driven or state-dependent styles (e.g., color variants, spacing overrides) will use `var(--css-variable)` expressions within static rules.
   - Theme values (e.g., colors, radii, shadows) will be defined as CSS custom properties at the root level (or scoped containers), enabling runtime theming without re-rendering.

3. **Theme Support**
   - Multiple themes (e.g. light/dark) will be supported through dynamic assignment of CSS variable values on a container element (e.g., `html`, `body`, or app root).
   - Theme switching becomes a simple DOM operation—changing class names or updating variables—without requiring React re-renders or style re-injection.

4. **Guardrails & Developer Experience**
   - The `sx` prop will remain but will map strictly to an allowed set of design tokens via CSS variables.
   - Invalid or arbitrary values will be rejected at build time or normalized at runtime to ensure consistency with Primer's design system.

5. **Migration Path**
   - A codemod will assist in converting existing runtime-styled components to the new pattern.
   - Hybrid support (mixed old/new) will be maintained temporarily via a compatibility layer that polyfills missing variables.

### Benefits
- ✅ 60% faster initial render (based on lab benchmarks)
- ✅ 55% reduction in style update cost
- ✅ Elimination of double-rendering in SSR
- ✅ First-class theming via declarative CSS variables
- ✅ Stronger design constraints and token enforcement
- ✅ Better long-term maintainability and debuggability

This decision aligns with modern web performance principles and positions Arachne applications for scalability, accessibility, and responsiveness across device capabilities.","## Decisions

1. Manually refactor styles to css files + css modules

   Use css variables from primer/primitives for inexpensive theming support, especially with server side rendering

2. Use css modules with hash

   By importing styles in the component, we can create an explicit dependency between the two. This enables applications using primer to have control over bundling and delivery.

   To begin with, we will ship a single css bundle that can be imported in the application. (example: `import '@primer/react/styles.css'`). When needed, we have the option to delegate bundling of css files to the application (they can be split by page or bundled as a common chunk for all react pages to share)

3. Keep styled-components underneath for supporting sx prop.

   Keep backward compatibility, we might change this in the future

Code example:

```jsx
import clsx from 'clsx'
import classNames from './ActionList.module.css'

const Button = ({className, sx, ...props}}) => {
  return (
    <Box
      as=""button""
      sx={sx}
      // it's important to pass both internal and props.className to the element
      className={clsx(classNames.ActionList, className)}
      {...props}
    >
    </Box>
  )
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision  
We will no longer use Storybook's `play` functions as the primary mechanism for authoring interaction-based tests.

Instead, we will adopt **standalone Playwright test files** co-located with components as the standard approach for writing and running user interaction tests.

This decision is based on the following rationale:

1. **Separation of concerns**: Storybook `play` functions conflate component documentation with automated testing logic. This leads to bloated stories and makes it harder to maintain complex test scenarios.
2. **Tooling limitations**: `play` functions run in the browser context of Storybook’s UI, which introduces constraints around debugging, parallel execution, and integration with CI/CD pipelines.
3. **Limited test capabilities**: The `play` function API lacks advanced features available in native Playwright, such as multi-page scenarios, precise network interception, video capture, and trace analysis.
4. **Discoverability and consistency**: Colocating `.spec.ts` or `.test.ts` files with component code improves developer discoverability and aligns with established patterns across the ecosystem.
5. **Scalability**: Standalone Playwright tests support modular setup, reusable fixtures, parameterized testing, and better organization at scale.

### Migration Plan
- Deprecate all new usage of `play` functions for testing purposes.
- Gradually migrate existing interaction tests from `play` functions to standalone Playwright specs.
- Update documentation, templates, and linting rules to reflect the new pattern.
- Retain use of `play` functions only where strictly necessary for interactive documentation within Storybook.

This change preserves the value of Storybook as a development and design tool while strengthening our test suite with more robust, maintainable, and debuggable interaction tests.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision 
We will standardize the use of a `useResponsiveValue` hook with built-in server-safe fallback behavior to prevent layout shifts during server-side rendering.

The hook will accept a responsive value (an object mapping breakpoints to values) or a static value, and return the appropriate value based on the current viewport. To avoid hydration mismatches:

1. **Server-side rendering (SSR)**: The hook will return a consistent, deterministic fallback value on the server—this fallback will be the value associated with the `regular` breakpoint by default.
2. **Client-side hydration**: The hook will immediately initialize with the same `regular` breakpoint value during first render (to match SSR output), then asynchronously subscribe to `matchMedia` listeners to detect the actual viewport size.
3. **Post-hydration updates**: After hydration, if the actual viewport does not match the `regular` breakpoint, the component will re-render with the correct responsive value. Any resulting visual change will occur *after* hydration and therefore will not cause a layout shift in the traditional sense (i.e., no DOM structure changes or content reflow that disrupts user interaction).

To further mitigate perceived instability:
- We will document that responsive props should only affect non-critical aspects of layout (e.g., spacing, font size within safe ranges) and must not alter element presence, order, or fundamental structure.
- We will provide a TypeScript utility type `Responsive<T>` defined as `T | { narrow?: T; regular?: T; wide?: T }`, ensuring type safety across components.
- We will recommend (and lint for) explicit specification of at least the `regular` key in responsive objects to ensure predictability.

This approach provides a consistent, developer-friendly abstraction for responsive design while eliminating SSR-induced layout shifts through deterministic server/client initial rendering.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision
We will support deprecated props by providing clear, actionable deprecation warnings in development mode, while maintaining backward compatibility during a defined migration period.

When a prop is deprecated in favor of a new API:
1. The component will detect use of the deprecated prop and log a console warning in development builds only. The warning will:
   - Clearly state that the prop is deprecated
   - Specify the version in which deprecation occurred
   - Point to the replacement API
   - Include a link to updated documentation or migration guide
2. Both the old and new APIs will coexist for at least one minor version cycle to allow consumers time to migrate
3. The deprecated prop will continue to function with the same behavior during the migration period
4. Documentation and TypeScript types will be updated to mark the prop as deprecated and guide users toward the preferred pattern
5. After sufficient adoption of the new API, the deprecated prop will be removed in a major version release

In cases like the `ActionList.Group` example above, where structural changes are involved:
- We will design migration paths that preserve accessibility improvements
- We will ensure the new API provides equal or greater flexibility
- We may provide codemods to assist with automated migration when feasible

This approach balances forward progress with practical concerns for downstream consumers, enabling us to improve our APIs while minimizing disruption.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

- Proxy support is required for some enterprises and organizations to start using their own self-hosted runners
- While there is not a standard convention, many applications support setting proxies via the environment variables `http_proxy`, `https_proxy`, `no_proxy`, such as curl, wget, perl, python, docker, git, and R
  - Some of these applications use `HTTPS_PROXY` versus `https_proxy`, but most understand or primarily support the lowercase variant","## Decision 
We will support proxy configuration through the environment variables `http_proxy`, `https_proxy`, and `no_proxy` in lowercase, aligning with the de facto standard used by widely adopted tools such as curl, wget, git, and Docker. This approach ensures familiarity for users and operators in enterprise environments where proxy settings are commonly managed via these variables.

The application will read and interpret these environment variables at startup to configure outbound HTTP and HTTPS traffic accordingly:
- `http_proxy`: Specifies the proxy server for HTTP traffic.
- `https_proxy`: Specifies the proxy server for HTTPS traffic.
- `no_proxy`: Comma-separated list of hosts that should bypass the proxy.

We will prioritize lowercase variable names over uppercase (e.g., `https_proxy` over `HTTPS_PROXY`) to maintain consistency with the majority of tools and reduce ambiguity, though implementations may optionally support uppercase variants if needed without overriding the lowercase when both are present.

This decision avoids introducing custom configuration mechanisms unless necessary, reduces onboarding friction, and increases compatibility with existing infrastructure automation and tooling. Support for these variables will be documented clearly in configuration documentation.","## Decision

We will update the Runner to use the conventional environment variables for proxies: `http_proxy`, `https_proxy`, and `no_proxy` if they are set.
These are described in detail below:
- `https_proxy` a proxy URL for all https traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `http_proxy` a proxy URL for all http traffic. It may contain basic authentication credentials. For example:
  - http://proxy.com
  - http://127.0.0.1:8080
  - http://user:password@proxy.com
- `no_proxy` a comma-separated list of hosts that should not use the proxy. An optional port may be specified. For example:
  - `google.com`
  - `yahoo.com:443`
  - `google.com,bing.com`

We won't use `http_proxy` for https traffic when `https_proxy` is not set, this behavior lines up with any libcurl based tools (curl, git) and wget.
Otherwise, action authors and workflow users need to adjust to differences between the runner proxy convention, and tools used by their actions and scripts.  

Example:  
  Customer sets `http_proxy=http://127.0.0.1:8888` and configures the runner against `https://github.com/owner/repo`, with the `https_proxy` -> `http_proxy` fallback, the runner will connect to the server without any problem. However, if a user runs `git push` to `https://github.com/owner/repo`, `git` won't use the proxy since it requires `https_proxy` to be set for any https traffic.

> `golang`, `node.js`, and other dev tools from the Linux community use `http_proxy` for both http and https traffic based on my research.

A majority of our users are using Linux where these variables are commonly required to be set by various programs. By reading these values, we simplify the process for self-hosted runners to set up a proxy and expose it in a way users are already familiar with.

A password provided for a proxy will be masked in the logs.

We will support the lowercase and uppercase variants, with lowercase taking priority if both are set."
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision  
We will disable command echoing by default in user logs to reduce clutter and improve log readability. Command output will only be echoed when:

- The command fails or produces an error, in which case the output is automatically included to aid troubleshooting.
- Explicitly enabled by the user via the `::echo::on` and `::echo::off` control commands, allowing fine-grained control over when output is displayed.

Additionally, echoing will be automatically enabled if the `ACTIONS_STEP_DEBUG` secret is set, providing a convenient mechanism for debugging across all steps without requiring changes to individual commands.

This approach balances clean default output with full transparency when needed, supporting both usability and debuggability while keeping control in the hands of the user.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision  
We will implement a structured logging extension mechanism in the runner's `Set up job` step to include additional host environment metadata in the logs. This mechanism will:

1. **Capture standardized environment information** from hosted runners, including:
   - Operating System (name, version, architecture)
   - Pre-installed software and tooling (with versions where applicable)
   - Image identifier/timestamp
   - Any relevant virtualization or container context

2. **Format log output consistently** using a tagged, human-readable format that is also machine-parsable (e.g., key-value pairs or JSON snippets), ensuring compatibility with existing log processing systems.

3. **Make the feature extensible**, allowing future additions of new metadata types without requiring changes to core logging infrastructure.

4. **Ensure transparency and debuggability**, enabling users to understand exactly which environment their job is running on, improving reproducibility and issue diagnosis.

5. **Keep the implementation lightweight and non-invasive**, avoiding runtime overhead or tight coupling between the runner and environment detection logic.

This approach supports operational visibility while maintaining system simplicity and scalability. The environment data will be logged early in the job lifecycle, before any user scripts execute.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision  
We will add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context in Arachne.

- `steps.<id>.outcome` will reflect the final disposition of a step, such as `""success""`, `""failure""`, `""skipped""`, or `""neutral""`. This field is set by the runtime upon completion of the step and is intended to be authoritative.
- `steps.<id>.conclusion` will represent a derived or interpreted result, potentially influenced by conditional logic, status checks, or heuristic evaluation (e.g., treating certain failures as acceptable under policy). This allows higher-level workflows to make nuanced decisions based on intent rather than raw success/failure.

These additions enable conditional execution in downstream steps using expressions like:
```clojure
(when (= (get-in ctx [:steps :previous-step :outcome]) ""success"") ...)
```

This change supports Arachne’s goals of **transparency**, **introspectability**, and **configurability** by making control flow dependencies explicit and inspectable within the data model. It also aligns with the “everything is data” principle—outcomes and conclusions are first-class values in the configuration context, not hidden side effects or exceptions.

Backward compatibility is preserved: existing workflows relying only on `steps.<id>.outputs` remain unaffected.

Documentation and schema definitions will be updated to reflect the new fields, and the schema will enforce valid outcome/conclusion values via enumeration constraints.

Future work may include adding utilities for expressing step dependencies declaratively (e.g., `needs: [""step-a"", ""step-b""]`) which would internally use these fields.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision
We will extend the self-hosted runner registration process to allow custom labels to be specified via command-line arguments during registration. This change will enable automation scripts to dynamically assign meaningful, environment-specific labels (e.g., `gpu`, `high-memory`, `testing`) at setup time, improving flexibility and integration with deployment workflows.

The implementation will:
- Accept a new optional `--labels` argument that takes a comma-separated list of label names.
- Validate label format (alphanumeric, hyphens, underscores; no spaces or special characters) during registration.
- Combine user-provided labels with built-in labels (`os`, `arch`) to form the complete initial label set.
- Store the full set of labels in the runner’s configuration file for consistency and reuse.

This approach maintains backward compatibility while supporting automated, configurable labeling. It avoids runtime mutations of labels post-registration, keeping label management predictable and auditable.

A future ADR may explore dynamic label updates after registration, but this is out of scope for now. Reference: [ADR275](https://github.com/actions/runner/pull/275).","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision  
We will implement a resilient authentication gateway pattern for handling third-party provider communication in SS??. This approach encapsulates all interactions with external identity providers (e.g., Google via OAuth2) behind a fault-tolerant interface that anticipates and gracefully handles transient failures.

The solution will include the following elements:

1. **Circuit Breaker Pattern**: All outbound authentication requests to third-party providers will be wrapped in a circuit breaker (e.g., using a library such as `clj-http-fuse` or similar). The circuit breaker will track failures and automatically open the circuit upon reaching a defined threshold, preventing cascading failures and giving the provider time to recover.

2. **Retry with Exponential Backoff**: Transient failures (e.g., 5xx errors, timeouts) will trigger retries using an exponential backoff strategy with jitter. The number of retries and backoff parameters will be configurable per provider.

3. **Provider Response Caching (where applicable)**: For non-sensitive, idempotent operations (e.g., fetching public keys for JWT verification), we will employ short-lived, in-memory caching to reduce dependency on upstream availability.

4. **Graceful Degradation**: When the circuit is open or repeated retries fail, the system will fall back to safe behaviors—such as allowing authenticated sessions to continue but disallowing new logins—and return appropriate user-facing messages indicating temporary login issues.

5. **Observability**: All faults, retries, and circuit state changes will be logged and instrumented with metrics (e.g., counters, histograms) to enable monitoring, alerting, and post-incident analysis.

6. **Configuration Overridability**: The thresholds, timeouts, retry counts, and other resilience parameters will be externally configurable, enabling operational tuning without code changes.

This decision supports our goals of reliability and transparency by ensuring that transient provider issues do not cascade into system-wide outages, while maintaining clear visibility into the state of external dependencies. It also preserves modularity by encapsulating provider interaction logic and failure handling within a well-defined boundary, making it easier to support additional providers in the future.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision  
We will use Architecture Decision Records (ADRs) to document significant architectural decisions made during the project lifecycle. Each ADR will capture the rationale, context, and consequences of a decision at the time it was made, ensuring transparency and traceability.

Every ADR will include:
- A unique identifier and title
- The date the decision was proposed
- The status of the decision (e.g., proposed, accepted, rejected, deprecated)
- The context that necessitated the decision
- A description of the decision made
- Considered alternatives and their trade-offs
- Potential impacts and consequences
- References to related ADRs or discussions

ADRs will be stored in version control alongside the codebase, making them easily accessible and auditable. New team members and external contributors can read through past ADRs to understand the evolution of the system and the reasoning behind key choices.

This practice supports onboarding, enables meaningful contributions from outside the core team, and invites feedback and critique, leading to higher-quality, well-considered decisions over time. While implementation details may change, the record of why a decision was made remains valuable and does not become outdated.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂÃÂ¡ÃÂÃÂª with all relevant discussion summarized ÃÂÃÂ¡ÃÂÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision  
We will use a standardized URI scheme to locate and identify Arachne extension definition files stored in decentralized storage.

The URI scheme `arachne-extension://` will be used to uniquely identify and resolve extension definitions. This scheme enables consistent, portable, and unambiguous referencing of extensions regardless of their physical location in the decentralized storage network.

The structure of the URI will be:

```
arachne-extension://<namespace>/<name>/<version>
```

Where:
- `namespace` is a logical grouping (e.g., a project, organization, or domain),
- `name` is the human-readable name of the extension,
- `version` is a semantic version string (e.g., `1.0.0`).

Resolution of the URI will be handled by a pluggable resolver system that maps the logical URI to a concrete retrieval operation in the appropriate decentralized storage backend (e.g., IPFS, Filecoin, or other content-addressed stores). The resolver will retrieve the extension's definition file, which must be an EDN document conforming to the Arachne extension schema.

This approach supports modularity and decentralization by:
- Decoupling logical identity from physical location,
- Enabling transparent verification and caching via content addressing,
- Supporting reproducible builds through version pinning,
- Allowing multiple implementations or sources for the same extension namespace.

Extension definitions retrieved in this way will be treated as immutable data and integrated into the application configuration model as first-class entities, preserving the ""everything is data"" principle.","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision  
Group chat functionality is out of scope for the Q2 beta release.  

We will focus exclusively on delivering a stable, secure, and high-quality 1-1 and public chat experience as the core of the MVP. This decision is based on:  

- **Time constraints**: Delivering group chat would require significant additional effort, diverting resources from critical path items needed for beta.  
- **Complexity and risk**: Group chats introduce greater cryptographic complexity (e.g. multi-party encryption, membership management), increasing the likelihood of security regressions.  
- **MVP value**: Public chat better serves the goal of enabling open interaction with strangers, aligning more closely with early user acquisition and engagement goals.  
- **Recurring scope ambiguity**: Explicitly removing group chat from scope eliminates ongoing debate and prevents wasted effort on non-critical features.  

Any work on group chat will be deferred to a future milestone. Outstanding group chat issues may be deprioritized or closed if they block progress on beta deliverables.  

If and when group chat is revisited, a new Architecture Decision Record will be created to reassess the context, requirements, and implementation approach.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision  
We will implement dynamic peer discovery using the Discovery v5 protocol to replace the current hardcoded peer configuration in the application.

This decision aligns with our architectural goals of modularity, transparency, and configurability by:
- Decoupling the application from fixed infrastructure assumptions
- Enabling transparent visibility into discovered peers and network topology
- Supporting runtime adaptability without requiring code changes or redeployment

The implementation will:
1. Integrate the Discovery v5 protocol as a pluggable networking module, allowing it to be enabled or configured per environment.
2. Define entity types in the Arachne configuration schema for:
   - Discovery service configuration (bootnodes, topic interests, etc)
   - Discovered peer metadata
   - Policies governing peer selection and connection limits
3. Ensure all discovery-related state is managed outside the Clojure runtime vars, maintaining separation between application configuration and ephemeral runtime state.
4. Provide introspection endpoints to observe current peer set, discovery activity, and routing table contents, supporting the goal of transparency.

Hardcoded peers will be deprecated but retained as an optional fallback mode for development and testing environments. Production deployments will require Discovery v5 to be enabled.

A new Architecture Decision Record will be created if future scaling requirements necessitate additional discovery mechanisms or hybrid approaches.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision  
We will adopt the standardized key derivation path `m/44'/60'/0'/0/0` for Ethereum-based wallet accounts generated from a 12-word BIP39 mnemonic phrase.

This decision aligns with the [Idea 142 - Wallet Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) and ensures interoperability with other Ethereum wallets (e.g., MetaMask, Ledger, Trust Wallet), allowing users to import their existing wallets into Status seamlessly.

### Rationale  
- **User Control & Portability**: Users can generate a wallet in any standards-compliant application and restore it in Status using only the 12-word recovery phrase.
- **Industry Standard**: The derivation path `m/44'/60'/0'/0/0` is widely adopted across the Ethereum ecosystem per BIP44 and EIP84 recommendations.
- **Reduced Confusion**: Eliminates discrepancies where the same seed phrase generates different addresses in different apps.
- **Security**: No additional risk is introduced, as the change is purely in key derivation—security remains dependent on the secrecy of the mnemonic.

### Implementation Notes  
- Existing accounts using legacy derivation paths will remain accessible; this change applies only to new wallet creation flows.
- Migration tools or guidance will be provided if users need to derive accounts under the new path from an existing seed.
- The ADR will be referenced in relevant modules (e.g., account management, onboarding flow) to ensure consistent implementation.

### Status  
Accepted","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision  
We will adopt **ULC (Ultra Light Client)** as the primary synchronization strategy for the Status app, with a phased transition plan to minimize trust in centralized infrastructure like Infura.

### Rationale:
- ULC offers a significantly lighter resource footprint compared to LES, addressing core performance concerns: reduced bandwidth usage, lower disk consumption, faster initial sync, and improved energy efficiency—directly resolving issues [#55](https://github.com/status-im/ideas/issues/55), [071](https://ideas.status.im/ideas/071-low-traffic), [083](https://ideas.status.im/ideas/083-energy-efficient), and contributing to [076](https://ideas.status.im/ideas/076-smooth-ui).
- Unlike Infura-dependent models, ULC supports connection to multiple upstream nodes, improving decentralization, resilience, and security through diversified trust.
- While LES could be optimized (e.g., via CHTs), it remains fundamentally heavier and more complex than ULC; investing further in LES is not sustainable long-term.
- ULC aligns with our architectural goal of client-side efficiency and offline-first capabilities.

### Transition Plan:
1. **Short Term**: Maintain Infura as a fallback while accelerating ULC development and testing on testnets. Allow users to configure custom upstream nodes to reduce reliance on single endpoints.
2. **Mid Term**: Deploy ULC as the default sync mechanism in beta releases. Support multi-node ULC configurations to enhance redundancy and trust distribution.
3. **Long Term**: Fully deprecate Infura as a default endpoint. Make local or user-provided node connections (via ULC) the standard. Continue improving ULC reliability, peer discovery, and sync robustness.

### Monitoring & Evaluation:
- Define success metrics: sync time < 30s on mobile networks, RAM usage < 50MB, disk usage < 100MB.
- Reassess this decision if ULC fails to meet performance targets within six months or if critical vulnerabilities are identified.

This decision prioritizes long-term sustainability, user sovereignty, and performance, while acknowledging current limitations and planning for incremental improvement.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision  
We will introduce a `Secret` type to encapsulate sensitive data such as passwords, API keys, and other credentials. This type will:

1. **Wrap sensitive values** (e.g., strings) in a dedicated, opaque wrapper that prevents accidental logging or inspection.
2. **Override string representation** so that when `str`, `print`, or similar functions are called on a `Secret`, it returns a redacted placeholder (e.g., `""***REDACTED***""`) instead of the actual value.
3. **Provide controlled access** to the underlying value via an explicit, auditable function (e.g., `arachne.core/secret-value`) that can only be used in intentional, secure contexts.
4. **Support static analysis and linting**: The use of the `Secret` type will make it easier for tooling to detect improper handling of sensitive data (e.g., passing a `Secret` directly into a logging function).
5. **Be immutable and thread-safe**, consistent with functional programming principles.

Developers must use the `Secret` type for any transient in-memory representation of sensitive information. Passing raw strings containing secrets—especially across module boundaries or into shared infrastructure—will be considered a violation of security policy and rejected during code review.

This approach ensures that:
- Sensitive data is not accidentally logged or exposed via debug output.
- The handling of secrets is explicit, traceable, and consistent across modules.
- We maintain transparency about where secrets are created, used, and eventually passed to external systems (e.g., network clients).

Additionally, documentation and module templates will include guidance on proper usage of the `Secret` type, and automated tests will verify that no raw secrets appear in log captures during test runs.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision
We will deprecate and remove support for JavaScript-based 3rd party status command extensions (Jail environment) in favor of a new, declarative extension system based on project Pluto.

The Jail-based extension mechanism has proven to be:
- Technically flawed due to its reliance on mutable state and poor integration with the host application’s re-frame/functional architecture.
- A source of technical debt, encouraging hard-coded, non-uniform logic across the codebase.
- Performance-intensive due to constant asynchronous communication over the React Native bridge.
- Poorly adopted, with no real-world third-party usage beyond basic examples.
- Underspecified and unmaintained, leading to broken APIs and documentation drift.

Instead, we will invest in **project Pluto**, which provides:
- A declarative, data-driven model for defining extensions.
- Deep, safe integration with the host application using ClojureScript and re-frame patterns.
- Support for multiple extension points (e.g., commands, wallet assets, chat contacts) through well-defined, testable interfaces.
- Better performance by eliminating cross-bridge execution for core logic.
- Strong tooling and testing support via the host language and runtime.

Migration plan:
1. Mark all existing Jail-based extension APIs as deprecated in documentation and code.
2. Freeze feature development on the Jail system; only critical security fixes will be applied.
3. Implement core command functionality (e.g., `/send`, `/request`) using the new Pluto extension model.
4. Provide migration guides and tools for any internal teams relying on Jail logic.
5. Remove the Jail execution environment and associated bridge code once all functionality is ported.

This decision aligns with our architectural principles of simplicity, maintainability, performance, and leveraging the strengths of our functional, data-oriented stack.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

The whitepaper describes Tribute to Talk (TtT) as an economics-based anti-spam filter for receiving messages and ""cold"" contact requests from user. Stakeholders would stake a minimum amount of SNT that another Status stakeholder must deposit in order to contact him directly.
The whitepaper also proposes that the deposit is only forfeited to the recipient if he replies.","## Decision  
We will implement Tribute to Talk (TtT) as a permissionless, economics-based anti-spam mechanism for direct messaging and cold contact requests in Status. 

The core mechanics will adhere to the following principles:

1. **Staking Requirement**: Any user wishing to initiate a direct message or contact request with another user who has enabled TtT must deposit a configurable amount of SNT into an escrow associated with that recipient.
2. **Recipient Control**: The recipient defines the minimum deposit amount they require for unsolicited contact. This setting is user-configurable, with reasonable defaults provided.
3. **Conditional Forfeiture**: The deposited SNT is only transferred to the recipient if they choose to reply to the message or accept the contact request. If the recipient does not respond within a defined period, the deposit is refunded to the sender.
4. **Non-Blocking**: Receiving a tribute does not obligate the recipient to respond. The decision to engage remains entirely at the recipient’s discretion.
5. **Transparency and Opt-In**: TtT is opt-in for recipients. When enabled, it is clearly indicated in the user interface, and senders are informed of the required deposit before attempting contact.

This design aligns with the whitepaper's vision by:
- Shifting the cost of attention extraction onto the sender,
- Empowering recipients with economic agency over their communication channels,
- Incentivizing thoughtful, relevant outreach while deterring spam at scale.

Future iterations may explore dynamic pricing models, reputation integration, or partial forfeiture mechanisms, but the initial implementation will prioritize simplicity, auditability, and alignment with Ethereum’s trust-minimized ethos.","## Decision

Considering:
- the absence of efficient ways to perform anonymous transactions (zk-snarks could be used in the future for that)
- the impossibility to prove that a recipient has made an actual reply and not some kind of automated reply (captcha solution was proposed, but wouldn't be practical until we can use a solution such as swarm feeds that allow users to make free updates to their captcha without on-chain transactions)
- the limited time to develop the feature

We opted for a solution that:
- doesn't establish a direct correlation between the tribute payment and an on-chain transaction, so it
- uses regular transactions for tributes payment, giving users plausible deniability about the nature of the payment
- doesn't store tribute information on-chain"
"## Context:

Currently, there is only a single team lead in the mobile development team, and after a meeting with the design team, it has been identified that the mobile app lacks polishness. One of the main issues identified is a lack of oversight in certain areas. To address this problem, it has been decided to create an additional role within the team. For each section of the mobile app (communities, syncing & onboarding, activity center & shell, wallet, messaging), a person will be selected to fulfill the responsibilities outlined below.","## Decision  
We will establish a ""Section Owner"" role for each major area of the mobile app (Communities, Syncing & Onboarding, Activity Center & Shell, Wallet, Messaging) to improve oversight, accountability, and quality across the application.

Each Section Owner will be responsible for:

- **Maintaining polish and consistency** within their section, including UI/UX fidelity, performance, and adherence to design specs.
- Acting as the **primary point of contact** between the mobile team and other teams (e.g. design, product, backend) for matters concerning their section.
- **Reviewing all changes** (code, design, copy) that affect their section, ensuring alignment with user needs and product goals.
- Identifying and **tracking technical debt, bugs, and polish issues** specific to their area.
- Proposing and prioritizing improvements and refinements in collaboration with product and design.

Section Owners are not necessarily managers or leads; this is a *technical leadership* and *stewardship* role focused on quality and coherence.

The Team Lead will coordinate the selection process in consultation with the team, aiming for:
- Clear ownership with no overlap or gaps.
- Distribution of ownership that accounts for expertise, bandwidth, and growth opportunities.

This change aims to distribute leadership responsibility, increase ownership, and directly address the lack of oversight contributing to the app’s current lack of polish. The effectiveness of this change will be reviewed after one sprint cycle.","## Decision

The decision is to create an additional role within the mobile development team for each section. The individuals selected for these roles will have the following responsibilities:

- Ensure GitHub issues are created for new features/designs of the section.
- Ensure issues are adequately described and linked as best as possible.
- Ensure issues have a grooming session where other developers can provide their feedback before working on the set of issues.
- Ensure all relevant parties are invited, such as designers, QA, Product Owner, leads, etc., as needed (including the designer who created the feature).
- Ensure there is consensus among the developers working on the section about the decided strategy approach. Ideally, this consensus should be documented.
- Ensure UI quality by organizing designer reviews and guaranteeing pixel perfection.
- Ensure overall implementation is satisfactory based on the predefined requirements.
- Ensure alignment between desktop, web, and mobile versions for the respective section in collaboration with the mobile developers and design team.
- Act as a point of contact for the design team with respect to the section.
- Ensure that someone goes through the designs once they are created with the respective designer.
- Assist the design team with technical feasibility analysis and address any product concerns.
- Facilitate the growth of the next team member into the role.

The people identified for each sections are as follow:

- Messaging @flexsurfer, followed by @alwx
- Syncing & Onboarding @siddarthkay
- Communities @ilmotta, followed by @ajayesivan
- Activity Center & Shell @J-Son89, followed by @Parveshdhull
- Wallet (once the vertical team is created) @J-Son89, followed by @briansztamfater

This will be a rotating post, every 4 months it will go to the next in line.

The wallet team might have longer times for rotation, as stability might be more important. That's still to be decided.

# Follow-up Actions

- Find suitable next in line for Messaging, Onboarding, Communities"
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision  
We will model Product Option values as a normalized, indexed collection within the Sylius API, leveraging API Platform's resource and relation capabilities to ensure efficient querying and serialization.

The `ProductOptionValue` entity will be treated as a first-class resource, related to `ProductOption` via a one-to-many relationship. This allows:
- Independent CRUD operations on option values when necessary.
- Efficient filtering and retrieval using API Platform’s built-in filtering, pagination, and sorting.
- Clear, RESTful endpoints (e.g., `/product_options/{id}/values`) through custom subresource operations.
- Reuse of validation and serialization groups per value.

To optimize performance:
- Database indexes will be added on foreign keys and commonly queried fields (e.g., `code`, `option_id`).
- Doctrine DQL fetch joins will be used in list operations to avoid N+1 query problems.
- Optional embedding of values in `ProductOption` responses will be supported using API Platform’s `iri hydrators` or `serialization contexts`, allowing clients to choose between linked references or inlined data.

This approach maintains data integrity, supports scalability, and aligns with JSON:API and HAL conventions where applicable. It also preserves flexibility for future features such as translations, value-level metadata, or constraints.

Configuration will remain declarative and overridable via YAML or attributes, consistent with Sylius and API Platform best practices.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision  
We will adopt a **locale-aware API design** that ensures consistent, predictable handling of translatable content across all endpoints in Sylius. The API will return only the translation corresponding to the requested locale by default, falling back to a configured fallback locale (e.g., the store's default locale) when the requested translation is not available.

To achieve this:

1. **Explicit Locale Negotiation**:  
   The API will determine the requested locale through an explicit `Accept-Language` header or a query parameter (e.g., `?locale=fr_FR`). If none is provided, it will use the system-defined default locale.

2. **Single Translation per Resource by Default**:  
   Endpoints returning resources with translatable fields (e.g., Product, PaymentMethod, ShippingCategory) will include only the translated values for the negotiated locale. This avoids over-fetching and places responsibility on the API to deliver correctly localized data.

3. **Consistent Embedding of Translations**:  
   All translatable resources must embed their current translation directly within the resource representation (i.e., denormalized into the main payload), rather than exposing translations as a separate collection. For example:
   ```json
   {
     ""id"": 1,
     ""code"": ""t-shirt"",
     ""name"": ""Red T-Shirt"",
     ""description"": ""A comfortable cotton t-shirt.""
   }
   ```
   where `name` and `description` are already resolved to the requested locale.

4. **Optional Access to All Translations (Admin-Only)**:  
   For administrative interfaces, an optional flag (e.g., `include_translations=all`) may be supported on certain endpoints to return all available translations. This should be restricted to admin contexts and not used in storefront APIs.

5. **Schema Consistency Across Endpoints**:  
   All existing and future endpoints must adhere to this pattern. Any inconsistency (e.g., some endpoints returning full translation lists) will be considered a bug and corrected.

6. **Documentation and Developer Guidance**:  
   Clear documentation will be added to explain how localization works in the API, including examples of locale negotiation, fallback behavior, and best practices for frontend developers.

This approach improves performance, simplifies frontend logic, and ensures a uniform experience across the API surface while maintaining flexibility for backend content management.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision  
We will use Architecture Decision Records (ADRs) to capture significant architectural and design decisions for the Terraform Controller project. ADRs will serve as a lightweight, discoverable, and persistent record of key decisions that are not covered by formal proposals.

Each ADR will be stored in a dedicated `docs/adr` directory using a numbered filename (e.g., `0001-use-adrs.md`) to ensure chronological clarity. The format of each ADR will follow the standard structure:

- **Title**: A short descriptive name
- **Status**: One of proposed, accepted, rejected, deprecated, superseded
- **Context**: The problem or issue being addressed, including background and constraints
- **Decision**: The chosen approach and rationale
- **Consequences**: The positive and negative outcomes, trade-offs, and follow-up actions

New decisions requiring an ADR include:
- Selection of core libraries or tools
- Major API design choices
- Changes to deployment or operational models
- Deviations from established patterns or practices

The process is:
1. When a significant decision is made (or needs to be made), create a new ADR with status ""proposed""
2. Share it with the team for feedback (via PR or discussion)
3. Update based on feedback, then mark as ""accepted"" or ""rejected""
4. Merge the ADR into main once accepted

ADRs are immutable after acceptance except for status updates (e.g., to mark as superseded). If a decision is reversed or updated, a new ADR must be created that references the original.

This practice ensures transparency, improves onboarding, prevents repeated debates, and preserves institutional knowledge across contributor changes.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision
We will use GitHub webhooks to detect pull request changes, rather than polling via the GitHub API.

This approach is more efficient and responsive, as it allows GitHub to notify our system immediately when a relevant event occurs (e.g., a new pull request, a push to an existing pull request, or a change in pull request status). This reduces latency in reacting to changes and avoids unnecessary API calls, helping us stay within rate limits and minimizing system load.

Webhooks also align with modern event-driven architectures and support Arachne’s goals of transparency and introspectability—each incoming webhook event can be logged and traced, providing clear insight into how and when the system learned about a pull request change.

We will subscribe to specific webhook events including `pull_request` (for actions like opened, closed, reopened, synchronized) and `pull_request_review`, ensuring comprehensive coverage of pull request lifecycle changes. Payloads will be validated and processed through a structured event-handling pipeline.

In cases where reliability is critical (e.g., missed webhook deliveries due to downtime), we will implement a fallback reconciliation mechanism using targeted API polling for out-of-sync pull requests, but this will not be the primary detection method.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision  
We will retain support for cross-namespace references in the tf-controller API, but with strict controls to mitigate security and operational risks.

Rationale:
- **Backward compatibility**: Removing cross-namespace references would break existing workflows, particularly those aligned with Flux’s established patterns where source definitions (e.g., `GitRepository`, `Bucket`) are intentionally separated from consuming resources by namespace.
- **Separation of concerns**: It is a legitimate operational pattern to centralize shared artifacts (like Terraform modules) in a dedicated namespace, enabling reuse and consistent governance across multiple teams or environments.

To address the associated risks, we will implement the following safeguards:

1. **Explicit opt-in via permissions**:
   - Cross-namespace references require explicit RBAC permissions on both the referencing object and the target object.
   - The controller will not attempt to resolve a reference unless it has `get` (and where applicable, `watch`) permissions on the target namespace and resource type.

2. **Allow-listing via Reference Grants**:
   - We adopt the [ReferenceGrant](https://gateway-api.sigs.k8s.io/references/reference-grants/) pattern from Gateway API.
   - A `ReferenceGrant` custom resource must exist in the target namespace, explicitly allowing the source namespace and resource to reference specific types or individual objects.
   - Example: To allow a `Terraform` object in namespace `team-a` to reference a `GitRepository` in namespace `sources`, a `ReferenceGrant` must be created in `sources` authorizing that access.

3. **Validation and visibility**:
   - All cross-namespace references are validated at admission time (via webhook) if possible, or during reconciliation with clear event logging.
   - Status conditions on referring objects will clearly indicate when a reference cannot be resolved due to permission or existence issues.

4. **Documentation and guidance**:
   - We will document the security implications and recommended practices for using cross-namespace references.
   - Encourage use of same-namespace references by default, reserving cross-namespace use for intentional, well-justified cases.

This approach preserves necessary flexibility while enforcing namespace isolation through explicit, auditable consent rather than implicit trust.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The TF-Controller currently faces challenges related to the deletion of Terraform resources.
These problems span across three categories:

1. Single object deletion,
2. Resources with dependencies deletion, and
3. Namespace deletion.

These problems must be fixed in the above order as (2) and (3) require single object deletion to be resolved first.

Deleting a single TF object can sometimes be obstructed because it's tied to other resources like Source objects, Secrets, and ConfigMaps. If we try to remove it without deleting these resources, the TF object gets stuck in an inconsistent state, making it harder for users to manage their infrastructure smoothly.
Therefore, the TF-Controller is being enhanced to address this problem more efficiently, using the contents of generated Workspace BLOBs. Each BLOB contains all necessary information from the associated Source, Secrets, and ConfigMaps to ensure that TF-Controller finalization procedures can delete objects correctly.

Currently, the TF-Controller downloads a Source BLOB and pushes it to a tf-runner. The tf-runner processes this BLOB to create a Workspace file system. It generates a backend configuration file, variable files, and other necessary files for the Workspace file system, using data from associated Secrets and ConfigMaps. This newly created Workspace file system is then compressed, sent back to the TF-Controller, and stored as a Workspace BLOB in the controller's storage.
A caching mechanism for these BLOBs is essential to fixing the single TF object deletion process.","## Decision  
We will implement a **BLOB caching mechanism** in the TF-Controller to enable reliable deletion of individual Terraform (TF) objects by preserving complete Workspace state, including dependencies on Sources, Secrets, and ConfigMaps.

The cache will store serialized Workspace BLOBs associated with each TF object, ensuring that during finalization—especially when dependent resources (e.g., Secrets, ConfigMaps) have already been deleted—the TF-Controller retains access to all necessary configuration data required for a clean `terraform destroy` operation.

### Key Design Points:
- The BLOB cache will be versioned and tied to the lifecycle of the TF object via owner references.
- Each cached BLOB contains a complete snapshot of the Workspace file system at plan/apply time, including:
  - Source code
  - Resolved secrets and config values
  - Backend and variable files
- The cache will be stored in durable, controller-managed storage (e.g., a dedicated bucket or persistent volume), not reliant on ephemeral runner environments.
- On deletion of a TF object, the TF-Controller will:
  1. Retrieve the latest Workspace BLOB from the cache
  2. Dispatch it to a tf-runner
  3. Execute `terraform destroy` using the exact environment in which the infrastructure was created

This ensures deterministic teardown even if original Secrets, ConfigMaps, or Source objects are no longer available.

### Rationale:
Without cached BLOBs, deletion fails when referenced Kubernetes resources are missing, leaving infrastructure dangling. By decoupling deletion-time requirements from live K8s dependencies, we ensure consistent finalization outcomes.

This caching layer forms the foundational capability needed to later address:
- Deletion of resources with inter-object dependencies (via dependency graph resolution using cached state)
- Bulk namespace-level deletions (via coordinated, BLOB-backed destruction across multiple TF objects)

Once implemented and validated for single-object deletion, this mechanism will be extended to support cascading deletion workflows.","## Decision

1. **BLOB Creation and Storage**
   * A gRPC function named `CreateWorkspaceBlob` will be invoked by the TF-Controller 
     to tell tf-runner to compress the Workspace file system into a tar.gz BLOB, which is then retrieved back to the controller.
   * The caching mechanism will be executed right before the Terraform Initialization step, ensuring that the latest and most relevant data is used.
   * Each Workspace Blob will be cached on the TF-Controller's local disk, using the UUID of the Terraform object as the filename,`${uuid}.tar.gz`.
   * To reduce the risk of unauthorized access to the cache entries, and cache collisions, the cache file will be deleted after the finalization process is complete.
2. **Persistence** 
   * [The persistence mechanism used by the Source Controller](https://fluxcd.io/flux/installation/configuration/vertical-scaling/#persistent-storage-for-flux-internal-artifacts) will be adopted for the TF-Controller's persistence volume.
3. **BLOB Encryption**
   * The encryption and decryption of the BLOBs will be tasked to the runner, with the controller solely responsible for storing encrypted BLOBs.
   * Each namespace will require a service account, preferably named ""tf-runner"".
   * The token of this service account, which is natively supported by Kubernetes, will serve as the most appropriate encryption key because it's stored in a Secret, access to which can be controlled by RBAC. Storing it in a Secret also allows the key to be rotated.
4. **Security Measures (Based on STRIDE Analysis)**
   * **Spoofing:** Implement Kubernetes RBAC for access restrictions and use mutual authentication for gRPC communications.
   * **Tampering:** Use checksums for integrity verification and 0600 permissions to write-protect local disk storage.
   * **Repudiation:** Ensure strong logging and auditing mechanisms for tracking activities.
   * **Information Disclosure:** Utilize robust encryption algorithms, rotate encryption keys periodically, and secure service account tokens.
   * **Denial of Service:** Monitor storage space and automate cleanup processes.
   * **Elevation of Privilege:** Minimize permissions associated with service account tokens.
5. **First MVP & Future Planning**
   * For the initial MVP, the default pod local volume will be used.
   * Since a controller restart will erase the BLOB cache, consideration for using persistent volumes should be made for subsequent versions."
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision

We will transition Zarf's E2E testing to a fully public, transparent model by removing the dependency on repository secrets and enabling automated execution of E2E tests for all pull requests from trusted contributors. This shift is enabled by the removal of registry1 image dependencies in v0.15, which previously necessitated secret-based authentication.

The new E2E testing workflow will be:

1. **Automated Public Execution**: All E2E tests will run automatically on every PR from members of the organization and approved contributors, without requiring maintainer-triggered commands via `slash-command-dispatch`.

2. **Multi-Distribution Testing Matrix**: We will expand our CI pipeline to include a testing matrix across:
   - Kubernetes distributions: Kind (default), K3s, MicroK8s, and EKS/Minikube where appropriate.
   - Base OS environments: Ubuntu (LTS), Amazon Linux 2, and Rocky Linux to validate cross-platform compatibility.

3. **Local E2E Development Support**: Introduce a lightweight, containerized test harness (`zarf test e2e-local`) that allows developers to run a subset of core E2E tests locally using Docker and Kind, significantly reducing feedback loop time during development.

4. **Secretless Design Principle**: Maintain a strict policy of not using repository secrets in any standard E2E workflow. Any future need for authenticated resources will require alternative solutions such as ephemeral, short-lived credentials or public mirror registries.

This decision improves developer experience, increases test coverage, accelerates contribution review cycles, and aligns with open-source best practices—while preserving security through design rather than access control.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision 
Zarf will deploy a temporary, internal image registry as part of the `zarf init` command. This registry will be deployed into the cluster and exposed via a ClusterIP service, accessible only within the cluster network.

All images provided in Zarf packages will be pushed to this registry during package deployment. To ensure nodes can authenticate and trust this registry:
- The registry will use TLS with certificates signed by a private CA bundle generated at `zarf init` time.
- The CA bundle will be automatically distributed to each node’s host-level certificate store or CRI trust configuration using privileged init containers during initialization.
- Node affinity or taints are not required, as all nodes will be configured to trust the registry.

This approach ensures compatibility across all Kubernetes environments — including air-gapped, restricted, or unknown clusters — without relying on external registries or pre-existing infrastructure. After initialization, Zarf-managed workloads pull images normally via K8s, with the CRI retrieving them from the trusted internal registry.

The temporary registry remains active for the lifetime of the Zarf environment and is removed only upon `zarf destroy`. This provides a consistent, dependency-free mechanism for image distribution that aligns with Zarf’s goal of zero external dependencies in isolated environments.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision  
We will generate and maintain a Software Bill of Materials (SBOM) for all Arachne-based applications and core modules to comply with Executive Order 14028, which mandates SBOM availability for software deployed on government systems.

The SBOM will be produced as part of the standard build pipeline using automated tooling that analyzes dependencies at both the language level (e.g., Maven/Leiningen/Clojars artifacts) and the operating system/library level (if applicable). We will use SPDX or CycloneDX formats—both recognized under NIST guidelines—for interoperability and toolchain support.

Key aspects of this decision:

1. **Automation**: SBOM generation will be integrated into the CI/CD workflow. No manual curation is required, reducing drift and human error.
2. **Immutability & Traceability**: Each SBOM will be tied to a specific build or release artifact, enabling traceability across deployments.
3. **Transparency**: The SBOM will be publicly available alongside release artifacts, supporting transparency and auditability.
4. **Modularity Alignment**: Given Arachne’s modular architecture, each module may produce its own SBOM, but a complete, aggregated SBOM will be generated for any full application assembly.
5. **Tooling Choice**: We will use tools such as Syft (from Chainguard) or Eclipse CycloneDX Maven/Gradle plugins depending on the packaging ecosystem, ensuring compatibility with Clojure’s JVM-based stack.

An ADR will be created to document the chosen toolchain and integration approach, and future changes to SBOM format, scope, or tooling will follow the standard ADR process.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision
We will implement a Zarf-native manifest transformer pipeline that operates independently of Helm’s Post Rendering, enabling reliable image, secret, and Git path mutations regardless of the deployment method (e.g., direct apply, Flux, ArgoCD, etc.).

This pipeline will:
1. Intercept and transform Kubernetes manifests at deploy time—before they are handed off to any apply mechanism or gitops tool.
2. Be driven by declarative transformation rules defined in the Zarf package configuration, such as:
   - Image rewrite rules mapping public images to internal registry paths using the Zarf Registry.
   - Secret injection or reference updates based on Zarf-provided credentials or generated values.
   - Git repository URL rewrites to point to the internal Gitea instance when applicable.
3. Operate on raw YAML manifests (either embedded in charts or provided as raw files), parsing them into structured data, applying transformations, and serializing them back.
4. Support both built-in transformation types (e.g. image path rewrite) and optional user-defined transformers via external executables (for advanced use cases).
5. Integrate with the existing `zarf prepare` functionality but supersede it as the primary mutation mechanism, reducing author burden and increasing consistency.

By decoupling transformation logic from the deployment toolchain and centralizing it within Zarf, we ensure consistent, predictable behavior across all deployment patterns—including gitops workflows—while minimizing the responsibility placed on package authors. The result is a more seamless, reliable experience when deploying air-gapped or isolated environments.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision  
We will replace the `stage2` Go binary with a lightweight, purpose-built Rust binary that directly implements the minimal functionality required to serve the `registry:2` image in read-write and read-only modes during initialization.

The new binary, to be called `zarf-regserve`, will:

1. Accept a path to the unpacked `registry:2` image content (from the crane tarball).
2. Support two operational modes:
   - **Write mode**: Start an HTTP server that accepts a single image push (the `registry:2` image) and stores it on disk.
   - **Read mode**: Serve the pushed image via the Docker Registry HTTP API (v2) for consumption by the `zarf-registry` package.
3. Implement only the necessary subset of the Docker Registry API (e.g., `GET /v2/`, `GET /v2/<name>/manifests/<tag>`, `GET /v2/<name>/blobs/<digest>`), rejecting all other endpoints.
4. Be compiled for all supported platforms as part of the Zarf build process, but with significantly smaller footprint and faster compile time than the full Go registry.

Rationale:
- Eliminates the complexity and overhead of embedding and managing a full Docker registry binary.
- Reduces the number of binaries in the Zarf ecosystem from three back toward two (effectively consolidating `stage2` into a more efficient replacement).
- Decreases the number of ConfigMaps needed, improving `zarf init` performance.
- Avoids cross-platform compilation burden of the Go binary, as Rust compilation is already well-integrated and optimized in the current pipeline.
- Aligns with the principle of doing one thing well—this binary exists solely to bootstrap the in-cluster registry.

Migration Plan:
- The `stage1` binary will continue to unpack configmaps and prepare the filesystem.
- `stage1` will invoke `zarf-regserve` in write mode, push the image, then restart it in read mode.
- Once `zarf-registry` successfully pulls the image, the injector pod terminates as before.
- A new ADR will be created if future use cases require expanding this service beyond init-time bootstrapping.

This change improves maintainability, reduces technical debt, and streamlines the Zarf initialization sequence without sacrificing reliability.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision  
We will adopt **Option 2: Introduce Unit Testing in a defined and limited capacity within Zarf**, with a strong emphasis on testing *interfaces* and *boundary conditions* rather than implementation details.

This approach provides the most balanced response to our evolving needs as Zarf transitions into a library used by other products. While end-to-end (e2e) tests remain essential for validating complete workflows, they are insufficient for ensuring reliability at the interface level and for covering edge cases in logic that are difficult or expensive to trigger through e2e scenarios.

By limiting unit testing to well-defined areas—particularly those that are:
- Difficult to test via e2e (e.g., error handling, internal validation, configuration parsing),
- Exposed as public APIs or extension points for external users,
- Prone to regressions due to complexity or frequent change,

—we gain increased confidence in correctness without incurring an unsustainable maintenance burden.

Unit tests will be required when:
- A new public API or extension mechanism is introduced,
- Logic contains non-trivial branching or data transformation,
- A defect is discovered that cannot be reasonably or efficiently covered by e2e testing.

To preserve long-term efficacy:
- Unit tests must target *behavior*, not implementation. Mocking should be used sparingly and only at module boundaries.
- Tests written against private functions must justify their existence and be reviewed carefully to avoid coupling tests to transient code structures.
- The team will establish clear guidelines and patterns for unit testing in the documentation, and maintain them as part of the contribution process.

Additionally, we recognize value in eventual integration testing (Option 3), and will explore lightweight integration-style tests for key subsystems in parallel. However, this remains secondary to strengthening interface-level guarantees through disciplined, focused unit testing.

This decision supports modularity, improves transparency, and increases confidence in Zarf’s stability as a library—all while respecting practical constraints around test maintainability and developer velocity.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision  
We will adopt **Option 2**: Run Syft against `files` and `dataInjections` within Zarf packages to generate SBOMs.

This approach strikes the right balance between completeness, accuracy, and usability. By focusing on `files` and `dataInjections`, we ensure that:
- Only artifacts *definitively included* in the package are analyzed, avoiding false positives from transient or intermediate build artifacts (e.g., git index files).
- We cover the most common and intended methods users employ to include non-image dependencies, including those used in our own init package (e.g., k3s binaries, injector scripts).

While this does not capture every possible dependency (e.g., files embedded in a git repo pulled at deploy time), it provides a reliable, reproducible, and meaningful SBOM for the core set of non-image assets that are explicitly declared and packaged.

We acknowledge that Option 1 could produce more comprehensive results but risks introducing noise and inaccuracies that reduce trust in the SBOM. Option 3, while flexible, would add significant complexity and cognitive load, potentially leading to inconsistent usage and reduced adoption.

Going forward, we will:
- Document clearly what is included in the SBOM and why.
- Monitor community feedback and usage patterns to evaluate future expansion (e.g., opt-in scanning of git repos or other sources).
- Ensure generated SBOMs are surfaced prominently during `zarf package create` and `zarf inspect`.

This decision aligns with our goal of providing transparent, actionable security information without overwhelming users with ambiguous or misleading data.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision  
We will introduce a new `zarf init-less` deployment mode that allows users to deploy Zarf packages in connected environments without requiring a prior `zarf init` step or an initialization package.

This mode will:
- Enable the direct application of Zarf packages that declare all necessary infrastructure components, package dependencies, and configuration inline.
- Rely on real-time access to upstream registries, repositories, and services (e.g., container images, Git repos, Helm charts) since airgap constraints are lifted.
- Validate required resources at deploy time rather than bundling them during package creation.
- Support the full Zarf declarative specification (e.g., components, manifests, images, hooks) but skip the staging and seeding steps traditionally handled by `zarf init`.
- Be explicitly opt-in via a flag (`--init-less`) or configuration field in the `zarf.yaml` to ensure users are aware of connectivity requirements.

This decision preserves Zarf’s core value—declarative, reproducible deployments—while increasing flexibility for connected use cases. It reduces operational overhead in online environments without compromising the integrity of disconnected workflows, which remain the default and primary supported model.  

A future ADR may explore further divergence between connected and disconnected feature sets, but this change ensures backward compatibility and keeps the two modes cleanly separated.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision  
We will refactor the `scripts` noun in components to support a more expressive, structured, and lifecycle-aware scripting model.

The updated `scripts` system will:

1. **Introduce distinct lifecycle phases** with well-defined semantics:
   - `prepare`: Runs during package creation (`zarf package create`). Used for asset generation, templating, or validation prior to packaging.
   - `init`: Runs once at component initialization time in the target environment. Replaces the old `before`/`after` init logic.
   - `pre-action`: Runs before each declared action (e.g., deploy, upgrade).
   - `post-action`: Runs after each declared action completes successfully.
   - `cleanup`: Runs during component teardown or rollback.

2. **Allow named script entries** instead of anonymous arrays, enabling better composition, override control, and introspection:
   ```yaml
   scripts:
     init:
       - name: setup-directories
         cmd: mkdir -p /var/lib/component
       - name: load-config
         cmd: cp config.template.yaml /etc/component.yaml
   ```

3. **Support conditional execution** via `when` clauses that evaluate environment or state conditions:
   ```yaml
   - name: migrate-database
     cmd: ./migrate.sh
     when: ""{{ .State.db_version }} < 2""
   ```

4. **Enable modular reuse** through script imports from other packages or shared libraries, promoting DRY practices across teams.

5. **Preserve immutability and declarative integrity**: Scripts must not mutate the component definition itself; side effects are limited to the execution environment.

This change supports Arachne’s goals by:
- Increasing **transparency**, as script intent and ordering are now explicit and inspectable.
- Enhancing **modularity**, allowing scripts to be composed, extended, or overridden per-component.
- Improving **configurability**, letting users adapt behavior without forking code.
- Avoiding entanglement with runtime state by keeping script definitions data-driven and external to code.

Existing `before`/`after` script blocks will be deprecated and automatically mapped to `init` for backward compatibility, with warnings emitted during package creation. New features will require updated Zarf versions and explicit opt-in via schema versioning.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision
We will revert the local image support feature implemented in [#1173](https://github.com/defenseunicorns/zarf/pull/1173) due to the critical OOM (Out of Memory) issue when handling large images via Crane's docker daemon integration. This reversion is necessary to maintain system stability and prevent crashes during normal operations.

Going forward, we will pursue a phased approach:
1. **Immediate**: Revert the change to eliminate the OOM risk. Local image support will be disabled until a robust solution is in place.
2. **Short-term**: Explore alternative implementations using `skopeo` or direct Docker CLI invocation with streaming/pipelining to avoid full in-memory buffering, accepting some performance trade-offs for reliability.
3. **Long-term**: Align with upcoming package schema changes ([#1319](https://github.com/defenseunicorns/zarf/issues/1319)) to adopt OCI-native image handling that avoids legacy formats and supports efficient, memory-safe transfers.

Performance will remain a key evaluation criterion, but not at the expense of basic system stability. Any new implementation must include memory usage testing, particularly with large (>10GB) images, before being merged.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision 
We will create a Big Bang-specific component generator for Zarf that dynamically analyzes the provided `values.yaml` configuration to determine the minimal set of required images and git repositories for a given Big Bang deployment.

This generator will:
1. Accept a user-provided `values.yaml` file as input
2. Parse the values to determine which Big Bang components/modules are enabled
3. Use Big Bang's internal dependency mapping to calculate the closure of required components
4. Query Big Bang's image and repository manifests to identify only the specific images and repositories needed for the enabled component set
5. Generate a tailored Zarf component that includes:
   - Flux installation (required prerequisite)
   - Only the necessary container images (dramatically reducing package size)
   - Only the necessary git repositories (reducing download overhead)
   - Properly injected values configuration at the appropriate layer in the kustomization hierarchy

The generator will be implemented as a standalone tool that outputs a Zarf component definition, allowing users to generate optimized packages without modifying core Zarf functionality. This approach maintains Zarf's general-purpose nature while providing specialized optimization for this common, complex use case.

Future consideration: This pattern could serve as a model for other complex Helm-based applications with similar superset-of-resources deployment patterns.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf packages are currently only available if built locally or through manual file transfers. This is not a scalable way to distribute packages. We wanted to find a way to distribute and publish packages in a way that is easily consumable for the majority of users. When considering the goal of being able to share packages, security and trust are very important considerations. We wanted our publishing solution and architecture changes to keep in mind signing of packages and the ability to verify the integrity of packages.

We know we are successful when:

1. (Priority) Users can use Zarf to natively publish a Zarf package to an OCI compliant registry
2. (Secondary goal) Package creators can sign Zarf packages to enable package deployers can trust a packages supply chain security","## Decision  
We will implement native support for publishing and pulling Zarf packages to and from OCI (Open Container Initiative) compliant registries, using the OCI distribution specification as the foundation for package distribution. This enables seamless integration with existing container registry infrastructure (e.g., Docker Hub, GitHub Container Registry, Artifactory, private registries), which are widely adopted, scalable, and accessible to the majority of users.

To satisfy the priority goal, Zarf will:
- Package the Zarf-generated airgap bundle as an OCI artifact using defined media types for Zarf packages.
- Support `zarf package publish <registry-url>` and `zarf package pull <registry-url>` commands to push and retrieve packages from any OCI-compliant registry.
- Leverage standard authentication mechanisms (e.g., docker config, bearer tokens, basic auth) already supported by registries and tools like `docker` or `oras`.

To address the secondary goal around supply chain security:
- Integrate Cosign to enable cryptographic signing and verification of Zarf packages.
- Allow package creators to sign packages during or after creation using `zarf package sign`.
- Enable deployers to verify signatures before deployment via `zarf package deploy --insecure=false` (default secure mode).
- Store signatures alongside the package in the registry as OCI artifacts, associated via subject reference.

The package configuration will include optional metadata for provenance, such as:
- Builder identity
- Timestamps
- Signing key identifiers

This approach ensures:
- **Scalability**: Leverages battle-tested, globally distributed registry ecosystems.
- **Transparency**: All operations are auditable and inspectable.
- **Trust**: End-to-end integrity and authenticity verification through signing.
- **Interoperability**: Uses open standards (OCI + SigStore) rather than building a proprietary distribution layer.

Future extensions may include support for TUF (The Update Framework) and automated signing in CI environments.","## Decision

We decided that changing the structure of Zarf packages to be an OCI artifact would be the best way to distribute and publish packages as registries are already an integral part of the container ecosystem."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision  
We will enhance Zarf to support **generic, offline-capable mirroring of artifact repositories** (e.g., npm, PyPI, Maven, NuGet, etc.) and improve native `git` proxying capabilities to better support airgapped development workflows.

This will be achieved by:

1. **Introducing a new `zarf mirror` command suite** that allows developers to declaratively define upstream artifact sources (including git hosts, npm registries, PyPI indexes, etc.) and mirror them into the airgapped environment as part of a Zarf package.
2. **Adding a transparent proxy layer in Zarf Deployments** that intercepts outbound requests to known upstream sources (e.g., `github.com`, `registry.npmjs.org`, `repo.maven.apache.org`) and routes them either:
   - To cached/mirrored content within the airgap if available.
   - Or returns a clear error message indicating the required asset must be mirrored via Zarf packaging.
3. **Extending Zarf’s Git support beyond Flux GitRepositories** to allow arbitrary `git clone`, `fetch`, and `push` operations through a proxied or sidecar-mirrored mechanism, preserving standard git tooling and SSH/HTTPS workflows.
4. **Allowing packages to bundle artifacts from multiple ecosystem package managers**, storing them in an indexed, metadata-preserving format that enables accurate reproduction of dependency resolution during build processes in the airgap.
5. **Documenting and providing templates** for common development stacks (e.g., Node.js + npm, Python + pip, Java + Maven) showing how to pre-seed dependencies using Zarf for seamless airgapped development.

This decision ensures that Zarf evolves from a deployment-focused tool into a full lifecycle enabler for secure, airgapped software development—without sacrificing usability or forcing non-standard workflows on developers.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision  
We will establish a formal versioning and release policy for Zarf to achieve General Availability (v1.0.0) stability, enabling reliable use in mission-critical environments.

This includes the following key commitments:

1. **Semantic Versioning Compliance**: Starting at v1.0.0, Zarf will strictly adhere to Semantic Versioning (SemVer 2.0). This means:
   - Major version increments (e.g., 1.x.x → 2.x.x) will occur only when introducing backward-incompatible changes.
   - Minor versions (x.1.x → x.2.x) will introduce new functionality in a backward-compatible manner.
   - Patch versions (x.x.1 → x.x.2) will be used solely for backward-compatible bug fixes.

2. **Freeze on Breaking Changes**: No breaking changes to core APIs, CLI interface, package format, or configuration schema will be introduced within the v1.x.x series. Any such changes must wait for v2.0.0.

3. **Comprehensive Testing Requirements**: Before v1.0.0 is released, we will:
   - Expand test coverage across all critical code paths.
   - Implement end-to-end testing for common deployment scenarios.
   - Establish CI/CD pipelines that validate compatibility across supported platforms and configurations.
   - Introduce regression testing for previously reported issues.

4. **Stability Period**: A minimum stabilization period of three months prior to v1.0.0, during which only bug fixes and documentation improvements are accepted—no new features.

5. **Deprecation Policy**: When future changes require removal or replacement of functionality, a deprecation cycle of at least two minor releases will be provided with clear migration guidance.

6. **Documentation Completeness**: All public interfaces, configuration options, and user workflows will be fully documented prior to v1.0.0.

By meeting these criteria, Zarf will transition from a rapidly evolving tool requiring expert involvement to a production-ready, maintainable platform suitable for broad adoption in high-stakes environments.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision  
We will implement a `hooks` system within Zarf that allows cluster maintainers to register pre-defined, reusable behaviors that automatically execute during the deployment lifecycle in response to specific conditions or events. These hooks will be configured at the cluster level during `zarf init`, and will operate transparently to package creators and deployers.

The `hooks` system will address the identified problems with ECR (and similar external registries) by enabling automatic, context-aware execution of required operations without placing the burden on individual packages or users. Specifically:

1. **Authentication Token Management**: A hook will be triggered periodically or prior to any image pull/push operation to refresh ECR authentication tokens using AWS IAM credentials provided at cluster initialization. This ensures long-lived clusters remain operational without manual intervention.

2. **Image Repository Provisioning**: A hook will intercept image push events and automatically create ECR repositories if they do not already exist, addressing the lack of 'push-to-create' support. This will use metadata extracted from the image reference and apply consistent naming and tagging policies.

These hooks will be:
- **System-specific but transparent**: Implemented and maintained by Zarf core or official modules, they encapsulate registry-specific logic (e.g., AWS SDK calls for ECR), so packages remain system-agnostic.
- **Configurable at init time**: Enabled automatically when ECR (or another supported registry type) is selected during `zarf init`, based on user-provided configuration (e.g., AWS role ARN, region).
- **Non-bypassing of core functionality**: Designed to extend Zarf’s behavior rather than work around it, ensuring actions remain focused on legitimate extensibility (e.g., custom scripting), not compensating for missing platform features.

This approach ensures:
- Packages are portable across different cluster configurations.
- Cluster-specific concerns are handled centrally, reducing duplication and errors.
- The intent of `actions` is preserved — they remain a mechanism for optional, user-defined extensions, not mandatory workarounds.

Future hooks may support other registries (e.g., GCR, ACR) or integration points (e.g., secret management systems, monitoring tools), following the same pattern of conditionally activated, well-scoped automation tied to cluster configuration.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision  
We will use WebAssembly (WASM) to compile the existing Go-based Zarf transformation library for execution in TypeScript/JavaScript environments.

This approach allows us to:
- Maintain a single source of truth for transformation logic in Go, preserving years of battle-tested reliability and edge-case handling.
- Reuse the exact same code path in both Zarf (Go-native) and Pepr (via WASM) without duplication or drift.
- Avoid introducing network dependencies (as would be required with gRPC or REST), enabling fully offline, low-latency transformations within the Pepr runtime.
- Prevent the high cost and risk of rewriting complex logic in TypeScript, which would introduce new bugs and require extensive testing and maintenance.

The WASM module will be compiled from Go using `tinygo` for smaller binary size and better performance in constrained environments. It will expose a minimal, well-typed interface to the TypeScript layer, accepting input data as JSON and returning transformed results likewise.

We accept the following trade-offs:
- Increased complexity in build tooling (cross-compilation to WASM).
- Slight overhead in marshaling/unmarshaling data between JS and WASM runtimes.
- Limitations in Go’s WASM support (e.g., no goroutines, larger binaries with standard `go` compiler — mitigated by using `tinygo`).

Future decisions may include optimizing bundle size, caching compiled modules, or exploring partial rewrites of non-critical paths in TypeScript, but the core transformation logic will remain centralized in Go and distributed via WASM.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision  
We will standardize and unify Zarf's package source handling by implementing a consistent set of operations across all supported package sources. Every package source type—whether local tarball, split tarball, OCI registry, in-cluster (deployed), or HTTPS URL—must support the following core operations:

- `inspect`: Display metadata and contents of the package without deploying it.
- `deploy`: Deploy the package to the target environment.
- `remove`: Remove a deployed package (where applicable).
- `list`: Show available packages, with source-type-specific details.

To achieve this:

1. **Extend `zarf package inspect`** to support:
   - Local and split tarballs
   - OCI references (`oci://...`)
   - HTTPS URLs pointing to valid Zarf packages
   - In-cluster packages (by querying deployed state)

2. **Enhance `zarf package remove`** to:
   - Accept both in-cluster package names and full source references where unambiguous
   - Support removal of packages previously deployed from any source

3. **Ensure `zarf package list`** provides a unified view of:
   - Locally available packages (on disk)
   - OCI-hosted packages (via remote listing when authenticated)
   - Deployed (in-cluster) packages

4. **Treat all package sources as first-class citizens** in the CLI design. The implementation behind each operation will be abstracted behind a common interface that encapsulates source-specific logic.

5. **Document and enforce** that any new package source added in the future must implement the full set of core operations unless technically impossible (with justification required).

This decision ensures predictable user experience, improves tooling reliability, and supports automation workflows such as CI/CD pipelines where inspection and cleanup are essential. It also aligns with Zarf’s goal of being usable in disconnected environments by enabling users to verify and manage packages regardless of origin.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision 
We will refactor the package composition system to address scalability, maintainability, and consistency issues by implementing a centralized composition engine. This engine will:

1. **Normalize all import operations** through a single composition pipeline that handles both traditional layers and OCI skeletons uniformly. All imports—regardless of source type—will be processed through the same validation, merging, and transformation logic.

2. **Introduce a directed acyclic graph (DAG) model for import resolution**, replacing the current linear import chain. This enables parallel processing of independent imports, improves error reporting for circular dependencies, and supports comprehensive test coverage across the full dependency tree.

3. **Centralize component filtering logic** within the composition engine. The `only` filter (and future filters) will be applied at a single canonical stage during composition, ensuring consistent behavior whether used at create-time or in nested imports.

4. **Enforce schema-aware merging** by validating each imported component against a unified schema before integration. This prevents silent conflicts and ensures that both core components and OCI skeleton definitions adhere to the same structural rules.

5. **Expose composition traces** as first-class artifacts, allowing tooling and tests to verify integrity across multi-layer scenarios. These traces will document the origin, transformation, and final state of every composed component.

This refactoring will decouple composition semantics from individual package types, enabling future extensibility while restoring confidence in complex, deeply composed packages.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision 
We will implement a `zarf dev` command that streamlines the package development lifecycle by combining and optimizing existing operations for rapid iteration.

The `zarf dev` command will:

1. **Skip air-gap constraints**: Operate under the assumption of a connected environment, allowing direct access to remote dependencies (e.g., container registries, git repos) during deployment without requiring them to be embedded in the package.
2. **Preserve cluster state**: Reuse an existing cluster (if available) and only redeploy changed components, avoiding full reinitialization via `zarf init`.
3. **Bypass full packaging**: Instead of rebuilding the full Zarf package tarball on every change, `zarf dev` will:
   - Validate the `zarf.yaml`
   - Resolve and inject dynamic values (e.g., image pulls, file references)
   - Deploy directly to the target cluster using live resources
4. **Support incremental updates**: Where possible, apply only deltas to deployed components (e.g., rolling restarts of updated manifests or injected images).
5. **Leverage YOLO mode semantics**: Internally use YOLO-mode-like behavior (no seed registry, no embedded images) but expose it through a dedicated, documented developer workflow rather than as a hidden mode.
6. **Enable teardown**: Include a `zarf dev cleanup` subcommand to remove deployed components and associated resources.

This approach reduces the feedback loop from minutes (due to full rebuilds and reinitialization) to seconds, while maintaining compatibility with the final `zarf package create` output.

Rationale:
- Improves developer experience without compromising the integrity of the production-oriented `zarf package create` flow.
- Maintains separation between development and air-gapped deployment workflows.
- Builds on existing concepts (like YOLO mode) in a more transparent, user-facing way.

Note: The `zarf dev` command is intended solely for local development and testing. It does not produce an air-gapped package and should not be used in secure or disconnected environments.","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision  
We will migrate the Arachne documentation site from Docusaurus 2 to a static site generator with stronger performance characteristics and more predictable dependency management, specifically evaluating options such as [Hugo](https://gohugo.io/) or [Eleventy (11ty)](https://www.11ty.dev/), with a preference for minimal JavaScript tooling where possible.

Rationale:
- **Performance**: The current Docusaurus setup has shown slow build times and suboptimal output for large documentation sets. Hugo and Eleventy are known for faster builds and lighter runtime overhead.
- **Dependency Management**: Docusaurus relies heavily on the Node.js/npm ecosystem, which introduces complexity in dependency resolution, version conflicts, and bundle size. A move toward lighter, more focused tools reduces this surface area.
- **Simplicity and Maintainability**: Both Hugo (Go-based) and Eleventy (JavaScript but zero-config by design) promote simpler architectures. Eleventy, in particular, aligns well with Arachne’s “everything is data” philosophy by allowing content and structure to be defined clearly and separately from presentation.
- **Decoupling from Framework Bloat**: Docusaurus includes many features we do not use. A leaner SSG allows us to include only what we need, improving long-term maintainability and reducing technical debt.

Next Steps:
1. Evaluate Hugo vs. Eleventy based on:
   - Build speed using our existing content corpus
   - Ease of integration with our CI/CD pipeline
   - Support for custom metadata, routing, and search
   - Developer experience for contributors
2. Prototype a subset of the documentation in the chosen tool.
3. Create an ADR for the selected SSG, including migration plan and tooling support.
4. Execute migration once the new ADR is accepted.

This decision does not require immediate action but establishes direction. The final choice of tool will be made in a follow-up ADR after evaluation.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
"## Context

Zarf is currently using an in-house built logging solution which in turn depends on pterm. This solution is used to output information to the end user who is using the Zarf CLI. This output is for the most part formatted with the purpose of CLI user experience. Logging function calls are done both in the CLI specific code as well as in the packages. The logging is implemented in such a way that different levels exist with different impacts and log destinations. A common pattern that is used is to call `message.Fatal` whenever an error occurs. This will output the message to STDERR while writing the actual error message to a debug log and exiting the program. Exiting the program in this manner makes unit testing difficult while also skipping proper context handling and skipping any clean-up that was intended to run before exiting the program. Some logging components like the progress bar and spinner are accessed through a shared global state that is not thread safe. The coupling to logging becomes complicated as disabling the progress bar is a challenge while multi threading these functions which access the global state resulting in complicated access patterns.","## Decision  
We will replace Zarf's current in-house logging solution with a structured, context-aware logging system that separates concerns between user-facing output, debugging information, and program flow control. This new approach will improve testability, support safe concurrency, and enable better lifecycle management.

The key aspects of this decision are:

1. **Decouple Logging from Program Termination**:  
   We will stop using `message.Fatal` (or equivalent functions) to exit the program. Instead, errors will be logged appropriately and propagated via return values or explicit error channels. Final termination decisions will be made at well-defined boundaries in the application (e.g., main entrypoint), allowing for proper cleanup, resource release, and testing.

2. **Adopt Structured Logging**:  
   We will adopt a structured logging library (e.g., [zap](https://github.com/uber-go/zap) or similar) that supports leveled logs, structured fields, and multiple output destinations. This enables:
   - Clear separation between human-readable CLI output and machine-parseable debug logs.
   - Flexible log routing (e.g., debug logs to file, user messages to stdout/stderr).
   - Better observability and post-processing of logs in automated environments.

3. **Eliminate Global State in Progress Reporting**:  
   The progress bar and spinner components will be refactored to avoid shared global state. These will instead be managed through explicit context objects or interfaces passed to components that need them. This allows:
   - Safe use in multithreaded or concurrent contexts.
   - Per-operation control (e.g., disabling progress during tests or automation).
   - Mocking and verification in unit tests.

4. **Introduce a Logging Interface Abstraction**:  
   A logging interface will be defined so that both CLI code and internal packages can log without depending on concrete implementations. This enables:
   - Dependency injection of loggers per component or context.
   - Test-specific loggers that capture output without side effects.
   - Dynamic reconfiguration of log levels and outputs.

5. **Ensure All Critical Paths Support Cleanup**:  
   Error handling paths must preserve context (e.g., via `context.Context`) and allow deferred cleanup functions (e.g., `defer` blocks, shutdown hooks) to execute before process exit.

6. **Migration Plan**:  
   - Begin by introducing the new logger alongside the existing one.
   - Gradually migrate packages to use injected loggers and structured output.
   - Refactor `Fatal`-style exits into error returns with centralized handling at top-level commands.
   - Update tests to verify behavior without relying on process termination.

This change improves maintainability, testability, and robustness while preserving—or enhancing—the end-user experience. It aligns with modern Go practices and prepares Zarf for more complex, concurrent, and automated usage patterns.","## Decision

I am proposing to completely refactor the logging functionality to follow a more standardized format using the newish slog interfaces. On top of that we would refactor the current internationalization by converting them to standardized errors.

* Replace the message package with a slog interface.
* Replace the lang package with static errors.
* Remove any use of `message.Fatal` and instead return errors.
* Refactor use of `message.WarnErr` to either return error or to log the error.
* Refactor existing functions that print formatted outputs to defer the printing to the CLI.
* Define a new interface for spinner and progress bar that is passed in as a function parameter.

The message package currently exports the following functions which should be replaced by its logr counterpart.

| Function | Replacement | Comment |
| --- | --- | --- |
| ZarfCommand | Info | Just formats a slice as a command. |
| Command | Info | Outputs a command with a prefix and style. |
| Debug | Debug | |
| Debugf | Debug | |
| ErrorWebf | N/A | Not used anywhere. |
| Warn | Warn | |
| Warnf | Warn | |
| WarnErr | Warn | |
| WarnErrf | Warn | |
| Fatal | N/A | Should not be used. |
| Fatalf | N/A | Should not be used. |
| Info | Info | |
| Infof | Info | |
| Success | Info | Seems to be a info log with a checkmark prefix. |
| Successf | Info | Seems to be a info log with a checkmark prefix. |
| Question | ? | Open question how to resolve this. |
| Questionf | ? | Open question how to resolve this. |
| Note | Info | Should just be an info or maybe a debug log. |
| Notef | Info | Should just be an info or maybe a debug log. |
| Title | ? | Not sure how this should be replaced as it formats with separator. |
| HeaderInfof | ? | |
| HorizontalRule | ? | |
| JSONValue | N/A | Should be replaced with a marshal. |
| Paragraph | ? | |
| Paragraphn | ? | |
| PrintDiff | ? | |
| Table | ? | Need to come up with a good syntax for functions to return output that can print as a table. |
| ColorWrap | ? | Should this be used? | 
| PrintConnectStringTable | N/A | This logic should not exist in the message package. |
| PrintCredentialTable | N/A | This logic should not exist in the message package. |
| PrintComponentCredential | N/A | This logic should not exist in the message package. |
| PrintCredentialUpdates | N/A | This logic should not exist in the message package. |
| Spinner | Interface | New Spinner interface. |
| ProgressBar | Interface | New progress bar interface. |

The majority of simple logging changes should be possible with little signature changes. Replacing the existing output with a slog interface would allow other implementations to be used. A challenge initially may be the changes to table output formatting to make it work properly. This change will require some refactoring of existing code. A requirement for the changes is that they have to improve the UX for users looking at log files. As I understand the present day the spinners will cause a new line everytime they update, resulting in a lot of bloat. A goal should be to make sure that this does not happen in the future.

Spinners and progress bars however are a bit more challenging. They need to be refactored so that they are no longer instantiated outside of the CLI code. They should instead be passed as a function parameter to each function that needs them. A good example of a project that solves the problem in a similar manner is Kind. In Kind they create a [status object from the logger](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/internal/cli/status.go#L39) which they then [pass to where it is needed](https://github.com/kubernetes-sigs/kind/blob/7799e72306db315ea4f4b1cac90ff68404da4f28/pkg/cluster/internal/create/create.go#L133). Doing so results in a single status object created which is reused where ever it is needed. A lot of inspiration can be take from how Kind deals with CLI output. While they use klog instead of slog there are a lot of similarities. They have for example a check if the output is in a terminal or not, and will disable the spinner accordingly. 

Here is a suggestion for how a thread safe spinner could be implemented with a shared logger. This also allows for parallel spinners and progress bars.

```golang
package main

import (
	""context""
	""fmt""
	""io""
	""log/slog""
	""os""
	""sync""
	""time""

	""github.com/pterm/pterm""
)

func main() {
	err := run()
	if err != nil {
		panic(err)
	}
}

func run() error {
	h := NewPtermHandler(os.Stderr)
	log := slog.New(h)

	log.Info(""before"")

	spinner := NewSpinner(log)
	spinner.Update(""Running some job"")
	log.Info(""after"")
	time.Sleep(1 * time.Second)
	spinner.Update(""Doing some update"")

	time.Sleep(2 * time.Second)

	spinner.Succeed()

	time.Sleep(2 * time.Second)

	return nil
}

type PtermHandler struct {
	printer *pterm.MultiPrinter
	attrs   []slog.Attr
	group   string
}

func NewPtermHandler(w io.Writer) *PtermHandler {
	printer, _ := pterm.DefaultMultiPrinter.WithWriter(w).Start()
	return &PtermHandler{
		printer: printer,
	}
}

func (h *PtermHandler) Enabled(context.Context, slog.Level) bool {
	return true
}

func (h *PtermHandler) Handle(ctx context.Context, r slog.Record) error {
	l := fmt.Sprintf(""%s: %s\n"", r.Level, r.Message)
	_, err := h.printer.NewWriter().Write([]byte(l))
	return err
}

func (h *PtermHandler) WithAttrs(attrs []slog.Attr) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   append(h.attrs, attrs...),
		group:   h.group,
	}
}

func (h *PtermHandler) WithGroup(name string) slog.Handler {
	return &PtermHandler{
		printer: h.printer,
		attrs:   h.attrs,
		group:   name,
	}
}

type Spinner struct {
	sequence    []string
	mx          sync.Mutex
	log         *slog.Logger
	printer     *pterm.MultiPrinter
	firstStatus string
	status      string
	spinner     *pterm.SpinnerPrinter
}

func NewSpinner(log *slog.Logger) *Spinner {
	h, ok := log.Handler().(*PtermHandler)
	if !ok {
		return &Spinner{
			log: log,
		}
	}
	return &Spinner{
		sequence: []string{`  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `, `  ? `},
		log:      log,
		printer:  h.printer,
	}
}

func (s *Spinner) Update(status string) {
	s.mx.Lock()
	defer s.mx.Unlock()

	// Do not update if status is the same.
	if s.status == status {
		return
	}
	if s.firstStatus == """" {
		s.firstStatus = status
	}
	s.status = status

	// If no printer we log normally.
	if s.printer == nil {
		s.log.Info(status)
		return
	}

	// Create or update the spinner.
	if s.spinner == nil {
		spinner, _ := pterm.DefaultSpinner.WithWriter(s.printer.NewWriter()).WithSequence(s.sequence...).Start(status)
		s.spinner = spinner
		return
	}
	s.spinner.UpdateText(status)
}

func (s *Spinner) Fail() {
	s.mx.Lock()
	defer s.mx.Lock()

	if s.printer == nil {
		return
	}
	s.spinner.Fail(s.firstStatus)
}

func (s *Spinner) Succeed() {
	s.mx.Lock()
	defer s.mx.Unlock()

	if s.printer == nil {
		return
	}
	s.spinner.Success(s.firstStatus)
}
```

The work will most likely have to be split into a couple of steps.

1. Remove any use of message fatal.
2. Refactor table printing functions.
3. Replace message logging with a structured logger.
4. Replace spinner and progress bars."
"## Context

Zarf currently does not have explicit schema versions. Any schema changes are embedded into Zarf and can change with new versions. There are several examples of deprecated keys throughout Zarf's lifespan such as:

- `setVariable` deprecated in favor of `setVariables`
- `scripts` deprecated in favor of `actions`
- `group` deprecated in favor of `flavor`, however these work functionally different and cannot be automatically migrated
- `cosignKeyPath` deprecated in favor of specifying the key path at create time

Zarf has not disabled any deprecated keys thus far. On create the user is always warned when using a deprecated field, however the field still exists in the schema and functions properly. Some of the deprecated keys can be migrated automatically as the old item is a subset or directly related to it's replacement. For example, setVariable is automatically migrated to a single item list of setVariables. This migration occurs on the zarf.yaml fed into the package during package create, however the original field is not deleted from the packaged zarf.yaml because the Zarf binary used in the airgap or delivery environment is not assumed to have the new schema fields that the deprecated key was migrated to.

The release of v1 will provide an opportunity to delete deprecated features that Zarf has warned will be dropped in v1.

Creating a v1 schema will allow Zarf to establish a contract with it's user base that features will be supported long term. When a feature is deprecated in the v1 schema, it will remain usable in the schema for the lifetime of v1.","## Decision 
Zarf will introduce explicit schema versioning starting with the v1 release. The schema version will be defined within the `zarf.yaml` file, allowing Zarf to validate and handle configuration files according to their declared version.

- The initial v1 schema will include all currently supported fields, including those previously marked as deprecated.
- Once a field is deprecated in the v1 schema, it will remain functional and officially supported for the entire lifecycle of v1. No schema fields will be removed or broken during the v1 series.
- Deprecation warnings will continue to be shown when using outdated fields, guiding users toward preferred patterns, but backward compatibility will be fully maintained.
- Future major versions (e.g., v2) may remove deprecated fields and introduce breaking changes, but such changes will require an explicit schema version bump in the configuration file.
- Schema version resolution will be handled by the Zarf CLI, which will parse the declared version and apply appropriate validation rules and transformations.

This approach establishes a clear contract with users: configurations written for v1 will continue to work across all v1 releases, enabling safe upgrades and reducing friction in long-term deployments. It also provides a foundation for more predictable evolution of the Zarf configuration format in future major versions.","## Decision

Zarf will begin having proper schema versions. A top level key, `apiVersion`, will be introduced to allow users to specify the schema. At the release of v1 the only valid user input for `apiVersion` will be v1. Zarf will not allow users to build using the v0 schema. `zarf package create` will fail if the user has deprecated keys or if `apiVersion` is missing and the user will be instructed to run the new `zarf dev migrate-schema` command. `zarf dev migrate-schema` will automatically migrate deprecated fields in the users `zarf.yaml` where possible. It will also add the apiVersion key and set it to v1.

The existing go types which comprise the Zarf schema will be moved to types/alpha and will never change. An updated copy of these types without the deprecated fields will be created in a package types/v1 and any future schema changes will affect these objects. Internally, Zarf will introduce translation functions which will take the alpha schema and return the v1 schema. From that point on, all function signatures that have a struct that is included in the Zarf schema will change from `types.structName` to `v1.structName`.

All deprecated features will cause an error on create. Deprecated features with a direct migration path will still be deployed if the package was created with schema v1, as migrations will add the non deprecated fields. If a feature does not have a direct automatic migration path (cosignKeyPath & groups) the package will fail on deploy. This will happen until the alpha schema is entirely removed from Zarf, which will happen one year after v1 is released.

At create time Zarf will package both a `zarf.yaml` and a `zarfv1.yaml`. If a `zarfv1.yaml` exists Zarf will use that. If a `zarfv1.yaml` does not exist, then Zarf will know that the package was created prior to v1 and use the regular `zarf.yaml`. If the package is deployed with v0 it will read the `zarf.yaml` as normal even if the package has a `zarfv1.yaml`. This will make it simpler to drop deprecated items with migration paths from the v1 schema while remaining backwards compatible as those deprecated items will exist in the `zarf.yaml`

When Zarf introduces new keys that they are not ready to promise long term support for they will mark them as experimental in the schema. A key is assumed to be stable if it's not experimental or deprecated.

There are several other keys Zarf will deprecate which will have automated migrations to new fields
- `.metadata.aggregateChecksum` -> `.build.aggregateChecksum`
- `.components[x].required` -> `.components[x].optional`. Optional will default to false. This is a change in behavior as currently `required` defaults to true. However, automatic migration will work since we can set optional to true on any components which do not already have required.
- Metadata fields `image`, `source`, `documentation`, `url`, `authors`, `vendors` -> will become a map of `annotations`
- `noWait` -> `wait` which will default to true. This change will happen on both `.components.[x].manifests` and `components.[x].charts`
- `yolo` -> `airgap` which will default to true
- `.components.[x].actions.[default/onAny].maxRetries` -> `.components.[x].actions.[default/onAny].retries`
- `.components.[x].actions.[default/onAny].maxTotalSeconds` -> `.components.[x].actions.[default/onAny].timeout`, which must be in a [Go recognized duration string format](https://pkg.go.dev/time#ParseDuration)
- charts will change to avoid [current confusion with keys](https://github.com/defenseunicorns/zarf/issues/2245). Exactly one of the following fields will exist for each `components.[x].charts`.
```yaml
  helm:
    url: https://stefanprodan.github.io/podinfo
    RepoName: podinfo # replaces repoName since it's only applicable for helm repos

  git:
    url: https://stefanprodan.github.io/podinfo
    path: charts/podinfo

  oci:
    url: oci://ghcr.io/stefanprodan/charts/podinfo

  local:
   path: chart
```
- A new field called `.components.[x].healthChecks` will be introduced and will utilize [kstatus](https://github.com/kubernetes-sigs/cli-utils/blob/master/pkg/kstatus/README.md) to wait for resources to be ready after deployment. Read [#2718](https://github.com/zarf-dev/zarf/issues/2718) for more detail.
- a new field `.components.[x].wait` will be introduced. It will default to true, and automatically run `healthchecks` for every object in the cluster.
- `.components.[x].extensions` will be removed in favor of `zarf dev generate bigbang`. See #2875"
