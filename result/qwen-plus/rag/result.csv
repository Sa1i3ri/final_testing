context,decision,predicted_decision
"## Context

Primer React components was originally released without TypeScript type definitions, making it difficult for engineers to consume the library in TypeScript applications. In [July 2019](https://github.com/primer/react/commit/2983c935ea9ad600c04078adb25e40c3624c11fa#diff-7aa4473ede4abd9ec099e87fec67fd57afafaf39e05d493ab4533acc38547eb8), we created an [ambient declaration](https://www.geeksforgeeks.org/typescript-ambients-declaration/) file (`index.d.ts`) file to provide type definitions for TypeScript applications without having to rewrite Primer React components in TypeScript.

`index.d.ts` has been an effective stopgap, enabling teams to build complex applications with Primer React components and TypeScript. However, because `index.d.ts` is disconnected from the implementation code, we've struggled to keep the type definitions up-to-date and accurate, as evidenced by [many](https://github.com/primer/react/issues/906) [TypeScript](https://github.com/primer/react/issues/540) [bug](https://github.com/primer/react/issues/520) [reports](https://github.com/primer/react/issues/534). As the library continues to grow in size and complexity, manually maintaining type definitions will become unsustainable.","## Decision 
## Decision

We will migrate Primer React components to TypeScript by incrementally rewriting the source files from JavaScript to TypeScript. This approach ensures that type definitions are co-located with implementation code, improving accuracy, maintainability, and developer confidence.

The migration will follow these principles:

1. **Incremental Adoption**: Components will be converted file-by-file to minimize risk and allow for continuous integration. Each converted file will include explicit types for props, state, and return values.
2. **Type Safety First**: During conversion, we will prioritize correct and comprehensive typing over syntactic cleanliness, ensuring public APIs are accurately represented.
3. **Remove Ambient Declarations**: Once all components are migrated, the standalone `index.d.ts` file will be removed in favor of generated type declaration files (`*.d.ts`) from the TypeScript compiler.
4. **Enforce Type Checks in CI**: The build process will enforce type checking, preventing new type-related regressions.
5. **Support Mixed Codebase During Transition**: The build system will be configured to handle both `.js` and `.ts/.tsx` files during the transition period using tools like `tsc` with `allowJs: true`.

This decision ensures long-term sustainability of the library in a TypeScript-first ecosystem while reducing the maintenance burden caused by out-of-sync type definitions.","## Decision

We will rewrite Primer React components in TypeScript."
"## Context

Throughout the last few years folks from the Design Infrastructure, Web Systems, and UI Platform teams have discussed the idea of using custom elements for behaviors in Primer React. The main goal of using custom elements in Primer React is to be able to author behaviors once and reuse them in any framework. Several experiments have been conducted which are listed above.","## Decision 
We will use custom elements for behaviors in Primer React to enable framework-agnostic reuse of interactive behaviors. This approach allows us to author behavior once as a custom element and integrate it consistently across different frameworks, improving maintainability and ensuring uniform user experience. We will formalize patterns for creating and consuming these custom elements within Primer React components.",## Decision
"## Context

Our component prop APIs have, at times been a bit of a mess. We've seen:

- Implicit conventions not documented anywhere but consistently reflected in our code (e.g., the type of the `sx` prop)
- Explicit plans to change some of those (e.g., the deprecation of Styled System props)
- Inconsistencies in our implementation (e.g., when components accept a `ref` prop)

This ADR aims to unify some of these conversations about prop APIs, codify our decisions, and sequence the work to get there.","## Decision 
## Decision

We standardize component prop APIs with the following guidelines:

- **Use explicit, documented prop interfaces**: All component props must be explicitly typed and documented in a shared TypeScript definition file or interface.
- **Deprecate Styled System props**: We formally deprecate the use of Styled System shorthand props (e.g., margin, padding props as strings) in favor of explicit `sx` or `style` objects that align with CSS-in-JS best practices.
- **Standardize `sx` prop typing**: The `sx` prop must conform to a unified `SxProps` type that supports theme-aware, responsive, and accessible styling across all components.
- **Ref forwarding via `ref`**: Only stateful or container components (e.g., `Input`, `Modal`) should accept a `ref` prop, and it must be implemented using React’s `forwardRef`. Stateless components should not expose `ref`.
- **Enforce consistency via linting**: Introduce ESLint rules to flag non-compliant prop usage and deprecated patterns.

These decisions will be rolled out in phases, starting with documentation updates, followed by codemods for deprecation warnings, and eventual removal of legacy patterns in the next major version.",## Decision
"## Context

In Primer React and consuming applications, we use many different patterns for creating React components. Two common patterns are:

1. Creating components with styled-components

   ```tsx
   const Avatar = styled.img.attrs<StyledAvatarProps>(props => ({
     height: props.size,
     width: props.size,
   }))<StyledAvatarProps>`
     display: inline-block;
     overflow: hidden;
     line-height: ${get('lineHeights.condensedUltra')};
     border-radius: ${props => getBorderRadius(props)};
     ${sx}
   `
   ```

   [Show full code example ÃÂ¡ÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

2. Creating components with Box

   ```tsx
   const Avatar: React.FC<AvatarProps> = ({size = 20, alt = '', square = false, sx = {}, ...props}) => {
     const styles: BetterSystemStyleObject = {
       display: 'inline-block',
       overflow: 'hidden',
       lineHeight: 'condensedUltra',
       borderRadius: getBorderRadius({size, square}),
     }

     return (
       <Box
         as=""img""
         alt={alt}
         sx={merge<BetterSystemStyleObject>(styles, sx)} // styles needs to merge with props.sx
         {...props}
       />
     )
   }
   ```

   [Show full code example ÃÂ¡ÃÂº](https://github.com/primer/react/pull/2019/files?diff=split&w=0)

&nbsp;","## Decision 
## Decision

We will standardize on using **Box with `sx` prop** as the preferred pattern for creating React components in Primer React and consuming applications.

### Rationale:
- **Consistency**: Using Box provides a uniform API across components, making code easier to read and maintain.
- **Theming Support**: The `sx` prop integrates seamlessly with our design system and theme object, enabling consistent spacing, typography, and responsive styles.
- **Flexibility**: Box supports the `as` prop, allowing us to change the underlying HTML element without rewriting styling logic.
- **Reduced Complexity**: Avoids the need for styled-components' template literals and attribute forwarding, reducing cognitive load and potential bugs.
- **Easier Composition**: Merging styles via `merge` or direct `sx` object composition is more predictable than CSS-in-JS cascade behavior.

### Migration Plan:
- New components will be implemented using the Box pattern.
- Existing styled-components will be migrated incrementally during component updates or refactors.
- Documentation and component templates will reflect the new standard.

Styled-components usage will be discouraged for new development unless there is a compelling reason (e.g., complex dynamic styling not feasible with `sx`).","## Decision

Prefer using method #2: Creating components with Box for the following reasons:

- Better authoring experience with Typescript. With Box, we can improve the API and autocomplete for consuming primitives. [See research](https://github.com/github/primer/discussions/755#discussioncomment-2318144)
- The styling library (i.e. styled-components) becomes an implementation detail and can be changed later with minimal breaking changes for consumers. (Avoids leaky abstractions)
- We have had issues with exporting types, we can increase confidence by keeping the exported types close to what we author.

See diff for moving Avatar from approach 1 to 2: https://github.com/primer/react/pull/2019/files?diff=split&w=0"
,Please provide the context for which you'd like me to generate an Architecture Decision Record (ADR) or decision.,"## Decision:

1. Prefer using children for ÃÂ¡ÃÂ°contentÃÂ¡ÃÂ±

2. For composite components, the API should be decided by how much customisation is available for children.

For components that have design decisions baked in, should use strict props. For example, the color of the icon inside a Button component is decided by the `variant` prop on the Button. The API does not allow for changing that.

```jsx
<Button variant=""danger"" leadingIcon={TrashIcon}>
  Delete branch
</Button>
```

On the other hand, if we want consumers to have more control over children, a composite API is the better choice.

```jsx
<ActionList.Item>
  <ActionList.LeadingVisual>
    <Avatar src=""https://github.com/mona.png"" />
  <ActionList.LeadingVisual>
  mona
</ActionList.Item>
```"
"## Context

As we work on maturity of our components, we will sometimes need to build components in a parallel track/bundle without breaking existing components. Eventually, the new components would replace the old ones in the main bundle.","## Decision  
## Decision

We will adopt a dual-bundle development strategy to enable parallel evolution of components without disrupting existing functionality. New or refactored components will be developed and shipped in a separate, opt-in bundle (e.g., `components-next` or `experimental`). This allows teams to adopt matured components incrementally while the stable main bundle remains unchanged.

The experimental bundle will:
- Coexist with the main component bundle
- Be versioned independently but aligned with the main release cycle
- Include clear documentation and deprecation warnings when components are marked for promotion
- Be built and tested using the same tooling and quality gates as the main bundle

Once components reach sufficient maturity, stability, and adoption, they will be promoted into the main bundle with proper deprecation cycles for the legacy versions. This approach ensures backward compatibility, reduces risk, and supports a smooth migration path.",
"## Context

`Drafts` were introduced in [ADR-006](./adrs/adr-006-drafts.md) as a way of creating components outside of the root bundle of `@primer/react`. This helps us create, test and maintain multiple versions of the same component in parallel.

Example:

```js
// default version of component exported from root of package
import {UnderlineNav} from '@primer/react'

// new version of component exported from /drafts
import {UnderlineNav} from '@primer/react/drafts'
```

We have also used this method for components that are still a work in progress and we don't want developers to start using them in production without collaborating with us first.

Example:

```js
// this component only exists in drafts and not in the roof of the package
import {TreeView} from '@primer/react/drafts'
```","## Decision 
## Decision

We will continue using the `drafts` directory to:

1. Develop and test new versions of existing components in parallel with the stable versions.
2. Host experimental or work-in-progress components that are not yet ready for public production use.

Access to components within `@primer/react/drafts` is intended for internal development, early feedback, and collaboration only. These components are not considered part of the public API and may change or be removed without following semver.

We will document the purpose and instability of the `drafts` directory clearly in the project README and contribution guidelines to set appropriate expectations.","## Decision

We should drop the `draft` label and use `experimental` instead.

While not completely accurate (UnderlineNav2 is much more polished than `experimental` maturity), the lack of confusion makes it a good trade-off.

Fine-grained information about component maturity, useful for maintainers, is also communicated by the [status checklist in the component documentation](https://primer.style/react/drafts/UnderlineNav2#status)."
"## Context

[Snapshot testing](https://jestjs.io/docs/snapshot-testing) is a strategy that allows a developer to ""snapshot"" a given value within a test suite. This value could be a primitive within JavaScript, an Array, an object, or the resulting HTML of rendering a component in React. The snapshot of the value is stored within the repo and is then compared against future test runs.

When a change is detected to a snapshot, the test will fail with a diff between the currently stored snapshot and the new snapshot. Within the space of React components, this may happen when a class name changes or the markup of a component changes. Is is then up to the developer to decide if the change is intentional and whether or not to accept this new snapshot as the baseline for future test runs.

A common practice is to snapshot a React component which will capture the HTML output of rendering a component. For example:

```jsx
import renderer from 'react-test-renderer'
import Link from '../Link'

it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.facebook.com"">Facebook</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

```js
exports[`renders correctly 1`] = `
<a
  className=""normal""
  href=""http://www.facebook.com""
  onMouseEnter={[Function]}
  onMouseLeave={[Function]}
>
  Facebook
</a>
`
```

[Source](https://jestjs.io/docs/snapshot-testing#snapshot-testing-with-jest)

As the number of snapshots within a project grows, there are a couple of challenges that emerge:

- Components with wide or deep trees emit large snapshots
- Debugging what change lead to a broken snapshot can be difficult
- It is unclear what the intent may be of a snapshot test when something does fail, in other words it does not always answer the question: what is this testing?","## Decision  
## Decision  
We will limit the use of snapshot testing to cases where the intent is clear and the output is stable, predictable, and concise. Snapshot tests should not be used as a default testing strategy for components or complex objects.

Instead, we prefer explicit assertions that test specific behavior or output, as they are more maintainable, easier to understand, and clearly communicate the expected outcome.

When snapshot testing is used:
- It must be justified by a clear need (e.g. verifying structure of configuration objects, ensuring no unintended regressions in public API shape).
- Snapshots must be small and focused—avoid capturing large or deeply nested structures.
- Component snapshots (e.g. rendered HTML) should only be used sparingly and never replace functional or behavioral tests.

Developers are responsible for reviewing and understanding every change to a snapshot before committing it. Blindly updating snapshots (`-u`) is discouraged and subject to review.","## Decision

- Avoid using ""catch-all"" snapshot tests for React components
- Snapshots may be used for static objects which have a clear signal as to what to do if they are modified (such as the exports of a package)

<table>
<thead><tr><th>Unpreferred</th><th>Preferred</th></tr></thead>
<tbody>
<tr><td>

```tsx
it('renders correctly', () => {
  const tree = renderer.create(<Link page=""http://www.github.com"">GitHub</Link>).toJSON()
  expect(tree).toMatchSnapshot()
})
```

</td><td>

```tsx
it('renders an element with role=""link""', () => {
  render(<Link page=""http://www.github.com"">GitHub</Link>)
  expect(screen.getByRole('link', {name: 'GitHub'})).toBeInTheDocument()
})
```

</td></tr>
</tbody></table>"
"## Context

There are situations where we would like to provide warnings to developers who
use `@primer/react` that something may be deprecated, unsupported, etc. Often,
these will be emitted using `console.warn()`. When using `console.warn()` by
itself, we run into a situation where code that is only meant for development
is included in production code. As a result, it would be helpful to establish
patterns around how to provide warnings to developers in order to make sure
that:

- Calls to `console.warn()` do not appear in production
- Code related to development checks, warnings, or messages are removed from
  production code","## Decision 
## Decision
We will use the `tiny-warning` npm package to conditionally emit warnings during development while ensuring that warning code is stripped out in production builds. This approach guarantees that:

- Warnings are only shown in development mode.
- All warning-related code, including condition checks and `console.warn()` calls, is completely removed from production bundles via dead code elimination (e.g., through webpack/UglifyJS/Terser).
- Developers receive informative feedback when using deprecated or unsupported features without impacting production bundle size or performance.","## Decision

Code that is meant for development-only warnings or checks **must** be wrapped within a
`__DEV__` block.

```tsx
function ExampleComponent() {
  if (__DEV__) {
    // This code only runs in development
  }
}
```

Under-the-hood, the `__DEV__` block will be compiled to a `NODE_ENV` check so
that it is stripped when `NODE_ENV` is set to `'production'`.

> **Note**
> Contributors may wrap hooks within a `__DEV__` block even though hooks are not
> meant to be called conditionally. This is because the `__DEV__` check can be
> considered constant in that it will always be true or false for the
> environment (development or production).

When a contributor would like to communicate a warning to a developer, they
should use the `warning()` helper.

```ts
warning(condition, 'This is the message that is logged when condition is truth-y')
```

This helper allows you to provide a `condition`. When the condition is truth-y
it will emit the message provided. This helper is automatically wrapped in a
`__DEV__` block and will be removed from production builds.

For more complex conditions, a contributor may combine `console.warn()` with
`__DEV__` when `warning()` does not suffice."
"## Context

Our testing infrastructure consists of Jest ""unit"" tests and automatic Chromatic visual testing. Both of these are super useful but we have been experiencing a few setbacks.
Unofficially we test more complex interactive scenarios using our storybooks setup.

1. Repeated markup in storybooks and jest tests - Since there is not automation of interaction in storybooks, we tend to test them out manually in stories and then transfer to jest. This results in us constructing similar test case scenarios in storybooks and in tests.

2. Debugging complex jest tests - There is no visual aspect to jest tests. Mostly everything is CLI based. This makes it hard to debug broken tests. Visibility into these tests is also low for people who want to enhance them.","## Decision 
## Decision

We decided to adopt Playwright for end-to-end and component testing to unify our testing approach and improve developer experience. Playwright allows us to:

- Reuse component stories as test fixtures, reducing duplication between Storybook and Jest tests.
- Run interactive, browser-based tests that validate both functionality and appearance, enabling automated testing of complex user flows previously only testable manually.
- Visually debug tests through built-in capabilities like live debugging, screenshots, videos, and trace recordings.
- Replace brittle or overly complex Jest DOM tests with more resilient, human-readable interaction patterns.
- Integrate with Chromatic for visual regression testing in a single pipeline.

This shift enables developers to write tests that are easier to understand, maintain, and verify visually—improving collaboration across engineering and design.","## Decisions

Going forward, we can use [storybook's play functions](https://storybook.js.org/docs/react/writing-stories/play-function) to write interaction tests for our components. Here's a document that describes [how to add and run these tests](../storybook-tests.md).
Guidelines to make this easier -

1. Create a new storybook file for your components and call them ""interactions"". Inside these files, we can import the stories from ""examples"" or ""fixtures"" and tack on play functions to them. This will create duplicate stories but with the added benefit of the tests appearing in the interactions add-on tab. This tab will let you step through your tests step by step, increasing visibility and debuggability.

2. Port complex jest tests onto interactive tests. This is up to developer discretion. My recommendation is to have basic ""unit"" tests like component existence and axe tests in jest. Anything that can benefit from a visual aspect to it can be moved to interaction tests.

3. Interaction tests are not perfect. Some aspects like mocking can be challenging and we will find solutions to these as when the need arises."
"## Context

While [ADR 002](https://github.com/primer/react/blob/main/contributor-docs/adrs/adr-002-behavior-isolation.md) landed on the decision to:

- Share JavaScript behaviour as ""vanilla"" functions which can be used between Primer View Components (PVC) and Primer React (PRC), consumed via React Hooks in PRC.
- Not use Custom Elements to drive behaviour of components.

Our perspective on the ADR has changed since its approval and this document will address those changes.","## Decision 
## Decision Outcome

Chosen option: ""Revisit and update ADR 002 to reflect current perspective"", because

* The original decision no longer aligns with evolving technical requirements and ecosystem trends.
* Maintaining outdated ADRs creates confusion for contributors and inconsistent implementation across Primer React and Primer View Components.
* There is renewed interest in leveraging Custom Elements for better encapsulation, interoperability, and framework-agnostic behaviour—especially as browser support and tooling have improved since the ADR was written.
* Clarifying the updated approach ensures alignment across teams and future-proofs component architecture.

This update will include:
- Evaluation of Custom Elements for behaviour isolation.
- Revised guidance on sharing logic between PVC and PRC.
- Deprecation plan for existing vanilla function patterns where appropriate.",## Decision
"## Context

> tl;dr
>
> We have workspaces within the project but no official way of connecting them or managing dependencies from the top-level. Similarly, scripts that grow in size or complexity have no dedicated home or ability to manage dependencies outside of the top-level workspace.

The `primer/react` repo is composed of several workspaces that use `npm` to manage dependencies. In order to install these dependencies, a contributor needs to visit each directory and run `npm install`. The project also has a convenience script for this task, invoked through `npm run setup`, that goes through this process for contributors.

These distinct workspaces may sometimes depend on each other. For example, the `docs` workspace depends on the `@primer/react` package which is defined at the top-level of the project. Similarly, examples in the `examples/*` folders may also depend on `@primer/react`. Currently these dependencies are expressed either through relative paths, file dependencies in `package.json`, or symlinks through `npm link`.

There is also a collection of scripts under the `script` directory that exist in the project that involve varying levels of complexity and depth.","## Decision 
## Decision

Adopt a monorepo tool such as `pnpm` with workspaces to manage dependencies and scripts holistically across the project.

This tool will enable:
- A single, top-level command (`pnpm install`) to install all dependencies across all workspaces.
- Proper versioned or symlinked inter-dependencies between workspaces (e.g., `docs` depending on `@primer/react`) using workspace protocols (e.g., `""workspace:*""`).
- Elimination of manual `npm link` or relative path hacks.
- Dedicated management of shared dev tools and scripts via the root `package.json`.
- Creation of reusable, isolated scripts under `scripts/` with their own dependencies if needed, managed through the same pnpm workspace.

Additionally, move complex scripts into their own workspace (e.g., `tools/*`) with individual `package.json` files to manage their specific dependencies, enabling better maintainability and testability.

This improves developer experience, ensures consistency, and scales with project growth.","## Decision

Setup the project to use [`npm` workspaces](https://docs.npmjs.com/cli/v7/using-npm/workspaces). This will require changing the project to have a top-level `package.json` file which will use `workspaces` to reference child workspaces. The layout for the project would then be:

```
- root
  # Root package.json, defines workspaces
  - package.json

  # Workspaces
  - docs
  - packages
    - react
    - *
  - examples
    - nextjs
    - consumer-test
    - *

  # Top-level scripts
  - script

  # Testing & Development
  - .storybook
  - .playwright

  # Repo config
  - .github
  - .vscode
```

The following items will be installed and ran at the root-level:

- Linting
  - `eslint`
  - `markdownlint`
- Testing
  - Jest
  - Playwright
- Development
  - Storybook
- Type checking
  - TypeScript

Certain configuration items, like for TypeScript or Babel, may need configuration defined at the top-level and extended by child workspaces.

The current `@primer/react` package will move to the `packages/react` package. Workspaces that depend on `@primer/react` should reference the version of that package."
"## Context

Currently all files live under the `src` directory. In the `npm` package for `@primer/react`, we specify the following export pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/*': {
      import: [
        // ...
      ],
      require: [
        // ...
      ],
    },
  },
}
```

This pattern, along with our Rollup setup, opts-in files and folders under the `src` directory into the public API of the package. This means that certain parts of the codebase which aren't intended to be used outside of `@primer/react` are considered part of its public API and can be imported and used by consumers.","## Decision 
## Decision
Restructure the `src` directory to separate internal and public code by introducing a dedicated `internal` folder for implementation details not intended for public use. Update the package exports configuration to only include explicitly intended entry points under `./lib-esm/*`, ensuring that files within `internal` are excluded from the public API. This prevents accidental exposure of internal modules and strengthens encapsulation.","## Decision

Adopt a convention in Primer React where internal modules live in the `src/internal` folder. This folder which would include all components, hooks, and other modules which are not intended for usage outside of the project. Files within `src/internal` may be grouped by area, such as `src/internal/components`, `src/internal/hooks`, etc.

In the `""exports""` field of our `npm` package, we can then add the following pattern:

```json5
{
  exports: {
    // ...
    './lib-esm/internal/*': null,
  },
}
```

This pattern would remove any files and folders within `src/internal` from the public API of the `npm` package. This pattern is inspired by [this section](https://nodejs.org/api/packages.html#package-entry-points) from Node.js, specifically this example:

```json
{
  ""name"": ""my-package"",
  ""exports"": {
    ""."": ""./lib/index.js"",
    ""./feature/*.js"": ""./feature/*.js"",
    ""./feature/internal/*"": null
  }
}
```"
"## Context

> **Note**
> This ADR supercedes [`ADR 009`](./adr-009-interaction-tests.md)

We author tests using a combination of Jest and Playwright. Typically, Jest is
used for unit and integration tests and Playwright is used for Visual Regression
Tests against our Storybook.

In `ADR 009`, we proposed using Storybook's [`play`
functions](https://storybook.js.org/docs/react/writing-stories/play-function) as
the recommended way to author tests that require tests based on user
interactions. In this ADR, we would like to revisit this decision.","## Decision 
## Decision
After further evaluation, we are deprecating the use of Storybook's `play` functions for interaction tests. While `play` functions offer a convenient way to define user interactions within stories, they blur the line between development aids and test code, making it harder to maintain clear testing boundaries.

Instead, we are standardizing on using Playwright for all user interaction and end-to-end testing scenarios. Interaction tests should be written as dedicated Playwright test files (`.e2e.spec.ts`) that run against rendered components in Storybook or actual application pages. This ensures tests are explicit, isolated, and run in a controlled environment with full access to Playwright’s assertion and debugging capabilities.

Jest will remain responsible for unit and integration tests only, while Playwright becomes the single source of truth for any test involving user actions, browser behavior, or visual regression.","## Decision

For tests involving interaction, use Playwright to visit a storybook story and
interact with the component directly. This will allow for easy integration with
Visual Regression Testing and our automated accessibility testing (through axe).
In order to perform interactions, use [actions from
Playwright](https://playwright.dev/docs/input)."
"## Context

There are situations where a component prop can support a value or a ""responsive
value"" that results in the value changing depending on the viewport size. For
example:

```tsx
// Value
<Stack gap=""condensed"" />

// Responsive value
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' />
```

Typically, a responsive value will support all of our named breakpoints: narrow,
regular, and wide. Authors can use a hook called `useResponsiveValue` to support
resolving the value of a prop based on the current viewport size. The
implementation of this hook uses [`matchMedia`](https://developer.mozilla.org/en-US/docs/Web/API/Window/matchMedia)
which, unfortunately, has a downside: the value of a prop may shift when the
component is server-side rendered. If the prop is used for styles or layout, then this will lead to a layout shift
when the component hydrates and the viewport size is different than the fallback
size on the server.

As a result, we would like to offer a paved path for providing responsive props
in a way tha twill not lead to layout shift during server-side rendering.","## Decision 
## Decision
We will adopt a CSS-first approach for responsive props to prevent layout shifts during server-side rendering. Instead of resolving responsive values on the client using `useResponsiveValue` and `matchMedia`, we will generate responsive utility classes that handle value changes based on media queries.

For example, instead of passing a responsive object to a prop like `gap`, authors will use dedicated class names or shorthand token syntax that maps to predefined CSS classes:

```tsx
// Preferred: Uses CSS classes that respond to breakpoints without JS
<Stack className=""gap-condensed@narrow gap-normal@regular gap-spacious@wide"" />

// Or via a transformed prop using design tokens (compiled to same result)
<Stack gap={{ narrow: 'condensed', regular: 'normal', wide: 'spacious' }} />
```

Under the hood, the latter will be statically analyzed or transformed at build time into the corresponding utility classes, ensuring consistent server and client output.

This approach eliminates reliance on `matchMedia` for initial render, avoids hydration mismatches, and ensures stable SSR output. The `useResponsiveValue` hook will be deprecated for layout-affecting props and only allowed in cases where dynamic behavior is essential and does not impact layout stability.","## Decision

Authors should use data attributes to bridge between responsive values in
JavaScript and the corresponding styles in CSS. Authors should avoid using
JavaScript-driven mechanisms for controlling responsive values in order to
prevent layout shift during SSR.

For example, consider a `Stack` component that has a `gap` prop. This prop can
receive a plain value or one that is a responsive value. To support this,
authors should use the following patterns:

```tsx
type GapScale = 'none' | 'condensed' | 'normal' | 'spacious'

type StackProps = React.PropsWithChildren<{
  // Responsive props should be a union of the value of the prop plus
  // `ResponsiveValue` over that value
  gap?: GapScale | ResponsiveValue<GapScale>
}>

function Stack({children, gap}: StackProps) {
  // The `getResponsiveAttributes` helper is useful for adding the corresponding
  // `data-*` attributes to the container for the compnoent.
  //
  // In this case, it will add `data-gap` and `data-gap-{breakpoint}` attributes
  // with the corresponding value set
  return <StyledStack {...getResponsiveAttributes('gap', gap)}>{children}</StyledStack>
}

const StyledStack = styled.div`
  &[data-gap='none'],
  &[data-gap-narrow='none'] {
    /* ... */
  }

  &[data-gap='condensed'],
  &[data-gap-narrow='condensed'] {
    /* ... */
  }

  /* ... */

  // @custom-media --viewportRange-regular
  @media (min-width: 768px) {
    &[data-gap-regular='none'] {
      /* ... */
    }

    &[data-gap-regular='condensed'] {
      /* ... */
    }

    /* ... */
  }

  // @custom-media --viewportRange-wide
  @media (min-width: 1400px) {
    &[data-gap-wide='none'] {
      /* ... */
    }

    &[data-gap-wide='condensed'] {
      /* ... */
    }

    /* ... */
  }
`
```

By default, this approach uses the following conventions:

- Props which may have responsive values should have a value that is
  serializable, ideally as a `string` or `boolean`
- Props will be provided on the component as a data attribute, using the format:
  `data-{propName}`
- For responsive values, the breakpoint name will be appended to the data
  attribute name specified above. For example: `data-{propName}-{breakpoint}`

Data attributes will act as a bridge between JavaScript and CSS. In CSS, authors
should use these data attributes to apply styles based on the value of the
attribute per-breakpoint.

> [!NOTE]
> There is a utility named `getResponsiveAttributes` that can be used to generate
> the corresponding `data-*` attributes for the props which may have responsive
> values."
"## Context

There are times when a prop is deprecated in favor of a more accessible API or a more flexible prop that can support a wider range of values. In these cases, we want to provide a clear path for authors to migrate from the deprecated prop to the new API.

For
example:

```tsx
// old API
<ActionList>
  <ActionList.Group title=""Group title"">
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```

```tsx
// new API
<ActionList>
  <ActionList.Group>
    <ActionList.GroupHeading as=""h2"">Group title</ActionList.GroupHeading>
    <ActionList.Item>Item 1</ActionList.Item>
    <ActionList.Item>Item 2</ActionList.Item>
  </ActionList.Group>
</ActionList>
```","## Decision 
## Decision

We will deprecate props and components that are superseded by more accessible or flexible alternatives by following a structured deprecation strategy:

1. **Introduce the New API First**: Ensure the replacement API (e.g., `ActionList.GroupHeading`) is fully implemented, documented, and available before marking anything as deprecated.

2. **Mark Deprecated Props/Components with Warnings**: When a deprecated prop or component (e.g., the `title` prop on `ActionList.Group`) is used, emit a runtime warning in development mode guiding users to the new API. The warning will include:
   - A clear message indicating the deprecation.
   - The recommended alternative.
   - A link to migration documentation.

3. **Maintain Backward Compatibility**: The deprecated API will continue to function during the deprecation period to avoid breaking changes.

4. **Document the Migration Path**: Provide clear documentation outlining:
   - Why the change was made (e.g., improved accessibility, flexibility).
   - Side-by-side examples of old vs. new usage.
   - Instructions for updating code.

5. **Remove in Next Major Version**: The deprecated prop or component will be removed in the next major version release, with proper communication in the changelog and migration guide.

This approach ensures a smooth transition for developers while allowing us to evolve the API toward better practices.","## Decision

1. Add the `@deprecated` TS notice, if possible, to the corresponding type
2. Mark the prop as deprecated in the component's documentation
3. Add an eslint rule or extend an existing one to warn against using the deprecated prop
4. Update the `primer/react` eslint plugin at github/github and evaluate the impact of the deprecation
5. If the change is minimal and safe, proceed with the auto fix of the eslint rule and create PRs by codeowners
6. If the change is large, add an eslint disable to each instance of the deprecated prop at github/github and make sure they show up on the accessibility score cards so that we can track the usage of the deprecated prop
7. Enable the eslint rule afterwards in github/github so that the new instances of the deprecated prop are heavily discouraged"
"## Context

Compilation failures during a CI build should surface good error messages.

For example, the actual compile errors from the typescript compiler should bubble as issues in the UI. And not simply ""tsc exited with exit code 1"".

VSCode has an extensible model for solving this type of problem. VSCode allows users to configure which [problems matchers](https://code.visualstudio.com/docs/editor/tasks#_defining-a-problem-matcher) to use, when scanning output. For example, a user can apply the `tsc` problem matcher to receive a rich error output experience in VSCode, when compiling their typescript project.

The problem-matcher concept fits well with ""setup"" actions. For example, the `setup-nodejs` action will download node.js, add it to the PATH, and register the `tsc` problem matcher. For the duration of the job, the `tsc` problem matcher will be applied against the output.","## Decision 
## Decision
We will leverage GitHub Actions' problem matcher feature to surface compilation errors clearly during CI builds. By using a `tsc`-compatible problem matcher, TypeScript compiler errors will be parsed and displayed as actionable annotations in the GitHub UI, rather than generic exit code failures. This will be implemented through a setup step (e.g., using `actions/setup-node` or a custom matcher registration) to ensure compile-time issues are visible, navigable, and traceable directly in pull requests.",## Decision
"## Context
run-actions run scripts using a platform specific shell:
`bash -eo pipefail` on non-windows, and `cmd.exe /c /d /s` on windows

The `shell` option overrides this to allow different flags or completely different shells/interpreters

A small example is:
```yml
jobs:
  bash-job:
    actions:
    - run: echo ""Hello""
      shell: bash
  python-job:
    actions:
    - run: print(""Hello"")
      shell: python {0}
```","## Decision 
## Decision

The `shell` option must support placeholder `{0}`, which is replaced with the path to the temporary script file containing the action's commands.

- On non-Windows platforms, the default shell is `bash -eo pipefail {0}`.
- On Windows, the default shell is `cmd.exe /d /c /s {0}`.
- When `shell` is specified (e.g., `bash`, `python {0}`, `sh -c`), it fully replaces the default command, and must include `{0}` if the script file should be passed as an argument.

This allows maximum flexibility for using alternative interpreters or custom shell flags while maintaining backward compatibility.","## Decision

___"
"## Context

User wants to reference workflow variables defined in workflow yaml file for action's input, displayName and condition.","## Decision 
We allow referencing workflow variables in the action's `input`, `displayName`, and `condition` fields using a `${{ }}` syntax. This enables dynamic configuration of actions based on workflow-level context such as environment, branch, or user-defined parameters.

For example:
```yaml
actions:
  - name: Deploy Application
    displayName: ${{ vars.ENVIRONMENT }} deployment for ${{ github.ref }}
    input:
      environment: ${{ vars.ENVIRONMENT }}
      image_tag: ${{ vars.IMAGE_TAG }}
    condition: ${{ vars.DEPLOY_ENABLED == 'true' }}
```

The variables are resolved at workflow execution time. If a referenced variable is undefined, the resolution fails early with a clear error message indicating the missing variable.",## Decision
"## Context
First party action `actions/cache` needs a input which is an explicit `key` used for restoring and saving the cache. For packages caching, the most common `key` might be the hash result of contents from all `package-lock.json` under `node_modules` folder.
  
There are serval different ways to get the hash `key` input for `actions/cache` action.

1. Customer calculate the `key` themselves from a different action, customer won't like this since it needs extra step for using cache feature
```yaml
  steps:
  - run: |
      hash=some_linux_hash_method(file1, file2, file3)
      echo ::set-output name=hash::$hash
    id: createHash
  - uses: actions/cache@v1
    with:
      key: ${{ steps.createHash.outputs.hash }}
``` 

2. Make the `key` input of `actions/cache` follow certain convention to calculate hash, this limited the `key` input to a certain format customer may not want.
```yaml
  steps:
  - uses: actions/cache@v1
    with:
      key: ${{ runner.os }}|${{ github.workspace }}|**/package-lock.json
```","## Decision 
## Decision

We decided to implement a solution where the `actions/cache` action automatically computes the cache key based on specified file patterns (e.g., `**/package-lock.json`) and environment context (e.g., `runner.os`) without requiring users to manually generate keys.

This approach strikes a balance between usability and flexibility:
- It avoids forcing users into extra steps by abstracting hash computation within the action.
- It supports common use cases (like caching `node_modules`) through declarative, convention-based syntax.
- It allows customization via explicit key composition while defaulting to secure, deterministic hashing of specified files.

Example usage:
```yaml
steps:
  - uses: actions/cache@v1
    with:
      path: node_modules
      key: ${{ runner.os }}-npm-cache-${{ hashFiles('**/package-lock.json') }}
```

The `hashFiles` function is provided as a built-in expression to compute file hashes consistently, making it easy for users to construct stable, content-aware keys without external steps.",## Decision
"## Context

Command echoing as a default behavior tends to clutter the user logs, so we want to swap to a system where users have to opt in to see this information.

Command outputs will still be echoed in the case there are any errors processing such commands. This is so the end user can have more context on why the command failed and help with troubleshooting.

Echo output in the user logs can be explicitly controlled by the new commands `::echo::on` and `::echo::off`. By default, echoing is enabled if `ACTIONS_STEP_DEBUG` secret is enabled, otherwise echoing is disabled.","## Decision  
## Decision

Change the default command echoing behavior to be **disabled** unless explicitly opted in.

Command echoing will now follow these rules:
- By default, commands are **not echoed** in logs.
- If the `ACTIONS_STEP_DEBUG` secret is enabled, echoing is **enabled by default** for debugging purposes.
- Users can explicitly control echoing using the new built-in commands:
  - `::echo::on` – enables command echoing
  - `::echo::off` – disables command echoing
- Regardless of echoing settings, **all command output (stdout/stderr) will be echoed if a command fails**, ensuring users have sufficient context for troubleshooting failures.

This change reduces log noise while preserving diagnostic information when needed.","## Decision
- The only commands that currently echo output are
  - `remove-matcher`
  - `add-matcher`
  - `add-path`
- These will no longer echo the command, if processed successfully
- All commands echo the input when any of these conditions is fulfilled:
  1. When such commands fail with an error
  2. When `::echo::on` is set
  3. When the `ACTIONS_STEP_DEBUG` is set, and echoing hasn't been explicitly disabled with `::echo::off`
- There are a few commands that won't be echoed, even when echo is enabled. These are (as of 2019/11/04):
   - `add-mask`
   - `debug`
   - `warning`
   - `error`
- The three commands above will not echo, either because echoing the command would leak secrets (e.g. `add-mask`), or it would not add any additional troubleshooting information to the logs (e.g. `debug`). It's expected that future commands would follow these ""echo-suppressing"" guidelines as well. Echo-suppressed commands are still free to output other information to the logs, as deemed fit."
"## Context

The Runner registers a number of Value Encoders, which mask various encodings of a provided secret. Currently, we register a 3 base64 Encoders:
- The base64 encoded secret
- The secret with the first character removed then base64 encoded
- The secret with the first two characters removed then base64 encoded

This gives us good coverage across the board for secrets and secrets with a prefix (i.e. `base64($user:$pass)`).

However, we don't have great coverage for cases where the secret has a string appended to it before it is base64 encoded (i.e.: `base64($pass\n))`). 

Most notably we've seen this as a result of user error where a user accidentally appends a newline or space character before encoding their secret in base64.","## Decision 
## Decision

To improve detection coverage for commonly misformatted secrets, we will extend the set of registered base64 Value Encoders in the Runner to include variations where common trailing characters (such as newline `\n`, carriage return `\r`, and space ` `) are appended to the secret before base64 encoding.

Specifically, the following encoders will be added:
- Secret + `\n` → base64
- Secret + `\r` → base64
- Secret + `\r\n` → base64
- Secret + space (` `) → base64

These additions address real-world cases stemming from user input errors during manual encoding, especially in shell or configuration file contexts where trailing whitespace is easily introduced.

The change maintains consistency with our existing strategy of proactively covering likely encoding variants, without introducing significant performance overhead. The expanded encoder set will be documented as part of the Runner's value masking logic.",## Decision
"## Context

- Provide a mechanism in the runner to include extra information in `Set up job` step's log.
  Ex: Include OS/Software info from Hosted image.","## Decision 
## Decision

Introduce a new phase in the runner execution lifecycle, named `logEnvironment`, which runs at the beginning of the `Set up job` step. This phase will collect and log relevant environment metadata such as operating system details, software versions, and image information from the hosted runner.

The collected data will be formatted in a structured, human-readable way and appended to the job setup logs. This ensures transparency and aids in debugging by providing immediate context about the execution environment without requiring additional steps or scripts in the user workflow.","## Decision

The runner will look for a file `.setup_info` under the runner's root directory, The file can be a JSON with a simple schema.
```json
[
  {
    ""group"": ""OS Detail"",
    ""detail"": ""........""
  },
  {
    ""group"": ""Software Detail"",
    ""detail"": ""........""
  }
]
```
The runner will use `::group` and `::endgroup` to fold all detail info into an expandable group.

Both [virtual-environments](https://github.com/actions/virtual-environments) and self-hosted runners can use this mechanism to add extra logging info to the `Set up job` step's log."
"## Context

This ADR proposes adding `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context.

This allows downstream a step to run based on whether a previous step succeeded or failed.

Reminder, currently the steps contains `steps.<id>.outputs`.","## Decision 
## Decision Outcome

Chosen option: ""Add `steps.<id>.outcome` and `steps.<id>.conclusion` to the steps context"", because it enables more flexible workflow control based on step results.","## Decision

For steps that have completed, populate `steps.<id>.outcome` and `steps.<id>.conclusion` with one of the following values:

- `success`
- `failure`
- `cancelled`
- `skipped`

When a continue-on-error step fails, the outcome will be `failure` even though the final conclusion is `success`."
"## Context

Since configuring self-hosted runners is commonly automated via scripts, the labels need to be able to be created during configuration.  The runner currently registers the built-in labels (os, arch) during registration but does not accept labels via command line args to extend the set registered.

See Issue: https://github.com/actions/runner/issues/262

This is another version of [ADR275](https://github.com/actions/runner/pull/275)","## Decision 
## Decision Outcome

Chosen option: ""Support command-line arguments for custom labels during runner registration"", because

- Enables automation scripts to fully configure runners without requiring post-registration label updates
- Aligns with infrastructure-as-code practices where all configuration is declared upfront
- Reduces setup complexity by eliminating the need for additional API calls or manual steps after registration
- Maintains consistency with other tools in the ecosystem that accept configuration via CLI arguments
- Builds upon lessons learned from ADR275 while addressing its limitations","## Decision

This ADR proposes that we add a `--labels` option to the `config`, which could be used to add custom additional labels to the configured runner.

For example, to add a single additional label the operator could run:
```bash
./config.sh --labels mylabel
```
> Note: the current runner command line parsing and envvar override algorithm only support a single argument (key).

This would add the label `mylabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel]
```

To add multiple labels the operator could run:
```bash
./config.sh --labels mylabel,anotherlabel
```
> Note: the current runner command line parsing and envvar override algorithm only supports a single argument (key).

This would add the label `mylabel` and `anotherlabel` to the runner, and enable users to select the runner in their workflow using this label:
```yaml
runs-on: [self-hosted, mylabel, anotherlabel]
```

It would not be possible to remove labels from an existing runner using `config.sh`, instead labels would have to be removed using the GitHub UI.

The labels argument will split on commas, trim and discard empty strings.  That effectively means don't use commas in unattended config label names.  Alternatively, we could choose to escape commas but it's a nice to have."
"## Context

In addition to action's regular execution, action author may wants their action to have a chance to participate in:
- Job initialization
  My Action will collect machine resource usage (CPU/RAM/Disk) during a workflow job execution, we need to start perf recorder at the beginning of the job.
- Job cleanup
  My Action will dirty local workspace or machine environment during execution, we need to cleanup these changes at the end of the job.  
  Ex: `actions/checkout@v2` will write `github.token` into local `.git/config` during execution, it has post job cleanup defined to undo the changes.","## Decision  
## Decision

We will introduce **job event hooks** (also known as `pre` and `post` steps) for GitHub Actions, allowing action authors to define behavior that executes during job initialization and job cleanup phases.

Specifically:
- Action authors can define a `pre` entry point in their action's metadata (`action.yml`) that runs at the beginning of the job, before any `runs:` step.
- They can also define a `post` entry point that runs at the end of the job, regardless of success or failure of earlier steps.
- These hooks are optional. Authors may implement either, both, or neither.
- The `pre` hook is intended for setup tasks such as starting background services, initializing monitoring tools, or preparing the environment.
- The `post` hook is intended for cleanup tasks such as removing secrets, restoring modified files, or stopping long-running processes.
- Both hooks follow the same syntax and execution model as regular actions (e.g., they can run shell commands, JavaScript, or Docker containers).

This design enables actions like performance monitors or workspace modifiers to properly encapsulate their side effects and ensure clean, reliable job execution without relying on user-defined cleanup steps.

> ⚠️ Note: This functionality must be explicitly enabled via the `actions-job-hooks` feature flag until it is fully stabilized.",## Decision
"## Context

Customers want to be able to compose actions from actions (ex: https://github.com/actions/runner/issues/438)

An important step towards meeting this goal is to build functionality for actions where users can simply execute any number of steps.","## Decision 
## Decision

We will introduce a new feature called **Composite Actions**, enabling users to define reusable action workflows composed of multiple steps. These composite actions can be versioned, shared, and invoked from other workflows, promoting modularity and reducing duplication.

A composite action is defined in a YAML file within a repository (e.g., `.github/actions/my-composite-action/action.yml`) and contains:
- A list of sequential `steps` that execute using the same runner environment.
- Optional inputs, outputs, and descriptions for reusability and clarity.

Each step in a composite action runs in the same job context and has access to the same filesystem and environment variables. Steps may use shell commands, run scripts, or invoke other actions—including other composite actions—enabling hierarchical composition.

Execution of steps within a composite action follows the same syntax and semantics as regular job steps, including support for conditionals, environment settings, and working directories.

This approach avoids introducing full script-based action definitions while still providing significant flexibility through composition. It also maintains consistency with existing GitHub Actions syntax and security model, as composite actions run entirely within the confines of the runner’s execution context without requiring containerization or external services.

With this change, users can encapsulate common workflows—such as setting up environments, running tests, or deploying services—and reuse them across multiple repositories and workflows.","## Decision

**In this ADR, we only support running multiple run steps in an Action.** In doing so, we build in support for mapping and flowing the inputs, outputs, and env variables (ex: All nested steps should have access to their parents' input variables and nested steps can overwrite the input variables)."
"## Context

We recently shipped composite actions, which allows you to reuse individual steps inside an action. 
However, one of the [most requested features](https://github.com/actions/runner/issues/834) has been a way to support the `if` keyword.","## Decision  
## Decision Outcome

Chosen option: ""Support the `if` keyword in composite actions"", because it addresses a highly requested feature and enables greater flexibility and control in reusable workflows by allowing conditional execution of steps within composite actions.",
"## Context

This ADR details the design changes for supporting custom configurable hooks for on various runner events. This has been a long requested user feature [here](https://github.com/actions/runner/issues/1543), [here](https://github.com/actions/runner/issues/699) and [here](https://github.com/actions/runner/issues/1116) for users to have more information on runner observability, and for the ability to run cleanup and teardown jobs. 

This feature is mainly intended for self hosted runner administrators.

**What we hope to solve with this feature**
1. A runner admininstrator is able to add custom scripts to cleanup their runner environment at the start or end of a job
2. A runner admininstrator is able to add custom scripts to help setup their runner environment at the beginning of a job, for reasons like [caching](https://github.com/actions/runner/issues/1543#issuecomment-1050346279)
3. A runner administrator is able to grab custom telemetry of jobs running on their self hosted runner

**What we don't think this will solve**
- Policy features that require certain steps run at the beginning or end of all jobs
  - This would be better solved to in a central place in settings, rather then decentralized on each runner. 
  - The Proposed `Notification Hooks for Runners` is limited to self hosted runners, we don't beileve Policy features should be
- Reuse scenarios between jobs are covered by [composite actions](https://docs.github.com/en/actions/creating-actions/creating-a-composite-action) and [resuable workflows](https://docs.github.com/en/actions/using-workflows/reusing-workflows)
- Security applications, security should be handled on the policy side on the server, not decentralized on each runner","## Decision 
## Decision

Introduce a configurable hooks system for self-hosted runners that allows administrators to define custom scripts triggered at specific lifecycle events of a job. The supported events will be:

- `job-start`: Executes before a job begins, enabling environment setup, caching operations, or telemetry initialization.
- `job-end`: Executes after a job completes (regardless of success or failure), enabling cleanup, log aggregation, or custom telemetry reporting.

Hooks are defined in a configuration file (e.g., `runner-hooks.yml` or via runner configuration parameters) and point to executable scripts on the runner host. These scripts run under the same execution context as the runner, with access to job metadata via environment variables (e.g., `GITHUB_REPOSITORY`, `GITHUB_RUN_ID`, `GITHUB_JOB_STATUS`).

The hooks system will:
- Be opt-in and only available to self-hosted runners.
- Run synchronously with timeouts (configurable or defaulted to 5 minutes).
- Log hook execution output alongside runner logs for observability.
- Not affect job result; failures in hooks do not fail the job unless explicitly configured.

This design fulfills administrator needs for observability, environment management, and customization while explicitly excluding policy enforcement and security controls, which remain server-side responsibilities.",
,Please provide the context for which you'd like me to return a decision.,
"## Context

Following the [adoption of actions-runner-controller by GitHub](https://github.com/actions/actions-runner-controller/discussions/2072) and the introduction of the new runner scale set autoscaling mode, we needed to provide a basic runner image that could be used off the shelf without much friction.

The [current runner image](https://github.com/actions/runner/pkgs/container/actions-runner) is published to GHCR. Each release of this image is tagged with the runner version and the most recent release is also tagged with `latest`.

While the use of `latest` is common practice, we recommend that users pin a specific version of the runner image for a predictable runtime and improved security posture. However, we still notice that a large number of end users are relying on the `latest` tag & raising issues when they encounter problems.

Add to that, the community actions-runner-controller maintainers have issued a [deprecation notice](https://github.com/actions/actions-runner-controller/issues/2056) of the `latest` tag for the existing runner images (https://github.com/orgs/actions-runner-controller/packages).","## Decision 
## Decision

We will deprecate the `latest` tag for the official GitHub Actions runner image on GHCR.

Going forward:
- The primary method of distribution will be versioned tags (e.g., `v2.304.0`, `2.304.0`).
- The `latest` tag will no longer be updated with new releases.
- Existing usage of `latest` will continue to work but will point to a fixed, older version to prevent silent breakage while encouraging migration.
- Documentation and examples will be updated to emphasize pinning specific versions.
- A warning message will be displayed in logs when `latest` is used, directing users to use versioned tags instead.

This decision aligns with security best practices, improves reproducibility, and supports the broader ecosystem shift led by actions-runner-controller toward explicit versioning.","## Decision

Proceed with Option 2, keeping the `latest` tag and adding the `NOTES.txt` file to our helm charts with the notice."
"## Context
SS?? fundamentally relies on a third-party provider service to authenticate requests (e.g. OAuth2
via Google). The provider may experience transient faults for any number of reasons - perhaps the
provider is experiencing a transient outage, or our traffic has been temporarily rate limited. We
wanted to build SS?? to anticipate and handle the presence of faults during communication with
authentication providers, no matter the underlying cause.","## Decision  
## Decision

We will implement a resilient authentication layer that gracefully handles transient faults when communicating with third-party identity providers. This includes:

* Introducing configurable retry policies with exponential backoff for failed authentication requests.
* Implementing circuit breaker patterns to prevent cascading failures during prolonged outages of the provider.
* Caching authentication results or tokens where security and usability allow, to reduce dependency on real-time provider availability.
* Clearly distinguishing between client errors (e.g. invalid credentials), permanent failures, and transient faults in error handling logic.

These measures ensure that SS?? remains robust and usable during intermittent connectivity issues or provider-side instabilities, without compromising security.","## Decision
We implemented a generic `circuit.Breaker` type, which implements the ""Circuit Breaker"" design
pattern made popular by Michael T. Nygard's book, [""Release It!""](
https://pragprog.com/book/mnee/release-it). All requests to the provider service are issued through
a stateful `circuit.Breaker` instance, which tracks the frequency of request failures. When the
`Breaker` sees that enough requests are failing, it temporarily suspends all outgoing traffic to the
provider (i.e. ""enters `Open` state). After some time, the `Breaker` transitions to a `HalfOpen`
state, in which a limited number of outbound requests are allowed. If failures persist, then the
`Breaker` will once again suspend outbound traffic, re-enter `Open` state, and typically will wait
for a longer interval of time before trying again. If instead the `Breaker` observes that requests
are consistently succeeding, then it will resume all outbound traffic (i.e. enter `Closed` state).

The SS?? [`docs`](/docs/) directory contains a [Circuit Breaker](
/docs/architecture/circuit-breaker.md) document with more details. The implementation can be found
in [`breaker.go`](/internal/auth/circuit/breaker.go)"
"## Context

We make a lot of decisions during the life of a project and
documenting those decisions would help new team members and outside
contributors follow our thinking. It also opens up an opportunity for
constructive criticism around those decisions which will result
in better decision making.

We want to develop a product in the open. Opening up our decision making process for outside contributors is an important part in providing the opportunity to take ownership and impact the project in non-trivial ways.

Recording decisions will also be a useful form of documentation in general as it does not get out of date. All that's documented is that a decision has been made at a certain point in time.","## Decision  
## Decision Outcome

We will use **Markdown Architecture Decision Records (MADR)** to document all project decisions—architectural, technical, and process-related.

This format is lightweight, human-readable, version-control-friendly, and well-suited for open-source collaboration. It enables transparent, traceable, and contextual decision-making that remains valuable over time.

Each decision will be recorded as a separate Markdown file in the `docs/decisions` directory, using the MADR structure with the following sections:
- **Status** – e.g., proposed, accepted, deprecated
- **Context** – the problem and forces shaping the decision
- **Decision** – the chosen path and rationale
- **Consequences** – impacts, trade-offs, and follow-up actions

By maintaining an open and accessible decision log, we support onboarding, accountability, community engagement, and continuous improvement.","## Decision

We will adopt a format similar to [Architecture Decision
Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) (ADR)
to propose and document notable decisions. In contrast to ADRs we will try to embrace this tool for more than just architectural concerns. Decisions regarding development practices and product direction are just as important.

What decisions are notable and which are not is left to common-sense and informal consensus among the team.

Decisions may be proposed and discussed informally, should however eventually end up ÃÂ¡ÃÂª with all relevant discussion summarized ÃÂ¡ÃÂª in the `doc/decisions/` directory of this repository.

Decisions are numbered to have a clear identifier."
"## Context

Extensions are defined by an EDN file accessed from decentralized storage. An URI schema allows to identify those files.","## Decision 
## Decision

We will use the `extension://` URI scheme to identify and locate extension definitions stored in decentralized storage.

An extension URI follows the format:  
```
extension://<namespace>/<extension-name>[/version]
```

Where:
- `namespace` uniquely identifies the publisher or domain of the extension
- `extension-name` is the logical name of the extension
- `version` (optional) specifies a particular version; if omitted, the latest available version is resolved

The EDN file located at the resolved decentralized storage address contains metadata and pointers to the extension’s assets, including code, configuration, and dependencies. Resolution is handled by a global Extension Registry contract or decentralized naming service that maps URIs to content identifiers (e.g., IPFS or Arweave hashes).

This approach ensures:
- Decentralized, tamper-resistant distribution of extensions
- Versioned, reproducible deployments via optional version pinning
- Human-readable identifiers while supporting cryptographic integrity through backend content addressing
- Interoperability across services and clients using a standardized resolution mechanism","## Decision

URI follows ethereum URI schema as specified in [EIP 831](https://github.com/ethereum/EIPs/blob/master/EIPS/eip-831.md)


```
uri                     = ""ethereum:"" path
path                    = ""status:extension"" "":"" storage ""@"" id
storage                 = STRING (e.g. IPFS)
id                      = STRING (e.g. IPFS hash)
```

An example of an extension available on IPFS is: `ethereum:status:extension:ipfs@QmTeW79w7QQ6Npa3b1d5tANreCDxF2iDaAPsDvW6KtLmfB`"
"## Context

We want to launch our beta in Q2. This requires us to keep a tight scope and stay on the critical path.

Group chat is not as vital as 1-1 chat for the chat experience. It is also
likely not as useful for an MVP as public chat is, since this gives people a
chance to talk to strangers and it is open by default.

Whether group chats are in scope or not has been a recurring discussing and a
lot of effort has been wasted related to this.

There are currently a lot of outstanding regressions related to group chat: https://github.com/status-im/status-mobile/labels/group-chat Additionally, since group chats are private the encryption and security of those are generally harder than 1-1 and public chat.","## Decision  
## Decision

Group chat is out of scope for the beta launch in Q2.  

We will focus on delivering a stable and secure 1-1 and public chat experience as part of the MVP. This decision reduces scope, avoids known technical debt and security complexities, and aligns with the goal of staying on the critical path.  

Revisiting group chat functionality will be considered post-beta, based on user feedback and prioritization in future planning cycles.","## Decision

Disable group chat for beta and don't work on bugs related to it until after. See https://github.com/status-im/status-mobile/issues/3995

This ensures we can release beta without blockers, and then we can take some
time to fix group chats properly."
"## Context

Currently peers for messaging are hardcoded in the app. This means we can't
easily scale, as some of these nodes might fill up when we have more than N
active users.

Discovery v5 protocol (https://ideas.status.im/ideas/092-disc-v5-research,
https://github.com/fjl/p2p-drafts/blob/master/discv5-eip.md) allow us to
discover new peers from the app and scale server peers in our cluster
independently. This is vital for high availability/reliability and security.","## Decision 
## Decision

We will implement support for Discovery v5 protocol to enable dynamic peer discovery in the app.

Hardcoded peers present a scalability and operational bottleneck, as they limit our ability to respond to load fluctuations and perform seamless cluster scaling. By adopting Discv5, we gain:

- Dynamic discovery of available messaging nodes
- Independent scaling of server-side peer clusters
- Improved resilience through automatic failover to healthy peers
- Enhanced security via authenticated node discovery

The implementation will involve:
1. Integrating a Discv5-compliant library into the client application
2. Setting up bootstrap nodes that participate in the discovery network
3. Modifying the peer selection logic to prefer discovered peers over static ones
4. Maintaining fallback to hardcoded peers during transition and for emergency access

This change supports our goals of high availability, reliability, and secure, scalable peer-to-peer communication.","## Decision

- Use and enable Discovery v5 for all networks
- Do this before beta is released

This means we have capacity to achieve 5000+ DAU / 500 concurrent connection. It
seems unlikely we can do so with reliable messaging in the current setup.

Trade-off for this is:
- Uncertainty in introducing a new protocol leading to possible delays
- Possible increase in CPU/Network (we can disable protocol once peers are found though)"
"## Context

Idea [142-Wallet-Compatibility](https://ideas.status.im/ideas/142-wallet-compatibility/README) changes the way we generate keys starting from the 12 mnemonic words.

This change allows users to create a wallet in other apps and import the same address in Status using the 12 mnemonic words.","## Decision 
## Decision

Adopt BIP-39 and BIP-44 standards for mnemonic-based key derivation to ensure cross-wallet compatibility.

This decision enables users to generate a wallet using 12-word mnemonics in any standards-compliant wallet (e.g., Ledger, MetaMask, Trust Wallet) and seamlessly import the same account into Status, and vice versa. The implementation will use:

- **BIP-39** for generating the mnemonic phrase and deriving a seed from it.
- **BIP-44** (with appropriate coin types for Cosmos and Ethereum chains) for hierarchical deterministic (HD) key derivation, ensuring consistent address generation across platforms.

This change aligns Status with industry standards, improves user experience, and reduces friction during onboarding by eliminating vendor lock-in. Existing non-standard key generation methods will be deprecated, and migration paths will be provided for current users.","## Decision

* update the BIP39 seed generation to use the salt `""mnemonic""` instead of `""status-im""` following [BIP39](https://github.com/bitcoin/bips/blob/master/bip-0039.mediawiki#from-mnemonic-to-seed).

* update the master key generation using the constant `""Bitcoin seed""` instead of `""status-im""`, following [BIP32](https://github.com/bitcoin/bips/blob/master/bip-0032.mediawiki#master-key-generation).

* remove the use of the password in the salt during the BIP39 seed generation (password is not removed from the app in general, and it's still required to encrypt the keys on the device)."
"## Context

On the path to releasing a usable beta we've faced a variety of performance
issues:

- https://github.com/status-im/ideas/issues/55
- https://ideas.status.im/ideas/071-low-traffic
- https://ideas.status.im/ideas/076-smooth-ui
- https://ideas.status.im/ideas/083-energy-efficient

Some of these performance issues are related to the use of LES, especially on
Mainnet. Examples of problems include: slow/hot sync, disk filling up, bandwidth
usage, etc. This can partly be remedied by the use of CHT to speed up sync
(https://github.com/status-im/status-go/wiki/Updating-CHT-BloomTrie).

In order to simplify the problem, we decided earlier this year to not spend too
much time looking into performance improvements related to LES, instead focusing
our efforts on improving the performance on Whisper and in client-side code.

This also means we don't have a full picture of to what extent LES is a
performance bottleneck for the Status app. It is possible proper use of CHT,
slowing sync down, vary load depending on if you are on WiFi or not, etc, can
solve these problems. However, the consensus right now is that ULC is the more
promising route to take.

However, since ULC isn't ready yet, this means we are relying on Infura. This
is a bad idea from a decentralization and security point of view. This can be
partly remedied by allowing the user to run an upstream node. It is still a bad
trust model, relying on third-party Ethereum nodes, and we want to get away
from this as soon as possible.

With ULC it is possible to connect to more than one node, which increases
decentralization and security.","## Decision 
## Decision

We will prioritize the development and integration of Ultra Light Client (ULC) as the primary path for Ethereum node connectivity in the Status app, deferring further optimization of LES (Light Ethereum Subprotocol) for Mainnet.

While LES remains functional and can be improved with techniques like CHT, sync throttling, and network-aware behavior, these are short-term mitigations that do not resolve the fundamental scalability, decentralization, and trust issues. ULC offers a more sustainable, secure, and decentralized alternative by enabling clients to verify data from multiple untrusted nodes, reducing reliance on centralized infrastructure like Infura.

In the interim, we will:
- Maintain basic LES functionality with minimal performance safeguards (e.g. disk usage limits, sync pauses on poor connectivity).
- Encourage and support users who wish to connect to self-hosted or community-run upstream nodes.
- Monitor real-world performance data to assess the true impact of LES bottlenecks.

All new protocol-level performance efforts will focus on advancing ULC to production readiness, including multi-node connectivity, proof verification, and resilience under low-bandwidth conditions. This aligns with our long-term goals of security, energy efficiency, and decentralized network access.","## Decision

1. Rely on upstream nodes for beta. For the closed beta scheduled to be releases
Q2 we will provide two ways of accessing the Ethereum network:

- Upstream RPC with Infura (default)
- Custom upstream node (https://github.com/status-im/status-mobile/issues/3994)

From a decentralization point of view we need the latter before we can release
the beta.

2. ULC MVP. Additionally, as soon as beta is released, assess and start to
   introduce ULC as an experimental non-default mode in the app.
   
By doing this we also support the network, since it is still a very new
protocol. ULC requires trusted peers to operate, and right now trust is
established by config (list of trusted peers).

Initially we could hardcode these nodes, with option to add your own. Later on
we can have some form of distributed directory with a rating system. This also
likely requires merging ULC upstream, so we aren't the only ones running trusted
nodes.

3. LES track (optional). If we still have resources and there's interest,
optionally run a parallel track to clean up LES usage (update CHT, cluster) and
understand exact performance impact and viablity of running LES on
resource-constrained devices.

These three decisions balances the following trade-offs:

- get a usable beta released as soon as possible
- provide users with options to get minimal amount of decentralization
- pave the way for trustless usage by default (ULC/LES node)"
"## Context

We have some data that we don't want to appear in the logs (user passwords are
a good example). Currently, they are passed around as strings, that could be
printed out by mistake in a log entry (see https://github.com/status-im/status-mobile/issues/4053)","## Decision 
## Decision

We should create a `Secret` type, which is a wrapper around a string that prevents accidental logging.

The `Secret` type will:
- Wrap a string value
- Implement common interfaces (Stringer, JSON marshal/unmarshal) 
- But override the string representation to prevent accidental exposure
- Provide explicit methods to access the underlying secret value when needed

This approach ensures secrets cannot be accidentally logged while still allowing controlled access when necessary. The type system will help catch potential leaks at compile time rather than runtime.","## Decision

To minimize the risk of leaking passwords through logs, we should not pass
passwords as strings in our codebase. We introduced a new type `MaskedData` in
`status-im.utils.security`.
update (16-Dec-2022) `status-im.utils.security` is now moved over to `utils.security.core` 

We use `(security/mask-data <data to hide>` to wrap sensitive data into this
type and then use `(security/unmask <masked-data>)` to get the plaintext back.

It is important to keep that sensitive data masked as much as possible, until
you need the plaintext to pass to the extenral APIs.

Example:
```clojure
(println (security/mask-data ""my-plaintext-password"")) ;; Outputs ""******""
(println (security/unmask (security/mask-data ""my-plaintext-password""))) ;; Outputs ""my-plaintext-password""
```"
"## Context

This change describes a methodology of development which we believe will help
us increase the confidence and speed in the development process.","## Decision 
## Decision

We will use test-driven development in our software creation process.",
"## Context

Classical release approach: we cut off the release branch, stabilize it, fix every critical issue and release. Some changes are cherry-picked to the release branch.

It has a couple of downsides:

- Resources are scattered across two branches;

- Unique fixes and unique combination of commits in the release branch;

- Unpredictable release schedule (any day can be a release day!).","## Decision 
## Decision

Adopt a time-based release model with feature flags and mainline development to improve predictability, streamline resource allocation, and reduce branch divergence.

### Key Elements of the Decision:

- **Mainline Development**: All development, including features and fixes, occurs on the main branch. This ensures a single source of truth and avoids scattering work across multiple long-lived branches.

- **Time-Based Releases**: Schedule regular, predictable releases (e.g., every 4 weeks). This enables better planning, stakeholder communication, and team alignment.

- **Feature Flags**: Use feature flags to control the visibility of incomplete or unstable features in production. This allows merging code early while keeping it disabled until release readiness.

- **Stable Release Preparation**:
  - A release branch is created only *after* the release date is reached and the main branch is tagged.
  - Only critical hotfixes are applied post-release; these are first committed to main and then cherry-picked or merged into the release branch.

- **No Long-Lived Release Branches**: Avoid maintaining separate stabilization branches during the release cycle. Stabilization happens on main via testing in staging environments with appropriate feature flag configurations.

### Rationale:

- **Predictability**: Time-boxed cycles eliminate ""any day can be a release day"" uncertainty.
- **Focus**: Teams stay on main; no context switching between release and development branches.
- **Reduced Complexity**: Eliminates need for frequent cherry-picks during stabilization and avoids inconsistent commit sets across branches.
- **Faster Feedback**: Continuous integration and deployment from main ensure issues are caught early.

This approach aligns with modern practices used by teams practicing continuous delivery and leverages automation and feature toggling to decouple deployment from release.",
"## Context

Originally, we supported 3rd party status extensions in form of javascript jailed execution environment (eq Jail) + small js library exposing chat commands API to js code.

While neat in theory, it has some serious downsides:

- Design of the API was quite poor, preferring mutable semantics over stateless API leveraging strength of the host application (cljs/re-frame).
  As a result of that, more dynamic/state requiring things (like the live tx detail in `/send` command messages) were very hard to do,
  so instead of ""eating our own dogfood"", we decided to side-step the API and implement such things as hard-coded logic in the app, while partly
  retaining the js code for ""easier"" things (like parameter declaration).
  Needles to say, such efforts produced code of very poor quality, riddling our app with hard-coded ""magic"" everywhere in the codebase, completely
  defeating the point of ""dogfooding"" while still requiring more effort and being much more error prone (no way to unit test jail logic) because 
  of the need to asynchronously communicate with jail for leftover logic in command messages (the parts not hardcoded in app).
  
- We were in a state where there was not even one command defined completely in jail, with all of them requiring custom hardcoded logic in app to
  actually work.
  Due to numerous changes and rush to get things working, half of the API was obsolete and was not working/working differently then described in
  the API documentation.
  
- We are quite tight stretched regarding performance and the Jail didn't particularly help with that - whenever some command message was defined in
  Jail, it required constant RN Bridge ""ping-pong"" whenever sending the message or receiving the message, eating more resources + breaking otherwise
  quite simple logic into harder to reason asynchronous call chains (execute method defined in Jail, wait for the result, execute next method in Jail...).
  
- Till now, there was no real interest in 3rd party bots anyway - the only ""real"" DAPPs submitted to status were ""normal"" DAPPs utilising just the `web3.js`
  API and built on top of regular web technologies, so they could be ran in any Ethereum client like Metamask or Mist, besides Status.
  
- There is a very promising new concept of extensions (project pluto), which will enable to extend status with much nicer and declarative extension
  definitions.
  Such extensions will not be able to only hook-in into the command messages logic, but also other application (host) hooks, like wallet assets/collectibles,
  chat contacts, etc.","## Decision 
We decided to deprecate and remove the JavaScript Jail execution environment for 3rd party status extensions due to its poor API design, performance overhead, lack of real-world adoption, and the emergence of a superior alternative (Project Pluto). 

Instead, we will invest in the development of **Project Pluto**, a new extension system based on declarative, secure, and host-integrated extension definitions. This new system will provide better performance, improved developer experience, full testability, and deeper integration with the host application (e.g., wallet, contacts, chat), while aligning with our architecture by leveraging the strengths of ClojureScript and re-frame.

All existing command logic previously relying on the Jail will be fully migrated into native app code, eliminating technical debt, reducing complexity, and enabling consistent behavior across all commands.","## Decision

In the light of points above, we decided to remove Jail and `Status.js` API from the application and re-work command messages in set of nicely
encapsulated app level protocols, so command related logic will be defined in one place, commands will be able to leverage strength of our platform
and development of any new functionality related to them will be much faster and more pleasant. 
We will address the 3rd party code issue separately by extensions."
"## Context

During the development of a new Sylius API (based on API Platform) for Product Options, we had to decide how we should handle
Product Option's values collection, to make it efficient and easy to use.","## Decision 
## Decision Drivers

* Need to efficiently manage and retrieve Product Option values in a scalable way.
* Desire for a clean, maintainable API that aligns with API Platform best practices.
* Importance of performance when handling large collections of option values.
* Requirement to support flexible filtering, sorting, and validation of Product Option values.","## Decision and reasoning

Product option values should always be provided with their IRIs. They can exist outside of the `ProductOption` resource 
(e.g. be [related with](https://github.com/Sylius/Sylius/blob/master/src/Sylius/Component/Product/Model/ProductVariant.php#L44) `ProductVariant`).
Moreover, values embedded together with their translations in the `ProductOption` that already has its translations would
result in a massive and unmanageable response."
"## Context and Problem Statement

We want our API to cover all the functionalities that are currently implemented in the UI.","## Decision 
## Decision

The API will expose endpoints that fully encompass all functionalities currently implemented in the UI. This includes both standard CRUD operations and custom workflows or actions specific to the application’s business logic.

To ensure completeness and maintainability:

- The API surface will be aligned with the UI feature set through close collaboration between frontend and backend teams.
- Any UI-specific logic that cannot be handled by generic entity endpoints will be implemented via dedicated API routes or command-style endpoints (e.g., `POST /orders/checkout`, `PUT /documents/approve`).
- Custom endpoints will follow consistent naming, versioning, and error-handling conventions as the rest of the API.
- Where appropriate, apps may extend or override default behavior using app-provided API extensions, as permitted by the module façade design.

This ensures the API remains a first-class citizen capable of supporting current and near-future UI requirements without forcing UI logic into the client layer.","## Decision Drivers

* All the functionalities implemented for API should be tested
* Tracking whether a feature has been covered in the API or not should be easy"
"## Context and Problem Statement
While developing the new, unified API, there weren't clear guidelines for structuring new API endpoints. The first approach
was introducing two different endpoint prefixes, similar to what is currently done in a regular shop. On the 30th of April,
we have merged a unification of our endpoints in https://github.com/Sylius/Sylius/pull/11397. This move was dictated by 
the fact that we wanted to push unification even further. Depending on the currently logged in user, we had to expose different
data for available entities in both contexts. Besides, it ought to simplify API usage (as it would reduce the number of endpoints)
and simplify the relations between resources. However, this change rose a lot of new issues to solve:

 * Different serialization for different users is not trivial. Some relations between objects are also contextual, so 
admin can see many more links than the regular visitor or shop user. 
 * Endpoints like ""Product Show"" are resolved differently depending on the context as well. The shop products are determined 
based on the slug (taking into account the current channel), while the administrator is browsing products by code. This
separation blurs the product identifier, and it is not clear which route should be considered as IRI. 
 * the split was not complete. Two separate log in endpoints have to be preserved, due to our user architecture and the
decision that we want to be explicit, which user is trying to log in.
 
Using one unified identifier in both admin and shop panel is a no go as well. In the shop we should use a `slug`, to be able
to expose product data based on URL (or provide redirect to proper resource based on the channel and locale), while in admin 
resources are channel and locale agnostic, and we should use `codes` instead.","## Decision  
## Decision

We will maintain separate API endpoints for admin and shop contexts despite the initial move towards unification. This separation is necessary to address context-specific requirements and ensure clarity in resource identification and serialization.

* **Admin API** will use `code` as the primary identifier for resources (e.g., `/api/admin/products/{code}`), reflecting its channel- and locale-agnostic nature.
* **Shop API** will use `slug` as the identifier (e.g., `/api/shop/products/{slug}`), enabling proper URL routing, SEO, and channel/locale resolution.

Different serialization contexts and available relationships will be handled separately per API, allowing each to expose only relevant data and links based on the user role and context.

While this introduces a degree of duplication, it ensures semantic clarity, avoids ambiguous identifiers, supports proper IRI stability, and aligns with actual usage patterns in both admin and shop environments. The split login endpoints will remain as they reflect intentional architectural clarity regarding user authentication context.","## Decision Drivers

* Driver 1: There is not strict requirement to have a direct 1 to 1 correlation between entities in database and exposed API Resources.
One resource in the database may aggregate several API Resources
* Driver 2: There should be a clear way to determine an IRI for each of exposed API Resources
* Driver 3: A clear separation of available resources and paths for visitor and admin may be expected"
"## Context and Problem Statement

While defining new handlers, we may define a bus to which particular handler should be assigned to. If not, handler will
be added to all existing buses. Right now, we have only one bus in Sylius, so decision will not change current behaviour.
However, we may add additional buses in the future and more than one bus is not uncommon thing.","## Decision 
## Decision

Explicitly assign handlers to specific buses during their definition to ensure predictable and maintainable message routing, even as the system scales with additional buses. 

If a bus assignment is not specified, the default behavior of assigning the handler to all existing buses may lead to unintended side effects, such as duplicated message processing or tight coupling across concerns. Therefore, we deprecate implicit assignment to all buses and require an explicit bus identifier in handler configuration.

This change improves clarity, supports future extensibility, and aligns with the principle of least surprise.","## Decision Drivers <!-- optional -->

* Driver 1 - easiness of changing default behaviour and usage of the bus
* Driver 2 - backward compatible possibility of changing behaviour"
"## Context and Problem Statement

To achieve 100% API coverage, we need to handle emails by API.","## Decision 
## Decision

We will implement a dedicated Email API endpoint to manage email operations programmatically. This endpoint will support core actions such as sending, scheduling, canceling, and retrieving email status. 

The design follows REST conventions and integrates with the existing authentication and authorization mechanisms to ensure secure access. Email content templates will be managed separately via the template service, allowing dynamic composition while maintaining separation of concerns.

By exposing email functionality through the API, we enable full automation, third-party integrations, and consistent behavior across UI and backend processes. This also aligns with our goal of achieving 100% API coverage and supports auditability, testability, and observability of email-related workflows.

Additionally, we will emit appropriate events when emails are sent or fail, enabling downstream systems to react accordingly. These events will be part of our event-driven architecture and can be used for logging, monitoring, and retry mechanisms.","## Decision Outcome

Chosen option: ""Using events"", because it allows us to send email using events, commands and handlers. Thanks to this we can queue few messages in async transport."
"## Context and Problem Statement

Initial implementation of Unified API used `/new-api` prefix, to aggregate all following endpoints. This prefix does not 
clearly state the version of it and is not future-proof. At some moment of time our ""new api"" can become ""old api"". We should 
have clear guidance how to version our APIs.","## Decision 
## Decision

Adopt a versioned URL prefix for the Unified API using the format `/api/v{major-version}` (e.g., `/api/v1`).  
This approach ensures clear, predictable, and future-proof API versioning.  
Versioning at the major level aligns with semantic versioning principles, where breaking changes require a new major version.  
The initial release of the Unified API will be versioned as `/api/v1`, replacing the ambiguous `/new-api` prefix.  
Future versions will increment the major version number in the URL path when backward-incompatible changes are introduced.  
Documentation and routing will be structured around versioned endpoints to support lifecycle management and deprecation.","## Decision Outcome

As the underlaying technology, structure and content have changed significantly and taking into account easiness of first solution
the decision is to go with the `/api/v2` endpoint path. In the future it does not block us from the usage of the `Accept`
header in addition to this path, however it may be misleading for consumers."
"## Context and Problem Statement

API Platform recommends using IRI as an identifier. That identifier gives more clarity than an id because it contains more information - a full endpoint path to the resource and its unique identifier. On resources, API Platform handles IRI out of the box.
While we were designing Sylius new API, we have decided to use commands in many endpoints.
In some cases, this solution is more flexible than the default creation approach (as we have control over what we are processing), but it doesn't handle the transformation of IRI to proper `code`/`id`.
In the past, we have been using `code`/`id` instead of IRI, then we have been using both approaches.
Now we are trying to unify the new API and replace codes and ids with IRI everywhere.
The main challenge is the usage of IRI in requests, where we want to have IRI in the request but `id`/`code`in its commands and command handlers.","## Decision  
## Decision

We will implement an **IRI resolver service** that automatically transforms IRIs into internal identifiers (`id`/`code`) during command handling.

This service will:
- Intercept incoming requests containing IRIs (e.g. in JSON payloads or URL references)
- Resolve each IRI to its corresponding resource by extracting the unique identifier (typically the last segment of the IRI path)
- Map the resolved identifier to the appropriate `id` or `code` used internally by the domain
- Populate commands with these internal identifiers before they are passed to command handlers

Example:  
Given IRI: `/api/v2/products/P123` → resolved to `code: P123`  
Given IRI: `/api/v2/orders/456` → resolved to `id: 456`

The transformation will be handled transparently via a request listener or middleware layer, ensuring that:
- The API contract uses IRIs consistently across all endpoints
- Command and command handlers remain decoupled from API concerns and work exclusively with internal identifiers
- Developers don’t need to manually resolve IRIs in each handler

Additionally, we will:
- Leverage Symfony’s PropertyInfo component and API Platform’s metadata system to automate resolution where possible
- Provide a reusable `ResolveIriFor` attribute/injection mechanism for complex or custom cases
- Document IRI usage as the standard for client-server communication

This approach ensures consistency with API Platform best practices while maintaining clean separation between the API surface and domain logic.","## Decision Outcome

Chosen option: ""Handling and transforming IRI to `id`/`code`"". Request that is based on command and needed information like `code`/`id` should get it as IRI"
"## Context and Problem Statement

Sylius is by design application that will be adjusted to customer needs. 
Therefore each part of it has to be crafted with replaceability or customizability at its core. 
Nonetheless, the current state of API Platform integration requires the replacement of the whole config of the whole resource. 
In terms of the more complicated cases like Order or Customer, this practice may become error-prone and hard to maintain for both maintainers and Sylius users.","## Decision 
## Decision

To improve the customizability and maintainability of Sylius' API Platform integration, we will adopt a modular and composable configuration approach that allows granular overrides and extensions per resource, rather than requiring full configuration replacement.

Specifically:

- Introduce **per-resource API configuration partials** that define only the necessary API Platform attributes (e.g. normalization groups, filters, pagination) for each Sylius resource.
- Allow users to **extend or override specific parts** of a resource’s API configuration (e.g. just the filters for `Customer`, or just the denormalization context for `Order`) without replacing the entire configuration.
- Implement configuration merging via **semantic configuration trees** using Symfony’s Config component, ensuring type safety and validation.
- Provide **extension points in the form of interfaces or abstract services** for common customization patterns (e.g. custom filters, output DTOs).
- Document clear **best practices and examples** for safely customizing complex resources like Order and Customer.

This approach ensures that:
- Users can make targeted changes with minimal risk of breaking existing functionality.
- Upgrades are safer, as core configuration changes can be merged independently.
- Maintainability is improved both for the core team and end users.

This decision aligns with Sylius’ philosophy of flexibility by design, reducing the cost of customization while preserving extensibility.","## Decision Outcome

Chosen option: ""Config merging"", because it allows us easily overwrite any endpoint, without getting deep into api platform resources."
"## Context and Problem Statement

A big chunk of resources available in Sylius have some part locale aware. Product descriptions, names of payment or shipping methods
should be translated to the customer to provide the best user experience. Right now most of the resources are returned with 
all available translations in the store and proper localization of content is left to the frontend developers. What is more,
this solution is not consistent, as some endpoints already embeds some part of translatables in them. We should provide clear,
easy to use and consistent way of handling customer locales.","## Decision 
## Decision

To ensure a consistent and developer-friendly approach to locale-aware content in Sylius, we will adopt a standardized pattern where:

- All API responses return data for the **requested locale only**, based on an explicit `Accept-Language` header or a provided locale parameter.
- If a translation is not available for the requested locale, the system will fall back to the store’s default locale.
- Each translatable resource (e.g., Product, Taxonomy, Payment Method) will expose its localized fields directly at the root level of the response (e.g., `name`, `description`) — not nested under a `translations` object.
- A dedicated `translations` subresource will remain available via separate endpoints for use cases requiring access to all translations (e.g., admin interfaces).
- Documentation and response schemas will clearly indicate which fields are translatable and subject to locale negotiation.

This improves frontend usability, reduces payload size, ensures consistency across endpoints, and aligns with standard internationalization practices in REST APIs.","## Decision Outcome

The first approach should be used in admin section of API, while the third one should be used in shop section. This way, we
will receive best of both worlds with the minimal effort. It is worth mentioning that Sylius have two distinguish entry points
to API - admin and shop. They are concerned about different things and resolves different problems. Unified view of a product
in the shop will make it easy and straightforward to build product pages. At the same time all translation approach should be used in
admin to provide full control over ""translation"" resource - as we should treat them. Therefore, every resource translation
should be linked to the main resource through IRI. Each of these translation resource may be a subject of all CRUD operations
accessible throughout HTTP Verbs."
"## Context and Problem Statement
Customer should have access only to locales available in their channel","## Decision 
## Decision Outcome

Implement locale filtering at the channel level by associating supported locales with each channel configuration. During customer session initialization, load only the locales defined for the respective channel.

## Rationale
- Ensures consistency between channel content and available languages
- Prevents customers from accessing untranslated or unsupported content
- Enables per-channel localization strategies (e.g., regional channels with specific language requirements)

## Implementation Approach
1. Extend channel configuration model to include a list of supported locale identifiers
2. Modify locale resolution service to filter available locales based on current channel
3. Cache channel-specific locale lists to optimize performance
4. Return a default locale (e.g., en-US) if requested locale is not available in the channel

## Consequences
- ✅ Customers only see languages available in their channel
- ✅ Supports flexible, channel-specific localization
- ⚠️ Requires validation to ensure at least one locale is always available per channel
- ⚠️ Channel configuration must be maintained alongside locale deployments","## Decision Outcome

Chosen option: Using Data Provider
Shops shouldn't have many locales for each channel, so lack of a pagination is smaller problem than creating overcomplicated 
query in Doctrine Collection extension"
"## Context and Problem Statement

Cart and its processing is one of the key aspects of Sylius. It turned out that it has a vulnerability and there is
possible for an anonymous user to override the cart of logged in customer by using only its email. This is because 
when entering an email, during addressing step, the customer with this email is assigned to the cart and from then, 
there is no simple way to distinguish between the carts created by the guest and the logged in user. The question is 
how should we distinguish the carts to solve the vulnerability.","## Decision 
## Decision

We introduce a new identifier, called `cart_token`, which acts as a unique session-bound reference to a cart instance. This `cart_token` is generated when a cart is first created (i.e., on the initial add-to-cart action) and remains constant throughout the lifecycle of that cart.

The `cart_token`:
- Is a randomly generated, cryptographically secure string (e.g., UUIDv4).
- Is stored server-side and bound to the current session for anonymous users.
- Persists in the session or as a secure, HTTP-only cookie to prevent client-side tampering.
- Is used to validate ownership whenever a cart is accessed or modified, especially during checkout steps such as addressing.

To resolve the vulnerability:
1. When a user enters an email during the addressing step, the system checks whether a customer with that email exists **and** whether they have an existing cart.
2. If such a cart exists, the system compares the `cart_token` of the current session with the one associated with the logged-in customer’s cart.
3. If the tokens do not match, the carts are considered distinct — the current session does not take over the authenticated user's cart.
4. The guest cart continues independently, and the user may be prompted to merge carts **only after successful authentication**, and with explicit consent.

Additionally:
- Upon login, if the user has an existing cart (from prior sessions), and the current session has a guest cart, a merge strategy is triggered **with user confirmation**.
- The merge respects quantity rules, pricing consistency, and applicable promotions.

This approach ensures:
- No unauthorized cart takeover via email input.
- Clear separation between guest and authenticated user contexts.
- Support for legitimate cart recovery and merging post-login.

Implementation impact:
- Add `cart_token` field to the cart entity.
- Generate it at cart creation (not at checkout).
- Bind it to session and use it in security validation logic during critical steps.
- Expose it only where necessary and never allow direct manipulation by clients.

This decision aligns with best practices in e-commerce security and session integrity.","## Decision Drivers

Provided solution should:
* solve the initial problem with overriding cart of logged in customer by anonymous user
* not break a backward compatibility, both code and business behaviour"
"## Context and Problem Statement
Customer should have access only to locales available in their channel","## Decision 
## Decision Outcome

Implement locale-based access control by validating user channel configurations against available locales.

## Rationale

To ensure customers only access locales available in their channel:
- Each channel will have a defined list of supported locales.
- During user authentication or session initialization, the system checks the user's assigned channel and retrieves its allowed locales.
- Any request for locale-specific content is validated against this list.
- Unauthorized locale access attempts are denied with an appropriate error response.

This approach ensures data consistency, respects regional constraints, and aligns with customer entitlement rules.","## Decision Outcome

Chosen option: **""Using Doctrine Collection extension""**

This option is consistent with current approach and does not omit Doctrine extensions like pagination."
"## Context and Problem Statement

While working with commands and handlers, we are always working with objects; therefore, these objects need to be 
present in handlers. We may either fetch them from the repository based on identifiers or pass them as a part of 
commands. The selected solution needs to take into account that our commands may be dispatched to external systems or 
processed asynchronously","## Decision  
## Decision

We recommend passing only identifiers in commands and retrieving the required objects from a repository within the handler. This approach ensures that:

- Commands remain small and serializable, which is essential for reliable messaging across distributed systems.
- The command payload does not become stale or inconsistent due to changes in the object state between dispatch and handling—especially important when processing is delayed or asynchronous.
- It supports location transparency: the handler (which may be in a different service or bounded context) can resolve the entity using its own data store, respecting domain boundaries and encapsulation.
- It avoids issues with versioning complex object graphs embedded in commands, simplifying schema evolution over time.

Passing full objects in commands is discouraged unless those objects are value objects that are part of the command’s intent and are immutable and self-contained.

This pattern aligns with Domain-Driven Design principles, where aggregate roots are accessed by identity, and application services (or handlers) reconstitute them as needed from repositories.","## Decision Drivers

* always operate on the actual state of the object
* ease to integrate with external systems
* be more consistent with CQRS pattern"
"## Context and Problem Statement

When working with entities and persisting the current state to DB one has to call a flush method on entity manager. 
By default, it starts and commits transactions to used DB. The other possibility is to start the transaction manually, 
which will suspend the auto-commit feature of Doctrine.","## Decision 
## Decision

We will adopt the pattern of explicit transaction management in all business-critical operations. This means that developers must manually start and control transactions using `beginTransaction()`, `commit()`, and `rollback()` when data consistency across multiple operations is required.

By default, Doctrine’s auto-commit mode (implicit transactions per flush) remains acceptable for simple, single-entity operations. However, for complex workflows involving multiple entities or cross-cutting concerns (e.g., order processing, inventory deduction, payment handling), an explicit transaction boundary must be established.

This decision improves data integrity, makes error handling more predictable, and increases clarity about which operations are atomic. It also aligns with best practices for working with Doctrine in Symfony applications and prepares the codebase for potential future use of distributed transactions or sagas in a microservices context.

Additionally, we introduce a PHPStan rule and code review guideline to flag cases where multiple `flush()` calls occur without an enclosing transaction, encouraging developers to either wrap them explicitly or justify their absence.","## Decision Drivers

* avoid inconsistent data in DB
* flexibility to rollback changes
* provide an easy way to interface with object state before committing transaction"
"## Context and Problem Statement

In our current system, every base form type is placed at the lowest possible level, either within a specific bundle or the core. 
Form extensions serve as the primary means of extending form types since all form types are final. 
However, this approach leads to a significant issue: form extensions work globally, which prevents us from reusing any form type easily in different contexts. 
For example, if a field is added through an extension in the AdminBundle (admin context), the same field will also appear in the shop's context.
This problem is compounded by the introduction of Symfony UX, which necessitates overriding some fields to enable its functionality. 
As a result, distinguishing which context added a specific change and when each change should be applied becomes exceedingly difficult.","## Decision 
## Decision

We will deprecate the use of form extensions for context-specific modifications and adopt a composition-based approach using explicit form type inheritance and contextual form type classes. 

Instead of relying on global form extensions, we will create dedicated form types for each context (e.g., `AdminProductFormType`, `ShopProductFormType`) that explicitly build upon base form types by composing desired fields and behaviors. These contextual form types will be responsible for integrating only the modifications relevant to their specific use case.

Additionally, we will introduce a naming convention and directory structure that reflects the context (e.g., `Form/Admin/`, `Form/Shop/`) to improve discoverability and maintainability. Symfony UX integrations will be implemented within these contextual form types, ensuring that enhancements like live updates or Turbo integration are scoped appropriately.

This change ensures that form alterations are predictable, isolated to their intended context, and easier to debug, while preserving the immutability of base form types.","## Decision Drivers

* Need for context-specific form type customization.
* Avoiding global side effects caused by form extensions.
* Clear separation between admin and shop form types.
* Simplification of managing and overriding form fields with Symfony UX."
"## Context and Problem Stratement

Until version 2.0, we supported API Platform version 2, which utilized
`DataProviders` to deliver data to the API. These `DataProviders` were all
located within the `Sylius\Bundle\ApiBundle\DataProvider` directory.
This structure lacked differentiation between the admin and shop sections
and did not provide clear organization for individual resources. 
As a result, it became challenging to manage and maintain the codebase
as the number of providers grew.

With Sylius 2.0, the API Platform has been upgraded to version 3.
One of the significant changes in API Platform 3 is the shift from 
`DataProviders` to `StateProviders`. This change necessitates a 
reconsideration of how we organize and structure our data-providing
classes to ensure clarity, maintainability, and scalability.","## Decision 
## Decision

To address the organizational and maintainability challenges introduced by the transition to API Platform 3, we will restructure our data-providing classes using a new directory and naming convention that emphasizes separation of concerns and resource-specific organization.

We will adopt the following structure:

- All `StateProvider` classes will be placed under `Sylius\Bundle\ApiBundle\StateProvider`, organized into subdirectories by interface type: `Provide`, `Process`, and `Respond`.
- Within each interface-type subdirectory, further organization will occur by section (`admin` or `shop`) and then by resource (e.g., `Product`, `Order`).
- The naming convention will follow the pattern: `{Section}{Resource}{Operation}StateProvider`, where Operation is one of `Provide`, `Process`, or `Respond`, depending on the implemented interface.

For example:
- `Sylius\Bundle\ApiBundle\StateProvider\Provide\Admin\ProductProvideStateProvider`
- `Sylius\Bundle\ApiBundle\StateProvider\Process\Shop\OrderProcessStateProvider`

This structure ensures:
- Clear separation between admin and shop contexts,
- Logical grouping by resource and operation,
- Scalability for future additions without clutter,
- Alignment with API Platform 3’s state machine-based approach.

Additionally, corresponding unit tests will mirror this directory structure under `tests/StateProvider`.

This decision improves code discoverability, simplifies onboarding for new developers, and supports long-term maintenance as the API evolves.","## Decision Drivers

* Moving from DataProviders to StateProviders
* Clear separation between admin and shop providers
* Clear separation between resources"
"## Context and Problem Statement

Until version 2.0, Sylius used API Platform version 2.x, 
which utilized `DataPersisters` to handle data persistence 
operations like saving, updating, and deleting resources. 
These `DataPersisters` were all located within a single 
directory, lacking clear organization.

With the upgrade to Sylius 2.0, the API Platform has been 
updated to version 3. One significant change in API Platform 3
is the shift from `DataPersisters` to `StateProcessors`. 
This necessitates a reconsideration of how we organize 
and structure our data-processing classes to ensure clarity, 
maintainability, and scalability.","## Decision  
## Decision

We will adopt a **resource-centric directory structure** for organizing `StateProcessors` in Sylius 2.0, aligning with the domain-driven design principles and improving discoverability and maintainability.

Each `StateProcessor` will be placed within a dedicated subdirectory under `src/Api/StateProcessor/{ResourceName}/`, where `{ResourceName}` corresponds to the Sylius resource it operates on (e.g., `Product`, `Order`, `Customer`). Within each resource directory, we will further categorize processors by operation type:

- `Write/` – for processors handling create and update operations (e.g., `ProductCreateProcessor.php`, `ProductUpdateProcessor.php`)
- `Delete/` – for processors handling deletion logic (e.g., `ProductDeleteProcessor.php`)
- `Refresh/` – if needed, for processors that recompute or sync state

Example structure:
```
src/Api/StateProcessor/Product/
├── Write/
│   ├── ProductCreateProcessor.php
│   └── ProductUpdateProcessor.php
├── Delete/
│   └── ProductDeleteProcessor.php
```

Additionally, shared utilities or cross-resource processors will be placed under `src/Api/StateProcessor/_Shared/`.

This structure provides clear ownership, simplifies testing, and scales well as the API surface grows. It also mirrors Symfony’s best practices for organizing service classes by domain and intent.","## Decision Drivers

* Migrate from DataPersisters to StateProcessors
* Clear separation between admin and shop processors
* Clear separation between resources
* Separation of Persist and Remove processors
* Ease of customization in end applications"
"## Context and Problem Statement

The current directory structure of Query Extensions in the codebase is disorganized and unclear.

```
/Doctrine
    /QueryCollectionExtension
        ExampleExtension.php
    /QueryExtension
    /QueryItemExtension
```

There is no differentiation between sections such as Admin, Shop, and resources, resulting in unrelated extensions being grouped together based solely on their type.","## Decision 
## Decision

We will restructure the Query Extensions directory to improve clarity, maintainability, and domain separation. The new structure will group extensions by business domain or feature area rather than by technical type.

The new directory structure will be:

```
/Doctrine
    /QueryCollectionExtension
        /Admin
        /Storefront
        /Domain
            Customer/
            Product/
            Order/
        /Infrastructure
```

Each subdirectory will contain only the query extensions relevant to that context:
- `/Admin` – for administration-specific queries
- `/Storefront` – for storefront/shop API related queries
- `/Domain/*` – grouped by core business domains (e.g. Customer, Product)
- `/Infrastructure` – for technical or cross-cutting query extensions

This change improves discoverability, reduces cognitive load, and supports better ownership and testing practices. New query extensions must follow this structure.","## Decision Drivers

* Improve code organization and clarity
* Clear separation of extensions by section
* Clear separation of extensions by resource"
"## Context and Problem Statement

Due to historical reasons, and being BC compliant, we had duplications with our message buses. 
We had `sylius.command_bus` and `sylius_default.bus` for commands, `sylius.event_bus` and `sylius_default.bus` for events.","## Decision 
## Decision

We decided to remove the duplication of message buses by consolidating them into a single, unified bus system. Specifically:

- Deprecate and eventually remove `sylius.command_bus` and `sylius.event_bus`.
- Use `sylius_default.bus` as the sole message bus for both commands and events.
- Clearly separate message types (commands, events) through message interfaces and routing configurations rather than through separate bus instances.
- Update documentation and upgrade guides to reflect the change and assist developers in migrating custom handlers and listeners.

This simplifies the architecture, reduces configuration overhead, and aligns with modern messaging practices where logical separation is preferred over physical duplication.","## Decision Drivers

* Unification of the message buses
* Simplification of configuration and usage
* Lesser cognitive load for developers"
"## Context and Problem Statement

Services in Sylius are named in different ways, depending on the time of their creation, and other services' names.
Also, some services named with using the `dot` notation are harder to be used with `autowire` feature.","## Decision 
## Decision

Standardize service naming using **PascalCase** for class names and **snake_case** for service IDs, avoiding dots in autowired services.

Specifically:

- Service classes should follow `PascalCase` (e.g. `OrderProcessingService`).
- Service IDs in the container should use `snake_case` without dots for autowired services (e.g. `order_processing_service`).
- Dot notation (e.g. `sylius.service_name`) is allowed only for non-autowired, framework-level or third-party services.
- New services must adhere to this convention; legacy services will be gradually renamed during refactoring.

This improves consistency, enhances IDE support, and ensures compatibility with Symfony’s `autowire` and `autoconfigure` features.","## Decision Drivers

* Make naming services predictable
* Provide a consistent way of naming services
* Support the `autowire` feature"
"## Context and Problem Statement

In previous versions, Sylius had API filters grouped by field type rather than by resource and section. This led to filters being applied inconsistently, making it hard to customize them per resource. The filters were also located in the `Filter/Doctrine/` directory, which didn't align with how we organize other Doctrine-related features, creating confusion.
To improve maintainability, clarity, and ease of customization, a refactor is required to group filters per resource and section (admin and shop) and move the custom ones into the `Doctrine/ORM/Filter/` directory, as other Doctrine-related features are organized.","## Decision  
## Decision

We will refactor the API filter system to group filters by resource and section (admin/shop) instead of by field type. Custom filters will be moved from `Filter/Doctrine/` to `Doctrine/ORM/Filter/` to align with the standard organization of Doctrine-related features in the codebase.

This change improves:

- **Maintainability**: Filters are now co-located with their respective resources and contexts.
- **Clarity**: Developers can easily locate and understand which filters apply to a given resource and section.
- **Customizability**: Overriding or extending filters per resource becomes straightforward.

The old directory structure and generic filter grouping will be deprecated in version 1.14 and removed in 2.0. A migration guide will be provided for plugins and custom implementations relying on the previous structure.","## Decision Drivers

- Need for a clearer structure to reflect resource-based filtering.
- Separation between admin and shop filters.
- Alignment with Doctrine's structure for better maintainability and understanding."
"## Context and Problem Statement

The use of flags alongside language selection in the application UI has caused usability and consistency issues. Flags do not always accurately represent languages, especially in cases where:

1. A language is used in multiple countries, leading to ambiguity.
2. The regional variation of a language (e.g., ""English (Germany)"") is represented by the regionÃÂ¡ÃÂ¯s flag, which may mislead users.

This raises the question: How can we ensure a clear and consistent language selection experience while avoiding potential confusion or technical complexity?","## Decision 
## Decision Outcome

Replace flags with language names in native script (e.g., ""Deutsch"", ""Español"", ""Français"") or, where necessary, use standardized language codes (e.g., ""en-US"", ""pt-BR"") to distinguish regional variants.

**Rationale:**
- Flags represent countries, not languages, and their use can be misleading or exclusionary.
- Using native language names improves clarity and inclusivity, especially for multilingual regions.
- This approach avoids technical complexity in flag management and reduces maintenance overhead.
- Consistency with internationalization best practices and modern UX guidelines.

**Implementation Notes:**
- Sort language options alphabetically by native name.
- For regional variants, include the region in parentheses using standard locale notation (e.g., ""English (Germany)"") without a flag.
- Ensure screen readers announce the full language name correctly.","## Decision Drivers

* **User Experience**: Avoid user confusion and ensure clarity in language selection.
* **Aesthetic Appeal**: Maintain a visually pleasing interface where possible.
* **Technical Simplicity**: Minimize implementation and maintenance overhead.
* **Consistency**: Provide a uniform and predictable experience across all languages."
"## Context and Problem Statement

In v4 we started using Apollo due to its appealing promises that it has better docs, is easier to begin, and does not enforce strict patterns like Relay does. Apollo was first used in a naive way where each component that requires data would execute it's own query resulting in a large number of queries and round trips which performed badly.

We then transitioned to use Fat Queries that utilize fragments to compose a big query whose data response would then be distributed manually to the components that needed the data (Colocation).

Huge effort and complexity needed to be introduced to support this, as Apollo provided limited to no support for this use case. The lack of documentation of our home grown graphql framework is hindering quality contributions from the community.

It also revealed large performance issues inside Apollo that required an elaborate fork to fix.

Apollo is moving in a different direction, preventing us from upgrading apollo itself.

It is very easy to deteriorate performance, introduce bugs and break maintainability due to lack of proper Colocation support and prevention of misuse from the framework.

We reached out to the Apollo team but haven't received the required attention to fix the issues upstream.

For v5 we needed a better solution.","## Decision 
## Decision

For v5, we will transition from Apollo to Urql as our GraphQL client.

Urql provides a more flexible and lightweight approach to GraphQL queries, with first-class support for query composition via fragments and better performance characteristics out of the box. It embraces modern React patterns such as hooks and has strong TypeScript support, which aligns well with our tech stack and goals for maintainability.

Urql’s modular design allows us to build abstractions that enforce colocation and prevent common performance pitfalls, while its active maintenance and clear extensibility points make it feasible to contribute fixes upstream or publish shared plugins for community use.

Additionally, Urql does not impose heavy assumptions on how data is fetched or cached, giving us greater control over optimization strategies. This enables us to implement efficient data fetching patterns without requiring complex workarounds or maintaining a fork.

We will incrementally migrate existing components to use Urql during the v5 development cycle, starting with new features, and provide documentation and tooling to ensure consistent usage across the codebase and improve onboarding for external contributors.","## Decision Drivers

A framework that:

- has first class support for Colocation
- is performant
- encourages best practices
- prevent bugs
- proven to work in a large and complex application
- easy to maintain
- good documentation"
"## Context

Decisions that affect the development of Terraform Controller that are not
captured via a proposal need to be captured in some way. We need a method that
is lightweight and easy to discover the decision that have been made. The record
of decisions will help future contributors to the project to understand why
something has been implemented or is done a certain way.","## Decision 
## Decision

We will use Architecture Decision Records (ADRs) to document significant architectural and design decisions for the Terraform Controller project. 

The [Michael Nygard ADR template](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions) will be adopted, consisting of:
- Status
- Context
- Decision
- Consequences

ADRs will be stored in a dedicated `docs/adrs/` directory using Markdown format with YAML front matter to track metadata such as status, date, and authors. This approach is lightweight, version-controllable via Git, and easily discoverable through the repository structure.

This practice ensures transparency, improves onboarding for new contributors, and provides historical context for future maintenance and evolution of the system.","## Decision

The project will use [Architectural Decision Records
(ADR)](https://adr.github.io/) to record decisions that are made outside of a
proposal.

A [template](./0000-template.md) has been created based on prior work:

* https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions
* https://adr.github.io/madr/"
"## Context

To detect pull request changes, we can use webhooks or polling using GitHub's
API.","## Decision 
## Decision

We will use webhooks to detect pull request changes.

Webhooks provide real-time notifications from GitHub whenever a relevant event occurs (e.g., push, pull request creation, or update), enabling immediate and efficient reaction to changes. This approach reduces latency and unnecessary API calls compared to polling, which would require periodic checks and could introduce delays or increase rate-limiting risks.

Additionally, webhooks integrate well with CI/CD systems and event-driven architectures, aligning with modern best practices for automation and scalability.","## Decision

We decided to start with polling for security reasons. Using webhooks would require users to
open an ingress to the cluster. Because of this requirement, we think security conscious folks may refuse to roll this out on production clusters especially in an
air-gapped environment. This does not mean that we will never consider using webhooks for this, but that initially, polling is what we have chosen to implement.

The Branch-Based Planner has two components:

1. Polling Server: Detect Pull Request changes and manage Teraform resource
   state.
2. Informer: Make a comment when new plan output is available."
"## Context

Like [Flux](https://fluxcd.io/), the tf-controller API has a handful
of places where it accepts cross-namespace references.

 - `Terraform.spec.sourceRef` -- refers to the Flux source object with
   the Terraform program
 - `Terraform.spec.dependsOn[]` -- refers to other objects that must
   be ready before this object can be run
 - `.data.resources[]` -- in the config struct used by the branch
   planner

In general in Kubernetes, references to objects in other namespaces
are frowned upon, because

 - they break namespace isolation assurances; and,
 - they encourage the proliferation of permissions.

Both of these effects make a system less secure.

However: removing cross-namespace refs entirely would break some
installations in a way that would be difficult to fix, because Flux
deployments often rely on defining sources away from objects that use
them.","## Decision  
We retain cross-namespace references in the tf-controller API, but with strict validation and scoped permissions to mitigate security risks.

Specifically:

- Cross-namespace references (e.g., `Terraform.spec.sourceRef`, `spec.dependsOn[]`) are allowed **only when explicitly permitted by a RoleBinding or equivalent RBAC construct** in the target namespace.
- The controller performs **read-time access checks**: before resolving a cross-namespace reference, it verifies that the controller’s service account has permission to *get* the referenced resource in the target namespace.
- All cross-namespace references must be **explicitly allowed via allowlists** defined at the namespace level (future extension point for policy enforcement).
- We add **clear warnings in documentation** about the security implications of cross-namespace refs and recommend colocation of resources whenever possible.

Rationale: While cross-namespace references violate strict namespace isolation, they are essential for existing Flux-based workflows (e.g., shared sources across environments). Removing them would impose significant operational burden. Instead, we opt for a secure-by-default approach that preserves functionality while enforcing least privilege.","## Decision

Deny cross-namespace references by default, but allow them to be
enabled with a flag.

So that the default value means the right thing, the flag name must be
`enable-cross-namespace-refs`, and the default `false`. To avoid
confusion when people try to use the Flux version of this flag
`--disable-cross-namespace-refs`, it should be supported too, but only
respected if supplied."
"## Context

The components provided by this package SHOULD primarily be designed
with the following aims:","## Decision 
## Decision

The components provided by this package SHOULD primarily be designed with the following aims:

1. **Reusability** – Components should be modular and self-contained, enabling use across multiple projects or contexts with minimal modification.
2. **Consistency** – Components should follow uniform design patterns, APIs, and behavior to ensure predictable integration and usage.
3. **Accessibility** – Components must adhere to accessibility standards (e.g., WCAG) to support inclusive user experiences.
4. **Performance** – Components should be lightweight and optimized to minimize impact on load time and runtime performance.
5. **Developer Experience (DX)** – Components should be well-documented, easy to configure, and provide clear error messages and TypeScript support where applicable.

These aims guide design and implementation decisions for all components within the package.","## Decision

Define all suitable components in a way which enables this uniform
workflow:"
"## Context

An alternative configuration procedure to ADR-0002, possibly better
suited for dynamic theming, theme changes and separating the component
configuration between behavioral and stylistic aspects. This new
approach utilizes the hdom context object to retrieve theme attributes,
whereas the previous solution ignored the context object entirely.

A live demo of the code discussed here is available at:

[demo.thi.ng/umbrella/hdom-theme-adr-0003](https://demo.thi.ng/umbrella/hdom-theme-adr-0003)","## Decision 
## Decision

The new approach using the hdom context object for theme attribute retrieval is adopted. This method improves support for dynamic theming and runtime theme switching by leveraging the existing context mechanism, allowing for cleaner separation between behavioral logic and stylistic configuration.

Key benefits:
- Enables real-time theme changes without re-rendering the entire component tree
- Decouples styling from component instantiation
- Aligns with established hdom patterns and reduces boilerplate
- Facilitates theme inheritance and overrides through context propagation

This decision supersedes aspects of ADR-0002 related to static theme configuration. The context-based solution proves more flexible and maintainable for the project's evolving theming requirements.",## Decision
"## Context and Problem Statement

On the Application Services team, we have successfully built several re-useable components for sync- and storage-related browser
functionality by following what we've dubbed the ""rust-components"" approach: write the bulk of the code in rust
so we can cross-compile it for different target platforms, have it expose a C-compatible FFI layer, then write a small amount
of FFI bindings code to expose the functionality to each of several different target languages (e.g. Swift and Kotlin).

The FFI layer and foreign-language bindings code is currently written by hand, a tedious and potentially error-prone
process.

Given that we expect to build additional components in this style in the future, and expect more teams at Mozilla to
do the same, can we increase the efficiency and reliability of this work by auto-generating some of this code?","## Decision  
## Decision

Adopt and integrate **UniFFI** to auto-generate FFI bindings for our Rust components.

- We will use [UniFFI](https://github.com/mozilla/uniffi-rs) to generate C-compatible FFI layers and corresponding language bindings (e.g. Kotlin, Swift, Python) from a single interface definition written in WebIDL-like syntax.
- This reduces the need for manual FFI and bindings code, minimizing errors and inconsistencies across platforms.
- The generated code will interoperate safely with hand-written glue code where necessary, allowing incremental adoption.
- We will establish a standard project structure and tooling pipeline for new rust-components using UniFFI, including CI/CD support for multi-platform builds and binding generation.
- Documentation and onboarding materials will be created to enable other Mozilla teams to adopt this approach.

This decision improves developer velocity, code maintainability, and cross-language compatibility while preserving the performance and portability benefits of the rust-components architecture.","## Decision Drivers

* Reduce time taken to launch a new rust component.
* Improve maintainability of existing rust components.
* Reduce possibility of errors in hand-written foreign language bindings code.
* Continue shipping components on a regular cadence."
"## Context and Problem Statement

When [deciding to build this tool](./0000-whats-the-big-idea.md), the main risk identified was that we'd spend too
much time on an ultimately unworkable or unmaintainable idea. What early design decisions can we make to mitigate
this risk? What things are an existential risk to the success of this project that must be included in the first
version, and what things can we safely defer to future work?

In other words: how do we build an MVP of this tool that is both *minimal* and *viable*?","## Decision 
## Decision

We will adopt a strict Minimum Viable Product (MVP) approach for the initial release of the tool. The core functionality will be limited to the smallest set of features that delivers tangible value and validates the fundamental assumptions of the project. Non-essential features, optimizations, and edge-case handling will be explicitly deferred.

To ensure viability:
- We will implement one complete end-to-end workflow that demonstrates the primary use case.
- We will include basic error handling and logging sufficient for debugging core failures.
- We will build with extensibility in mind (e.g., modular architecture), but avoid over-engineering.

To ensure minimalism:
- All features must pass a ""core value"" test—directly contributing to the primary user outcome.
- Infrastructure and tooling will be kept simple (e.g., single binary output, minimal dependencies).
- UI/UX will be functional but not polished.

This approach allows us to validate the concept quickly, gather feedback early, and pivot if necessary—reducing the risk of investing in an unworkable solution.","## Decision Drivers

* Strictly timebox our efforts to ""prove out"" the approach.
* Establish whether we can effectively maintain this kind of tool as a team.
* Support initial development of a new rust component with externally-imposed, near-term deadlines."
"## Context and Problem Statement

Passing complex data-types from Rust to foreign-language code and back again can be, well, *complex*.
Given a Rust struct with named fields, and a corresponding autogenerated data class in the foreign language
bindings, how does one turn into the other?","## Decision 
## Decision

Use the `serde` library to serialize complex data types to JSON when passing them between Rust and foreign-language code.

- All structs that need to be passed across the FFI boundary must implement `serde::Serialize` and `serde::Deserialize`.
- Data is serialized to a JSON string in Rust, passed as a C-compatible string (`*const c_char`) over the FFI boundary, and then deserialized in the target language (e.g., Kotlin, Swift, JavaScript).
- The reverse process is used for returning complex types from foreign code to Rust, if needed.
- Simple types (integers, booleans, strings) continue to be passed directly through FFI without serialization.

This approach:
- Leverages existing, well-tested serialization infrastructure (`serde`, `serde_json`).
- Ensures type safety through derived (de)serializers.
- Reduces the need for manual memory management or unsafe bridging logic.
- Is human-readable and debuggable.
- Is compatible with a wide range of languages and platforms.

**Note:** Performance-critical paths may later optimize this using raw binary formats (e.g., CBOR, Bincode), but JSON is preferred during initial development for its simplicity and interoperability.","## Decision Drivers

* Ensuring safety of the generated code.
* Fitting with our [MVP goal](./0001-mvp-webidl.md) of favouring initial-time-to-ship over performance."
"## Context and Problem Statement

Uniffi currently uses a very coarse locking strategy for managing concurrent access to object instances,
which has caused us to accidentally ship code in a product that [blocked the main thread on network I/O](https://jira.mozilla.com/browse/SDK-157).
We need to enable finer-grained concurrency control in order to provide the desired API for a key consumer.

Currently, every interface has a corresponding [ffi_support::ConcurrentHandleMap](https://docs.rs/ffi-support/0.4.0/ffi_support/handle_map/struct.ConcurrentHandleMap.html) that is responsible for owning all instances of
that interface and for handing out references to them in a mutability-safe and threadsafe manner. This
ensures that the generated code is safe in the face of concurrent operations, but has a substantial
runtime cost: only one method call can be executed on an instance at any time. Any attempt to call an
object method while a concurrent method is already executing, will block until the previous call has completed.

The desired API for Project Nimbus includes methods that will be called synchronously from the main thread,
and hence must not block on network or disk I/O. Such an API cannot be built with uniffi as currently
implemented.","## Decision 

We will refactor Uniffi's concurrency model to replace the coarse-grained `ConcurrentHandleMap` locking with fine-grained per-instance or per-method concurrency control. Instead of a single global lock per interface, each object instance will manage its own synchronization, allowing truly concurrent method calls on different instances.

For methods that perform potentially blocking operations (e.g., network or disk I/O), we will require them to be marked as `async` in the UDL (Uniffi Definition Language). The generated bindings will expose these as asynchronous functions in the foreign language (e.g., `async`/`await` in Kotlin, `Task` in Swift via callbacks or async primitives). This ensures that synchronous main-thread APIs do not block on long-running operations.

Additionally, we will introduce an attribute in UDL (`[blocking]`) to explicitly mark methods that may perform blocking work, triggering appropriate runtime warnings or requiring async wrappers during code generation. For maximum safety and clarity, all methods that interact with external resources must either be async or include explicit documentation and opt-in annotations if they must remain sync.

This change enables Project Nimbus to implement non-blocking, responsive APIs while maintaining memory and thread safety across FFI boundaries.","## Decision Drivers <!-- optional -->

* Enabling consumers to control the potential blocking behaviour of their generated APIs.
* Ensure safety of the generated code.
* Ship a solution in a timely manner to unblock Project Nimbus."
"## Context and Problem Statement

[ADR-0003](0003-threadsafe-interfaces.md) introduced support for ""thread-safe
interfaces"" - possibly leading to the impression that there is such a thing as
non-threadsafe interfaces and confusion about exactly what the attribute means.

However, the entire concept of non-threadsafe interfaces is a misconception -
the Rust compiler insists that everything wrapped by uniffi is thread-safe -
the only question is who manages this thread-safety. Interfaces which are not
marked as thread-safe cause uniffi to wrap the interface in a mutex which is
hidden in the generated code and therefore not obvious to the casual reader.

The `[Threadsafe]` marker acts as a way for the component author to opt out of
the overhead and blocking behaviour of this mutex, at the cost of opting in to
managing their own locking internally. This ADR proposes that uniffi forces
component authors to explicitly manage that locking in all cases - or to put
this in Rust terms, that all structs supported by uniffi must already be
`Send+Sync`

Note that this ADR will hence-forth use the term `Send+Sync` instead of
""Threadsafe"" because it more accurately describes the actual intent and avoids
any misunderstandings that might be caused by using the somewhat broad and
generic ""Threadsafe"".","## Decision 
## Decision

We will remove the `[Threadsafe]` attribute from UniFFI and require that all interface types are `Send + Sync`.

This means:

* The `[Threadsafe]` attribute is removed from the UniFFI IDL.
* All structs exposed through UniFFI must be safe to send between threads (`Send`) and safe to share between threads (`Sync`).
* The automatic internal mutex wrapping — previously applied to non-`[Threadsafe]` types — is removed entirely.
* Component authors are now responsible for ensuring thread safety, either by:
  * Designing their types to be naturally `Send + Sync` (e.g., by avoiding interior mutability or using `Arc<Mutex<T>>`, `RwLock<T>`, etc.), or
  * Using thread-safe smart pointers and concurrency primitives explicitly in their Rust code.

### Rationale

* **Clarity**: Removing the implicit mutex eliminates confusion about who is responsible for thread safety. With the old model, users could not easily tell whether a type was internally synchronized or protected by an auto-generated wrapper.
* **Performance**: Eliminating a potentially unnecessary mutex layer avoids hidden performance costs and contention.
* **Predictability**: All exported types now have consistent, explicit concurrency semantics.
* **Alignment with Rust idioms**: Requiring `Send + Sync` aligns with Rust’s ownership and concurrency model, encouraging correct-by-construction designs.

### Migration

* Existing types that relied on automatic synchronization will need to be updated to wrap internal state in `Mutex`, `RwLock`, or another appropriate concurrency primitive.
* Generated foreign language bindings will no longer include implicit locking behavior — any required synchronization must be documented and managed at the application level if needed.

This change enforces explicit, maintainable, and performant concurrency design in UniFFI components.","## Decision Drivers

* Supporting non-`Send+Sync` structs means uniffi must add hidden locking to make
  them `Send+Sync`. We consider this a ""foot-gun"" as it may lead to accidentally
  having method calls unexpectedly block for long periods, such as
  [this Fenix bug](https://github.com/mozilla-mobile/fenix/issues/17086)
  (with more details available in [this JIRA ticket](https://jira.mozilla.com/browse/SDK-157)).

* Supporting such structs will hinder uniffi growing in directions that we've
  found are desired in practice, such as allowing structs to use [alternative
  method receivers](https://github.com/mozilla/uniffi-rs/issues/417) or to
  [pass interface references over the FFI](https://github.com/mozilla/uniffi-rs/issues/419)."
"## Context and Problem Statement

UniFFI currently manages object instances using the `HandleMap` struct in the ffi-support crate.
This means that external consumers of UniFFI-wrapped interfaces never see
any pointers to structs - instead, they get what is (roughly) an index into
an array, with the struct being stored in (and owned by) that array.

This has a number of safety characteristics which are particularly important for
hand-written FFI interfaces, but it does cause some issues in evolving UniFFI in
directions we consider important. In addition to the slight performance overhead,
the use of `HandleMap`s makes it difficult to support:

* Passing object instances as arguments ([#40](https://github.com/mozilla/uniffi-rs/issues/40)).
  Getting objects out of a `HandleMap` involves a closure, so accepting multiple
  object-typed arguments would involve code-generating nested closures.
* Returning object instances from functions ([#197](https://github.com/mozilla/uniffi-rs/issues/197)).
  Does the returned object already exist in the handlemap? If so, what is its handle?
  How will we manage the lifetime of multiple references to the object?

These restrictions mean that UniFFI's `Object` type is currently only suitable
as the `self` argument for method calls, and is forbidden in argument position,
as record fields, etc.

This ADR considers ways to evolve the handling of object instances and their
lifetimes, so that references to structs can be used more widely than currently allowed.","## Decision 
We will transition from `HandleMap`-based object management to a raw pointer + reference counting model for managing object instances in UniFFI.

This approach will:
- Use `Box::into_raw` to convert Rust objects into raw pointers for FFI export
- Wrap raw pointers in language-specific smart pointers (e.g., `std::shared_ptr` in C++, `Arc` in Rust) when crossing language boundaries
- Implement `free` methods that call `Box::from_raw` to properly clean up objects
- Generate additional binding code to manage reference counting across language boundaries

The decision is based on the following rationale:
1. Raw pointers enable passing objects as arguments and returning them from functions
2. Reference counting provides automatic memory management while maintaining safety
3. This approach scales to complex object graphs and multiple references
4. It aligns with common practices in other FFI systems
5. The performance overhead is acceptable compared to the current HandleMap approach

We accept the increased complexity in generated code and the need for careful null-pointer checking as trade-offs for the significantly improved API flexibility.","## Decision Drivers

* We desire the ability to have more flexible lifetimes for object interfaces, so
  they can be stored in dictionaries or other interfaces, and be returned by
  functions or methods other than constructors.

* We would like to keep the UniFFI implementation as simple as possible while
  providing a suitable degree of safety - in particular, a promise that it
  should be impossible to misuse the generated bindings in a way that triggers
  Rust's ""undefined behavior"" or otherwise defeats Rust's safety
  characteristics and ownership model (and in particular, avoiding things like
  use-after-free issues).

* We would like to keep the overhead of UniFFI as small as possible so that it
  is a viable solution to more use-cases."
"## Context and Problem Statement

UniFFI was not able to support types from external crates because Rust's orphan
rule prevents implementing the `ViaFfi` trait.  In order to add support we
needed to choose between updating the `uniffi` traits or updating the
`lift_py` and `lower_py` scaffolding functions.

The same general question comes up often.  When adding new features we often
need to choose between two paths:

  * Updating the code in the target language
  * Updating the template wrapping code

This ADR discusses this particular decision and also the general pros and cons of each
path.","## Decision  
## Decision Drivers

* **Respect for downstream ecosystems**: Modifying external crates or language-specific code (e.g., Python libraries) can introduce maintenance burden, compatibility issues, and resistance from maintainers. It risks creating forks or non-standard behavior.
* **Maintainability and control**: Changes to UniFFI’s own scaffolding (`lift_py`, `lower_py`) are under our control, easier to test, and do not depend on third-party approval or release cycles.
* **Long-term flexibility**: Template-based solutions can evolve independently and support multiple versions of target languages or libraries without requiring changes upstream.
* **Community contribution barriers**: Proposing changes to widely-used external crates may face high review overhead, slow iteration, or rejection, delaying development.

## Decision

Prefer modifying UniFFI's internal scaffolding (e.g., `lift_py`, `lower_py`) over extending external crates with `ViaFfi` implementations. This approach:

- Avoids violating the orphan rule by keeping trait implementations local
- Keeps integration logic encapsulated within UniFFI
- Enables faster iteration and broader type support without upstream dependencies

This preference applies generally when choosing between augmenting target-language code versus enhancing UniFFI’s own binding layer: favor changes that preserve ecosystem compatibility and reduce external coupling.","## Decision Drivers

* We wanted to support external crates that define new types by wrapping an
  UniFFI primitive type.  For example supporting `serde_json::Value` that wraps
  `string` or a `Handle` that wraps `int32`.  We wanted this kind of wrapping
  code to exist outside of `uniffi` to allow for more experimentation with
  wrapped types and to support types that were specific to particular libraries
  (for example the application-services `Guid` type)."
"## Context and Problem Statement
All the binding generators currently live in the [`uniffi_bindgen`](../../uniffi_bindgen/src/bindings) crate. This creates the following difficulties:

- All the bindings live in the `uniffi` repository, so the `uniffi` team has to maintain them (or at the very least review changes to them). This makes it difficult to support third-party developers writing bindings for languages the core team does not wish to maintain.
- Any change to a specific binding generator requires a new `uniffi_bindgen` release for it to be accessible by consumers. Even if it doesn't impact any of the other bindings.
- Some bindings require complex build systems to test. Including those build systems in `uniffi` would require developers to install those build systems, and CI to do the same. For example, any type of `gecko-js` bindings would require the mozilla-central build system to build and test.
- We currently run all the tests for the bindings in our CI and through `cargo test`. This means that if one binding target gets outdated and fails, or if a developer doesn't have the needed libraries installed for one of the targets, the tests would fail.

Before [PR 1201](https://github.com/mozilla/uniffi-rs/pull/1201), it was also impossible to write new bindings that did not live in the [`uniffi_bindgen`](../../uniffi_bindgen/src/bindings) crate.

This ADR proposes enabling third-party crates to implement binding generators, and describes the necessary uniffi changes to enable this.","## Decision 
## Decision
We will refactor the `uniffi_bindgen` crate to allow third-party crates to implement and distribute their own binding generators. This will be achieved by:

- Defining a stable, public API/traits in `uniffi_bindgen` that binding generators must implement.
- Moving language-specific binding generators out of the core `uniffi_bindgen` crate and into separate, optionally included modules or external crates.
- Publishing documentation and examples for how third-party developers can create and maintain their own binding generators.
- Updating CI to only test officially supported bindings, reducing overhead and allowing optional dependencies.

This enables:
- Faster iteration on individual bindings without requiring full `uniffi_bindgen` releases.
- Reduced maintenance burden on the core team.
- Support for experimental or niche language bindings (e.g., gecko-js) without bloating the main repository or requiring complex build dependencies from all contributors.
- Isolated testing and build environments per binding generator.

Third-party binding crates will be responsible for their own testing, versioning, and distribution, while adhering to the contract defined by the core `uniffi_bindgen` traits.","## Decision Drivers

* Support Firefox Desktop JavaScript binding generation
* Testability, it should be easy for developers to test the bindings they care about. Without having to navigate and install unfamiliar libraries and build systems.
* Developer experience, it should be easier to write and maintain a new binding generator than it currently is.
* Releases, cutting releases for changes in one binding generator shouldn't harm another.

**NOTE**: Version compatibility is handled in a [separate ADR](https://github.com/mozilla/uniffi-rs/pull/1203)"
"## Context

In previous releases of Zarf, the creation of the initialization package at the core of many of our E2E tests required repository secrets to login to registry1. Since this is an open-source project, anyone could submit a change to one of our GitHub workflows that could steal our secrets. In order to protect our secrets from any bad-actors we used [peter-evans/slash-command-dispatch@v2](https://github.com/peter-evans/slash-command-dispatch) so that only a maintainer would have the ability to run the E2E tests when a PR is submitted for review.

In the current version of Zarf (v0.15) images from registry1 are no longer needed to create the zarf-init-{{arch}}.tar.zst. This means, given our current span of E2E tests, we no longer need to use repository secrets when running tests. This gives us the ability to reassess the way we do our E2E testing.

When considering how to handle the tests, some of the important additions we were considering were:
  1. Ability to test against different kubernetes distributions
  2. Ability to test against different linux distributions
  3. Ability to run (at least some of) the E2E tests locally without relying on an ec2 instance - for quicker feedback loops when developing new features","## Decision 
## Decision

We will transition our E2E testing strategy to run automatically on all pull requests without requiring maintainer intervention via slash commands.

Since Zarf v0.15 no longer requires access to registry1 secrets to create the initialization package, the primary security concern that necessitated manual approval has been removed. This allows us to safely trigger E2E workflows directly on PRs from both internal and external contributors.

As part of this change, we will:

1. Remove dependency on `peter-evans/slash-command-dispatch@v2` for E2E test triggering.
2. Reconfigure GitHub Actions workflows to run E2E tests automatically on `pull_request` events.
3. Begin incremental implementation of multi-distribution testing:
   - Add matrix-based testing across supported Kubernetes distributions (e.g., K3s, KinD, MicroK8s) in CI.
   - Lay groundwork for Linux distribution testing using containerized environments.
4. Introduce local E2E test runners using KinD or similar lightweight tools, enabling developers to execute a subset of E2E tests locally for faster iteration.

This shift improves contributor experience, shortens feedback loops, and aligns with open-source best practices—while maintaining security through the elimination of secret dependencies in public workflows.","## Decision

The previous E2E test code was not extensible enough to be reused to test Zarf against different kubernetes distributions. The test suite was refactored so that we could write a setup and teardown function for each kubernetes distribution we wanted to verify against and the test suite was then responsible for cycling through the different distributions. This gives us the ability to test multiple kubernetes distributions against the same exact test cases.

The individual test cases were also rewritten to not rely on terratest running a bash command over ssh. Instead, the test uses the locally built Zarf binary and example packages to validate expected behavior. This approach works both on local dev machines (linux/mac) and on the Ubuntu GitHub Runner that gets triggered when a pull request is created. This also has the positive side effect of not needing to wait several minutes for an ec2 instance to spin up for testing.

Since we no longer need repository secrets to run the E2E tests, we removed the requirement for a maintainer to use a `/test all` chatops command to dispatch the tests. Instead, there is a new test workflow defined for each kubernetes distribution we are verifying against and the tests get run automatically whenever a PR is created or updated.

When looking back at the list of 'important additions' we were considering above. All three are addressed with this approach. Testing against different kubernetes distributions is as simple as defining how to create and destroy the cluster. All of the test cases are runnable locally and then because of that testing on a linux distribution is possible by just switching over to another machine and running the same `make test-e2e` there. This also gives us the ability to test against cloud distributions like EKS! All you need is a valid kubeconfig and running `go test ./...` in the `./src/test/e2e` directory will run all of the test cases against the EKS cluster."
"## Context

In order to create any workloads in K8s, an image has to exist on the node or be pulled in from an OCI Distribution server (docker registry). Local K8s distros such as KIND, K3D, K3S, Microk8s have CLI support for injecting or pushing images into the CRIs of cluster nodes. No standard or tool exists to do this generically in K8s as the CRI sits outside the problem of K8s beyond just communicating with it. For Zarf to push images into a K8s cluster that does not have a CLI option to inject the image, we have to establish some mechanism to create a temporary registry, use an existing one, or inject the image in the cluster. Zarf must also support unknown environments with no other dependencies; we cannot assume a registry exists. Typically when K8s pulls images from a remote registry, it sends the request to the CRI that then does the pull outside of the K8s context. The CRI runs at the host level on a per-node basis, so to access a private registry, the TLS trust needs to be modified on any host/node that would attempt to pull the image. The two primary ways to do this are modifying the node's root certificate authorities or the CRIs configuration if it has an option for TLS root CA. Lastly, as this is per node, all nodes in the cluster would need to be modified or some affinity/taint to force the pod to use a single node during bootstrapping.","## Decision 
We will create a temporary, ephemeral registry within the cluster as part of the Zarf agent during package deployment. This internal registry will be used to distribute container images to the nodes without relying on external registries or pre-existing infrastructure.

* The temporary registry will be automatically created and destroyed per deployment, ensuring minimal footprint and no dependency on external systems.
* Images will be pushed directly into this registry by the Zarf CLI during deployment initialization.
* Kubernetes pods will pull images from this local registry using standard image pull mechanisms, avoiding the need to modify node-level CRI configurations or TLS trust stores.
* To ensure all nodes can access the images, we will leverage Kubernetes Services and networking (e.g., ClusterIP or NodePort) to expose the registry internally, combined with tolerations and node selectors if needed for bootstrapping scenarios.
* This approach supports air-gapped environments and unknown clusters without requiring administrative access to host systems or CRI configuration.

This solution ensures portability, security, and compatibility across diverse Kubernetes distributions while adhering to the principle of least external dependency.","## Decision

Because we cannot require third-party dependencies, the requirement from some cloud-managed or distro-managed registry is not an option as not every K8s cluster will have that available.  Running an in-memory registry while zarf is performing `zarf init` was initially explored to resolve the external dependency issue.  However, this solution proved too complex when dealing with network traversal (especially behind NATs), firewall, and NACL rulesets.  Additionally, because of the complexities around TLS trust, the in-memory registry was not a viable solution.

After iterating on several solutions, including `kubectl cp`, `kubectl exec`, a busybox configmap injection that brought in netcat to stream files, and a few variations on those ideas, we found using only configmaps to be the most predictable method for injection. The challenge with this mode is that the ETCD limit is 1 MB total, with binary data base64-encoded leaving roughly 670 KBs of data per configmap, far too small to bring the registry tarball in a single configmap. So instead, we used a combination of a custom Rust binary compiled with MUSL to perform file concatenation, untarring, and verification of a set of split tarball chunks across a variable number of configmaps. We found that chunking by less than the near-maximum configmap size was more performant on some resource-constrained control planes. The details of this solution and a drawing are located in the main README of this repo."
"## Context

SBOM are required for software running on government hardware per EO14028.","## Decision 
## Decision

We will generate SBOMs for all our software releases using [Syft](https://github.com/anchore/syft) and include them in the release artifacts. The SBOMs will be generated in SPDX format as it is widely supported and recommended by NIST.

The SBOM generation will be integrated into our CI/CD pipeline to ensure they are created for every build. We will also implement validation steps to verify the integrity of the SBOMs and ensure they accurately reflect the dependencies in each release.

For containerized applications, we will generate SBOMs for both the source code dependencies and the container image contents. The SBOMs will be stored alongside the binaries and images in our artifact repository with clear version matching.

We will maintain a record of all SBOMs for audit purposes and establish a process for responding to security vulnerabilities identified through SBOM analysis. This includes regular scanning of our SBOMs against vulnerability databases and establishing patch timelines based on criticality.","## Decision

Using Witness' Syft attestor functionality allows Zarf to continue to get more refined SBOM capabilities as Witness' capabilities expand over time. Syft is capable of finding installed packages and some binaries for statically compiled dependencies over each image within a Zarf package. This allows for SBOMs for each image to be generated and packaged along with the Zarf package.  Abilities to export the SBOM to SDPX and CycloneDX formatted documents as well as a browse-able web page are in works."
"## Context

Currently Zarf leverages [Helm Post Rendering](https://helm.sh/docs/topics/advanced/#post-rendering) to mutate image paths and secrets for K8s to use the internal [Zarf Registry](../packages/zarf-registry/). This works well for simple K8s deployments where Zarf is performing the actual manifest apply but fails when using a secondary gitops tools suchs as [Flux](https://github.com/fluxcd/flux2), [ArgoCD](https://argo-cd.readthedocs.io/en/stable/), etc. At that point, Zarf is unable to provide mutation and it is dependent on the package author to do the mutations themselves using rudimentary templating. Further, this issue also exists when for CRDs that references the [git server](../packages/gitea/). A `zarf prepare` command was added previously to make this less painful, but it still requires additional burden on package authors to do something we are able to prescribe in code.","## Decision 
## Decision

We will implement a **Zarf Pre-Patch Hook System** to programmatically mutate manifests and CRDs before they are applied, regardless of the deployment method (direct apply or GitOps).

This system will:

1. Allow Zarf to inject image path rewrites and secret references (e.g., for the internal Zarf Registry and Gitea) during package deployment, even when using external GitOps tools like Flux or ArgoCD.
2. Replace reliance on Helm post-rendering and manual templating by package authors for common mutations.
3. Use structured patch definitions (strategic merge or JSON 6902 patches) defined in the Zarf package manifest (`zarf.yaml`) under a new `hooks.prePatch` section.
4. Support patching both standard Kubernetes resources and CRDs (e.g., Flux Kustomizations, ArgoCD Applications, Gitea webhook configurations).
5. Execute pre-patch operations during `zarf deploy`, after unpacking but before applying manifests.

Rationale:
- Removes burden from package authors who currently must manually template image paths and endpoints.
- Enables consistent behavior across all deployment patterns, including GitOps.
- Builds upon lessons from `zarf prepare`, automating what was previously a manual step.
- Maintains backward compatibility: Helm post-rendering remains supported for direct applies, but pre-patch hooks are recommended for GitOps scenarios.

Example usage in `zarf.yaml`:
```yaml
components:
  - name: flux-monitored-app
    hooks:
      prePatch:
        - target:
            kind: Kustomization
            name: app-kustomization
            namespace: flux-system
          patchFile: patches/images.yaml
        - target:
            kind: Repository
            name: app-repo
          patchFile: patches/gitea-url.json
```

This change increases Zarf’s autonomy and reliability in complex deployment topologies.","## Decision

A [mutating webhook](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) is standard practice in K8s and there [are a lot of them](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#what-does-each-admission-controller-do). Using the normal Zarf component structure and deployment strategy we can leverage a mutating webhook to perform automatic imagePullSecret binding and image path updates as well as add additional as-needed mutations such as updating the [GitRepository](https://fluxcd.io/docs/components/source/gitrepositories/) CRD with the appropriate secret and custom URL for the git server if someone is using Flux."
"## Context

Currently, Zarf only allows variables to be specified within components which introduces the following limitations:

 - Variables are scoped to the component making reuse between components impossible
 - Variables cannot be used for elements within the package definition itself
 - Variables can only be set at create time, not by a user at deploy time

This forces a package creator to copy information within their package/component definitions and also requires them to make bespoke packages per environment even if only small changes are needed to the overall spec (such as changing a domain name).","## Decision 
## Decision

We will introduce a new `variables` section at the package level that allows defining variables outside of components. These variables will be globally accessible across all components within the package and can be referenced in both component definitions and package-level fields.

Additionally, we will support user-defined variable values at deploy time through a new `--set-variable` flag or an interactive prompt if variables are marked as required but not provided. This enables environment-agnostic packages where specific values (e.g., domain names, replica counts) can be injected during deployment.

Variable precedence will follow this order:
1. Values set via `--set-variable` at deploy time
2. Values defined in a deploy-time variables file (e.g., `zarf.variables.yaml`)
3. Default values defined in the package’s `variables` section
4. Empty string if no value is provided and no default exists (with validation failure for required variables)

This change increases flexibility, promotes reuse, and reduces duplication in Zarf package definitions.","## Decision

The decision was made to move variable definitions to the package level and to split ""variables"" into three distinct types:

- Variables (specified with the `variables` yaml key) allow for the templating of component files similar to the component variables before them.  The main changes are that they are now specified at the package level (allowing for reuse between components) and have additional prompting and defaulting features to allow a package creator to ask for more information during `zarf package deploy`.
- Constants (specified with the `constants` yaml key) also template component files, but must be specified at package create time.  This allows a package creator to use the same value in multiple places without the need for copying it and without the package deployer being able to override it.
- Package Variables (specified by using `"
"## Context

In ADR 3, the decision was made to create a rust binary (`stage1`) that would re-assemble a `registry:2` image and a go registry binary (`stage2`) from a series of configmaps. While this solution works, it is overkill for the operations that `stage2` performs. The `stage2` binary is only responsible for 1. starting a docker registry in `rw` mode, 2. pushing the `registry:2` crane tarball into said registry, 3. starting the docker registry in `r` mode. This `registry:2` image is then immediately consumed by the `zarf-registry` package, creating a true in-cluster docker registry. The injector pod is then destroyed. The overhead this operation creates:

- having to keep track of another binary (making the total number 3 for the zarf ecosystem)
- nearly doubling the amount of configmaps loaded into the cluster (makes init slower)
- having to compile the binary for each platform (adds to the build time + ci)
- using a full-featured docker registry to host a single image (wasteful)","## Decision 
## Decision

We will replace the `stage2` Go binary with a lightweight Rust binary that performs the minimal necessary operations to bootstrap the in-cluster registry. This new binary will:

- Directly unpack and serve the `registry:2` crane tarball using a minimal HTTP server
- Eliminate the need to run a full Docker registry during initialization
- Serve the image content directly from memory or temporary storage
- Support read-only mode only, as write capabilities are no longer needed
- Be compiled for all required platforms but maintained as a single, simple codebase

This change reduces complexity, decreases initialization time, and removes unnecessary dependencies while maintaining the same end result. The binary will be integrated into the existing `stage1` process to further streamline the bootstrapping sequence.","## Decision

The rust binary is already being injected via configmap and unpacking the tarball. There is little need to bring everything but the kitchen sink to just serve a single image. Therefore the decision is to use the rust binary to perform the entire injection process. This required a few changes to the rust binary:

- not only re-assemble + unpack the tarball, but also unpack the `registry:2` image (stored as a [crane tarball format](https://github.com/google/go-containerregistry/tree/main/pkg/v1/tarball))
- transform the `registry:2` crane manifest to a docker v2 manifest
- spin up an HTTP server compliant with the v2 docker registry API to serve the `registry:2` image"
"## Context

There are sections of the code that are not easily able to be tested by our end-to-end testing and now that Zarf is moving into being a library for other products there will be more scrutiny around our defined interfaces.  This ADR explores ways that we could adjust our testing strategy to address these concerns.

Potential considerations:

1. Introduce Unit-Testing Zarf-wide
    Pros:
        - More of the codebase is covered with tests and will force us to take more consideration when updating it
    Cons:
        - As the codebase evolves the efficacy of these tests is likely to go down, especially if they are written to the code rather than the interface

2. Introduce Unit Testing in a defined and limited capacity within Zarf
    Pros:
        - A decent amount of the codebase will be tested (specifically we can target those areas difficult to e2e test)
        - We can be picky about what/where we unit test so that the maintenance burden of the tests is not too high
    Cons:
        - We will need to be vigilant when PRs come through with Unit Tests that these are correct applications of them

3. Introduce Integration Testing or partial End-to-End testing
    Pros:
        - This is closer to how we already test within Zarf
    Cons:
        - These can be relatively complex to orchestrate and maintain over time requiring complex mocks or other mechanisms to make them work","## Decision 
## Decision

We will adopt **Option 2: Introduce Unit Testing in a defined and limited capacity within Zarf**.

We will strategically apply unit testing to areas of the codebase that are difficult or impossible to cover with end-to-end tests—particularly core logic, utilities, and well-defined interfaces that are critical for stability and reuse across products. These tests will focus on public interfaces and contracts rather than implementation details to ensure resilience to internal refactoring.

Unit tests must be:
- Isolated and fast
- Focused on pure logic, input validation, error handling, and interface behavior
- Avoid tight coupling to internal structures (e.g., private methods, concrete types)

We will establish clear guidelines in the contribution documentation to help reviewers evaluate appropriate use of unit tests in PRs. This approach balances improved test coverage with manageable maintenance overhead, supporting Zarf’s evolution into a reusable library while maintaining development velocity.","## Decision

It was decided to introduce a limited set of unit tests to the Zarf codebase (2) because this will enable us the most flexibility and control over how we test while also allowing us to specifically target specific units without a ton of overhead.  Specifically we will ask the following before implementing a Unit Test:

1. Is what I want to test a true unit (i.e. a single function or file)?
2. Does what I want to test have a clearly defined interface (i.e. a public specification)?
3. Is this code inside of the `src/pkg` folder or should it be?"
"## Context

Images are not the only way that dependencies can be brought in via a Zarf package (our own init package has two components that are not images yet could have vulnerabilities within them, k3s and the injector).  We should SBOM these in addition to images so that we are providing a more complete picture to our users.

Potential considerations:

1. Run Syft against the entire zarf temp build directory
    Pros:
        - This would provide the most information for what _might_ be in a Zarf package
    Cons:
        - If git repos are brought in you could catch many index files that are actually not in the package but are dependencies of the repos leading to confusion

2. Run Syft against files and dataInjections
    Pros:
        - We know these files are actually inside of the package and won't just be noise
        - This is the most common two ways for people to include additional artifacts in packages (we use files in our own init package)
    Cons:
        - This is only a subset of what Zarf provides and someone could commit an artifact to a git repo for example

3. Allow user flexibility in what gets SBOMed
    Pros:
        - This provides the user the most flexibility
    Cons:
        - This could be complex to get right on implementation and if it were optional may be a forgotten feature by most users","## Decision 
## Decision

We will implement option 2 - Run Syft against files and dataInjections.

This approach provides a good balance between comprehensive vulnerability reporting and implementation complexity. Since files and dataInjections are the primary mechanisms for including non-image artifacts in Zarf packages (and are used even in our own init package), focusing on these ensures we capture meaningful, actual package contents without introducing noise from transient build artifacts.

While this doesn't cover every possible way artifacts could be included (like git repos), it addresses the most common and impactful cases. The guarantee that we're only scanning files actually included in the final package prevents false positives and user confusion, which outweighs the benefit of broader but noisier scanning.

We can revisit extending SBOM coverage to other artifact types based on user feedback and observed usage patterns.","## Decision

It was decided that we would SBOM the files and dataInjections inclusions by component (2) and include them into our SBOM viewer that way.  This will allow us to characterize what SBOMing in Zarf may look like going forward without introducing something that is just optional or that might make too much noise at first blush."
"## Context

Zarf was rooted in the idea of declarative K8s deployments for disconnected environments. Many of the design decisions made in Zarf are based on this idea. However, in certain connected environments, Zarf can still be leveraged as a way to define declarative deployments and upgrades without the constraints of disconnected environments. To that end, providing a declarative way to deploy Zarf packages without the need for a Zarf init package would be useful in such environments.","## Decision  
## Decision

Introduce an optional `--no-init` flag in the `zarf package deploy` command to allow deployment of Zarf packages without requiring a prior `zarf init` step in connected environments.

This mode will:
- Skip the initialization sequence (including secret generation, registry seeding, and Git server setup) normally performed by `zarf init`.
- Assume that required services (e.g., image registry, Git server, artifact storage) are already available and accessible in the target environment.
- Rely on standard Kubernetes service accounts and network policies for authentication and access control.
- Only be allowed when deploying packages with remote references (e.g., images from public/private registries, Helm charts from remote repos) rather than embedded assets.

This functionality is intended for use in trusted, connected environments where infrastructure prerequisites are managed externally, enabling Zarf to function as a pure declarative deployment tool without the overhead of self-contained airgap support.

Rationale:
- Reduces friction for developers and CI/CD pipelines operating in online environments.
- Maintains backward compatibility — the default behavior still requires `zarf init`.
- Preserves security boundaries by restricting `--no-init` deployments to configurations that do not rely on Zarf-managed secrets or embedded components.
- Aligns with GitOps practices where cluster state is defined declaratively and external systems manage dependencies.","## Decision

YOLO mode is an optional boolean config set in the `metadata` section of the Zarf package manifest. Setting `metadata.yolo=true` will deploy the Zarf package ""as is"" without needing the Zarf state to exist or the Zarf Agent mutating webhook. Zarf packages with YOLO mode enabled are not allowed to specify components with container images or Git repos and validation will prevent the package from being created."
"## Context

Originally, the `scripts` noun was added to components to allow us to move hard-coded init business logic out of the codebase and into the package system. At the time there was only a `before` and `after` section with simply an array entry per command. Later, `prepare` was added as a way to do something during `zarf package create`. As teams began to find new ways to use the capabilities, their limitations became more obvious.","## Decision  
## Decision

We will expand and formalize the `scripts` functionality in components by introducing a more structured and flexible execution model with distinct lifecycle hooks: `pre`, `post`, `prepare`, and `validate`.

Each script section will support:
- A list of commands or scripts to execute
- Environment variable injection
- Conditional execution based on target architecture or OS
- Timeout configuration
- Failure behavior (continue/on-error vs. fail-fast)

The `before` and `after` sections are deprecated and replaced with `pre` and `post`, respectively, to align with common CLI terminology and reduce ambiguity.

Additionally, we introduce a new `validate` script hook that runs during deployment to ensure prerequisites are met before proceeding. This enables better error reporting and user feedback early in the deployment process.

Scripts are now defined as structured objects rather than simple arrays, allowing for greater configurability while maintaining backward compatibility through schema translation during package creation.

Documentation and examples will be updated to guide users toward using the new structure and deprecating the old array-style syntax.","## Decision

The `scripts` section of the `zarf.yaml` will be replaced with a new `actions` section. The `actions` section will be a map of action names to a list of commands to run. `actions` will contain `action sets` that map to the following lifecycle events:

- `onCreate` - Runs during `zarf package create`
- `onDeploy` - Runs during `zarf package deploy`
- `onRemove` - Runs during `zarf package remove`

In addition to adding more lifecycle events, the `actions` section will also allow for more complex actions to be defined. New configurations include, setting the cmd directory, defining custom env variables, setting the number of retries, setting the max total seconds, muting the output, and [setting a variable](../docs/3-create-a-zarf-package/7-component-actions.md#creating-dynamic-variables-from-actions) to be used in other actions or components.

Further details can be found in the `component-actions` [component actions documentation](../docs/3-create-a-zarf-package/7-component-actions.md), [package create lifecycle documentation](../docs/3-create-a-zarf-package/5-package-create-lifecycle.md), [package deploy lifecycle documentation](../docs/4-deploy-a-zarf-package/1-package-deploy-lifecycle.md), and the [example package](../examples/component-actions/README.md)."
"## Context

There has been a long-standing usability gap with Zarf when doing local development due to a lack of local image support. A solution was merged in, [#1173](https://github.com/defenseunicorns/zarf/pull/1173), and released in [v0.23.4](https://github.com/defenseunicorns/zarf/releases/tag/v0.23.4) to support this feature. Unfortunately, we didn't realize there is a [glaring issue](https://github.com/defenseunicorns/zarf/issues/1214) with the implementation that causes Zarf to crash when trying to load large images into the local docker daemon.  The docker daemon support in Crane is somewhat naive and can send a machine into an OOM condition due to how the tar stream is loaded into memory from the docker save action. Crane does have an option to avoid this issue, but at the cost of being much slower to load images from docker.

We did extensive investigation into various strategies of loading docker images from the daemon including: crane, skopeo, the docker go client and executing the docker cli directly with varying levels of success. Unfortunately, some of the methods that did work well were up to 3 times slower than the current implementation, though they avoided the OOM issue. Lastly, the docker daemon save operations directly still ended up being slower than crane and docker produced a legacy format that would cause issues with [future package schema changes](https://github.com/defenseunicorns/zarf/issues/1319) we are planning for oci imports.

|                                                    | **Docker** | **Crane** |
| -------------------------------------------------- | ---------- | --------- |
| Big Bang Core (cached)                             | 3m 1s      | 1m 58s    |
| Big Bang Core (cached + skip-sbom)                 | 1m 51s     | 56s       |
| 20 GB Single-Layer Image (local registry)          |            | 6m 14s    |
| 20 GB Single-Layer Image (local registry + cached) | 5m 2s      | 2m 10s    |","## Decision 
## Decision

We will revert the local image support feature in Zarf until a more stable and performant solution can be implemented. The current implementation, while fast, introduces a critical risk of OOM crashes when handling large images, which outweighs its performance benefits. 

We acknowledge the usability gap this creates for local development workflows, but stability and system safety are prioritized. In the interim, users should rely on remote image references or pre-built packages.

Future work will focus on evaluating alternative image loading strategies—such as using `skopeo` with streaming support or a hybrid approach leveraging partial tar stream processing—that balance performance, memory safety, and compatibility with upcoming OCI schema changes. A new implementation will be required before re-introducing local image support.","## Decision

We had hoped to leverage docker caching and avoid crane caching more, but realized that caching was still occurring via Syft for SBOM. Additionally, the extremely-large, local-only image is actually the edge case here and we created a recommended workaround in the FAQs as well as an inline alert when a large docker image is detected. This restores behavior to what it was before the docker daemon support was added, but with the added benefit of being able to load images from the docker daemon when they are available locally."
"## Context

One primary application component that end users of Zarf are deploying is [Big Bang](https://repo1.dso.mil/big-bang/bigbang). The installation of Big Bang is complicated for several reasons:

- It requires Flux to be installed to deploy correctly due to the use of Flux CRDs.
- The [images](https://umbrella-bigbang-releases.s3-us-gov-west-1.amazonaws.com/umbrella/1.51.0/package-images.yaml) defined within Big Bang are normally a superset of the images needed for any individual deployment.
- All images that Big Bang might need takes 10s of gigabytes of storage to include in a Zarf package.
- The git repositories defined within Big Bang are normally a superset of the git repositories needed for any individual deployment.
- Injecting a `values.yaml` file into the [default deployment structure](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/base/kustomization.yaml) is complicated and the discovery of which images are needed is a function of the values that are provided to the Big Bang chart","## Decision 
## Decision

We will create a new `zarf init` option called `--components=big-bang` that allows users to select Big Bang as an optional component during Zarf initialization. This option will:

- Automatically deploy Flux in the cluster prior to package deployment, satisfying the CRD requirements for Big Bang.
- Prompt the user for a custom `values.yaml` file during `zarf init`, enabling tailored configuration upfront.
- Use the provided `values.yaml` to perform image and Git repository discovery, filtering only the necessary dependencies (images and repos) based on the enabled Big Bang components.
- Dynamically generate a lean Zarf package manifest containing only the required artifacts, significantly reducing package size (avoiding inclusion of all 10s of GB of potential images).
- Integrate with Zarf’s existing templating and transformation capabilities to inject the `values.yaml` into the appropriate layer of the Kustomize stack.

This approach ensures that Big Bang deployments remain flexible and configurable while minimizing resource overhead and simplifying the user experience through automation and intelligent dependency resolution.","## Decision

Deployments of Big Bang can be managed with a new `bigbang` noun in the zarf.yaml that manages the complexity of the deployment. This capability will take the values provided to the big bang chart, template them during the package phase to identify which [Big Bang packages](https://repo1.dso.mil/big-bang/bigbang/-/blob/master/docs/packages.md) are being configured in the Zarf package. The code then includes only the git repositories and images needed for the configured packages, and does not include the git repositories and images for packages that would not be deployed.

The `bigbang` section will provide the following configurations for managing a big bang deployment:

- `version` - Identifies the particular version of Big Bang to deploy, which corresponds to git tags in the provided `repo`. See versions of Big Bang [here](https://repo1.dso.mil/big-bang/bigbang/-/releases).
- `repo` - Identifies the git repository Big Bang is hosted on. Defaults to https://repo1.dso.mil/big-bang/bigbang.git
- `valuesFiles` - list of local files that get passed to the Big Bang helm chart for deployment.
- `skipFlux` - boolean to determine if the flux installation for Big Bang should be skipped. Only set this to true if flux has been deployed in a different way already in the cluster."
"## Context

Zarf currently supports git servers and container registries within the airgap to host dependencies for applications, and while this works well for most production deployments, it doesn't cater as well to airgap development where you may need artifacts and libraries for various coding languages to compile and develop software.  Zarf's support of `git` is also somewhat lacking in that it only supports flux `GitRepository` objects and does not support more generic use cases where clients try to reach out to upstream `git` hosts in a more native way.","## Decision 
## Decision

Introduce a Zarf package type specifically designed for development in airgapped environments, called the `dev` package type.

This `dev` package will:
- Bundle language-specific artifacts (e.g., npm, pip, Maven, Cargo, RubyGems) and their metadata to support offline dependency resolution.
- Include a local proxy/cache mirror for common public registries (e.g., npmjs.org, pypi.org, crates.io), pre-seeded with required packages.
- Support generic `git` server functionality within the airgap, allowing developers to clone, fetch, and push to internal git endpoints using standard git protocols (HTTPS/SSH).
- Enable bidirectional sync of git repositories via Zarf's existing git server integration, but expose them through native git URLs instead of being limited to Flux GitRepository CRDs.
- Allow developers to define upstream sources in their tooling (e.g., `.npmrc`, `pip.conf`) that point to the local Zarf-provided mirrors.

The `dev` package is intended for use only in airgapped development environments and will not be included or enabled by default in production Zarf packages. It will be documented as an optional, opt-in capability for teams requiring full-stack airgapped development support.","## Decision

Since we already had an artifact registry available to us in Gitea, it was decided to utilize that as the default provider for this functionality in addition to matching the external support for `git` and `registry` servers with a new `artifact` server specification on `init`.  From here, to access the configured server we had two main options:

1. Transform any artifact references statically (i.e. swap upstream URLs in build scripts before bringing them into the environment)
2. Transform any artifact references dynamically (i.e. swap upstream URLs in an active proxy that any build scripts could pickup through DNS)

It was decided to go with #2 since this would allow us to support a wider array of build technologies (including those that wrap existing commands like Buck/Bazel/Make) as well as support builds that may be coming from resources not brought in by Zarf.  This allows for more flexibility in how Zarf could transform URLs while allowing these commands to run as they would on the internet side without any modification."
"## Context

Eventually, Zarf needs to become a ""Generally Available"" v1.x.x product that people can rely on for mission-critical operations.  Today, Zarf can be used in these environments; though, it requires someone more involved in the Zarf lifecycle than a normal consumer/user to make that successful due to the regular introduction of breaking changes and the lack of testing in certain areas.","## Decision 

We will establish a formal deprecation and versioning policy to stabilize Zarf ahead of its v1.0.0 release. This includes:

- Introducing semantic versioning (SemVer) with clear guidelines for major, minor, and patch releases.
- Requiring a deprecation period of at least one minor version before removing or breaking existing functionality.
- Expanding test coverage across core features, particularly in air-gapped and production-like environments.
- Publishing a public roadmap and stability guarantee for the v1.x.x series.

This will reduce breaking changes, increase predictability, and enable broader adoption by mission-critical users who require reliability and long-term support.","## Decision

To make Zarf a Generally Available product we need to focus on overall stability and mechanisms to ensure long-term stability.  ""Stability,"" in this case, is both that of the features we release and of the APIs and schemas we present to Zarf consumers.

To increase this stability, we decided to implement the following:

- [ ] Mechanism/branching strategy to backport patch fixes to older minor releases
- [x] Clear definition of `released`, `beta`, and `alpha` features, including a matrix of their support across OSes
- [ ] Clear definition of when backward compatibility checks are going to be removed with clear messaging to users
- [ ] End to End testing that covers the `released` features outlined in that feature matrix - this should also be done:
  - across operating systems (specifically: Windows, macOS, Linux)
  - across major k8s distros (specifically: K3d, K3s, Minikube, Kind, EKS, AKS, OpenShift)
  - across registry providers (specifically: Docker Distribution, ECR, ACR)
  - across git providers (specifically: Gitea, GitLab)
- [ ] Unit testing that covers our library code (`src/pkg`) for people using Zarf as a library (code coverage metric TBD)
- [ ] Mechanisms and tests to not break compatibility with packages built with older versions of Zarf
- [ ] Mechanisms to notify users when they may need to upgrade the Zarf Agent (or Pepr capability)
- [ ] Mechanisms to ensure users can easily access documentation specific to the version of Zarf they use
- [ ] Mechanisms to ensure a more seamless Zarf install experience (i.e., macOS binary signing, `sudo apk add zarf`, `asdf install zarf X.X.X`)
- [ ] Regularly published/maintained example package(s) for tutorials/quick install
- [ ] Clear definition/strategy for ""what Zarf is"" including clear docs on how to use `released` features

>  **Note**: A [x] checkmark denotes items already addressed in Zarf."
"## Context

Orchestrating capabilities from multiple Zarf packages into meta-packages is a current weak point for Zarf. The core of Zarf was built around components as capabilities, but as Zarf packages have scaled, there has been a need to create a new boundary to manage these capabilities efficiently.

Currently there is no official way to enable the deployment, publishing, pulling, and creation of multiple Zarf packages together, and due to this some in the community have resorted to patterns such as:

```yaml
- name: init
  required: true
  files:
    - source: zarf-init-amd64-v0.27.0.tar.zst
      target: zarf-init-amd64-v0.27.0.tar.zst
  actions:
    onDeploy:
      after:
        - cmd: zarf package deploy zarf-init-amd64-v0.27.0.tar.zst --components git-server --confirm -l warn
```

While this _does_ fulfill the need to deploy two packages in one command, it does so in such a way that is verbose within the `zarf.yaml`, brittle across Zarf versions, inefficient within the package structure (it doesn't share layers), and is difficult to use `variables` with.","## Decision 
## Decision

Introduce a new `includes` field in the `zarf.yaml` to support composing multiple Zarf packages together into a single, unified package. This approach enables:

- **Atomic deployment**: Multiple packages can be deployed as one, with consistent state management.
- **Efficient layer sharing**: Unlike embedding packages via files and scripts, `includes` allows for deduplication of shared components and layers.
- **Cleaner configuration**: Eliminates the need for workarounds like `files` + `actions` to deploy nested packages.
- **Version compatibility handling**: The `includes` field supports version constraints to prevent incompatible compositions.
- **Seamless variable propagation**: Variables from the parent package can be passed down or overridden in included packages.
- **Unified publishing and pulling**: A meta-package with `includes` can be published and pulled as a single unit, simplifying distribution.

This solution maintains backward compatibility while addressing scalability, maintainability, and usability concerns, making it the most sustainable path forward for multi-package orchestration in Zarf.","## Decision

> :warning: **NOTE**: This functionality was migrated to [uds-cli](https://github.com/defenseunicorns/uds-cli) - this ADR is kept here for historical purposes.

The current proposition (subject to change before acceptance) is **Zarf Bundles**, which a following PR will focus on and create a POC of.

In essence the `zarf-bundle.yaml` would look something like so:

```yaml
metadata:
  name: omnibus
  description: an example Zarf bundle
  version: 0.0.1
  architecture: amd64

packages:
  - repository: localhost:888/init
    ref: """
"## Context

The idea of `hooks` is to provide a way for cluster maintainers to register functionality that runs during the deployment lifecycle. Zarf packages already have the concept of `actions` that can execute commands on the host machine's shell during certain package lifecycle events. As `actions` gain more adoption, the team has noticed they are being used to add functionality to Zarf in unexpected ways. We want `actions` to be a tool that extends upon the functionality of Zarf and its packages, not a tool that works around missing or clunky functionality.

We want package creators to be able to create system agnostic packages by leveraging core Zarf functionality. The following is one such scenario:

- _IF_ ECR is chosen as the external registry during `zarf init` / cluster creation, _THEN_ Zarf will seamlessly leverage ECR without requiring advanced user effort.

Using ECR as a remote registry creates 2 problems that Zarf will need to solve:

 1. ECR authentication tokens expire after 12 hours and need to be refreshed. This means the cluster will need to constantly be refreshing its tokens and the user deploying packages will need to make sure they have a valid token.
 2. ECR Image Repositories do not support 'push-to-create'. This means we will need to explicitly create an image repository for every image that is being pushed within the Zarf package.

Packages that get deployed onto a cluster initialized with ECR as its remote registry will need to make sure it solves these 2 problems.

Currently there are 2 solutions:

1. The package deployer solves the problem pre-deployment (creating needed repos, secrets, etc...)
2. The package itself solves these problems with `actions` that are custom built for ECR clusters.

Neither one of these current solutions are ideal. We don't want to require overly complex external + prior actions for Zarf package deployments, and we don't want package creators to have to create and distribute packages that are specific to ECR.

Potential considerations:","## Decision 
## Decision

Introduce `hooks` as a core Zarf feature to handle lifecycle events during package deployment, specifically to address ECR-related challenges in a system-agnostic and automated way.

Hooks will be executed by Zarf itself during predefined stages of the deployment lifecycle (e.g., pre-deploy, post-deploy) and will be triggered conditionally based on cluster state or configuration—such as the use of ECR as the remote registry.

For ECR support, Zarf will implement built-in hooks that:

1. **Automatically refresh ECR authentication tokens** before image operations, using AWS IAM credentials provided at deploy time. This ensures long-running or delayed deployments do not fail due to expired tokens.
2. **Pre-create ECR repositories** for each image referenced in the Zarf package, via the AWS SDK or CLI, prior to attempting image pushes. This resolves the lack of 'push-to-create' support in ECR.

These hooks will be:
- **Transparent to users**: No manual setup or scripting required.
- **Conditional**: Only activated when ECR is detected as the target registry.
- **Built into Zarf**: Not exposed as user-defined actions, ensuring consistent, secure, and maintainable behavior.

This approach removes the need for:
- Manual pre-deployment setup by operators.
- ECR-specific `actions` authored by package creators.

As a result, Zarf packages remain portable and system-agnostic, while complex platform-specific logic is centralized and managed within the Zarf runtime.

Future extensibility: The hook system may later be opened to advanced users via a controlled API, but for now it remains internal and purpose-built for critical platform integration needs like ECR.","## Decision

[Pepr](https://github.com/defenseunicorns/pepr) will be used to enable custom, or environment-specific, automation tasks to be integrated in the Zarf package deployment lifecycle. Pepr also allows the Zarf codebase to remain agnostic to any third-party APIs or dependencies that may be used.

A `--skip-webhooks` flag has been added to `zarf package deploy` to allow users to opt out of Zarf checking and waiting for any webhooks to complete during package deployments."
"## Context

We need a way to decouple transformations from Zarf. We know the Zarf transformation library is battle tested, hardened, and reliable. As Pepr takes admission responsibilities from the Zarf Agent, we need a way to synchronize the TypeScript with the Go code instead of maintaining two disparate libraries which will be expected to grow.

We considered:

- WASM
- gRPC
- REST
- Rewrite the code in TypeScript","## Decision 
## Decision

We will use WebAssembly (WASM) to compile the Go-based Zarf transformation library into a WASM binary that can be executed directly within Pepr's TypeScript runtime. This approach allows us to:

1. Maintain a single source of truth for transformation logic in Go, avoiding duplication and drift between languages.
2. Leverage the existing, battle-tested Zarf transformation code without rewriting it in TypeScript.
3. Achieve near-native performance compared to REST or gRPC inter-process communication.
4. Enable synchronous function calls from TypeScript to Go logic, simplifying the programming model.
5. Reduce operational complexity compared to maintaining a separate gRPC/REST service.

The WASM module will be compiled from the Go transformation library and bundled with the Pepr runtime, allowing admission controllers to invoke transformation functions directly during webhook processing. This ensures consistent behavior between Zarf and Pepr while enabling safe, isolated execution of the Go code within the Node.js environment.","## Decision

We conformed around using WASM because we can leverage the battle tested transform library from Zarf without incurring network overhead cost from pod to pod or container to container communications."
"## Context

Zarf natively supports creating the following package sources:

- Local Tarball (`.tar` and `.tar.zst`)
  - Via `zarf package create <dir> -o <dir>`, with compression determined by `metadata.uncompressed` in `zarf.yaml`
- Split Tarball (`.part...`)
  - Via `zarf package create <dir> --max-package-size <size> -o <dir>`
- OCI package (`oci://`)
  - Via `zarf package publish <source> oci://` or `zarf package create <dir> -o oci://...`
- In-cluster (Deployed) package
  - Post `zarf package deploy <source>` the package is show in `zarf package list`

However, the current loading abilities of Zarf have been inconsistent depending upon the action specified. For example:

- Split tarball packages could be created, deployed, but not inspected, or removed
- In-cluster packages could be removed (by name), but not inspected
- HTTPs URLs could be deployed, but not inspected, or removed
- etc...","## Decision 
## Decision

Standardize Zarf package loading to support all native package sources across all applicable commands (`deploy`, `remove`, `inspect`, etc.).

Each command that operates on packages will be updated to detect the package source type and handle it appropriately, enabling consistent behavior regardless of whether the package is a local tarball, split tarball, OCI reference, or in-cluster deployment.

This includes:
- Adding support for inspecting split tarball and HTTPS-hosted packages
- Enabling inspection of in-cluster (deployed) packages
- Allowing removal of packages referenced by HTTPS URLs, where supported
- Ensuring uniform parsing and validation of package references via a centralized package loader

The change ensures that users can inspect and manage packages uniformly based on their source, improving usability and reducing confusion around package lifecycle operations.","## Decision

Zarf must support the `deploy`, `inspect`, `remove`, `publish`, `pull`, and `mirror-resources` commands across package sources.

For common behavior to be exhibited by all sources, the `PackageSource` interface has been introduced along with the `layout` library.

```go
// src/pkg/packager/sources/new.go

// PackageSource is an interface for package sources.
//
// While this interface defines three functions, LoadPackage, LoadPackageMetadata, and Collect; only one of them should be used within a packager function.
//
// These functions currently do not promise repeatability due to the side effect nature of loading a package.
type PackageSource interface {
    // LoadPackage loads a package from a source.
    //
    // For the default sources included in Zarf, package integrity (checksums, signatures, etc.) is validated during this function
    // and expects the package structure to follow the default Zarf package structure.
    //
    // If your package does not follow the default Zarf package structure, you will need to implement your own source.
    LoadPackage(*layout.PackagePaths) error
    // LoadPackageMetadata loads a package's metadata from a source.
    //
    // This function follows the same principles as LoadPackage, with a few exceptions:
    //
    // - Package integrity validation will display a warning instead of returning an error if
    //   the package is signed but no public key is provided. This is to allow for the inspection and removal of packages
    //   that are signed but the user does not have the public key for.
    LoadPackageMetadata(dst *layout.PackagePaths, wantSBOM bool, skipValidation bool) error

    // Collect relocates a package from its source to a tarball in a given destination directory.
    Collect(destinationDirectory string) (tarball string, err error)
}
```

The following sources have been implemented:

- Local Tarball (`.tar` and `.tar.zst`)
- Split Tarball (`.part...`)
- HTTP(S) URL
- Published OCI package (`oci://`)
- In-cluster (Deployed) package (`inspect` and `remove` only)

The `layout` library contains the `PackagePaths` struct which supersedes the prior `TempPaths` struct. This new struct contains access methods to different aspects of Zarf's internal package layout. This struct is passed to the `PackageSource` functions to allow for the loading of packages into the correct layout. In order for a package to be loaded into the correct layout, the package must follow the default Zarf package structure, or be converted to the expected structure during loading operations."
"## Context

Zarf has supports composing components together between packages on `zarf package create` since v0.16.0.  This has allowed package creators to make more complex packages from smaller reusable bits.  As this functionality grew however there were a few problems that developed:

1. Import chains did not handle scaling to larger numbers of layers with test coverage usually only covering the first import.
2. When OCI skeletons were added they were largely bolted on after the fact without rethinking how they would impact composability.
3. Component filtering via the `only` filter was not implemented in a central location leading to bugs with create-time filters.","## Decision 
## Decision

To address the scalability, maintainability, and correctness issues in Zarf's package composition system, we will refactor the composition logic around a centralized, recursive import resolution engine with the following changes:

1. **Unified Import Resolution Layer**: Introduce a dedicated composition service that handles all import operations—whether from local packages or OCI skeletons—in a single, well-tested code path. This service will recursively resolve imports up to a configurable depth limit (default: 5 layers), ensuring consistent behavior and enabling end-to-end testing of multi-layer chains.

2. **First-Class OCI Support**: Treat OCI-based imports as a native source type within the composition engine rather than a post-hoc addition. This includes standardized metadata fetching, digest verification, and layer unpacking aligned with local package handling.

3. **Centralized Component Filtering**: Move all `only`-based component filtering into the composition service. Filters will be applied during the import resolution phase, ensuring they work consistently across both create-time and runtime contexts, and eliminating edge cases where filters were previously bypassed.

4. **Schema Validation at Each Layer**: Enforce schema validation and dependency checks at every level of the import chain, with clear error messaging indicating the source package and component when conflicts arise.

This refactoring will improve reliability, enable better test coverage, and lay the groundwork for future features like override semantics and import-time patching.","## Decision

We decided to separate composability into its own package that represents a composability import chain as a doubly linked list.  This allows us to represent the whole chain as it exists relative to the ""head"" Zarf package (the definition that Zarf was asked to build) to more easily handle packages that are in different locations (such as OCI skeletons in one's cache).  We also run the compose functions on all components so that the additional filter logic that is needed for these components can be handled more concisely and built upon (as it might for `flavor` https://github.com/defenseunicorns/zarf/issues/2101)."
"## Context

> Feature request: <https://github.com/defenseunicorns/zarf/issues/2169>

The current package development lifecycle is:

1. Create a `zarf.yaml` file and add components
2. Create a package with `zarf package create <dir>`
3. Debug any create errors and resolve by editing `zarf.yaml` and repeating step 2
4. Run `zarf init` to initialize the cluster
5. Deploy the package with `zarf package deploy <tarball>`
6. Debug any deploy errors and resolve by editing `zarf.yaml` and repeating step 2 or 5

If there are deployment errors, the common pattern is to reset the cluster (ex: `k3d cluster delete && k3d cluster create`) and repeat steps 4-6. Re-initializing the cluster, recreating the package, and redeploying the package is tedious and time consuming; especially when the package is large or the change was small.

`zarf package create` is designed around air-gapped environments where the package is created in one environment and deployed in another. Due to this architecture, a package's dependencies _must_ be retrieved and assembled _each_ and _every_ time.

There already exists the concept of [`YOLO` mode](0010-yolo-mode.md), which can build + deploy a package without the need for `zarf init`, and builds without fetching certain heavy dependencies (like Docker images). However, `YOLO` mode is not exposed via CLI flags, and is meant to develop and deploy packages in fully connected environments.","## Decision 
## Decision

We will introduce a new `--mode` flag to the `zarf package create` command that allows users to specify the mode in which the package is created. The `--mode` flag will accept two values: `airgap` (default) and `connected`.

When `--mode=connected` is specified, the package creation process will skip fetching dependencies that are expected to be available in the target environment (e.g. Docker images, Git repositories). This mode assumes the target cluster has network access to pull these dependencies directly during deployment.

This change enables a faster development loop by allowing developers to:
1. Make changes to `zarf.yaml`
2. Run `zarf package create --mode=connected` (skipping lengthy dependency pulls)
3. Deploy with `zarf package deploy`

The existing `YOLO` mode functionality will be preserved but repositioned as an internal implementation detail of `--mode=connected`. The CLI will remain the primary interface for users.

This decision balances the needs of air-gapped environments (maintaining default safe behavior) with developer productivity in connected environments (enabling faster iteration).","## Decision

Introduce a `dev deploy` command that will combine the lifecycle of `package create` and `package deploy` into a single command. This command will:

- Not result in a re-usable tarball / OCI artifact
- Not have any interactive prompts
- Not require `zarf init` to be run by default (override-able with a CLI flag)
- Be able to create+deploy a package in either YOLO mode (default) or prod mode (exposed via a CLI flag)
- Only build + deploy components that _will_ be deployed (contrasting with `package create` which builds _all_ components regardless of whether they will be deployed)"
"## Context

The process of deploying applications with Helm charts in Kubernetes environments often necessitates the customization of chart values to align with specific operational or environmental requirements. The current method for customizing these valuesÃÂ¡ÃÂªeither through manual edits or `","## Decision 
## Decision
We will use `values.yaml` overlays to manage Helm chart customizations. Custom values for a deployment will be stored in version-controlled YAML files, named according to environment or use case (e.g., `values-prod.yaml`, `values-staging.yaml`). These files will be applied during Helm install/upgrade using the `-f` flag.

To support composability and avoid duplication, we allow multiple value files to be layered (e.g., base `values.yaml` + environment-specific override). Sensitive values (e.g., passwords, tokens) will not be stored directly in these files; instead, they will be injected via Helm's integration with Kubernetes Secrets, managed through SealedSecrets or external secret managers like Hashicorp Vault.

Automation tools (e.g., Argo CD, Flux) will apply these value overlays declaratively, ensuring consistent, auditable, and reproducible deployments across environments. Documentation for each overlay will be maintained alongside the values file to clarify intent and usage.","## Decision

To address this issue, we propose the introduction of a feature designed to simplify the process of overriding chart values at the time of deployment. This feature would allow users to easily specify overrides for any chart values directly via command-line arguments, eliminating the need to alter the chart's default values file or manage multiple command-line arguments for each override.

Key aspects of the proposed implementation include:
- Use existing `--set`  flags to specify overrides for chart values.
- The ability to list all overrides in a structured and easily understandable format within `zarf.yaml`.
- Ensuring that during deployment, these specified overrides take precedence over the chart's default values, thus facilitating customized deployments without necessitating permanent modifications to the chart."
"## Context

The current documentation site is built with Docusaurus 2. While Docusaurus has served us well, it has some limitations, primarily around performance and dependency management.","## Decision 
## Decision

Migrate the documentation site from Docusaurus 2 to VitePress.

### Rationale:
- **Performance**: VitePress offers faster build times and a significantly improved development experience due to its foundation on Vite.
- **Simplicity & Maintainability**: VitePress has a lighter footprint, fewer dependencies, and a more straightforward configuration compared to Docusaurus.
- **Developer Experience**: Hot module replacement (HMR) is faster, and the overall tooling aligns better with modern Vue-based workflows.
- **Extensibility**: While both tools support customization, VitePress integrates seamlessly with the broader Vite ecosystem, making it easier to extend with plugins and custom scripts.

This migration will improve load times, streamline the contribution process, and reduce technical debt in the docs infrastructure. Documentation content (Markdown files) will be preserved and adapted to VitePress conventions with minimal changes.","## Decision

To simplify the development process and improve the performance of the documentation site, we have decided to migrate to Starlight. This is based upon a number of factors:

- Small dependency footprint.
- Fast performance.
- Easy to extend with custom components.
- Built-in support for MDX.
- The majority of the features we need are built-in and require minimal configuration.
  - Client-side static site search
  - Sane spacing and typography
  - Dark mode + light mode
  - Syntax highlighting
  - Responsive design + mobile-friendly
  - Tabs + Admonitions"
